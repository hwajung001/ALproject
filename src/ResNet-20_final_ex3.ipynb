{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-20 final Train code\n",
    "\n",
    "### ex 3 : train dataset 100,000 = original + horizontal crop\n",
    "##### (random seed = 104729)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - CIFAR-100 데이터 다운로드 및 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 already downloaded.\n",
      "Generating augmented dataset with horizontal flip only...\n",
      "train_flip: [(90000, 3, 32, 32), (90000,)]\n",
      "val_flip: [(10000, 3, 32, 32), (10000,)]\n",
      "test: [(10000, 3, 32, 32), (10000,)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(104729)  # ex3 random seed\n",
    "\n",
    "def download_cifar100(save_path='cifar-100-python'):\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "        return\n",
    "\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
    "    filename = 'cifar-100-python.tar.gz'\n",
    "    print(\"Downloading CIFAR-100...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "    with tarfile.open(filename, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "    os.remove(filename)\n",
    "    print(\"Download and extraction completed.\")\n",
    "\n",
    "def load_batch(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "    data = data_dict[b'data']\n",
    "    fine_labels = np.array(data_dict[b'fine_labels'])\n",
    "    data = data.reshape(-1, 3, 32, 32)\n",
    "    return data, fine_labels\n",
    "\n",
    "def normalize_images(images):\n",
    "    return images.astype(np.float32) / 255.0\n",
    "\n",
    "def split_validation(images, labels, val_ratio=0.1):\n",
    "    num_samples = images.shape[0]\n",
    "    val_size = int(num_samples * val_ratio)\n",
    "\n",
    "    idx = np.random.permutation(num_samples)\n",
    "    images = images[idx]\n",
    "    labels = labels[idx]\n",
    "\n",
    "    val_images = images[:val_size]\n",
    "    val_labels = labels[:val_size]\n",
    "    train_images = images[val_size:]\n",
    "    train_labels = labels[val_size:]\n",
    "\n",
    "    return (train_images, train_labels), (val_images, val_labels)\n",
    "\n",
    "def random_crop(x, crop_size=32, padding=4):\n",
    "    n, c, h, w = x.shape\n",
    "    padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='reflect')\n",
    "    cropped = np.empty((n, c, crop_size, crop_size), dtype=x.dtype)\n",
    "    for i in range(n):\n",
    "        top = np.random.randint(0, padding * 2 + 1)\n",
    "        left = np.random.randint(0, padding * 2 + 1)\n",
    "        cropped[i] = padded[i, :, top:top+crop_size, left:left+crop_size]\n",
    "    return cropped\n",
    "\n",
    "def horizontal_flip(x):\n",
    "    return x[:, :, :, ::-1]\n",
    "\n",
    "def load_cifar100_dataset():\n",
    "    download_cifar100()\n",
    "    train_data, train_fine = load_batch('cifar-100-python/train')\n",
    "    test_data, test_fine = load_batch('cifar-100-python/test')\n",
    "    train_data = normalize_images(train_data)\n",
    "    test_data = normalize_images(test_data)\n",
    "    return (train_data, train_fine), (test_data, test_fine)\n",
    "\n",
    "def generate_augmented_dataset(images, labels, target_size):\n",
    "    N = images.shape[0]\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    repeat = target_size // (N * 2) + 1  # original + flip\n",
    "\n",
    "    for _ in range(repeat):\n",
    "        imgs_original = images.copy()\n",
    "        imgs_flip = horizontal_flip(images.copy())\n",
    "\n",
    "        augmented_images.append(imgs_original)\n",
    "        augmented_labels.append(labels.copy())\n",
    "\n",
    "        augmented_images.append(imgs_flip)\n",
    "        augmented_labels.append(labels.copy())\n",
    "\n",
    "        if sum(x.shape[0] for x in augmented_images) >= target_size:\n",
    "            break\n",
    "\n",
    "    X = np.concatenate(augmented_images, axis=0)[:target_size]\n",
    "    y = np.concatenate(augmented_labels, axis=0)[:target_size]\n",
    "    return X, y\n",
    "\n",
    "def prepare_dataset():\n",
    "    (full_train_images, full_train_labels), (test_images, test_labels) = load_cifar100_dataset()\n",
    "    print(\"Generating augmented dataset with horizontal flip only...\")\n",
    "\n",
    "    X_aug, y_aug = generate_augmented_dataset(full_train_images, full_train_labels, target_size=100000)\n",
    "    train_aug, val_aug = split_validation(X_aug, y_aug)\n",
    "\n",
    "    return {\n",
    "        'train_flip': train_aug,\n",
    "        'val_flip': val_aug,\n",
    "        'test': (test_images, test_labels)\n",
    "    }\n",
    "\n",
    "data = prepare_dataset()\n",
    "for k, v in data.items():\n",
    "    if isinstance(v, tuple):\n",
    "        print(f\"{k}: {[x.shape for x in v]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - ResNet-20 모델 구조 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type)                       Output Shape                Param #\n",
      "===========================================================================\n",
      " 1. Conv1                           (1, 16, 32, 32)                 448\n",
      " 2. Block[1-1]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 3. Block[1-1]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 4. Block[1-2]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 5. Block[1-2]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 6. Block[1-3]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 7. Block[1-3]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 8. Block[2-1]_Conv1                (1, 32, 16, 16)               4,640\n",
      " 9. Block[2-1]_Conv2                (1, 32, 16, 16)               9,248\n",
      "    └─ Shortcut[2-1]                (1, 32, 16, 16)                 544\n",
      "10. Block[2-2]_Conv1                (1, 32, 16, 16)               9,248\n",
      "11. Block[2-2]_Conv2                (1, 32, 16, 16)               9,248\n",
      "12. Block[2-3]_Conv1                (1, 32, 16, 16)               9,248\n",
      "13. Block[2-3]_Conv2                (1, 32, 16, 16)               9,248\n",
      "14. Block[3-1]_Conv1                (1, 64, 8, 8)                18,496\n",
      "15. Block[3-1]_Conv2                (1, 64, 8, 8)                36,928\n",
      "    └─ Shortcut[3-1]                (1, 64, 8, 8)                 2,112\n",
      "16. Block[3-2]_Conv1                (1, 64, 8, 8)                36,928\n",
      "17. Block[3-2]_Conv2                (1, 64, 8, 8)                36,928\n",
      "18. Block[3-3]_Conv1                (1, 64, 8, 8)                36,928\n",
      "19. Block[3-3]_Conv2                (1, 64, 8, 8)                36,928\n",
      "    GlobalAvgPool                   (1, 64)                           0\n",
      "20. FC                              (1, 100)                      6,500\n",
      "===========================================================================\n",
      "Total weight layers:                                        20\n",
      "Total params:                                               277,540\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "from common.ResNet20 import ResNet20\n",
    "\n",
    "def count_params(layer):\n",
    "    count = 0\n",
    "    if hasattr(layer, 'W'):\n",
    "        count += np.prod(layer.W.shape)\n",
    "    if hasattr(layer, 'b'):\n",
    "        count += np.prod(layer.b.shape)\n",
    "    return count\n",
    "\n",
    "def print_resnet20_summary(model, input_shape=(1, 3, 32, 32)):\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Layer (type)':<35}{'Output Shape':<25}{'Param #':>10}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "    x = np.zeros(input_shape)\n",
    "    total_params = 0\n",
    "    layer_idx = 1\n",
    "\n",
    "    x = model.conv1.forward(x)\n",
    "    p = count_params(model.conv1)\n",
    "    print(f\"{layer_idx:>2}. {'Conv1':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "    layer_idx += 1\n",
    "\n",
    "    x = model.bn1.forward(x, train_flg=False)\n",
    "    x = model.relu1.forward(x)\n",
    "\n",
    "    for i, layer_block in enumerate([model.layer1, model.layer2, model.layer3]):\n",
    "        for j, block in enumerate(layer_block):\n",
    "            residual = x.copy()\n",
    "\n",
    "            # Conv1\n",
    "            x = block.conv1.forward(x)\n",
    "            p = count_params(block.conv1)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv1\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn1.forward(x, train_flg=False)\n",
    "            x = block.relu1.forward(x)\n",
    "\n",
    "            x = block.conv2.forward(x)\n",
    "            p = count_params(block.conv2)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv2\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn2.forward(x, train_flg=False)\n",
    "\n",
    "            if not block.equal_in_out:\n",
    "                x_sc = block.shortcut.forward(residual)\n",
    "                p = count_params(block.shortcut)\n",
    "                name = f\"└─ Shortcut[{i+1}-{j+1}]\"\n",
    "                print(f\"{'':>3} {name:<32}{str(x_sc.shape):<25}{p:>10,}\", flush=True)\n",
    "                total_params += p\n",
    "                x = x + x_sc\n",
    "                x = block.bn_shortcut.forward(x, train_flg=False)\n",
    "            else:\n",
    "                x = x + residual\n",
    "\n",
    "            x = block.relu2.forward(x)\n",
    "\n",
    "    x = x.mean(axis=(2, 3))\n",
    "    print(f\"{'':>3} {'GlobalAvgPool':<32}{str(x.shape):<25}{'0':>10}\", flush=True)\n",
    "\n",
    "    x = model.fc.forward(x)\n",
    "    p = count_params(model.fc)\n",
    "    print(f\"{layer_idx:>2}. {'FC':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Total weight layers:':<60}{'20'}\", flush=True)\n",
    "    print(f\"{'Total params:':<60}{total_params:,}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "model = ResNet20()\n",
    "print_resnet20_summary(model, input_shape=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - ResNet-20 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from common.optimizer import Adam\n",
    "from common.functions import softmax\n",
    "\n",
    "# label smoothing\n",
    "def smooth_labels(y, smoothing=0.1, num_classes=100):\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = (y.shape[0], num_classes)\n",
    "    smooth = np.full(label_shape, smoothing / (num_classes - 1))\n",
    "    smooth[np.arange(y.shape[0]), y] = confidence\n",
    "    return smooth\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name,\n",
    "                 train_data, val_data, test_data,\n",
    "                 epochs=20, batch_size=64, lr=0.01,\n",
    "                 smoothing=0.15):\n",
    "\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.val_x, self.val_t = val_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "\n",
    "        self.optimizer = Adam(lr=lr)\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def smooth_labels(self, y, num_classes=100):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        label_shape = (y.shape[0], num_classes)\n",
    "        smooth = np.full(label_shape, self.smoothing / (num_classes - 1), dtype=np.float32)\n",
    "        smooth[np.arange(y.shape[0]), y] = confidence\n",
    "        return smooth\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            dx = (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            dx /= batch_size\n",
    "        return dx, y\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "        if hasattr(self.model.fc, 'W'):\n",
    "            param_dict['fc_W'] = self.model.fc.W\n",
    "            param_dict['fc_b'] = self.model.fc.b\n",
    "            grad_dict['fc_W'] = self.model.fc.dW\n",
    "            grad_dict['fc_b'] = self.model.fc.db\n",
    "\n",
    "        idx = 0\n",
    "        for layer in self.model.layer1 + self.model.layer2 + self.model.layer3:\n",
    "            for attr in ['conv1', 'conv2', 'shortcut']:\n",
    "                if hasattr(layer, attr):\n",
    "                    conv = getattr(layer, attr)\n",
    "                    param_dict[f'{idx}_W'] = conv.W\n",
    "                    param_dict[f'{idx}_b'] = conv.b\n",
    "                    grad_dict[f'{idx}_W'] = conv.dW\n",
    "                    grad_dict[f'{idx}_b'] = conv.db\n",
    "                    idx += 1\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        if t_batch.ndim == 1:\n",
    "            t_batch = self.smooth_labels(t_batch)\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        dx, y = self.loss_grad(x_batch, t_batch)\n",
    "        self.model.backward(dx)\n",
    "\n",
    "        if hasattr(self.model, 'clip_weights'):\n",
    "            self.model.clip_weights(clip_value=1.0)\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        patience = 10\n",
    "        best_val_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "    \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\n[Epoch {epoch + 1}/{self.epochs}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0 or i == self.iter_per_epoch - 1:\n",
    "                    print(f\"  Iter {i+1:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            val_acc = self.model.accuracy(self.val_x, self.val_t)\n",
    "\n",
    "            val_loss = self.batched_loss(self.val_x, self.val_t, batch_size=128)\n",
    "\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.val_acc_list.append(val_acc)\n",
    "            self.val_loss_list.append(val_loss)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Fine Train Loss: {avg_loss:.4f}, Fine Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}\", flush=True)\n",
    "            print(f\"Time: {elapsed:.2f}s\", flush=True)\n",
    "\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                model_filename = f\"{self.model_name}_epoch{epoch+1}.pkl\"\n",
    "                self.save_model(model_filename)\n",
    "                print(f\">>> Model saved to {model_filename}\", flush=True)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improve_count = 0\n",
    "                self.save_model(f\"{self.model_name}_best.pkl\")\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "\n",
    "    def batched_loss(self, x, t, batch_size=128):\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "            loss = self.model.loss(x_batch, t_batch)\n",
    "            total_loss += loss * len(x_batch)\n",
    "            total_count += len(x_batch)\n",
    "        return total_loss / total_count\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "\n",
    "        model_state['conv1_W'] = self.model.conv1.W.copy()\n",
    "        model_state['conv1_b'] = self.model.conv1.b.copy()\n",
    "\n",
    "        def extract_bn_params(model):\n",
    "            bn_params = {}\n",
    "            bn_count = 0\n",
    "            for layer in model.layer1 + model.layer2 + model.layer3:\n",
    "                for bn_attr in ['bn1', 'bn2']:\n",
    "                    if hasattr(layer, bn_attr):\n",
    "                        bn = getattr(layer, bn_attr)\n",
    "                        bn_params[f'{bn_count}_gamma'] = bn.gamma.copy()\n",
    "                        bn_params[f'{bn_count}_beta'] = bn.beta.copy()\n",
    "                        bn_params[f'{bn_count}_running_mean'] = bn.running_mean.copy()\n",
    "                        bn_params[f'{bn_count}_running_var'] = bn.running_var.copy()\n",
    "                        bn_count += 1\n",
    "                if hasattr(layer, 'bn_shortcut'):\n",
    "                    bn = layer.bn_shortcut\n",
    "                    bn_params[f'{bn_count}_gamma'] = bn.gamma.copy()\n",
    "                    bn_params[f'{bn_count}_beta'] = bn.beta.copy()\n",
    "                    bn_params[f'{bn_count}_running_mean'] = bn.running_mean.copy()\n",
    "                    bn_params[f'{bn_count}_running_var'] = bn.running_var.copy()\n",
    "                    bn_count += 1\n",
    "            bn = model.bn1\n",
    "            bn_params[f'{bn_count}_gamma'] = bn.gamma.copy()\n",
    "            bn_params[f'{bn_count}_beta'] = bn.beta.copy()\n",
    "            bn_params[f'{bn_count}_running_mean'] = bn.running_mean.copy()\n",
    "            bn_params[f'{bn_count}_running_var'] = bn.running_var.copy()\n",
    "            return bn_params\n",
    "\n",
    "        model_state.update(extract_bn_params(self.model))\n",
    "\n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': self.optimizer.beta1,\n",
    "            'beta2': self.optimizer.beta2,\n",
    "            'm': self.optimizer.m,\n",
    "            'v': self.optimizer.v,\n",
    "            't': self.optimizer.iter\n",
    "        }\n",
    "\n",
    "        save_data = {\n",
    "            'model': model_state,\n",
    "            'optimizer': optimizer_state,\n",
    "            'train_loss_list': self.train_loss_list,\n",
    "            'train_acc_list': self.train_acc_list,\n",
    "            'val_acc_list': self.val_acc_list,\n",
    "            'val_loss_list': self.val_loss_list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename,\n",
    "                 loss=np.array(self.train_loss_list),\n",
    "                 train_acc=np.array(self.train_acc_list),\n",
    "                 val_acc=np.array(self.val_acc_list),\n",
    "                 val_loss=np.array(self.val_loss_list))\n",
    "        print(f\"Log saved to {filename}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - ResNet-20_ex1 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Running ex3 : train dataset 100,000 = original + horizontal flip ====\n",
      "\n",
      "[Epoch 1/100]\n",
      "  Iter   1/1406: Loss 5.3838\n",
      "  Iter  11/1406: Loss 4.6716\n",
      "  Iter  21/1406: Loss 4.6090\n",
      "  Iter  31/1406: Loss 4.5481\n",
      "  Iter  41/1406: Loss 4.5677\n",
      "  Iter  51/1406: Loss 4.3817\n",
      "  Iter  61/1406: Loss 4.3026\n",
      "  Iter  71/1406: Loss 4.0237\n",
      "  Iter  81/1406: Loss 4.1144\n",
      "  Iter  91/1406: Loss 4.0962\n",
      "  Iter 101/1406: Loss 3.9886\n",
      "  Iter 111/1406: Loss 3.9998\n",
      "  Iter 121/1406: Loss 4.1299\n",
      "  Iter 131/1406: Loss 3.8621\n",
      "  Iter 141/1406: Loss 3.9984\n",
      "  Iter 151/1406: Loss 4.0580\n",
      "  Iter 161/1406: Loss 4.1268\n",
      "  Iter 171/1406: Loss 3.8738\n",
      "  Iter 181/1406: Loss 4.0512\n",
      "  Iter 191/1406: Loss 4.0164\n",
      "  Iter 201/1406: Loss 4.0057\n",
      "  Iter 211/1406: Loss 3.7973\n",
      "  Iter 221/1406: Loss 3.9423\n",
      "  Iter 231/1406: Loss 3.8715\n",
      "  Iter 241/1406: Loss 3.7846\n",
      "  Iter 251/1406: Loss 3.8666\n",
      "  Iter 261/1406: Loss 3.9779\n",
      "  Iter 271/1406: Loss 3.9549\n",
      "  Iter 281/1406: Loss 4.0291\n",
      "  Iter 291/1406: Loss 3.9358\n",
      "  Iter 301/1406: Loss 3.4741\n",
      "  Iter 311/1406: Loss 3.6471\n",
      "  Iter 321/1406: Loss 3.5786\n",
      "  Iter 331/1406: Loss 3.8494\n",
      "  Iter 341/1406: Loss 3.3409\n",
      "  Iter 351/1406: Loss 3.5270\n",
      "  Iter 361/1406: Loss 3.6149\n",
      "  Iter 371/1406: Loss 3.5258\n",
      "  Iter 381/1406: Loss 3.6874\n",
      "  Iter 391/1406: Loss 3.9569\n",
      "  Iter 401/1406: Loss 3.8019\n",
      "  Iter 411/1406: Loss 3.6053\n",
      "  Iter 421/1406: Loss 3.6189\n",
      "  Iter 431/1406: Loss 3.8090\n",
      "  Iter 441/1406: Loss 3.5302\n",
      "  Iter 451/1406: Loss 3.7039\n",
      "  Iter 461/1406: Loss 3.5189\n",
      "  Iter 471/1406: Loss 3.6174\n",
      "  Iter 481/1406: Loss 3.4863\n",
      "  Iter 491/1406: Loss 3.3232\n",
      "  Iter 501/1406: Loss 3.5535\n",
      "  Iter 511/1406: Loss 3.2733\n",
      "  Iter 521/1406: Loss 3.5164\n",
      "  Iter 531/1406: Loss 3.0326\n",
      "  Iter 541/1406: Loss 3.5630\n",
      "  Iter 551/1406: Loss 3.4959\n",
      "  Iter 561/1406: Loss 3.4641\n",
      "  Iter 571/1406: Loss 3.1827\n",
      "  Iter 581/1406: Loss 3.6810\n",
      "  Iter 591/1406: Loss 3.4809\n",
      "  Iter 601/1406: Loss 3.3345\n",
      "  Iter 611/1406: Loss 3.4592\n",
      "  Iter 621/1406: Loss 3.5131\n",
      "  Iter 631/1406: Loss 3.2112\n",
      "  Iter 641/1406: Loss 3.2701\n",
      "  Iter 651/1406: Loss 3.3946\n",
      "  Iter 661/1406: Loss 3.2885\n",
      "  Iter 671/1406: Loss 3.2357\n",
      "  Iter 681/1406: Loss 3.3878\n",
      "  Iter 691/1406: Loss 3.1618\n",
      "  Iter 701/1406: Loss 3.2956\n",
      "  Iter 711/1406: Loss 3.0630\n",
      "  Iter 721/1406: Loss 3.0457\n",
      "  Iter 731/1406: Loss 3.2579\n",
      "  Iter 741/1406: Loss 3.2878\n",
      "  Iter 751/1406: Loss 3.2348\n",
      "  Iter 761/1406: Loss 3.4043\n",
      "  Iter 771/1406: Loss 3.3227\n",
      "  Iter 781/1406: Loss 2.8622\n",
      "  Iter 791/1406: Loss 3.3011\n",
      "  Iter 801/1406: Loss 3.1791\n",
      "  Iter 811/1406: Loss 3.1204\n",
      "  Iter 821/1406: Loss 2.9928\n",
      "  Iter 831/1406: Loss 3.4285\n",
      "  Iter 841/1406: Loss 3.1813\n",
      "  Iter 851/1406: Loss 2.7777\n",
      "  Iter 861/1406: Loss 3.3454\n",
      "  Iter 871/1406: Loss 3.0568\n",
      "  Iter 881/1406: Loss 3.0207\n",
      "  Iter 891/1406: Loss 3.1867\n",
      "  Iter 901/1406: Loss 3.4963\n",
      "  Iter 911/1406: Loss 2.9575\n",
      "  Iter 921/1406: Loss 3.1286\n",
      "  Iter 931/1406: Loss 3.1934\n",
      "  Iter 941/1406: Loss 3.1509\n",
      "  Iter 951/1406: Loss 2.9733\n",
      "  Iter 961/1406: Loss 2.8878\n",
      "  Iter 971/1406: Loss 3.0827\n",
      "  Iter 981/1406: Loss 3.1505\n",
      "  Iter 991/1406: Loss 3.1178\n",
      "  Iter 1001/1406: Loss 3.0674\n",
      "  Iter 1011/1406: Loss 2.9120\n",
      "  Iter 1021/1406: Loss 3.0817\n",
      "  Iter 1031/1406: Loss 2.8275\n",
      "  Iter 1041/1406: Loss 3.0919\n",
      "  Iter 1051/1406: Loss 2.8655\n",
      "  Iter 1061/1406: Loss 2.9595\n",
      "  Iter 1071/1406: Loss 2.9927\n",
      "  Iter 1081/1406: Loss 3.0845\n",
      "  Iter 1091/1406: Loss 2.8450\n",
      "  Iter 1101/1406: Loss 2.6825\n",
      "  Iter 1111/1406: Loss 2.9018\n",
      "  Iter 1121/1406: Loss 2.9576\n",
      "  Iter 1131/1406: Loss 2.6463\n",
      "  Iter 1141/1406: Loss 3.0164\n",
      "  Iter 1151/1406: Loss 2.6364\n",
      "  Iter 1161/1406: Loss 3.3513\n",
      "  Iter 1171/1406: Loss 2.9133\n",
      "  Iter 1181/1406: Loss 2.8536\n",
      "  Iter 1191/1406: Loss 2.7922\n",
      "  Iter 1201/1406: Loss 2.8973\n",
      "  Iter 1211/1406: Loss 2.7348\n",
      "  Iter 1221/1406: Loss 3.0891\n",
      "  Iter 1231/1406: Loss 3.0383\n",
      "  Iter 1241/1406: Loss 2.9182\n",
      "  Iter 1251/1406: Loss 2.9215\n",
      "  Iter 1261/1406: Loss 2.7268\n",
      "  Iter 1271/1406: Loss 3.0181\n",
      "  Iter 1281/1406: Loss 2.8234\n",
      "  Iter 1291/1406: Loss 2.9404\n",
      "  Iter 1301/1406: Loss 2.8979\n",
      "  Iter 1311/1406: Loss 3.1061\n",
      "  Iter 1321/1406: Loss 2.8633\n",
      "  Iter 1331/1406: Loss 2.9191\n",
      "  Iter 1341/1406: Loss 2.8747\n",
      "  Iter 1351/1406: Loss 2.5399\n",
      "  Iter 1361/1406: Loss 2.7733\n",
      "  Iter 1371/1406: Loss 2.8675\n",
      "  Iter 1381/1406: Loss 2.9713\n",
      "  Iter 1391/1406: Loss 2.6615\n",
      "  Iter 1401/1406: Loss 2.2644\n",
      "  Iter 1406/1406: Loss 2.8526\n",
      "Fine Train Loss: 3.3658, Fine Train Acc: 0.2760, Val Acc: 0.2802, Val Loss: 2.7701\n",
      "Time: 3071.30s\n",
      "\n",
      "[Epoch 2/100]\n",
      "  Iter   1/1406: Loss 2.7401\n",
      "  Iter  11/1406: Loss 2.6773\n",
      "  Iter  21/1406: Loss 2.7556\n",
      "  Iter  31/1406: Loss 3.0194\n",
      "  Iter  41/1406: Loss 2.8779\n",
      "  Iter  51/1406: Loss 2.5300\n",
      "  Iter  61/1406: Loss 2.8988\n",
      "  Iter  71/1406: Loss 2.7465\n",
      "  Iter  81/1406: Loss 2.8008\n",
      "  Iter  91/1406: Loss 2.7513\n",
      "  Iter 101/1406: Loss 2.7071\n",
      "  Iter 111/1406: Loss 2.5178\n",
      "  Iter 121/1406: Loss 2.6118\n",
      "  Iter 131/1406: Loss 2.9597\n",
      "  Iter 141/1406: Loss 2.5157\n",
      "  Iter 151/1406: Loss 2.4937\n",
      "  Iter 161/1406: Loss 2.6422\n",
      "  Iter 171/1406: Loss 2.3691\n",
      "  Iter 181/1406: Loss 2.3691\n",
      "  Iter 191/1406: Loss 2.5679\n",
      "  Iter 201/1406: Loss 2.5482\n",
      "  Iter 211/1406: Loss 2.5661\n",
      "  Iter 221/1406: Loss 2.3992\n",
      "  Iter 231/1406: Loss 2.4756\n",
      "  Iter 241/1406: Loss 2.7376\n",
      "  Iter 251/1406: Loss 2.5586\n",
      "  Iter 261/1406: Loss 2.6413\n",
      "  Iter 271/1406: Loss 2.7207\n",
      "  Iter 281/1406: Loss 2.4728\n",
      "  Iter 291/1406: Loss 2.8612\n",
      "  Iter 301/1406: Loss 2.8932\n",
      "  Iter 311/1406: Loss 2.5118\n",
      "  Iter 321/1406: Loss 2.5980\n",
      "  Iter 331/1406: Loss 2.5283\n",
      "  Iter 341/1406: Loss 2.7770\n",
      "  Iter 351/1406: Loss 2.4918\n",
      "  Iter 361/1406: Loss 2.4114\n",
      "  Iter 371/1406: Loss 2.3154\n",
      "  Iter 381/1406: Loss 2.8759\n",
      "  Iter 391/1406: Loss 2.7507\n",
      "  Iter 401/1406: Loss 2.7019\n",
      "  Iter 411/1406: Loss 2.8459\n",
      "  Iter 421/1406: Loss 2.7955\n",
      "  Iter 431/1406: Loss 2.4681\n",
      "  Iter 441/1406: Loss 2.6803\n",
      "  Iter 451/1406: Loss 2.6981\n",
      "  Iter 461/1406: Loss 2.6117\n",
      "  Iter 471/1406: Loss 2.7692\n",
      "  Iter 481/1406: Loss 2.6546\n",
      "  Iter 491/1406: Loss 2.2721\n",
      "  Iter 501/1406: Loss 2.4287\n",
      "  Iter 511/1406: Loss 2.6330\n",
      "  Iter 521/1406: Loss 2.6685\n",
      "  Iter 531/1406: Loss 2.9028\n",
      "  Iter 541/1406: Loss 2.6763\n",
      "  Iter 551/1406: Loss 2.5512\n",
      "  Iter 561/1406: Loss 2.4962\n",
      "  Iter 571/1406: Loss 2.4184\n",
      "  Iter 581/1406: Loss 2.5263\n",
      "  Iter 591/1406: Loss 2.5866\n",
      "  Iter 601/1406: Loss 2.5251\n",
      "  Iter 611/1406: Loss 2.3431\n",
      "  Iter 621/1406: Loss 2.4352\n",
      "  Iter 631/1406: Loss 2.2278\n",
      "  Iter 641/1406: Loss 2.1935\n",
      "  Iter 651/1406: Loss 2.5250\n",
      "  Iter 661/1406: Loss 2.1275\n",
      "  Iter 671/1406: Loss 2.3621\n",
      "  Iter 681/1406: Loss 2.5350\n",
      "  Iter 691/1406: Loss 2.3577\n",
      "  Iter 701/1406: Loss 2.6505\n",
      "  Iter 711/1406: Loss 2.6175\n",
      "  Iter 721/1406: Loss 2.5736\n",
      "  Iter 731/1406: Loss 2.4785\n",
      "  Iter 741/1406: Loss 2.5080\n",
      "  Iter 751/1406: Loss 2.3617\n",
      "  Iter 761/1406: Loss 2.7384\n",
      "  Iter 771/1406: Loss 2.7802\n",
      "  Iter 781/1406: Loss 2.3601\n",
      "  Iter 791/1406: Loss 2.6266\n",
      "  Iter 801/1406: Loss 2.2523\n",
      "  Iter 811/1406: Loss 2.3004\n",
      "  Iter 821/1406: Loss 2.5132\n",
      "  Iter 831/1406: Loss 2.6260\n",
      "  Iter 841/1406: Loss 2.4450\n",
      "  Iter 851/1406: Loss 2.5047\n",
      "  Iter 861/1406: Loss 2.2831\n",
      "  Iter 871/1406: Loss 2.6743\n",
      "  Iter 881/1406: Loss 2.6997\n",
      "  Iter 891/1406: Loss 2.2730\n",
      "  Iter 901/1406: Loss 2.5827\n",
      "  Iter 911/1406: Loss 2.6439\n",
      "  Iter 921/1406: Loss 2.1516\n",
      "  Iter 931/1406: Loss 2.5488\n",
      "  Iter 941/1406: Loss 2.5073\n",
      "  Iter 951/1406: Loss 2.1757\n",
      "  Iter 961/1406: Loss 2.1767\n",
      "  Iter 971/1406: Loss 2.7100\n",
      "  Iter 981/1406: Loss 2.4742\n",
      "  Iter 991/1406: Loss 2.4568\n",
      "  Iter 1001/1406: Loss 2.1822\n",
      "  Iter 1011/1406: Loss 2.3603\n",
      "  Iter 1021/1406: Loss 2.3640\n",
      "  Iter 1031/1406: Loss 2.4219\n",
      "  Iter 1041/1406: Loss 2.3416\n",
      "  Iter 1051/1406: Loss 2.5474\n",
      "  Iter 1061/1406: Loss 2.2982\n",
      "  Iter 1071/1406: Loss 2.4168\n",
      "  Iter 1081/1406: Loss 2.7315\n",
      "  Iter 1091/1406: Loss 2.5573\n",
      "  Iter 1101/1406: Loss 2.3291\n",
      "  Iter 1111/1406: Loss 2.4548\n",
      "  Iter 1121/1406: Loss 2.1986\n",
      "  Iter 1131/1406: Loss 2.1780\n",
      "  Iter 1141/1406: Loss 2.3386\n",
      "  Iter 1151/1406: Loss 2.6052\n",
      "  Iter 1161/1406: Loss 1.9044\n",
      "  Iter 1171/1406: Loss 2.2968\n",
      "  Iter 1181/1406: Loss 2.4664\n",
      "  Iter 1191/1406: Loss 2.2896\n",
      "  Iter 1201/1406: Loss 2.3882\n",
      "  Iter 1211/1406: Loss 2.3431\n",
      "  Iter 1221/1406: Loss 2.4748\n",
      "  Iter 1231/1406: Loss 2.5793\n",
      "  Iter 1241/1406: Loss 2.4718\n",
      "  Iter 1251/1406: Loss 2.5149\n",
      "  Iter 1261/1406: Loss 2.1323\n",
      "  Iter 1271/1406: Loss 2.3929\n",
      "  Iter 1281/1406: Loss 2.4530\n",
      "  Iter 1291/1406: Loss 2.8195\n",
      "  Iter 1301/1406: Loss 2.7524\n",
      "  Iter 1311/1406: Loss 2.2323\n",
      "  Iter 1321/1406: Loss 2.3164\n",
      "  Iter 1331/1406: Loss 2.6525\n",
      "  Iter 1341/1406: Loss 2.1057\n",
      "  Iter 1351/1406: Loss 2.1854\n",
      "  Iter 1361/1406: Loss 2.4918\n",
      "  Iter 1371/1406: Loss 2.3121\n",
      "  Iter 1381/1406: Loss 2.5809\n",
      "  Iter 1391/1406: Loss 2.2476\n",
      "  Iter 1401/1406: Loss 2.3692\n",
      "  Iter 1406/1406: Loss 2.4466\n",
      "Fine Train Loss: 2.5163, Fine Train Acc: 0.3370, Val Acc: 0.3521, Val Loss: 2.4030\n",
      "Time: 2134.48s\n",
      "\n",
      "[Epoch 3/100]\n",
      "  Iter   1/1406: Loss 2.2878\n",
      "  Iter  11/1406: Loss 2.5027\n",
      "  Iter  21/1406: Loss 2.3405\n",
      "  Iter  31/1406: Loss 2.8065\n",
      "  Iter  41/1406: Loss 2.2623\n",
      "  Iter  51/1406: Loss 2.2433\n",
      "  Iter  61/1406: Loss 2.5410\n",
      "  Iter  71/1406: Loss 2.6179\n",
      "  Iter  81/1406: Loss 2.2978\n",
      "  Iter  91/1406: Loss 2.4350\n",
      "  Iter 101/1406: Loss 2.4055\n",
      "  Iter 111/1406: Loss 2.3299\n",
      "  Iter 121/1406: Loss 2.1588\n",
      "  Iter 131/1406: Loss 2.2892\n",
      "  Iter 141/1406: Loss 2.3991\n",
      "  Iter 151/1406: Loss 2.2727\n",
      "  Iter 161/1406: Loss 2.5493\n",
      "  Iter 171/1406: Loss 2.4146\n",
      "  Iter 181/1406: Loss 2.1060\n",
      "  Iter 191/1406: Loss 2.4059\n",
      "  Iter 201/1406: Loss 2.7703\n",
      "  Iter 211/1406: Loss 2.1779\n",
      "  Iter 221/1406: Loss 2.4187\n",
      "  Iter 231/1406: Loss 2.2272\n",
      "  Iter 241/1406: Loss 2.1963\n",
      "  Iter 251/1406: Loss 2.3057\n",
      "  Iter 261/1406: Loss 2.2426\n",
      "  Iter 271/1406: Loss 2.1658\n",
      "  Iter 281/1406: Loss 2.2648\n",
      "  Iter 291/1406: Loss 2.1583\n",
      "  Iter 301/1406: Loss 1.9336\n",
      "  Iter 311/1406: Loss 2.4300\n",
      "  Iter 321/1406: Loss 2.1927\n",
      "  Iter 331/1406: Loss 2.5048\n",
      "  Iter 341/1406: Loss 1.8844\n",
      "  Iter 351/1406: Loss 2.6705\n",
      "  Iter 361/1406: Loss 2.0611\n",
      "  Iter 371/1406: Loss 2.3464\n",
      "  Iter 381/1406: Loss 2.3459\n",
      "  Iter 391/1406: Loss 2.2591\n",
      "  Iter 401/1406: Loss 2.1844\n",
      "  Iter 411/1406: Loss 2.3915\n",
      "  Iter 421/1406: Loss 2.2418\n",
      "  Iter 431/1406: Loss 2.4350\n",
      "  Iter 441/1406: Loss 2.1046\n",
      "  Iter 451/1406: Loss 1.8632\n",
      "  Iter 461/1406: Loss 2.2341\n",
      "  Iter 471/1406: Loss 2.2165\n",
      "  Iter 481/1406: Loss 1.8589\n",
      "  Iter 491/1406: Loss 2.2885\n",
      "  Iter 501/1406: Loss 2.2061\n",
      "  Iter 511/1406: Loss 2.1469\n",
      "  Iter 521/1406: Loss 2.3916\n",
      "  Iter 531/1406: Loss 2.1959\n",
      "  Iter 541/1406: Loss 2.1911\n",
      "  Iter 551/1406: Loss 2.1708\n",
      "  Iter 561/1406: Loss 2.4970\n",
      "  Iter 571/1406: Loss 2.1559\n",
      "  Iter 581/1406: Loss 2.5215\n",
      "  Iter 591/1406: Loss 2.2906\n",
      "  Iter 601/1406: Loss 2.4939\n",
      "  Iter 611/1406: Loss 2.1619\n",
      "  Iter 621/1406: Loss 2.1692\n",
      "  Iter 631/1406: Loss 1.9876\n",
      "  Iter 641/1406: Loss 2.2273\n",
      "  Iter 651/1406: Loss 2.3889\n",
      "  Iter 661/1406: Loss 2.0826\n",
      "  Iter 671/1406: Loss 2.1970\n",
      "  Iter 681/1406: Loss 1.7667\n",
      "  Iter 691/1406: Loss 2.1448\n",
      "  Iter 701/1406: Loss 2.2555\n",
      "  Iter 711/1406: Loss 2.1254\n",
      "  Iter 721/1406: Loss 2.0976\n",
      "  Iter 731/1406: Loss 2.0274\n",
      "  Iter 741/1406: Loss 1.9556\n",
      "  Iter 751/1406: Loss 2.0542\n",
      "  Iter 761/1406: Loss 2.2039\n",
      "  Iter 771/1406: Loss 2.2040\n",
      "  Iter 781/1406: Loss 1.8400\n",
      "  Iter 791/1406: Loss 2.1600\n",
      "  Iter 801/1406: Loss 2.0700\n",
      "  Iter 811/1406: Loss 1.8887\n",
      "  Iter 821/1406: Loss 2.1979\n",
      "  Iter 831/1406: Loss 2.0834\n",
      "  Iter 841/1406: Loss 2.0363\n",
      "  Iter 851/1406: Loss 1.8991\n",
      "  Iter 861/1406: Loss 2.3429\n",
      "  Iter 871/1406: Loss 1.9398\n",
      "  Iter 881/1406: Loss 2.0729\n",
      "  Iter 891/1406: Loss 2.4675\n",
      "  Iter 901/1406: Loss 2.3647\n",
      "  Iter 911/1406: Loss 1.9289\n",
      "  Iter 921/1406: Loss 1.9160\n",
      "  Iter 931/1406: Loss 2.2129\n",
      "  Iter 941/1406: Loss 2.3475\n",
      "  Iter 951/1406: Loss 2.3784\n",
      "  Iter 961/1406: Loss 2.3839\n",
      "  Iter 971/1406: Loss 2.4094\n",
      "  Iter 981/1406: Loss 2.2861\n",
      "  Iter 991/1406: Loss 2.3573\n",
      "  Iter 1001/1406: Loss 1.9873\n",
      "  Iter 1011/1406: Loss 2.0213\n",
      "  Iter 1021/1406: Loss 2.4900\n",
      "  Iter 1031/1406: Loss 2.1170\n",
      "  Iter 1041/1406: Loss 2.4206\n",
      "  Iter 1051/1406: Loss 2.3946\n",
      "  Iter 1061/1406: Loss 2.1813\n",
      "  Iter 1071/1406: Loss 2.1931\n",
      "  Iter 1081/1406: Loss 2.4710\n",
      "  Iter 1091/1406: Loss 2.4159\n",
      "  Iter 1101/1406: Loss 2.1883\n",
      "  Iter 1111/1406: Loss 1.9923\n",
      "  Iter 1121/1406: Loss 1.9425\n",
      "  Iter 1131/1406: Loss 2.1645\n",
      "  Iter 1141/1406: Loss 2.1631\n",
      "  Iter 1151/1406: Loss 2.0023\n",
      "  Iter 1161/1406: Loss 2.1808\n",
      "  Iter 1171/1406: Loss 2.3385\n",
      "  Iter 1181/1406: Loss 1.9307\n",
      "  Iter 1191/1406: Loss 1.8511\n",
      "  Iter 1201/1406: Loss 1.7301\n",
      "  Iter 1211/1406: Loss 2.0465\n",
      "  Iter 1221/1406: Loss 2.1466\n",
      "  Iter 1231/1406: Loss 2.3863\n",
      "  Iter 1241/1406: Loss 2.0977\n",
      "  Iter 1251/1406: Loss 2.0486\n",
      "  Iter 1261/1406: Loss 1.9076\n",
      "  Iter 1271/1406: Loss 2.2560\n",
      "  Iter 1281/1406: Loss 2.0519\n",
      "  Iter 1291/1406: Loss 1.8903\n",
      "  Iter 1301/1406: Loss 2.1860\n",
      "  Iter 1311/1406: Loss 1.9493\n",
      "  Iter 1321/1406: Loss 2.1259\n",
      "  Iter 1331/1406: Loss 2.0564\n",
      "  Iter 1341/1406: Loss 2.1717\n",
      "  Iter 1351/1406: Loss 2.1167\n",
      "  Iter 1361/1406: Loss 1.6944\n",
      "  Iter 1371/1406: Loss 2.2839\n",
      "  Iter 1381/1406: Loss 2.2163\n",
      "  Iter 1391/1406: Loss 2.0755\n",
      "  Iter 1401/1406: Loss 1.7812\n",
      "  Iter 1406/1406: Loss 1.8244\n",
      "Fine Train Loss: 2.2002, Fine Train Acc: 0.4520, Val Acc: 0.4298, Val Loss: 2.1937\n",
      "Time: 2146.98s\n",
      "\n",
      "[Epoch 4/100]\n",
      "  Iter   1/1406: Loss 2.0868\n",
      "  Iter  11/1406: Loss 2.1125\n",
      "  Iter  21/1406: Loss 2.0842\n",
      "  Iter  31/1406: Loss 2.2083\n",
      "  Iter  41/1406: Loss 2.1194\n",
      "  Iter  51/1406: Loss 2.1799\n",
      "  Iter  61/1406: Loss 2.2190\n",
      "  Iter  71/1406: Loss 1.9518\n",
      "  Iter  81/1406: Loss 2.1824\n",
      "  Iter  91/1406: Loss 1.9765\n",
      "  Iter 101/1406: Loss 1.8836\n",
      "  Iter 111/1406: Loss 2.1916\n",
      "  Iter 121/1406: Loss 1.8613\n",
      "  Iter 131/1406: Loss 2.0789\n",
      "  Iter 141/1406: Loss 1.9133\n",
      "  Iter 151/1406: Loss 1.7807\n",
      "  Iter 161/1406: Loss 2.1624\n",
      "  Iter 171/1406: Loss 2.1220\n",
      "  Iter 181/1406: Loss 2.3828\n",
      "  Iter 191/1406: Loss 2.2716\n",
      "  Iter 201/1406: Loss 2.1480\n",
      "  Iter 211/1406: Loss 2.2728\n",
      "  Iter 221/1406: Loss 2.1644\n",
      "  Iter 231/1406: Loss 2.5164\n",
      "  Iter 241/1406: Loss 1.9632\n",
      "  Iter 251/1406: Loss 1.8332\n",
      "  Iter 261/1406: Loss 2.0735\n",
      "  Iter 271/1406: Loss 1.7705\n",
      "  Iter 281/1406: Loss 2.1490\n",
      "  Iter 291/1406: Loss 2.1079\n",
      "  Iter 301/1406: Loss 2.2814\n",
      "  Iter 311/1406: Loss 2.1664\n",
      "  Iter 321/1406: Loss 1.8807\n",
      "  Iter 331/1406: Loss 2.1689\n",
      "  Iter 341/1406: Loss 1.8653\n",
      "  Iter 351/1406: Loss 2.0267\n",
      "  Iter 361/1406: Loss 2.3294\n",
      "  Iter 371/1406: Loss 2.0867\n",
      "  Iter 381/1406: Loss 1.9717\n",
      "  Iter 391/1406: Loss 2.1074\n",
      "  Iter 401/1406: Loss 1.6945\n",
      "  Iter 411/1406: Loss 2.2517\n",
      "  Iter 421/1406: Loss 2.0351\n",
      "  Iter 431/1406: Loss 1.9006\n",
      "  Iter 441/1406: Loss 2.0588\n",
      "  Iter 451/1406: Loss 1.9437\n",
      "  Iter 461/1406: Loss 1.8135\n",
      "  Iter 471/1406: Loss 1.8900\n",
      "  Iter 481/1406: Loss 1.9586\n",
      "  Iter 491/1406: Loss 2.1316\n",
      "  Iter 501/1406: Loss 2.0663\n",
      "  Iter 511/1406: Loss 2.0566\n",
      "  Iter 521/1406: Loss 1.9498\n",
      "  Iter 531/1406: Loss 1.8809\n",
      "  Iter 541/1406: Loss 2.0062\n",
      "  Iter 551/1406: Loss 2.2510\n",
      "  Iter 561/1406: Loss 1.8825\n",
      "  Iter 571/1406: Loss 1.9921\n",
      "  Iter 581/1406: Loss 1.8904\n",
      "  Iter 591/1406: Loss 2.1213\n",
      "  Iter 601/1406: Loss 2.0801\n",
      "  Iter 611/1406: Loss 1.8655\n",
      "  Iter 621/1406: Loss 1.9897\n",
      "  Iter 631/1406: Loss 2.0122\n",
      "  Iter 641/1406: Loss 2.1705\n",
      "  Iter 651/1406: Loss 1.7752\n",
      "  Iter 661/1406: Loss 2.2631\n",
      "  Iter 671/1406: Loss 2.0645\n",
      "  Iter 681/1406: Loss 1.7688\n",
      "  Iter 691/1406: Loss 1.9735\n",
      "  Iter 701/1406: Loss 2.0677\n",
      "  Iter 711/1406: Loss 2.1960\n",
      "  Iter 721/1406: Loss 1.7841\n",
      "  Iter 731/1406: Loss 1.9732\n",
      "  Iter 741/1406: Loss 2.2460\n",
      "  Iter 751/1406: Loss 1.6953\n",
      "  Iter 761/1406: Loss 1.9487\n",
      "  Iter 771/1406: Loss 2.1378\n",
      "  Iter 781/1406: Loss 2.1027\n",
      "  Iter 791/1406: Loss 1.8299\n",
      "  Iter 801/1406: Loss 2.1170\n",
      "  Iter 811/1406: Loss 2.1985\n",
      "  Iter 821/1406: Loss 2.0316\n",
      "  Iter 831/1406: Loss 2.0525\n",
      "  Iter 841/1406: Loss 2.2132\n",
      "  Iter 851/1406: Loss 2.0858\n",
      "  Iter 861/1406: Loss 2.1451\n",
      "  Iter 871/1406: Loss 1.7998\n",
      "  Iter 881/1406: Loss 1.9176\n",
      "  Iter 891/1406: Loss 1.9315\n",
      "  Iter 901/1406: Loss 1.7457\n",
      "  Iter 911/1406: Loss 1.9423\n",
      "  Iter 921/1406: Loss 1.7140\n",
      "  Iter 931/1406: Loss 2.0294\n",
      "  Iter 941/1406: Loss 2.0112\n",
      "  Iter 951/1406: Loss 2.1369\n",
      "  Iter 961/1406: Loss 1.8489\n",
      "  Iter 971/1406: Loss 1.9767\n",
      "  Iter 981/1406: Loss 1.9433\n",
      "  Iter 991/1406: Loss 1.7021\n",
      "  Iter 1001/1406: Loss 1.9503\n",
      "  Iter 1011/1406: Loss 1.9010\n",
      "  Iter 1021/1406: Loss 1.9705\n",
      "  Iter 1031/1406: Loss 1.9499\n",
      "  Iter 1041/1406: Loss 1.9812\n",
      "  Iter 1051/1406: Loss 1.7975\n",
      "  Iter 1061/1406: Loss 1.8433\n",
      "  Iter 1071/1406: Loss 1.9784\n",
      "  Iter 1081/1406: Loss 2.0822\n",
      "  Iter 1091/1406: Loss 1.8819\n",
      "  Iter 1101/1406: Loss 1.9674\n",
      "  Iter 1111/1406: Loss 2.1223\n",
      "  Iter 1121/1406: Loss 1.9394\n",
      "  Iter 1131/1406: Loss 2.2886\n",
      "  Iter 1141/1406: Loss 2.1921\n",
      "  Iter 1151/1406: Loss 1.8262\n",
      "  Iter 1161/1406: Loss 2.1228\n",
      "  Iter 1171/1406: Loss 1.9845\n",
      "  Iter 1181/1406: Loss 1.8133\n",
      "  Iter 1191/1406: Loss 1.7713\n",
      "  Iter 1201/1406: Loss 1.9037\n",
      "  Iter 1211/1406: Loss 2.3268\n",
      "  Iter 1221/1406: Loss 1.6583\n",
      "  Iter 1231/1406: Loss 2.0484\n",
      "  Iter 1241/1406: Loss 1.7474\n",
      "  Iter 1251/1406: Loss 2.3839\n",
      "  Iter 1261/1406: Loss 1.8517\n",
      "  Iter 1271/1406: Loss 2.0200\n",
      "  Iter 1281/1406: Loss 1.9729\n",
      "  Iter 1291/1406: Loss 1.9353\n",
      "  Iter 1301/1406: Loss 1.8390\n",
      "  Iter 1311/1406: Loss 2.0314\n",
      "  Iter 1321/1406: Loss 1.9007\n",
      "  Iter 1331/1406: Loss 1.9888\n",
      "  Iter 1341/1406: Loss 1.9508\n",
      "  Iter 1351/1406: Loss 1.8849\n",
      "  Iter 1361/1406: Loss 1.7801\n",
      "  Iter 1371/1406: Loss 1.9268\n",
      "  Iter 1381/1406: Loss 1.7085\n",
      "  Iter 1391/1406: Loss 2.0406\n",
      "  Iter 1401/1406: Loss 1.8924\n",
      "  Iter 1406/1406: Loss 2.3863\n",
      "Fine Train Loss: 2.0213, Fine Train Acc: 0.4770, Val Acc: 0.4472, Val Loss: 2.0750\n",
      "Time: 2516.71s\n",
      "\n",
      "[Epoch 5/100]\n",
      "  Iter   1/1406: Loss 1.7294\n",
      "  Iter  11/1406: Loss 2.3305\n",
      "  Iter  21/1406: Loss 2.1484\n",
      "  Iter  31/1406: Loss 1.7372\n",
      "  Iter  41/1406: Loss 1.9487\n",
      "  Iter  51/1406: Loss 2.1709\n",
      "  Iter  61/1406: Loss 2.1421\n",
      "  Iter  71/1406: Loss 1.8784\n",
      "  Iter  81/1406: Loss 1.9487\n",
      "  Iter  91/1406: Loss 2.0234\n",
      "  Iter 101/1406: Loss 1.9237\n",
      "  Iter 111/1406: Loss 1.7080\n",
      "  Iter 121/1406: Loss 1.6916\n",
      "  Iter 131/1406: Loss 2.1259\n",
      "  Iter 141/1406: Loss 2.0323\n",
      "  Iter 151/1406: Loss 1.9705\n",
      "  Iter 161/1406: Loss 1.9468\n",
      "  Iter 171/1406: Loss 1.9825\n",
      "  Iter 181/1406: Loss 1.8590\n",
      "  Iter 191/1406: Loss 2.2170\n",
      "  Iter 201/1406: Loss 2.0018\n",
      "  Iter 211/1406: Loss 1.8719\n",
      "  Iter 221/1406: Loss 1.8911\n",
      "  Iter 231/1406: Loss 1.9306\n",
      "  Iter 241/1406: Loss 2.0622\n",
      "  Iter 251/1406: Loss 2.3166\n",
      "  Iter 261/1406: Loss 1.9884\n",
      "  Iter 271/1406: Loss 2.0217\n",
      "  Iter 281/1406: Loss 1.8146\n",
      "  Iter 291/1406: Loss 1.8848\n",
      "  Iter 301/1406: Loss 1.6960\n",
      "  Iter 311/1406: Loss 1.6600\n",
      "  Iter 321/1406: Loss 1.9250\n",
      "  Iter 331/1406: Loss 1.9876\n",
      "  Iter 341/1406: Loss 2.0712\n",
      "  Iter 351/1406: Loss 1.8944\n",
      "  Iter 361/1406: Loss 2.2224\n",
      "  Iter 371/1406: Loss 1.8052\n",
      "  Iter 381/1406: Loss 2.0717\n",
      "  Iter 391/1406: Loss 1.8388\n",
      "  Iter 401/1406: Loss 2.1926\n",
      "  Iter 411/1406: Loss 2.2355\n",
      "  Iter 421/1406: Loss 1.9071\n",
      "  Iter 431/1406: Loss 1.8516\n",
      "  Iter 441/1406: Loss 1.9984\n",
      "  Iter 451/1406: Loss 1.9619\n",
      "  Iter 461/1406: Loss 2.1462\n",
      "  Iter 471/1406: Loss 1.9099\n",
      "  Iter 481/1406: Loss 1.9186\n",
      "  Iter 491/1406: Loss 1.6867\n",
      "  Iter 501/1406: Loss 1.8916\n",
      "  Iter 511/1406: Loss 1.9120\n",
      "  Iter 521/1406: Loss 1.8508\n",
      "  Iter 531/1406: Loss 1.9203\n",
      "  Iter 541/1406: Loss 1.6223\n",
      "  Iter 551/1406: Loss 1.8403\n",
      "  Iter 561/1406: Loss 1.8591\n",
      "  Iter 571/1406: Loss 2.0821\n",
      "  Iter 581/1406: Loss 1.9503\n",
      "  Iter 591/1406: Loss 1.8029\n",
      "  Iter 601/1406: Loss 2.1880\n",
      "  Iter 611/1406: Loss 1.7116\n",
      "  Iter 621/1406: Loss 1.6850\n",
      "  Iter 631/1406: Loss 2.0728\n",
      "  Iter 641/1406: Loss 1.9492\n",
      "  Iter 651/1406: Loss 1.8971\n",
      "  Iter 661/1406: Loss 1.9869\n",
      "  Iter 671/1406: Loss 2.1172\n",
      "  Iter 681/1406: Loss 1.8604\n",
      "  Iter 691/1406: Loss 1.8168\n",
      "  Iter 701/1406: Loss 1.7363\n",
      "  Iter 711/1406: Loss 1.9827\n",
      "  Iter 721/1406: Loss 2.2066\n",
      "  Iter 731/1406: Loss 1.4592\n",
      "  Iter 741/1406: Loss 2.4294\n",
      "  Iter 751/1406: Loss 1.8065\n",
      "  Iter 761/1406: Loss 1.9880\n",
      "  Iter 771/1406: Loss 1.7203\n",
      "  Iter 781/1406: Loss 1.9203\n",
      "  Iter 791/1406: Loss 2.1599\n",
      "  Iter 801/1406: Loss 1.8685\n",
      "  Iter 811/1406: Loss 1.8646\n",
      "  Iter 821/1406: Loss 1.8714\n",
      "  Iter 831/1406: Loss 2.0023\n",
      "  Iter 841/1406: Loss 1.8726\n",
      "  Iter 851/1406: Loss 2.1325\n",
      "  Iter 861/1406: Loss 1.9192\n",
      "  Iter 871/1406: Loss 2.0003\n",
      "  Iter 881/1406: Loss 1.9021\n",
      "  Iter 891/1406: Loss 2.2242\n",
      "  Iter 901/1406: Loss 1.8940\n",
      "  Iter 911/1406: Loss 1.8287\n",
      "  Iter 921/1406: Loss 1.8637\n",
      "  Iter 931/1406: Loss 2.0269\n",
      "  Iter 941/1406: Loss 1.8568\n",
      "  Iter 951/1406: Loss 2.0393\n",
      "  Iter 961/1406: Loss 1.6517\n",
      "  Iter 971/1406: Loss 1.8398\n",
      "  Iter 981/1406: Loss 1.8023\n",
      "  Iter 991/1406: Loss 1.8113\n",
      "  Iter 1001/1406: Loss 1.9643\n",
      "  Iter 1011/1406: Loss 1.5426\n",
      "  Iter 1021/1406: Loss 1.9231\n",
      "  Iter 1031/1406: Loss 1.7908\n",
      "  Iter 1041/1406: Loss 1.9580\n",
      "  Iter 1051/1406: Loss 2.1350\n",
      "  Iter 1061/1406: Loss 2.0000\n",
      "  Iter 1071/1406: Loss 1.9506\n",
      "  Iter 1081/1406: Loss 1.9430\n",
      "  Iter 1091/1406: Loss 1.5820\n",
      "  Iter 1101/1406: Loss 1.6255\n",
      "  Iter 1111/1406: Loss 1.9241\n",
      "  Iter 1121/1406: Loss 2.0319\n",
      "  Iter 1131/1406: Loss 1.7744\n",
      "  Iter 1141/1406: Loss 1.8458\n",
      "  Iter 1151/1406: Loss 1.7578\n",
      "  Iter 1161/1406: Loss 2.0199\n",
      "  Iter 1171/1406: Loss 1.6946\n",
      "  Iter 1181/1406: Loss 1.8735\n",
      "  Iter 1191/1406: Loss 1.8343\n",
      "  Iter 1201/1406: Loss 1.9573\n",
      "  Iter 1211/1406: Loss 1.7846\n",
      "  Iter 1221/1406: Loss 1.8467\n",
      "  Iter 1231/1406: Loss 1.4847\n",
      "  Iter 1241/1406: Loss 2.0805\n",
      "  Iter 1251/1406: Loss 1.7602\n",
      "  Iter 1261/1406: Loss 1.8377\n",
      "  Iter 1271/1406: Loss 1.6996\n",
      "  Iter 1281/1406: Loss 1.8612\n",
      "  Iter 1291/1406: Loss 1.8375\n",
      "  Iter 1301/1406: Loss 2.1690\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==== Running ex3 : train dataset 100,000 = original + horizontal flip ====\")\n",
    "model = ResNet20()\n",
    "\n",
    "x_train, y_train = data['train_flip']\n",
    "x_val, y_val = data['val_flip']\n",
    "x_test, y_test = data['test']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    model_name='ResNet-20_ex3',\n",
    "    train_data=(x_train, y_train),\n",
    "    val_data=(x_val, y_val),\n",
    "    test_data=(x_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    lr=0.01,\n",
    "    smoothing=0.15\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_log(\"ResNet-20_ex3_log.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"ResNet-20_ex3_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss (smoothed)\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss (smoothed)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc (smoothed)\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc (smoothed)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

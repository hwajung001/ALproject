{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniVGGNet final Train code\n",
    "\n",
    "### ex 2 : train dataset 50,000 = original + on-the-fly (random crop + horizontal flip)\n",
    "##### (random seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - CIFAR-100 데이터 다운로드 및 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 already downloaded.\n",
      "train: [(45000, 3, 32, 32), (45000,)]\n",
      "val: [(5000, 3, 32, 32), (5000,)]\n",
      "test: [(10000, 3, 32, 32), (10000,)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def download_cifar100(save_path='cifar-100-python'):\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "        return\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
    "    filename = 'cifar-100-python.tar.gz'\n",
    "    print(\"Downloading CIFAR-100...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    with tarfile.open(filename, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "    os.remove(filename)\n",
    "    print(\"Download and extraction completed.\")\n",
    "\n",
    "def load_batch(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "    data = data_dict[b'data']\n",
    "    fine_labels = np.array(data_dict[b'fine_labels'])\n",
    "    data = data.reshape(-1, 3, 32, 32)\n",
    "    return data, fine_labels\n",
    "\n",
    "def normalize_images(images):\n",
    "    mean = np.array([0.5071, 0.4865, 0.4409]).reshape(1, 3, 1, 1)\n",
    "    std  = np.array([0.2673, 0.2564, 0.2762]).reshape(1, 3, 1, 1)\n",
    "    return (images - mean) / std\n",
    "\n",
    "def split_validation(images, labels, val_ratio=0.1):\n",
    "    num_samples = images.shape[0]\n",
    "    val_size = int(num_samples * val_ratio)\n",
    "    idx = np.random.permutation(num_samples)\n",
    "    images = images[idx]\n",
    "    labels = labels[idx]\n",
    "    val_images = images[:val_size]\n",
    "    val_labels = labels[:val_size]\n",
    "    train_images = images[val_size:]\n",
    "    train_labels = labels[val_size:]\n",
    "    return (train_images, train_labels), (val_images, val_labels)\n",
    "\n",
    "def load_cifar100_dataset():\n",
    "    download_cifar100()\n",
    "    train_data, train_fine = load_batch('cifar-100-python/train')\n",
    "    test_data, test_fine = load_batch('cifar-100-python/test')\n",
    "    train_data = normalize_images(train_data)\n",
    "    test_data = normalize_images(test_data)\n",
    "    return (train_data, train_fine), (test_data, test_fine)\n",
    "\n",
    "def prepare_dataset_on_the_fly():\n",
    "    (train_images, train_labels), (test_images, test_labels) = load_cifar100_dataset()\n",
    "    (train_images, train_labels), (val_images, val_labels) = split_validation(train_images, train_labels)\n",
    "    return {\n",
    "        'train': (train_images, train_labels),\n",
    "        'val': (val_images, val_labels),\n",
    "        'test': (test_images, test_labels)\n",
    "    }\n",
    "\n",
    "data = prepare_dataset_on_the_fly()\n",
    "for k, v in data.items():\n",
    "    if isinstance(v, tuple):\n",
    "        print(f\"{k}: {[x.shape for x in v]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Convolution, BatchNormalization, Relu, Pooling, Affine\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "def fake_quantize(x, num_bits=8):\n",
    "    qmin, qmax = 0., 2.**num_bits - 1.\n",
    "    x_min, x_max = np.min(x), np.max(x)\n",
    "    if x_max == x_min:\n",
    "        return x\n",
    "    scale = (x_max - x_min) / (qmax - qmin)\n",
    "    zero_point = np.clip(np.round(qmin - x_min / scale), qmin, qmax)\n",
    "    q_x = np.clip(np.round(zero_point + x / scale), qmin, qmax)\n",
    "    return scale * (q_x - zero_point)\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.orig_shape = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.orig_shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.orig_shape)\n",
    "\n",
    "\n",
    "class MiniVGGNet:\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_classes=100):\n",
    "        in_channels, _, _ = input_dim\n",
    "        weight_std = np.sqrt(2. / in_channels)\n",
    "\n",
    "        self.conv1 = Convolution(np.random.randn(64, in_channels, 3, 3) * weight_std, np.zeros(64), stride=1, pad=1)\n",
    "        self.bn1   = BatchNormalization(np.ones(64), np.zeros(64))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.conv2 = Convolution(np.random.randn(64, 64, 3, 3) * weight_std, np.zeros(64), stride=1, pad=1)\n",
    "        self.bn2   = BatchNormalization(np.ones(64), np.zeros(64))\n",
    "        self.relu2 = Relu()\n",
    "        self.pool1 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        self.conv3 = Convolution(np.random.randn(128, 64, 3, 3) * weight_std, np.zeros(128), stride=1, pad=1)\n",
    "        self.bn3   = BatchNormalization(np.ones(128), np.zeros(128))\n",
    "        self.relu3 = Relu()\n",
    "\n",
    "        self.conv4 = Convolution(np.random.randn(128, 128, 3, 3) * weight_std, np.zeros(128), stride=1, pad=1)\n",
    "        self.bn4   = BatchNormalization(np.ones(128), np.zeros(128))\n",
    "        self.relu4 = Relu()\n",
    "        self.pool2 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        self.conv5 = Convolution(np.random.randn(256, 128, 3, 3) * weight_std, np.zeros(256), stride=1, pad=1)\n",
    "        self.bn5   = BatchNormalization(np.ones(256), np.zeros(256)) #conv5 also \n",
    "        self.relu5 = Relu()\n",
    "        self.pool3 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Affine(np.random.randn(4096, 512) * weight_std, np.zeros(512))\n",
    "        self.relu6 = Relu()\n",
    "        self.fc2 = Affine(np.random.randn(512, num_classes) * 0.01, np.zeros(num_classes))\n",
    "\n",
    "        self.layers = [\n",
    "            self.conv1, self.bn1, self.relu1,\n",
    "            self.conv2, self.bn2, self.relu2, self.pool1,\n",
    "            self.conv3, self.bn3, self.relu3,\n",
    "            self.conv4, self.bn4, self.relu4, self.pool2,\n",
    "            self.conv5, self.bn5, self.relu5, self.pool3, #conv5\n",
    "            self.flatten, self.fc1, self.relu6, self.fc2\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNormalization):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.relu6.backward(dout)\n",
    "        dout = self.fc1.backward(dout)\n",
    "        dout = self.flatten.backward(dout)\n",
    "\n",
    "        dout = self.pool3.backward(dout)\n",
    "        dout = self.relu5.backward(dout)\n",
    "        dout = self.bn5.backward(dout)\n",
    "        dout = self.conv5.backward(dout)\n",
    "\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.relu4.backward(dout)\n",
    "        dout = self.bn4.backward(dout)\n",
    "        dout = self.conv4.backward(dout)\n",
    "\n",
    "        dout = self.relu3.backward(dout)\n",
    "        dout = self.bn3.backward(dout)\n",
    "        dout = self.conv3.backward(dout)\n",
    "\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "\n",
    "        return dout\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.fc2.backward(dout)\n",
    "        dout = self.relu6.backward(dout)\n",
    "        dout = self.fc1.backward(dout)\n",
    "        dout = self.flatten.backward(dout)\n",
    "\n",
    "        dout = self.pool3.backward(dout)\n",
    "        dout = self.relu5.backward(dout)\n",
    "        dout = self.bn5.backward(dout)\n",
    "        dout = self.conv5.backward(dout)\n",
    "\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.relu4.backward(dout)\n",
    "        dout = self.bn4.backward(dout)\n",
    "        dout = self.conv4.backward(dout)\n",
    "\n",
    "        dout = self.relu3.backward(dout)\n",
    "        dout = self.bn3.backward(dout)\n",
    "        dout = self.conv3.backward(dout)\n",
    "\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "\n",
    "        grads = {\n",
    "            'W1': self.conv1.W, 'b1': self.conv1.b,\n",
    "            'gamma1': self.bn1.gamma, 'beta1': self.bn1.beta,\n",
    "            'W2': self.conv2.W, 'b2': self.conv2.b,\n",
    "            'gamma2': self.bn2.gamma, 'beta2': self.bn2.beta,\n",
    "            'W3': self.conv3.W, 'b3': self.conv3.b,\n",
    "            'gamma3': self.bn3.gamma, 'beta3': self.bn3.beta,\n",
    "            'W4': self.conv4.W, 'b4': self.conv4.b,\n",
    "            'gamma4': self.bn4.gamma, 'beta4': self.bn4.beta,\n",
    "            'W5': self.conv5.W, 'b5': self.conv5.b,\n",
    "            'gamma5': self.bn5.gamma, 'beta5': self.bn5.beta,  \n",
    "            'W6': self.fc1.W, 'b6': self.fc1.b,\n",
    "            'W7': self.fc2.W, 'b7': self.fc2.b,\n",
    "        }\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def predict(self, x, batch_size=100):\n",
    "        return np.concatenate([self.forward(x[i:i+batch_size], False) for i in range(0, x.shape[0], batch_size)], axis=0)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.forward(x, True)\n",
    "        y_softmax = softmax(y)  \n",
    "        return cross_entropy_error(y_softmax, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        pred = np.argmax(self.predict(x, batch_size), axis=1)\n",
    "        true = t if t.ndim == 1 else np.argmax(t, axis=1)\n",
    "        return np.mean(pred == true)\n",
    "\n",
    "    def clip_weights(self, clip_value=1.0): \n",
    "        for layer in [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.fc1, self.fc2]:\n",
    "            layer.W = np.clip(layer.W, -clip_value, clip_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 구조 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type)                       Output Shape                Param #\n",
      "===========================================================================\n",
      " 1. Conv1                           (1, 64, 32, 32)               1,792\n",
      " 2. BN1                             (1, 64, 32, 32)                 128\n",
      " 3. Conv2                           (1, 64, 32, 32)              36,928\n",
      " 4. BN2                             (1, 64, 32, 32)                 128\n",
      " 5. Conv3                           (1, 128, 16, 16)             73,856\n",
      " 6. BN3                             (1, 128, 16, 16)                256\n",
      " 7. Conv4                           (1, 128, 16, 16)            147,584\n",
      " 8. BN4                             (1, 128, 16, 16)                256\n",
      " 9. Conv5                           (1, 256, 8, 8)              295,168\n",
      "10. Flatten                         (1, 4096)                         0\n",
      "11. FC1                             (1, 512)                  2,097,664\n",
      "12. FC2                             (1, 100)                     51,300\n",
      "===========================================================================\n",
      "Total weight layers:                                        13\n",
      "Total params:                                               2,705,060\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "from common.layers import Convolution, BatchNormalization, Relu, Pooling, Affine\n",
    "import numpy as np\n",
    "\n",
    "def count_params(layer):\n",
    "    count = 0\n",
    "    if hasattr(layer, 'W'):\n",
    "        count += np.prod(layer.W.shape)\n",
    "    if hasattr(layer, 'b'):\n",
    "        count += np.prod(layer.b.shape)\n",
    "    if hasattr(layer, 'gamma'):\n",
    "        count += np.prod(layer.gamma.shape)\n",
    "    if hasattr(layer, 'beta'):\n",
    "        count += np.prod(layer.beta.shape)\n",
    "    return count\n",
    "\n",
    "def print_vggnet_summary(model, input_shape=(1, 3, 32, 32)):\n",
    "    print(\"=\" * 75)\n",
    "    print(f\"{'Layer (type)':<35}{'Output Shape':<25}{'Param #':>10}\")\n",
    "    print(\"=\" * 75)\n",
    "\n",
    "    x = np.zeros(input_shape)\n",
    "    total_params = 0\n",
    "    layer_idx = 1\n",
    "\n",
    "    def log(name, x, p):\n",
    "        nonlocal total_params, layer_idx\n",
    "        print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\")\n",
    "        total_params += p\n",
    "        layer_idx += 1\n",
    "\n",
    "    x = model.conv1.forward(x)\n",
    "    log(\"Conv1\", x, count_params(model.conv1))\n",
    "    x = model.bn1.forward(x, train_flg=False)\n",
    "    log(\"BN1\", x, count_params(model.bn1))\n",
    "    x = model.relu1.forward(x)\n",
    "\n",
    "    x = model.conv2.forward(x)\n",
    "    log(\"Conv2\", x, count_params(model.conv2))\n",
    "    x = model.bn2.forward(x, train_flg=False)\n",
    "    log(\"BN2\", x, count_params(model.bn2))\n",
    "    x = model.relu2.forward(x)\n",
    "    x = model.pool1.forward(x)\n",
    "\n",
    "    x = model.conv3.forward(x)\n",
    "    log(\"Conv3\", x, count_params(model.conv3))\n",
    "    x = model.bn3.forward(x, train_flg=False)\n",
    "    log(\"BN3\", x, count_params(model.bn3))\n",
    "    x = model.relu3.forward(x)\n",
    "\n",
    "    x = model.conv4.forward(x)\n",
    "    log(\"Conv4\", x, count_params(model.conv4))\n",
    "    x = model.bn4.forward(x, train_flg=False)\n",
    "    log(\"BN4\", x, count_params(model.bn4))\n",
    "    x = model.relu4.forward(x)\n",
    "    x = model.pool2.forward(x)\n",
    "\n",
    "    x = model.conv5.forward(x)\n",
    "    log(\"Conv5\", x, count_params(model.conv5))\n",
    "    x = model.relu5.forward(x)\n",
    "    x = model.pool3.forward(x)\n",
    "\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    log(\"Flatten\", x, 0)\n",
    "\n",
    "    x = model.fc1.forward(x)\n",
    "    log(\"FC1\", x, count_params(model.fc1))\n",
    "    x = model.relu6.forward(x)\n",
    "    x = model.fc2.forward(x)\n",
    "    log(\"FC2\", x, count_params(model.fc2))\n",
    "\n",
    "    print(\"=\" * 75)\n",
    "    print(f\"{'Total weight layers:':<60}{layer_idx}\")\n",
    "    print(f\"{'Total params:':<60}{total_params:,}\")\n",
    "    print(\"=\" * 75)\n",
    "\n",
    "model = MiniVGGNet()\n",
    "print_vggnet_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from common.optimizer import Adam\n",
    "from common.functions import softmax\n",
    "\n",
    "def smooth_labels(y, smoothing=0.1, num_classes=100):\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = (y.shape[0], num_classes)\n",
    "    smooth = np.full(label_shape, smoothing / (num_classes - 1))\n",
    "    smooth[np.arange(y.shape[0]), y] = confidence\n",
    "    return smooth\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name,\n",
    "                 train_data, val_data, test_data,\n",
    "                 epochs=20, batch_size=64, lr=0.01,\n",
    "                 smoothing=0.15):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.val_x, self.val_t = val_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "        self.optimizer = Adam(lr=lr)\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def smooth_labels(self, y, num_classes=100):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        label_shape = (y.shape[0], num_classes)\n",
    "        smooth = np.full(label_shape, self.smoothing / (num_classes - 1), dtype=np.float32)\n",
    "        smooth[np.arange(y.shape[0]), y] = confidence\n",
    "        return smooth\n",
    "\n",
    "    def random_crop(self, x, crop_size=32, padding=4):\n",
    "        n, c, h, w = x.shape\n",
    "        padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='reflect')\n",
    "        cropped = np.empty((n, c, crop_size, crop_size), dtype=x.dtype)\n",
    "        for i in range(n):\n",
    "            top = np.random.randint(0, padding * 2 + 1)\n",
    "            left = np.random.randint(0, padding * 2 + 1)\n",
    "            cropped[i] = padded[i, :, top:top+crop_size, left:left+crop_size]\n",
    "        return cropped\n",
    "    \n",
    "    def horizontal_flip(self, x):\n",
    "        if np.random.rand() < 0.5:\n",
    "            return x[:, :, :, ::-1]\n",
    "        return x\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            dx = (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            dx /= batch_size\n",
    "        return dx, y\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "\n",
    "        for name in ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc1', 'fc2']:\n",
    "            layer = getattr(self.model, name)\n",
    "            if hasattr(layer, 'W'):\n",
    "                param_dict[f'{name}_W'] = layer.W\n",
    "                param_dict[f'{name}_b'] = layer.b\n",
    "                grad_dict[f'{name}_W'] = layer.dW\n",
    "                grad_dict[f'{name}_b'] = layer.db\n",
    "\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def get_bn_param_dict(self):\n",
    "        bn_dict = {}\n",
    "        idx = 0\n",
    "        for name in ['bn1', 'bn2', 'bn3', 'bn4', 'bn5']:\n",
    "            bn = getattr(self.model, name)\n",
    "            bn_dict[f'bn{idx}_gamma'] = bn.gamma.copy()\n",
    "            bn_dict[f'bn{idx}_beta'] = bn.beta.copy()\n",
    "            bn_dict[f'bn{idx}_running_mean'] = bn.running_mean.copy()\n",
    "            bn_dict[f'bn{idx}_running_var'] = bn.running_var.copy()\n",
    "            idx += 1\n",
    "        return bn_dict\n",
    "    \n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        x_batch = self.random_crop(x_batch)\n",
    "        x_batch = self.horizontal_flip(x_batch)\n",
    "\n",
    "        if t_batch.ndim == 1:\n",
    "            t_batch = self.smooth_labels(t_batch)\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        dx, _ = self.loss_grad(x_batch, t_batch)\n",
    "        dout = self.model.fc2.backward(dx)\n",
    "\n",
    "        self.model.backward(dout)\n",
    "\n",
    "        if hasattr(self.model, 'clip_weights'):\n",
    "            self.model.clip_weights(clip_value=1.0)\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        patience = 10\n",
    "        best_val_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\n[Epoch {epoch + 1}/{self.epochs}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0 or i == self.iter_per_epoch - 1:\n",
    "                    print(f\"  Iter {i+1:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            val_acc = self.model.accuracy(self.val_x, self.val_t)\n",
    "            val_loss = self.batched_loss(self.val_x, self.val_t, batch_size=128)\n",
    "\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.val_acc_list.append(val_acc)\n",
    "            self.val_loss_list.append(val_loss)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Fine Train Loss: {avg_loss:.4f}, Fine Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}\", flush=True)\n",
    "            print(f\"Time: {elapsed:.2f}s\", flush=True)\n",
    "\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_model(f\"{self.model_name}_epoch{epoch+1}.pkl\")\n",
    "                print(f\">>> Model saved to {self.model_name}_epoch{epoch+1}.pkl\", flush=True)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improve_count = 0\n",
    "                self.save_model(f\"{self.model_name}_best.pkl\")\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "    def batched_loss(self, x, t, batch_size=128):\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        num_classes = 100  \n",
    "    \n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "    \n",
    "            if t_batch.ndim == 1:\n",
    "                t_onehot = np.zeros((t_batch.size, num_classes), dtype=np.float32)\n",
    "                t_onehot[np.arange(t_batch.size), t_batch] = 1.0\n",
    "            else:\n",
    "                t_onehot = t_batch  \n",
    "    \n",
    "            loss = self.model.loss(x_batch, t_onehot)\n",
    "            total_loss += loss * len(x_batch)\n",
    "            total_count += len(x_batch)\n",
    "    \n",
    "        return total_loss / total_count\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "        bn_state = self.get_bn_param_dict()\n",
    "        \n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': self.optimizer.beta1,\n",
    "            'beta2': self.optimizer.beta2,\n",
    "            'm': self.optimizer.m,\n",
    "            'v': self.optimizer.v,\n",
    "            't': self.optimizer.iter\n",
    "        }\n",
    "\n",
    "        save_data = {\n",
    "            'model': model_state,\n",
    "            'bn': bn_state,\n",
    "            'optimizer': optimizer_state,\n",
    "            'train_loss_list': self.train_loss_list,\n",
    "            'train_acc_list': self.train_acc_list,\n",
    "            'val_acc_list': self.val_acc_list,\n",
    "            'val_loss_list': self.val_loss_list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename,\n",
    "                 loss=np.array(self.train_loss_list),\n",
    "                 train_acc=np.array(self.train_acc_list),\n",
    "                 val_acc=np.array(self.val_acc_list),\n",
    "                 val_loss=np.array(self.val_loss_list))\n",
    "        print(f\"Log saved to {filename}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Running ex2 : train dataset 50,000 = original + on-the-fly (random crop + horizontal flip) ====\n",
      "\n",
      "[Epoch 1/100]\n",
      "  Iter   1/703: Loss 14.9239\n",
      "  Iter  11/703: Loss 10.7589\n",
      "  Iter  21/703: Loss 10.3796\n",
      "  Iter  31/703: Loss 6.4044\n",
      "  Iter  41/703: Loss 5.6508\n",
      "  Iter  51/703: Loss 5.0497\n",
      "  Iter  61/703: Loss 4.7871\n",
      "  Iter  71/703: Loss 4.4200\n",
      "  Iter  81/703: Loss 4.5971\n",
      "  Iter  91/703: Loss 4.2888\n",
      "  Iter 101/703: Loss 4.4235\n",
      "  Iter 111/703: Loss 4.3538\n",
      "  Iter 121/703: Loss 4.3979\n",
      "  Iter 131/703: Loss 4.2548\n",
      "  Iter 141/703: Loss 4.3201\n",
      "  Iter 151/703: Loss 4.2268\n",
      "  Iter 161/703: Loss 4.2349\n",
      "  Iter 171/703: Loss 4.1208\n",
      "  Iter 181/703: Loss 4.0762\n",
      "  Iter 191/703: Loss 3.9106\n",
      "  Iter 201/703: Loss 4.2852\n",
      "  Iter 211/703: Loss 4.0228\n",
      "  Iter 221/703: Loss 4.0590\n",
      "  Iter 231/703: Loss 4.0767\n",
      "  Iter 241/703: Loss 3.9861\n",
      "  Iter 251/703: Loss 3.9492\n",
      "  Iter 261/703: Loss 3.9104\n",
      "  Iter 271/703: Loss 4.0484\n",
      "  Iter 281/703: Loss 3.9156\n",
      "  Iter 291/703: Loss 4.2410\n",
      "  Iter 301/703: Loss 4.2139\n",
      "  Iter 311/703: Loss 3.9186\n",
      "  Iter 321/703: Loss 3.8441\n",
      "  Iter 331/703: Loss 4.0927\n",
      "  Iter 341/703: Loss 3.6853\n",
      "  Iter 351/703: Loss 3.9351\n",
      "  Iter 361/703: Loss 3.9803\n",
      "  Iter 371/703: Loss 3.9361\n",
      "  Iter 381/703: Loss 3.7296\n",
      "  Iter 391/703: Loss 4.0384\n",
      "  Iter 401/703: Loss 3.6382\n",
      "  Iter 411/703: Loss 3.5923\n",
      "  Iter 421/703: Loss 4.0872\n",
      "  Iter 431/703: Loss 3.6786\n",
      "  Iter 441/703: Loss 3.8587\n",
      "  Iter 451/703: Loss 3.6544\n",
      "  Iter 461/703: Loss 3.7081\n",
      "  Iter 471/703: Loss 3.7506\n",
      "  Iter 481/703: Loss 3.7645\n",
      "  Iter 491/703: Loss 3.9996\n",
      "  Iter 501/703: Loss 3.6645\n",
      "  Iter 511/703: Loss 3.8422\n",
      "  Iter 521/703: Loss 3.5184\n",
      "  Iter 531/703: Loss 3.6932\n",
      "  Iter 541/703: Loss 3.7612\n",
      "  Iter 551/703: Loss 3.4978\n",
      "  Iter 561/703: Loss 3.7914\n",
      "  Iter 571/703: Loss 3.8831\n",
      "  Iter 581/703: Loss 3.6537\n",
      "  Iter 591/703: Loss 3.8623\n",
      "  Iter 601/703: Loss 3.9329\n",
      "  Iter 611/703: Loss 3.5533\n",
      "  Iter 621/703: Loss 3.6715\n",
      "  Iter 631/703: Loss 3.5091\n",
      "  Iter 641/703: Loss 3.6526\n",
      "  Iter 651/703: Loss 3.9202\n",
      "  Iter 661/703: Loss 3.5524\n",
      "  Iter 671/703: Loss 3.4355\n",
      "  Iter 681/703: Loss 3.6166\n",
      "  Iter 691/703: Loss 3.5093\n",
      "  Iter 701/703: Loss 3.8505\n",
      "  Iter 703/703: Loss 3.7138\n",
      "Fine Train Loss: 4.2786, Fine Train Acc: 0.1760, Val Acc: 0.1674, Val Loss: 3.5663\n",
      "Time: 2837.98s\n",
      "\n",
      "[Epoch 2/100]\n",
      "  Iter   1/703: Loss 3.7442\n",
      "  Iter  11/703: Loss 3.9748\n",
      "  Iter  21/703: Loss 3.6916\n",
      "  Iter  31/703: Loss 3.7752\n",
      "  Iter  41/703: Loss 3.6308\n",
      "  Iter  51/703: Loss 3.4265\n",
      "  Iter  61/703: Loss 3.6375\n",
      "  Iter  71/703: Loss 3.6120\n",
      "  Iter  81/703: Loss 3.6704\n",
      "  Iter  91/703: Loss 3.5716\n",
      "  Iter 101/703: Loss 3.7407\n",
      "  Iter 111/703: Loss 3.7081\n",
      "  Iter 121/703: Loss 3.5623\n",
      "  Iter 131/703: Loss 3.6314\n",
      "  Iter 141/703: Loss 3.7408\n",
      "  Iter 151/703: Loss 3.4422\n",
      "  Iter 161/703: Loss 3.5315\n",
      "  Iter 171/703: Loss 3.5508\n",
      "  Iter 181/703: Loss 3.7070\n",
      "  Iter 191/703: Loss 3.7416\n",
      "  Iter 201/703: Loss 3.4882\n",
      "  Iter 211/703: Loss 3.6045\n",
      "  Iter 221/703: Loss 3.3499\n",
      "  Iter 231/703: Loss 3.3721\n",
      "  Iter 241/703: Loss 3.7015\n",
      "  Iter 251/703: Loss 3.6650\n",
      "  Iter 261/703: Loss 3.8456\n",
      "  Iter 271/703: Loss 3.6370\n",
      "  Iter 281/703: Loss 3.5015\n",
      "  Iter 291/703: Loss 3.5033\n",
      "  Iter 301/703: Loss 3.5488\n",
      "  Iter 311/703: Loss 3.7620\n",
      "  Iter 321/703: Loss 3.4451\n",
      "  Iter 331/703: Loss 3.5545\n",
      "  Iter 341/703: Loss 3.4432\n",
      "  Iter 351/703: Loss 3.8469\n",
      "  Iter 361/703: Loss 3.3470\n",
      "  Iter 371/703: Loss 3.4905\n",
      "  Iter 381/703: Loss 3.5094\n",
      "  Iter 391/703: Loss 3.4257\n",
      "  Iter 401/703: Loss 3.6662\n",
      "  Iter 411/703: Loss 3.2633\n",
      "  Iter 421/703: Loss 3.8379\n",
      "  Iter 431/703: Loss 3.1597\n",
      "  Iter 441/703: Loss 3.4550\n",
      "  Iter 451/703: Loss 3.1748\n",
      "  Iter 461/703: Loss 3.6099\n",
      "  Iter 471/703: Loss 3.6747\n",
      "  Iter 481/703: Loss 3.5955\n",
      "  Iter 491/703: Loss 3.5312\n",
      "  Iter 501/703: Loss 3.6323\n",
      "  Iter 511/703: Loss 3.6793\n",
      "  Iter 521/703: Loss 3.7524\n",
      "  Iter 531/703: Loss 3.4369\n",
      "  Iter 541/703: Loss 3.7306\n",
      "  Iter 551/703: Loss 3.6576\n",
      "  Iter 561/703: Loss 3.1947\n",
      "  Iter 571/703: Loss 3.8800\n",
      "  Iter 581/703: Loss 3.4748\n",
      "  Iter 591/703: Loss 3.5577\n",
      "  Iter 601/703: Loss 3.2914\n",
      "  Iter 611/703: Loss 3.6714\n",
      "  Iter 621/703: Loss 3.2037\n",
      "  Iter 631/703: Loss 3.4048\n",
      "  Iter 641/703: Loss 3.7133\n",
      "  Iter 651/703: Loss 3.7154\n",
      "  Iter 661/703: Loss 3.3124\n",
      "  Iter 671/703: Loss 3.2398\n",
      "  Iter 681/703: Loss 3.6615\n",
      "  Iter 691/703: Loss 3.6405\n",
      "  Iter 701/703: Loss 3.5425\n",
      "  Iter 703/703: Loss 3.5238\n",
      "Fine Train Loss: 3.5666, Fine Train Acc: 0.2220, Val Acc: 0.1968, Val Loss: 3.3794\n",
      "Time: 2822.42s\n",
      "\n",
      "[Epoch 3/100]\n",
      "  Iter   1/703: Loss 3.3789\n",
      "  Iter  11/703: Loss 3.4108\n",
      "  Iter  21/703: Loss 3.4893\n",
      "  Iter  31/703: Loss 3.2040\n",
      "  Iter  41/703: Loss 3.4679\n",
      "  Iter  51/703: Loss 3.4712\n",
      "  Iter  61/703: Loss 3.3034\n",
      "  Iter  71/703: Loss 3.4005\n",
      "  Iter  81/703: Loss 3.7348\n",
      "  Iter  91/703: Loss 3.4604\n",
      "  Iter 101/703: Loss 3.2306\n",
      "  Iter 111/703: Loss 3.6211\n",
      "  Iter 121/703: Loss 3.4847\n",
      "  Iter 131/703: Loss 3.4363\n",
      "  Iter 141/703: Loss 3.3086\n",
      "  Iter 151/703: Loss 3.4651\n",
      "  Iter 161/703: Loss 3.3590\n",
      "  Iter 171/703: Loss 3.4125\n",
      "  Iter 181/703: Loss 3.5650\n",
      "  Iter 191/703: Loss 3.3874\n",
      "  Iter 201/703: Loss 3.2983\n",
      "  Iter 211/703: Loss 3.3907\n",
      "  Iter 221/703: Loss 3.4107\n",
      "  Iter 231/703: Loss 3.4774\n",
      "  Iter 241/703: Loss 3.3669\n",
      "  Iter 251/703: Loss 3.4206\n",
      "  Iter 261/703: Loss 3.1964\n",
      "  Iter 271/703: Loss 3.1457\n",
      "  Iter 281/703: Loss 3.5870\n",
      "  Iter 291/703: Loss 3.1951\n",
      "  Iter 301/703: Loss 3.2417\n",
      "  Iter 311/703: Loss 3.4508\n",
      "  Iter 321/703: Loss 3.4902\n",
      "  Iter 331/703: Loss 3.1895\n",
      "  Iter 341/703: Loss 3.5673\n",
      "  Iter 351/703: Loss 3.6679\n",
      "  Iter 361/703: Loss 3.2991\n",
      "  Iter 371/703: Loss 3.3983\n",
      "  Iter 381/703: Loss 3.1634\n",
      "  Iter 391/703: Loss 3.4727\n",
      "  Iter 401/703: Loss 3.4497\n",
      "  Iter 411/703: Loss 3.4586\n",
      "  Iter 421/703: Loss 3.4589\n",
      "  Iter 431/703: Loss 3.7521\n",
      "  Iter 441/703: Loss 3.4909\n",
      "  Iter 451/703: Loss 3.1006\n",
      "  Iter 461/703: Loss 3.2664\n",
      "  Iter 471/703: Loss 3.2587\n",
      "  Iter 481/703: Loss 3.0489\n",
      "  Iter 491/703: Loss 3.3703\n",
      "  Iter 501/703: Loss 3.1825\n",
      "  Iter 511/703: Loss 3.6174\n",
      "  Iter 521/703: Loss 3.2669\n",
      "  Iter 531/703: Loss 3.3471\n",
      "  Iter 541/703: Loss 3.2571\n",
      "  Iter 551/703: Loss 3.6939\n",
      "  Iter 561/703: Loss 3.1224\n",
      "  Iter 571/703: Loss 3.6597\n",
      "  Iter 581/703: Loss 3.3780\n",
      "  Iter 591/703: Loss 3.6544\n",
      "  Iter 601/703: Loss 3.7435\n",
      "  Iter 611/703: Loss 3.5379\n",
      "  Iter 621/703: Loss 3.5565\n",
      "  Iter 631/703: Loss 3.3980\n",
      "  Iter 641/703: Loss 3.2799\n",
      "  Iter 651/703: Loss 3.2640\n",
      "  Iter 661/703: Loss 3.5560\n",
      "  Iter 671/703: Loss 3.4667\n",
      "  Iter 681/703: Loss 3.3138\n",
      "  Iter 691/703: Loss 3.5095\n",
      "  Iter 701/703: Loss 3.4438\n",
      "  Iter 703/703: Loss 3.1233\n",
      "Fine Train Loss: 3.4067, Fine Train Acc: 0.2190, Val Acc: 0.2114, Val Loss: 3.2783\n",
      "Time: 2827.85s\n",
      "\n",
      "[Epoch 4/100]\n",
      "  Iter   1/703: Loss 3.2217\n",
      "  Iter  11/703: Loss 3.1915\n",
      "  Iter  21/703: Loss 3.3294\n",
      "  Iter  31/703: Loss 3.3763\n",
      "  Iter  41/703: Loss 2.8672\n",
      "  Iter  51/703: Loss 3.4968\n",
      "  Iter  61/703: Loss 3.3720\n",
      "  Iter  71/703: Loss 3.4625\n",
      "  Iter  81/703: Loss 3.4259\n",
      "  Iter  91/703: Loss 3.2565\n",
      "  Iter 101/703: Loss 3.2841\n",
      "  Iter 111/703: Loss 3.0907\n",
      "  Iter 121/703: Loss 3.2030\n",
      "  Iter 131/703: Loss 3.1314\n",
      "  Iter 141/703: Loss 3.4627\n",
      "  Iter 151/703: Loss 3.1859\n",
      "  Iter 161/703: Loss 3.2072\n",
      "  Iter 171/703: Loss 3.1906\n",
      "  Iter 181/703: Loss 3.1455\n",
      "  Iter 191/703: Loss 3.2144\n",
      "  Iter 201/703: Loss 3.3855\n",
      "  Iter 211/703: Loss 3.1601\n",
      "  Iter 221/703: Loss 3.5091\n",
      "  Iter 231/703: Loss 3.4424\n",
      "  Iter 241/703: Loss 3.4110\n",
      "  Iter 251/703: Loss 3.4627\n",
      "  Iter 261/703: Loss 3.1795\n",
      "  Iter 271/703: Loss 3.1400\n",
      "  Iter 281/703: Loss 3.4178\n",
      "  Iter 291/703: Loss 3.3832\n",
      "  Iter 301/703: Loss 3.1075\n",
      "  Iter 311/703: Loss 3.2905\n",
      "  Iter 321/703: Loss 3.2894\n",
      "  Iter 331/703: Loss 3.0695\n",
      "  Iter 341/703: Loss 3.4396\n",
      "  Iter 351/703: Loss 3.4073\n",
      "  Iter 361/703: Loss 3.4125\n",
      "  Iter 371/703: Loss 2.7268\n",
      "  Iter 381/703: Loss 3.3166\n",
      "  Iter 391/703: Loss 3.3257\n",
      "  Iter 401/703: Loss 3.3078\n",
      "  Iter 411/703: Loss 3.3410\n",
      "  Iter 421/703: Loss 3.1916\n",
      "  Iter 431/703: Loss 3.2369\n",
      "  Iter 441/703: Loss 3.3719\n",
      "  Iter 451/703: Loss 3.2433\n",
      "  Iter 461/703: Loss 3.4123\n",
      "  Iter 471/703: Loss 3.0136\n",
      "  Iter 481/703: Loss 2.9023\n",
      "  Iter 491/703: Loss 3.6154\n",
      "  Iter 501/703: Loss 3.6862\n",
      "  Iter 511/703: Loss 2.9953\n",
      "  Iter 521/703: Loss 3.4389\n",
      "  Iter 531/703: Loss 3.4514\n",
      "  Iter 541/703: Loss 3.0885\n",
      "  Iter 551/703: Loss 3.4215\n",
      "  Iter 561/703: Loss 3.2936\n",
      "  Iter 571/703: Loss 3.0039\n",
      "  Iter 581/703: Loss 3.2582\n",
      "  Iter 591/703: Loss 3.5734\n",
      "  Iter 601/703: Loss 3.3976\n",
      "  Iter 611/703: Loss 3.6316\n",
      "  Iter 621/703: Loss 3.1934\n",
      "  Iter 631/703: Loss 3.4273\n",
      "  Iter 641/703: Loss 2.9348\n",
      "  Iter 651/703: Loss 3.3588\n",
      "  Iter 661/703: Loss 3.2569\n",
      "  Iter 671/703: Loss 3.3219\n",
      "  Iter 681/703: Loss 3.2546\n",
      "  Iter 691/703: Loss 2.9904\n",
      "  Iter 701/703: Loss 3.3943\n",
      "  Iter 703/703: Loss 2.8093\n",
      "Fine Train Loss: 3.2969, Fine Train Acc: 0.2350, Val Acc: 0.2226, Val Loss: 3.2236\n",
      "Time: 2841.92s\n",
      "\n",
      "[Epoch 5/100]\n",
      "  Iter   1/703: Loss 3.0482\n",
      "  Iter  11/703: Loss 2.9948\n",
      "  Iter  21/703: Loss 3.1614\n",
      "  Iter  31/703: Loss 3.0048\n",
      "  Iter  41/703: Loss 3.3116\n",
      "  Iter  51/703: Loss 3.1849\n",
      "  Iter  61/703: Loss 3.3152\n",
      "  Iter  71/703: Loss 3.3708\n",
      "  Iter  81/703: Loss 3.0908\n",
      "  Iter  91/703: Loss 2.9566\n",
      "  Iter 101/703: Loss 3.2168\n",
      "  Iter 111/703: Loss 3.5594\n",
      "  Iter 121/703: Loss 3.3267\n",
      "  Iter 131/703: Loss 3.0933\n",
      "  Iter 141/703: Loss 2.5934\n",
      "  Iter 151/703: Loss 3.2098\n",
      "  Iter 161/703: Loss 3.1461\n",
      "  Iter 171/703: Loss 3.4777\n",
      "  Iter 181/703: Loss 3.3580\n",
      "  Iter 191/703: Loss 3.4208\n",
      "  Iter 201/703: Loss 3.2838\n",
      "  Iter 211/703: Loss 3.1760\n",
      "  Iter 221/703: Loss 3.5111\n",
      "  Iter 231/703: Loss 3.2138\n",
      "  Iter 241/703: Loss 3.2725\n",
      "  Iter 251/703: Loss 2.8297\n",
      "  Iter 261/703: Loss 3.4422\n",
      "  Iter 271/703: Loss 3.7302\n",
      "  Iter 281/703: Loss 3.1088\n",
      "  Iter 291/703: Loss 2.9000\n",
      "  Iter 301/703: Loss 3.2137\n",
      "  Iter 311/703: Loss 3.4157\n",
      "  Iter 321/703: Loss 3.0549\n",
      "  Iter 331/703: Loss 3.3833\n",
      "  Iter 341/703: Loss 3.2751\n",
      "  Iter 351/703: Loss 3.0887\n",
      "  Iter 361/703: Loss 3.3409\n",
      "  Iter 371/703: Loss 3.1864\n",
      "  Iter 381/703: Loss 3.3316\n",
      "  Iter 391/703: Loss 3.2137\n",
      "  Iter 401/703: Loss 3.2501\n",
      "  Iter 411/703: Loss 3.3258\n",
      "  Iter 421/703: Loss 2.9238\n",
      "  Iter 431/703: Loss 3.3638\n",
      "  Iter 441/703: Loss 3.1448\n",
      "  Iter 451/703: Loss 3.3154\n",
      "  Iter 461/703: Loss 3.0811\n",
      "  Iter 471/703: Loss 2.8869\n",
      "  Iter 481/703: Loss 3.0588\n",
      "  Iter 491/703: Loss 3.3800\n",
      "  Iter 501/703: Loss 3.2017\n",
      "  Iter 511/703: Loss 3.2294\n",
      "  Iter 521/703: Loss 3.4629\n",
      "  Iter 531/703: Loss 3.2293\n",
      "  Iter 541/703: Loss 3.1435\n",
      "  Iter 551/703: Loss 3.1491\n",
      "  Iter 561/703: Loss 3.3384\n",
      "  Iter 571/703: Loss 3.2481\n",
      "  Iter 581/703: Loss 3.3042\n",
      "  Iter 591/703: Loss 3.0124\n",
      "  Iter 601/703: Loss 2.9794\n",
      "  Iter 611/703: Loss 3.4322\n",
      "  Iter 621/703: Loss 3.2130\n",
      "  Iter 631/703: Loss 2.9984\n",
      "  Iter 641/703: Loss 2.8714\n",
      "  Iter 651/703: Loss 3.3864\n",
      "  Iter 661/703: Loss 3.3431\n",
      "  Iter 671/703: Loss 3.1179\n",
      "  Iter 681/703: Loss 3.1151\n",
      "  Iter 691/703: Loss 3.1381\n",
      "  Iter 701/703: Loss 3.2395\n",
      "  Iter 703/703: Loss 3.1009\n",
      "Fine Train Loss: 3.2168, Fine Train Acc: 0.2610, Val Acc: 0.2400, Val Loss: 3.1530\n",
      "Time: 2843.77s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch5.pkl\n",
      "\n",
      "[Epoch 6/100]\n",
      "  Iter   1/703: Loss 3.1224\n",
      "  Iter  11/703: Loss 3.2831\n",
      "  Iter  21/703: Loss 3.4131\n",
      "  Iter  31/703: Loss 2.9351\n",
      "  Iter  41/703: Loss 3.3718\n",
      "  Iter  51/703: Loss 3.2181\n",
      "  Iter  61/703: Loss 3.3324\n",
      "  Iter  71/703: Loss 3.3684\n",
      "  Iter  81/703: Loss 2.8025\n",
      "  Iter  91/703: Loss 3.2337\n",
      "  Iter 101/703: Loss 3.1826\n",
      "  Iter 111/703: Loss 3.1438\n",
      "  Iter 121/703: Loss 3.2272\n",
      "  Iter 131/703: Loss 3.5141\n",
      "  Iter 141/703: Loss 3.2848\n",
      "  Iter 151/703: Loss 3.4235\n",
      "  Iter 161/703: Loss 3.1107\n",
      "  Iter 171/703: Loss 2.9157\n",
      "  Iter 181/703: Loss 2.8748\n",
      "  Iter 191/703: Loss 2.9401\n",
      "  Iter 201/703: Loss 3.1445\n",
      "  Iter 211/703: Loss 3.1357\n",
      "  Iter 221/703: Loss 3.3048\n",
      "  Iter 231/703: Loss 2.7798\n",
      "  Iter 241/703: Loss 3.1722\n",
      "  Iter 251/703: Loss 3.2454\n",
      "  Iter 261/703: Loss 3.4218\n",
      "  Iter 271/703: Loss 3.2039\n",
      "  Iter 281/703: Loss 3.0272\n",
      "  Iter 291/703: Loss 3.2693\n",
      "  Iter 301/703: Loss 3.3544\n",
      "  Iter 311/703: Loss 2.9279\n",
      "  Iter 321/703: Loss 2.9669\n",
      "  Iter 331/703: Loss 2.7374\n",
      "  Iter 341/703: Loss 2.9619\n",
      "  Iter 351/703: Loss 3.0084\n",
      "  Iter 361/703: Loss 3.3036\n",
      "  Iter 371/703: Loss 3.1327\n",
      "  Iter 381/703: Loss 3.0863\n",
      "  Iter 391/703: Loss 3.2171\n",
      "  Iter 401/703: Loss 3.2828\n",
      "  Iter 411/703: Loss 3.1055\n",
      "  Iter 421/703: Loss 2.6923\n",
      "  Iter 431/703: Loss 2.8799\n",
      "  Iter 441/703: Loss 2.8310\n",
      "  Iter 451/703: Loss 2.8370\n",
      "  Iter 461/703: Loss 3.0524\n",
      "  Iter 471/703: Loss 3.3118\n",
      "  Iter 481/703: Loss 3.1794\n",
      "  Iter 491/703: Loss 2.7781\n",
      "  Iter 501/703: Loss 3.0247\n",
      "  Iter 511/703: Loss 2.7386\n",
      "  Iter 521/703: Loss 3.3417\n",
      "  Iter 531/703: Loss 2.9651\n",
      "  Iter 541/703: Loss 3.2664\n",
      "  Iter 551/703: Loss 3.2487\n",
      "  Iter 561/703: Loss 3.4148\n",
      "  Iter 571/703: Loss 2.9936\n",
      "  Iter 581/703: Loss 3.3531\n",
      "  Iter 591/703: Loss 3.2091\n",
      "  Iter 601/703: Loss 3.5874\n",
      "  Iter 611/703: Loss 3.6278\n",
      "  Iter 621/703: Loss 3.3477\n",
      "  Iter 631/703: Loss 3.2615\n",
      "  Iter 641/703: Loss 2.8666\n",
      "  Iter 651/703: Loss 2.9602\n",
      "  Iter 661/703: Loss 3.0535\n",
      "  Iter 671/703: Loss 3.1481\n",
      "  Iter 681/703: Loss 3.2871\n",
      "  Iter 691/703: Loss 3.2977\n",
      "  Iter 701/703: Loss 3.1486\n",
      "  Iter 703/703: Loss 3.0045\n",
      "Fine Train Loss: 3.1302, Fine Train Acc: 0.2700, Val Acc: 0.2536, Val Loss: 3.0422\n",
      "Time: 2834.90s\n",
      "\n",
      "[Epoch 7/100]\n",
      "  Iter   1/703: Loss 3.0739\n",
      "  Iter  11/703: Loss 3.2032\n",
      "  Iter  21/703: Loss 3.4696\n",
      "  Iter  31/703: Loss 3.2050\n",
      "  Iter  41/703: Loss 2.8769\n",
      "  Iter  51/703: Loss 3.0859\n",
      "  Iter  61/703: Loss 2.9208\n",
      "  Iter  71/703: Loss 2.9883\n",
      "  Iter  81/703: Loss 3.0465\n",
      "  Iter  91/703: Loss 2.9345\n",
      "  Iter 101/703: Loss 3.1607\n",
      "  Iter 111/703: Loss 3.1108\n",
      "  Iter 121/703: Loss 3.0602\n",
      "  Iter 131/703: Loss 3.0998\n",
      "  Iter 141/703: Loss 3.1172\n",
      "  Iter 151/703: Loss 2.6186\n",
      "  Iter 161/703: Loss 3.0498\n",
      "  Iter 171/703: Loss 2.9525\n",
      "  Iter 181/703: Loss 2.8063\n",
      "  Iter 191/703: Loss 3.1941\n",
      "  Iter 201/703: Loss 2.8342\n",
      "  Iter 211/703: Loss 3.3260\n",
      "  Iter 221/703: Loss 3.2194\n",
      "  Iter 231/703: Loss 2.7512\n",
      "  Iter 241/703: Loss 2.7031\n",
      "  Iter 251/703: Loss 2.9725\n",
      "  Iter 261/703: Loss 3.0688\n",
      "  Iter 271/703: Loss 2.9914\n",
      "  Iter 281/703: Loss 3.1932\n",
      "  Iter 291/703: Loss 3.1473\n",
      "  Iter 301/703: Loss 3.1174\n",
      "  Iter 311/703: Loss 3.3120\n",
      "  Iter 321/703: Loss 3.1647\n",
      "  Iter 331/703: Loss 2.8571\n",
      "  Iter 341/703: Loss 2.9931\n",
      "  Iter 351/703: Loss 2.8618\n",
      "  Iter 361/703: Loss 3.0283\n",
      "  Iter 371/703: Loss 3.0998\n",
      "  Iter 381/703: Loss 2.9939\n",
      "  Iter 391/703: Loss 3.1865\n",
      "  Iter 401/703: Loss 3.2592\n",
      "  Iter 411/703: Loss 3.1001\n",
      "  Iter 421/703: Loss 2.8921\n",
      "  Iter 431/703: Loss 3.1034\n",
      "  Iter 441/703: Loss 3.3134\n",
      "  Iter 451/703: Loss 3.4010\n",
      "  Iter 461/703: Loss 3.2755\n",
      "  Iter 471/703: Loss 3.4039\n",
      "  Iter 481/703: Loss 2.9684\n",
      "  Iter 491/703: Loss 2.9349\n",
      "  Iter 501/703: Loss 2.7002\n",
      "  Iter 511/703: Loss 2.7403\n",
      "  Iter 521/703: Loss 3.4854\n",
      "  Iter 531/703: Loss 3.1475\n",
      "  Iter 541/703: Loss 3.0277\n",
      "  Iter 551/703: Loss 3.3270\n",
      "  Iter 561/703: Loss 3.0444\n",
      "  Iter 571/703: Loss 3.3407\n",
      "  Iter 581/703: Loss 2.8271\n",
      "  Iter 591/703: Loss 3.0856\n",
      "  Iter 601/703: Loss 3.2861\n",
      "  Iter 611/703: Loss 3.2025\n",
      "  Iter 621/703: Loss 2.9016\n",
      "  Iter 631/703: Loss 2.9848\n",
      "  Iter 641/703: Loss 3.0503\n",
      "  Iter 651/703: Loss 3.0066\n",
      "  Iter 661/703: Loss 3.5798\n",
      "  Iter 671/703: Loss 2.8211\n",
      "  Iter 681/703: Loss 3.1428\n",
      "  Iter 691/703: Loss 3.0224\n",
      "  Iter 701/703: Loss 3.4966\n",
      "  Iter 703/703: Loss 3.1329\n",
      "Fine Train Loss: 3.0917, Fine Train Acc: 0.3020, Val Acc: 0.2540, Val Loss: 3.0419\n",
      "Time: 2848.30s\n",
      "\n",
      "[Epoch 8/100]\n",
      "  Iter   1/703: Loss 3.1027\n",
      "  Iter  11/703: Loss 2.7114\n",
      "  Iter  21/703: Loss 2.5351\n",
      "  Iter  31/703: Loss 3.2053\n",
      "  Iter  41/703: Loss 3.0583\n",
      "  Iter  51/703: Loss 2.7703\n",
      "  Iter  61/703: Loss 2.8996\n",
      "  Iter  71/703: Loss 2.9854\n",
      "  Iter  81/703: Loss 3.4022\n",
      "  Iter  91/703: Loss 3.4265\n",
      "  Iter 101/703: Loss 3.1698\n",
      "  Iter 111/703: Loss 3.2841\n",
      "  Iter 121/703: Loss 3.1952\n",
      "  Iter 131/703: Loss 3.0616\n",
      "  Iter 141/703: Loss 2.7635\n",
      "  Iter 151/703: Loss 2.8070\n",
      "  Iter 161/703: Loss 2.8252\n",
      "  Iter 171/703: Loss 3.1142\n",
      "  Iter 181/703: Loss 3.2783\n",
      "  Iter 191/703: Loss 3.3156\n",
      "  Iter 201/703: Loss 2.9749\n",
      "  Iter 211/703: Loss 3.3354\n",
      "  Iter 221/703: Loss 3.3328\n",
      "  Iter 231/703: Loss 2.9798\n",
      "  Iter 241/703: Loss 2.9264\n",
      "  Iter 251/703: Loss 3.0996\n",
      "  Iter 261/703: Loss 2.9666\n",
      "  Iter 271/703: Loss 2.6894\n",
      "  Iter 281/703: Loss 3.5661\n",
      "  Iter 291/703: Loss 2.9524\n",
      "  Iter 301/703: Loss 3.2049\n",
      "  Iter 311/703: Loss 2.9483\n",
      "  Iter 321/703: Loss 3.0218\n",
      "  Iter 331/703: Loss 2.9627\n",
      "  Iter 341/703: Loss 3.1077\n",
      "  Iter 351/703: Loss 3.1831\n",
      "  Iter 361/703: Loss 3.2497\n",
      "  Iter 371/703: Loss 2.7641\n",
      "  Iter 381/703: Loss 3.0388\n",
      "  Iter 391/703: Loss 3.4956\n",
      "  Iter 401/703: Loss 2.8249\n",
      "  Iter 411/703: Loss 3.0677\n",
      "  Iter 421/703: Loss 2.8127\n",
      "  Iter 431/703: Loss 2.8657\n",
      "  Iter 441/703: Loss 2.9816\n",
      "  Iter 451/703: Loss 2.9031\n",
      "  Iter 461/703: Loss 2.8716\n",
      "  Iter 471/703: Loss 2.8381\n",
      "  Iter 481/703: Loss 3.0126\n",
      "  Iter 491/703: Loss 3.0859\n",
      "  Iter 501/703: Loss 2.9459\n",
      "  Iter 511/703: Loss 3.0392\n",
      "  Iter 521/703: Loss 3.0535\n",
      "  Iter 531/703: Loss 2.9355\n",
      "  Iter 541/703: Loss 2.9169\n",
      "  Iter 551/703: Loss 3.2628\n",
      "  Iter 561/703: Loss 2.7814\n",
      "  Iter 571/703: Loss 3.0033\n",
      "  Iter 581/703: Loss 3.1204\n",
      "  Iter 591/703: Loss 3.1182\n",
      "  Iter 601/703: Loss 3.2044\n",
      "  Iter 611/703: Loss 2.9332\n",
      "  Iter 621/703: Loss 3.1191\n",
      "  Iter 631/703: Loss 2.5172\n",
      "  Iter 641/703: Loss 3.1365\n",
      "  Iter 651/703: Loss 3.0316\n",
      "  Iter 661/703: Loss 2.9618\n",
      "  Iter 671/703: Loss 3.1147\n",
      "  Iter 681/703: Loss 3.1444\n",
      "  Iter 691/703: Loss 2.9837\n",
      "  Iter 701/703: Loss 3.0642\n",
      "  Iter 703/703: Loss 3.2505\n",
      "Fine Train Loss: 3.0129, Fine Train Acc: 0.3070, Val Acc: 0.2692, Val Loss: 2.9256\n",
      "Time: 2838.63s\n",
      "\n",
      "[Epoch 9/100]\n",
      "  Iter   1/703: Loss 3.1445\n",
      "  Iter  11/703: Loss 2.8293\n",
      "  Iter  21/703: Loss 3.3403\n",
      "  Iter  31/703: Loss 2.9070\n",
      "  Iter  41/703: Loss 3.0801\n",
      "  Iter  51/703: Loss 2.6514\n",
      "  Iter  61/703: Loss 2.9001\n",
      "  Iter  71/703: Loss 3.0926\n",
      "  Iter  81/703: Loss 2.7711\n",
      "  Iter  91/703: Loss 3.0345\n",
      "  Iter 101/703: Loss 3.1178\n",
      "  Iter 111/703: Loss 2.9673\n",
      "  Iter 121/703: Loss 3.2210\n",
      "  Iter 131/703: Loss 3.0697\n",
      "  Iter 141/703: Loss 2.9696\n",
      "  Iter 151/703: Loss 2.9710\n",
      "  Iter 161/703: Loss 3.0122\n",
      "  Iter 171/703: Loss 3.2461\n",
      "  Iter 181/703: Loss 3.1465\n",
      "  Iter 191/703: Loss 2.9435\n",
      "  Iter 201/703: Loss 2.7895\n",
      "  Iter 211/703: Loss 2.8770\n",
      "  Iter 221/703: Loss 3.2444\n",
      "  Iter 231/703: Loss 3.1256\n",
      "  Iter 241/703: Loss 2.6649\n",
      "  Iter 251/703: Loss 2.7958\n",
      "  Iter 261/703: Loss 3.2576\n",
      "  Iter 271/703: Loss 2.7914\n",
      "  Iter 281/703: Loss 3.1562\n",
      "  Iter 291/703: Loss 2.8810\n",
      "  Iter 301/703: Loss 3.0170\n",
      "  Iter 311/703: Loss 3.4814\n",
      "  Iter 321/703: Loss 3.1439\n",
      "  Iter 331/703: Loss 3.4916\n",
      "  Iter 341/703: Loss 3.0083\n",
      "  Iter 351/703: Loss 2.7372\n",
      "  Iter 361/703: Loss 3.3844\n",
      "  Iter 371/703: Loss 2.8318\n",
      "  Iter 381/703: Loss 3.1477\n",
      "  Iter 391/703: Loss 2.9092\n",
      "  Iter 401/703: Loss 2.7697\n",
      "  Iter 411/703: Loss 3.0228\n",
      "  Iter 421/703: Loss 2.8860\n",
      "  Iter 431/703: Loss 3.1247\n",
      "  Iter 441/703: Loss 2.6542\n",
      "  Iter 451/703: Loss 2.5445\n",
      "  Iter 461/703: Loss 2.6976\n",
      "  Iter 471/703: Loss 2.8385\n",
      "  Iter 481/703: Loss 2.8962\n",
      "  Iter 491/703: Loss 2.9700\n",
      "  Iter 501/703: Loss 2.9079\n",
      "  Iter 511/703: Loss 2.9845\n",
      "  Iter 521/703: Loss 2.9436\n",
      "  Iter 531/703: Loss 2.7381\n",
      "  Iter 541/703: Loss 2.6005\n",
      "  Iter 551/703: Loss 2.6820\n",
      "  Iter 561/703: Loss 2.7525\n",
      "  Iter 571/703: Loss 2.7340\n",
      "  Iter 581/703: Loss 3.0002\n",
      "  Iter 591/703: Loss 2.7028\n",
      "  Iter 601/703: Loss 2.7041\n",
      "  Iter 611/703: Loss 2.9283\n",
      "  Iter 621/703: Loss 3.1579\n",
      "  Iter 631/703: Loss 3.0476\n",
      "  Iter 641/703: Loss 2.7189\n",
      "  Iter 651/703: Loss 3.2564\n",
      "  Iter 661/703: Loss 2.9818\n",
      "  Iter 671/703: Loss 2.9283\n",
      "  Iter 681/703: Loss 2.7340\n",
      "  Iter 691/703: Loss 2.6905\n",
      "  Iter 701/703: Loss 2.6985\n",
      "  Iter 703/703: Loss 3.2186\n",
      "Fine Train Loss: 2.9336, Fine Train Acc: 0.3190, Val Acc: 0.2886, Val Loss: 2.8561\n",
      "Time: 2853.09s\n",
      "\n",
      "[Epoch 10/100]\n",
      "  Iter   1/703: Loss 3.0937\n",
      "  Iter  11/703: Loss 2.5570\n",
      "  Iter  21/703: Loss 2.9839\n",
      "  Iter  31/703: Loss 2.9500\n",
      "  Iter  41/703: Loss 2.6740\n",
      "  Iter  51/703: Loss 2.8378\n",
      "  Iter  61/703: Loss 3.0952\n",
      "  Iter  71/703: Loss 3.1670\n",
      "  Iter  81/703: Loss 2.8680\n",
      "  Iter  91/703: Loss 3.1021\n",
      "  Iter 101/703: Loss 2.9960\n",
      "  Iter 111/703: Loss 3.0492\n",
      "  Iter 121/703: Loss 3.0611\n",
      "  Iter 131/703: Loss 2.7287\n",
      "  Iter 141/703: Loss 2.8318\n",
      "  Iter 151/703: Loss 2.7777\n",
      "  Iter 161/703: Loss 3.1284\n",
      "  Iter 171/703: Loss 2.8385\n",
      "  Iter 181/703: Loss 2.6222\n",
      "  Iter 191/703: Loss 3.0435\n",
      "  Iter 201/703: Loss 2.6124\n",
      "  Iter 211/703: Loss 2.9647\n",
      "  Iter 221/703: Loss 2.6808\n",
      "  Iter 231/703: Loss 2.5108\n",
      "  Iter 241/703: Loss 2.6847\n",
      "  Iter 251/703: Loss 3.0069\n",
      "  Iter 261/703: Loss 2.9084\n",
      "  Iter 271/703: Loss 2.8232\n",
      "  Iter 281/703: Loss 2.7286\n",
      "  Iter 291/703: Loss 2.8923\n",
      "  Iter 301/703: Loss 3.0116\n",
      "  Iter 311/703: Loss 2.8455\n",
      "  Iter 321/703: Loss 2.9717\n",
      "  Iter 331/703: Loss 2.5708\n",
      "  Iter 341/703: Loss 2.9512\n",
      "  Iter 351/703: Loss 3.0257\n",
      "  Iter 361/703: Loss 2.8696\n",
      "  Iter 371/703: Loss 3.3297\n",
      "  Iter 381/703: Loss 3.2798\n",
      "  Iter 391/703: Loss 2.7191\n",
      "  Iter 401/703: Loss 2.8136\n",
      "  Iter 411/703: Loss 2.8485\n",
      "  Iter 421/703: Loss 3.0034\n",
      "  Iter 431/703: Loss 2.9040\n",
      "  Iter 441/703: Loss 3.0464\n",
      "  Iter 451/703: Loss 3.0670\n",
      "  Iter 461/703: Loss 2.6897\n",
      "  Iter 471/703: Loss 3.0535\n",
      "  Iter 481/703: Loss 2.9755\n",
      "  Iter 491/703: Loss 2.6919\n",
      "  Iter 501/703: Loss 2.7089\n",
      "  Iter 511/703: Loss 2.7539\n",
      "  Iter 521/703: Loss 2.9195\n",
      "  Iter 531/703: Loss 3.3530\n",
      "  Iter 541/703: Loss 2.8492\n",
      "  Iter 551/703: Loss 3.0771\n",
      "  Iter 561/703: Loss 2.5716\n",
      "  Iter 571/703: Loss 3.0108\n",
      "  Iter 581/703: Loss 2.9402\n",
      "  Iter 591/703: Loss 2.6804\n",
      "  Iter 601/703: Loss 2.7571\n",
      "  Iter 611/703: Loss 2.9340\n",
      "  Iter 621/703: Loss 2.9326\n",
      "  Iter 631/703: Loss 2.9026\n",
      "  Iter 641/703: Loss 2.7691\n",
      "  Iter 651/703: Loss 2.8644\n",
      "  Iter 661/703: Loss 2.7544\n",
      "  Iter 671/703: Loss 3.1445\n",
      "  Iter 681/703: Loss 2.9332\n",
      "  Iter 691/703: Loss 2.9605\n",
      "  Iter 701/703: Loss 2.7750\n",
      "  Iter 703/703: Loss 3.0744\n",
      "Fine Train Loss: 2.8978, Fine Train Acc: 0.3220, Val Acc: 0.2896, Val Loss: 2.7943\n",
      "Time: 2845.26s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch10.pkl\n",
      "\n",
      "[Epoch 11/100]\n",
      "  Iter   1/703: Loss 2.8424\n",
      "  Iter  11/703: Loss 3.1956\n",
      "  Iter  21/703: Loss 2.6472\n",
      "  Iter  31/703: Loss 2.8036\n",
      "  Iter  41/703: Loss 2.7381\n",
      "  Iter  51/703: Loss 2.9034\n",
      "  Iter  61/703: Loss 2.8786\n",
      "  Iter  71/703: Loss 2.7949\n",
      "  Iter  81/703: Loss 2.6718\n",
      "  Iter  91/703: Loss 3.3223\n",
      "  Iter 101/703: Loss 3.0028\n",
      "  Iter 111/703: Loss 2.9985\n",
      "  Iter 121/703: Loss 2.9591\n",
      "  Iter 131/703: Loss 2.9102\n",
      "  Iter 141/703: Loss 2.5591\n",
      "  Iter 151/703: Loss 2.4324\n",
      "  Iter 161/703: Loss 2.9237\n",
      "  Iter 171/703: Loss 2.9264\n",
      "  Iter 181/703: Loss 2.7967\n",
      "  Iter 191/703: Loss 2.8638\n",
      "  Iter 201/703: Loss 2.8190\n",
      "  Iter 211/703: Loss 2.7120\n",
      "  Iter 221/703: Loss 2.9210\n",
      "  Iter 231/703: Loss 2.9436\n",
      "  Iter 241/703: Loss 2.5818\n",
      "  Iter 251/703: Loss 2.7249\n",
      "  Iter 261/703: Loss 2.7913\n",
      "  Iter 271/703: Loss 2.9450\n",
      "  Iter 281/703: Loss 2.8664\n",
      "  Iter 291/703: Loss 2.6290\n",
      "  Iter 301/703: Loss 2.8362\n",
      "  Iter 311/703: Loss 2.2249\n",
      "  Iter 321/703: Loss 2.9021\n",
      "  Iter 331/703: Loss 2.9650\n",
      "  Iter 341/703: Loss 2.8465\n",
      "  Iter 351/703: Loss 2.6058\n",
      "  Iter 361/703: Loss 3.0826\n",
      "  Iter 371/703: Loss 2.7289\n",
      "  Iter 381/703: Loss 2.8517\n",
      "  Iter 391/703: Loss 2.6349\n",
      "  Iter 401/703: Loss 2.7264\n",
      "  Iter 411/703: Loss 2.6973\n",
      "  Iter 421/703: Loss 2.7818\n",
      "  Iter 431/703: Loss 2.7983\n",
      "  Iter 441/703: Loss 2.6338\n",
      "  Iter 451/703: Loss 2.5466\n",
      "  Iter 461/703: Loss 2.9922\n",
      "  Iter 471/703: Loss 2.4640\n",
      "  Iter 481/703: Loss 3.0365\n",
      "  Iter 491/703: Loss 3.0630\n",
      "  Iter 501/703: Loss 3.0697\n",
      "  Iter 511/703: Loss 2.3115\n",
      "  Iter 521/703: Loss 2.8666\n",
      "  Iter 531/703: Loss 2.7057\n",
      "  Iter 541/703: Loss 3.0736\n",
      "  Iter 551/703: Loss 3.2497\n",
      "  Iter 561/703: Loss 2.4811\n",
      "  Iter 571/703: Loss 2.5542\n",
      "  Iter 581/703: Loss 2.7967\n",
      "  Iter 591/703: Loss 2.8156\n",
      "  Iter 601/703: Loss 2.8017\n",
      "  Iter 611/703: Loss 2.6077\n",
      "  Iter 621/703: Loss 2.7770\n",
      "  Iter 631/703: Loss 2.4905\n",
      "  Iter 641/703: Loss 2.4231\n",
      "  Iter 651/703: Loss 2.7130\n",
      "  Iter 661/703: Loss 2.7088\n",
      "  Iter 671/703: Loss 2.6304\n",
      "  Iter 681/703: Loss 2.8366\n",
      "  Iter 691/703: Loss 3.1629\n",
      "  Iter 701/703: Loss 2.5326\n",
      "  Iter 703/703: Loss 2.8390\n",
      "Fine Train Loss: 2.8305, Fine Train Acc: 0.3350, Val Acc: 0.3076, Val Loss: 2.7664\n",
      "Time: 2854.04s\n",
      "\n",
      "[Epoch 12/100]\n",
      "  Iter   1/703: Loss 3.1093\n",
      "  Iter  11/703: Loss 2.9275\n",
      "  Iter  21/703: Loss 2.6635\n",
      "  Iter  31/703: Loss 3.0407\n",
      "  Iter  41/703: Loss 2.7880\n",
      "  Iter  51/703: Loss 2.6295\n",
      "  Iter  61/703: Loss 2.8335\n",
      "  Iter  71/703: Loss 2.4375\n",
      "  Iter  81/703: Loss 2.5191\n",
      "  Iter  91/703: Loss 2.5906\n",
      "  Iter 101/703: Loss 2.9221\n",
      "  Iter 111/703: Loss 2.9416\n",
      "  Iter 121/703: Loss 2.7537\n",
      "  Iter 131/703: Loss 2.2637\n",
      "  Iter 141/703: Loss 2.7053\n",
      "  Iter 151/703: Loss 2.6263\n",
      "  Iter 161/703: Loss 3.0951\n",
      "  Iter 171/703: Loss 2.8781\n",
      "  Iter 181/703: Loss 2.7728\n",
      "  Iter 191/703: Loss 2.8861\n",
      "  Iter 201/703: Loss 2.7278\n",
      "  Iter 211/703: Loss 2.5646\n",
      "  Iter 221/703: Loss 3.0877\n",
      "  Iter 231/703: Loss 2.7033\n",
      "  Iter 241/703: Loss 3.0643\n",
      "  Iter 251/703: Loss 2.9970\n",
      "  Iter 261/703: Loss 2.8967\n",
      "  Iter 271/703: Loss 2.7588\n",
      "  Iter 281/703: Loss 2.6199\n",
      "  Iter 291/703: Loss 3.0142\n",
      "  Iter 301/703: Loss 2.7218\n",
      "  Iter 311/703: Loss 2.7721\n",
      "  Iter 321/703: Loss 2.7040\n",
      "  Iter 331/703: Loss 2.7477\n",
      "  Iter 341/703: Loss 2.7240\n",
      "  Iter 351/703: Loss 2.9077\n",
      "  Iter 361/703: Loss 2.7093\n",
      "  Iter 371/703: Loss 2.4056\n",
      "  Iter 381/703: Loss 3.2555\n",
      "  Iter 391/703: Loss 2.9334\n",
      "  Iter 401/703: Loss 2.9969\n",
      "  Iter 411/703: Loss 3.0660\n",
      "  Iter 421/703: Loss 2.7990\n",
      "  Iter 431/703: Loss 3.0284\n",
      "  Iter 441/703: Loss 2.9533\n",
      "  Iter 451/703: Loss 2.6614\n",
      "  Iter 461/703: Loss 2.5248\n",
      "  Iter 471/703: Loss 2.8273\n",
      "  Iter 481/703: Loss 2.7978\n",
      "  Iter 491/703: Loss 2.8566\n",
      "  Iter 501/703: Loss 2.7895\n",
      "  Iter 511/703: Loss 2.8486\n",
      "  Iter 521/703: Loss 2.6436\n",
      "  Iter 531/703: Loss 2.9016\n",
      "  Iter 541/703: Loss 2.8570\n",
      "  Iter 551/703: Loss 2.5946\n",
      "  Iter 561/703: Loss 2.6928\n",
      "  Iter 571/703: Loss 2.7862\n",
      "  Iter 581/703: Loss 2.4147\n",
      "  Iter 591/703: Loss 2.6572\n",
      "  Iter 601/703: Loss 3.0237\n",
      "  Iter 611/703: Loss 2.9137\n",
      "  Iter 621/703: Loss 2.4762\n",
      "  Iter 631/703: Loss 3.3043\n",
      "  Iter 641/703: Loss 2.7391\n",
      "  Iter 651/703: Loss 2.5585\n",
      "  Iter 661/703: Loss 2.7379\n",
      "  Iter 671/703: Loss 2.4312\n",
      "  Iter 681/703: Loss 2.8790\n",
      "  Iter 691/703: Loss 2.4974\n",
      "  Iter 701/703: Loss 2.8146\n",
      "  Iter 703/703: Loss 2.8166\n",
      "Fine Train Loss: 2.7735, Fine Train Acc: 0.3610, Val Acc: 0.3184, Val Loss: 2.7085\n",
      "Time: 2855.25s\n",
      "\n",
      "[Epoch 13/100]\n",
      "  Iter   1/703: Loss 2.7055\n",
      "  Iter  11/703: Loss 2.5514\n",
      "  Iter  21/703: Loss 3.0546\n",
      "  Iter  31/703: Loss 2.6848\n",
      "  Iter  41/703: Loss 2.6474\n",
      "  Iter  51/703: Loss 2.8297\n",
      "  Iter  61/703: Loss 2.6837\n",
      "  Iter  71/703: Loss 2.6027\n",
      "  Iter  81/703: Loss 2.5886\n",
      "  Iter  91/703: Loss 2.7125\n",
      "  Iter 101/703: Loss 2.6016\n",
      "  Iter 111/703: Loss 2.7961\n",
      "  Iter 121/703: Loss 2.7395\n",
      "  Iter 131/703: Loss 2.5643\n",
      "  Iter 141/703: Loss 2.7750\n",
      "  Iter 151/703: Loss 2.6209\n",
      "  Iter 161/703: Loss 2.5944\n",
      "  Iter 171/703: Loss 2.6989\n",
      "  Iter 181/703: Loss 2.6779\n",
      "  Iter 191/703: Loss 2.8364\n",
      "  Iter 201/703: Loss 2.9840\n",
      "  Iter 211/703: Loss 3.2210\n",
      "  Iter 221/703: Loss 2.8168\n",
      "  Iter 231/703: Loss 2.5466\n",
      "  Iter 241/703: Loss 2.6638\n",
      "  Iter 251/703: Loss 2.9723\n",
      "  Iter 261/703: Loss 2.5722\n",
      "  Iter 271/703: Loss 2.6779\n",
      "  Iter 281/703: Loss 2.7073\n",
      "  Iter 291/703: Loss 2.6099\n",
      "  Iter 301/703: Loss 2.3620\n",
      "  Iter 311/703: Loss 2.8680\n",
      "  Iter 321/703: Loss 3.0306\n",
      "  Iter 331/703: Loss 2.9512\n",
      "  Iter 341/703: Loss 2.8793\n",
      "  Iter 351/703: Loss 2.7400\n",
      "  Iter 361/703: Loss 3.0917\n",
      "  Iter 371/703: Loss 2.7471\n",
      "  Iter 381/703: Loss 2.5679\n",
      "  Iter 391/703: Loss 2.7377\n",
      "  Iter 401/703: Loss 2.5641\n",
      "  Iter 411/703: Loss 2.9129\n",
      "  Iter 421/703: Loss 3.0039\n",
      "  Iter 431/703: Loss 2.7152\n",
      "  Iter 441/703: Loss 2.7437\n",
      "  Iter 451/703: Loss 2.1255\n",
      "  Iter 461/703: Loss 2.6607\n",
      "  Iter 471/703: Loss 2.5966\n",
      "  Iter 481/703: Loss 3.0223\n",
      "  Iter 491/703: Loss 2.5479\n",
      "  Iter 501/703: Loss 2.7917\n",
      "  Iter 511/703: Loss 2.5184\n",
      "  Iter 521/703: Loss 2.5848\n",
      "  Iter 531/703: Loss 2.6132\n",
      "  Iter 541/703: Loss 2.6794\n",
      "  Iter 551/703: Loss 2.5708\n",
      "  Iter 561/703: Loss 2.6783\n",
      "  Iter 571/703: Loss 2.6437\n",
      "  Iter 581/703: Loss 2.9925\n",
      "  Iter 591/703: Loss 2.5491\n",
      "  Iter 601/703: Loss 2.5362\n",
      "  Iter 611/703: Loss 2.5897\n",
      "  Iter 621/703: Loss 2.3069\n",
      "  Iter 631/703: Loss 2.4689\n",
      "  Iter 641/703: Loss 2.5999\n",
      "  Iter 651/703: Loss 2.9932\n",
      "  Iter 661/703: Loss 2.7900\n",
      "  Iter 671/703: Loss 3.1337\n",
      "  Iter 681/703: Loss 2.5559\n",
      "  Iter 691/703: Loss 2.8096\n",
      "  Iter 701/703: Loss 2.4860\n",
      "  Iter 703/703: Loss 2.8942\n",
      "Fine Train Loss: 2.7491, Fine Train Acc: 0.3360, Val Acc: 0.3188, Val Loss: 2.6782\n",
      "Time: 2868.70s\n",
      "\n",
      "[Epoch 14/100]\n",
      "  Iter   1/703: Loss 2.9195\n",
      "  Iter  11/703: Loss 2.7069\n",
      "  Iter  21/703: Loss 2.7878\n",
      "  Iter  31/703: Loss 2.3252\n",
      "  Iter  41/703: Loss 2.3621\n",
      "  Iter  51/703: Loss 2.8980\n",
      "  Iter  61/703: Loss 2.2614\n",
      "  Iter  71/703: Loss 2.6679\n",
      "  Iter  81/703: Loss 2.8642\n",
      "  Iter  91/703: Loss 2.4686\n",
      "  Iter 101/703: Loss 2.6681\n",
      "  Iter 111/703: Loss 2.7831\n",
      "  Iter 121/703: Loss 2.9390\n",
      "  Iter 131/703: Loss 2.8077\n",
      "  Iter 141/703: Loss 2.9342\n",
      "  Iter 151/703: Loss 2.3681\n",
      "  Iter 161/703: Loss 2.6381\n",
      "  Iter 171/703: Loss 2.5639\n",
      "  Iter 181/703: Loss 2.4942\n",
      "  Iter 191/703: Loss 2.6216\n",
      "  Iter 201/703: Loss 2.6460\n",
      "  Iter 211/703: Loss 2.6243\n",
      "  Iter 221/703: Loss 2.8418\n",
      "  Iter 231/703: Loss 2.5956\n",
      "  Iter 241/703: Loss 2.4640\n",
      "  Iter 251/703: Loss 3.1127\n",
      "  Iter 261/703: Loss 2.5684\n",
      "  Iter 271/703: Loss 2.6952\n",
      "  Iter 281/703: Loss 2.4267\n",
      "  Iter 291/703: Loss 2.4671\n",
      "  Iter 301/703: Loss 2.4975\n",
      "  Iter 311/703: Loss 2.4715\n",
      "  Iter 321/703: Loss 2.9728\n",
      "  Iter 331/703: Loss 3.1211\n",
      "  Iter 341/703: Loss 2.8578\n",
      "  Iter 351/703: Loss 2.8774\n",
      "  Iter 361/703: Loss 2.4973\n",
      "  Iter 371/703: Loss 2.7594\n",
      "  Iter 381/703: Loss 2.5258\n",
      "  Iter 391/703: Loss 2.9947\n",
      "  Iter 401/703: Loss 2.5849\n",
      "  Iter 411/703: Loss 3.0527\n",
      "  Iter 421/703: Loss 2.6394\n",
      "  Iter 431/703: Loss 2.3738\n",
      "  Iter 441/703: Loss 2.6024\n",
      "  Iter 451/703: Loss 2.1717\n",
      "  Iter 461/703: Loss 2.3823\n",
      "  Iter 471/703: Loss 2.9447\n",
      "  Iter 481/703: Loss 2.8293\n",
      "  Iter 491/703: Loss 2.6300\n",
      "  Iter 501/703: Loss 2.9911\n",
      "  Iter 511/703: Loss 2.5863\n",
      "  Iter 521/703: Loss 2.6051\n",
      "  Iter 531/703: Loss 2.7494\n",
      "  Iter 541/703: Loss 2.8122\n",
      "  Iter 551/703: Loss 2.1269\n",
      "  Iter 561/703: Loss 2.5867\n",
      "  Iter 571/703: Loss 3.0693\n",
      "  Iter 581/703: Loss 2.8537\n",
      "  Iter 591/703: Loss 2.8841\n",
      "  Iter 601/703: Loss 2.6043\n",
      "  Iter 611/703: Loss 2.6475\n",
      "  Iter 621/703: Loss 2.8194\n",
      "  Iter 631/703: Loss 2.5800\n",
      "  Iter 641/703: Loss 2.4417\n",
      "  Iter 651/703: Loss 3.0994\n",
      "  Iter 661/703: Loss 3.0162\n",
      "  Iter 671/703: Loss 2.5605\n",
      "  Iter 681/703: Loss 2.4704\n",
      "  Iter 691/703: Loss 2.5514\n",
      "  Iter 701/703: Loss 3.4215\n",
      "  Iter 703/703: Loss 2.6921\n",
      "Fine Train Loss: 2.7110, Fine Train Acc: 0.3630, Val Acc: 0.3292, Val Loss: 2.6647\n",
      "Time: 2881.93s\n",
      "\n",
      "[Epoch 15/100]\n",
      "  Iter   1/703: Loss 2.4151\n",
      "  Iter  11/703: Loss 2.4446\n",
      "  Iter  21/703: Loss 2.6684\n",
      "  Iter  31/703: Loss 2.6702\n",
      "  Iter  41/703: Loss 2.7211\n",
      "  Iter  51/703: Loss 2.4491\n",
      "  Iter  61/703: Loss 2.8668\n",
      "  Iter  71/703: Loss 2.7805\n",
      "  Iter  81/703: Loss 2.8082\n",
      "  Iter  91/703: Loss 2.8173\n",
      "  Iter 101/703: Loss 2.6939\n",
      "  Iter 111/703: Loss 2.6785\n",
      "  Iter 121/703: Loss 2.7064\n",
      "  Iter 131/703: Loss 2.6837\n",
      "  Iter 141/703: Loss 2.4657\n",
      "  Iter 151/703: Loss 2.8141\n",
      "  Iter 161/703: Loss 2.4635\n",
      "  Iter 171/703: Loss 2.3609\n",
      "  Iter 181/703: Loss 2.7507\n",
      "  Iter 191/703: Loss 2.7202\n",
      "  Iter 201/703: Loss 2.8112\n",
      "  Iter 211/703: Loss 2.7876\n",
      "  Iter 221/703: Loss 2.5255\n",
      "  Iter 231/703: Loss 2.6127\n",
      "  Iter 241/703: Loss 2.2707\n",
      "  Iter 251/703: Loss 2.6194\n",
      "  Iter 261/703: Loss 2.5774\n",
      "  Iter 271/703: Loss 2.8776\n",
      "  Iter 281/703: Loss 2.6788\n",
      "  Iter 291/703: Loss 2.7672\n",
      "  Iter 301/703: Loss 2.3793\n",
      "  Iter 311/703: Loss 2.4843\n",
      "  Iter 321/703: Loss 2.6754\n",
      "  Iter 331/703: Loss 2.8226\n",
      "  Iter 341/703: Loss 3.0149\n",
      "  Iter 351/703: Loss 2.9443\n",
      "  Iter 361/703: Loss 2.5176\n",
      "  Iter 371/703: Loss 2.6895\n",
      "  Iter 381/703: Loss 2.8582\n",
      "  Iter 391/703: Loss 2.8493\n",
      "  Iter 401/703: Loss 2.5325\n",
      "  Iter 411/703: Loss 2.5543\n",
      "  Iter 421/703: Loss 2.2239\n",
      "  Iter 431/703: Loss 2.8917\n",
      "  Iter 441/703: Loss 2.5553\n",
      "  Iter 451/703: Loss 2.4914\n",
      "  Iter 461/703: Loss 2.8988\n",
      "  Iter 471/703: Loss 2.5981\n",
      "  Iter 481/703: Loss 2.5614\n",
      "  Iter 491/703: Loss 2.6378\n",
      "  Iter 501/703: Loss 2.7322\n",
      "  Iter 511/703: Loss 2.8237\n",
      "  Iter 521/703: Loss 2.4564\n",
      "  Iter 531/703: Loss 2.7833\n",
      "  Iter 541/703: Loss 2.3376\n",
      "  Iter 551/703: Loss 2.5368\n",
      "  Iter 561/703: Loss 2.6449\n",
      "  Iter 571/703: Loss 2.6136\n",
      "  Iter 581/703: Loss 2.6971\n",
      "  Iter 591/703: Loss 2.5427\n",
      "  Iter 601/703: Loss 2.5270\n",
      "  Iter 611/703: Loss 2.6545\n",
      "  Iter 621/703: Loss 2.4660\n",
      "  Iter 631/703: Loss 2.4994\n",
      "  Iter 641/703: Loss 2.5829\n",
      "  Iter 651/703: Loss 2.6076\n",
      "  Iter 661/703: Loss 2.5353\n",
      "  Iter 671/703: Loss 2.7954\n",
      "  Iter 681/703: Loss 2.6355\n",
      "  Iter 691/703: Loss 2.3747\n",
      "  Iter 701/703: Loss 2.4318\n",
      "  Iter 703/703: Loss 2.4262\n",
      "Fine Train Loss: 2.6667, Fine Train Acc: 0.3540, Val Acc: 0.3238, Val Loss: 2.6549\n",
      "Time: 2895.39s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch15.pkl\n",
      "\n",
      "[Epoch 16/100]\n",
      "  Iter   1/703: Loss 2.7734\n",
      "  Iter  11/703: Loss 2.3360\n",
      "  Iter  21/703: Loss 2.5331\n",
      "  Iter  31/703: Loss 2.3965\n",
      "  Iter  41/703: Loss 2.6288\n",
      "  Iter  51/703: Loss 2.6481\n",
      "  Iter  61/703: Loss 3.0536\n",
      "  Iter  71/703: Loss 2.2357\n",
      "  Iter  81/703: Loss 2.7524\n",
      "  Iter  91/703: Loss 2.6800\n",
      "  Iter 101/703: Loss 2.8816\n",
      "  Iter 111/703: Loss 2.4855\n",
      "  Iter 121/703: Loss 2.7870\n",
      "  Iter 131/703: Loss 2.8122\n",
      "  Iter 141/703: Loss 2.4328\n",
      "  Iter 151/703: Loss 2.4565\n",
      "  Iter 161/703: Loss 2.4196\n",
      "  Iter 171/703: Loss 3.1552\n",
      "  Iter 181/703: Loss 2.7077\n",
      "  Iter 191/703: Loss 2.8268\n",
      "  Iter 201/703: Loss 2.7556\n",
      "  Iter 211/703: Loss 2.6265\n",
      "  Iter 221/703: Loss 2.6483\n",
      "  Iter 231/703: Loss 2.4121\n",
      "  Iter 241/703: Loss 2.7961\n",
      "  Iter 251/703: Loss 2.5731\n",
      "  Iter 261/703: Loss 2.6025\n",
      "  Iter 271/703: Loss 2.4116\n",
      "  Iter 281/703: Loss 2.6405\n",
      "  Iter 291/703: Loss 2.7470\n",
      "  Iter 301/703: Loss 2.6962\n",
      "  Iter 311/703: Loss 2.5568\n",
      "  Iter 321/703: Loss 2.1647\n",
      "  Iter 331/703: Loss 2.5730\n",
      "  Iter 341/703: Loss 2.4323\n",
      "  Iter 351/703: Loss 2.6809\n",
      "  Iter 361/703: Loss 2.6075\n",
      "  Iter 371/703: Loss 2.4946\n",
      "  Iter 381/703: Loss 2.3762\n",
      "  Iter 391/703: Loss 2.5053\n",
      "  Iter 401/703: Loss 2.5667\n",
      "  Iter 411/703: Loss 2.7686\n",
      "  Iter 421/703: Loss 2.8239\n",
      "  Iter 431/703: Loss 3.0487\n",
      "  Iter 441/703: Loss 2.3928\n",
      "  Iter 451/703: Loss 2.5613\n",
      "  Iter 461/703: Loss 2.7543\n",
      "  Iter 471/703: Loss 2.3625\n",
      "  Iter 481/703: Loss 2.3530\n",
      "  Iter 491/703: Loss 2.6641\n",
      "  Iter 501/703: Loss 2.3871\n",
      "  Iter 511/703: Loss 2.7862\n",
      "  Iter 521/703: Loss 2.9648\n",
      "  Iter 531/703: Loss 2.8664\n",
      "  Iter 541/703: Loss 2.5483\n",
      "  Iter 551/703: Loss 2.8330\n",
      "  Iter 561/703: Loss 2.5692\n",
      "  Iter 571/703: Loss 2.7899\n",
      "  Iter 581/703: Loss 2.8461\n",
      "  Iter 591/703: Loss 2.6431\n",
      "  Iter 601/703: Loss 2.8888\n",
      "  Iter 611/703: Loss 2.4079\n",
      "  Iter 621/703: Loss 2.9462\n",
      "  Iter 631/703: Loss 2.7611\n",
      "  Iter 641/703: Loss 2.3838\n",
      "  Iter 651/703: Loss 2.6027\n",
      "  Iter 661/703: Loss 2.8177\n",
      "  Iter 671/703: Loss 2.4540\n",
      "  Iter 681/703: Loss 2.7431\n",
      "  Iter 691/703: Loss 2.6206\n",
      "  Iter 701/703: Loss 2.6089\n",
      "  Iter 703/703: Loss 2.3301\n",
      "Fine Train Loss: 2.6444, Fine Train Acc: 0.3860, Val Acc: 0.3352, Val Loss: 2.5781\n",
      "Time: 2906.66s\n",
      "\n",
      "[Epoch 17/100]\n",
      "  Iter   1/703: Loss 2.1959\n",
      "  Iter  11/703: Loss 2.5953\n",
      "  Iter  21/703: Loss 2.3204\n",
      "  Iter  31/703: Loss 2.3819\n",
      "  Iter  41/703: Loss 2.5587\n",
      "  Iter  51/703: Loss 2.8256\n",
      "  Iter  61/703: Loss 2.7681\n",
      "  Iter  71/703: Loss 2.6693\n",
      "  Iter  81/703: Loss 2.6802\n",
      "  Iter  91/703: Loss 2.5005\n",
      "  Iter 101/703: Loss 2.4376\n",
      "  Iter 111/703: Loss 2.7014\n",
      "  Iter 121/703: Loss 2.6081\n",
      "  Iter 131/703: Loss 2.7573\n",
      "  Iter 141/703: Loss 2.3764\n",
      "  Iter 151/703: Loss 2.5000\n",
      "  Iter 161/703: Loss 2.7788\n",
      "  Iter 171/703: Loss 2.1795\n",
      "  Iter 181/703: Loss 2.9570\n",
      "  Iter 191/703: Loss 2.5121\n",
      "  Iter 201/703: Loss 2.5938\n",
      "  Iter 211/703: Loss 2.6334\n",
      "  Iter 221/703: Loss 2.6370\n",
      "  Iter 231/703: Loss 2.5489\n",
      "  Iter 241/703: Loss 2.3227\n",
      "  Iter 251/703: Loss 2.3371\n",
      "  Iter 261/703: Loss 2.5388\n",
      "  Iter 271/703: Loss 2.4959\n",
      "  Iter 281/703: Loss 2.4399\n",
      "  Iter 291/703: Loss 2.6495\n",
      "  Iter 301/703: Loss 2.6130\n",
      "  Iter 311/703: Loss 2.7024\n",
      "  Iter 321/703: Loss 2.6198\n",
      "  Iter 331/703: Loss 2.6202\n",
      "  Iter 341/703: Loss 2.5716\n",
      "  Iter 351/703: Loss 2.4667\n",
      "  Iter 361/703: Loss 2.6557\n",
      "  Iter 371/703: Loss 2.7206\n",
      "  Iter 381/703: Loss 3.1411\n",
      "  Iter 391/703: Loss 2.5688\n",
      "  Iter 401/703: Loss 2.7986\n",
      "  Iter 411/703: Loss 2.9406\n",
      "  Iter 421/703: Loss 2.6619\n",
      "  Iter 431/703: Loss 2.6436\n",
      "  Iter 441/703: Loss 2.8927\n",
      "  Iter 451/703: Loss 2.0092\n",
      "  Iter 461/703: Loss 2.6240\n",
      "  Iter 471/703: Loss 2.5849\n",
      "  Iter 481/703: Loss 2.4586\n",
      "  Iter 491/703: Loss 2.8101\n",
      "  Iter 501/703: Loss 2.6699\n",
      "  Iter 511/703: Loss 2.4295\n",
      "  Iter 521/703: Loss 2.3509\n",
      "  Iter 531/703: Loss 2.4976\n",
      "  Iter 541/703: Loss 2.5590\n",
      "  Iter 551/703: Loss 2.5114\n",
      "  Iter 561/703: Loss 2.4483\n",
      "  Iter 571/703: Loss 2.8331\n",
      "  Iter 581/703: Loss 2.8191\n",
      "  Iter 591/703: Loss 2.6383\n",
      "  Iter 601/703: Loss 2.4810\n",
      "  Iter 611/703: Loss 2.6941\n",
      "  Iter 621/703: Loss 2.6581\n",
      "  Iter 631/703: Loss 2.3308\n",
      "  Iter 641/703: Loss 2.8296\n",
      "  Iter 651/703: Loss 2.2725\n",
      "  Iter 661/703: Loss 2.6379\n",
      "  Iter 671/703: Loss 2.8531\n",
      "  Iter 681/703: Loss 2.4856\n",
      "  Iter 691/703: Loss 2.5831\n",
      "  Iter 701/703: Loss 2.4597\n",
      "  Iter 703/703: Loss 2.4872\n",
      "Fine Train Loss: 2.5958, Fine Train Acc: 0.3820, Val Acc: 0.3294, Val Loss: 2.6065\n",
      "Time: 2920.81s\n",
      "\n",
      "[Epoch 18/100]\n",
      "  Iter   1/703: Loss 2.4345\n",
      "  Iter  11/703: Loss 2.5161\n",
      "  Iter  21/703: Loss 2.6490\n",
      "  Iter  31/703: Loss 3.0010\n",
      "  Iter  41/703: Loss 2.8691\n",
      "  Iter  51/703: Loss 2.6656\n",
      "  Iter  61/703: Loss 2.5363\n",
      "  Iter  71/703: Loss 2.4158\n",
      "  Iter  81/703: Loss 2.2634\n",
      "  Iter  91/703: Loss 2.2883\n",
      "  Iter 101/703: Loss 2.7103\n",
      "  Iter 111/703: Loss 2.6427\n",
      "  Iter 121/703: Loss 2.6239\n",
      "  Iter 131/703: Loss 2.7475\n",
      "  Iter 141/703: Loss 2.7726\n",
      "  Iter 151/703: Loss 2.6139\n",
      "  Iter 161/703: Loss 2.5270\n",
      "  Iter 171/703: Loss 2.6745\n",
      "  Iter 181/703: Loss 2.5182\n",
      "  Iter 191/703: Loss 2.7652\n",
      "  Iter 201/703: Loss 2.9285\n",
      "  Iter 211/703: Loss 2.6724\n",
      "  Iter 221/703: Loss 2.4779\n",
      "  Iter 231/703: Loss 2.6196\n",
      "  Iter 241/703: Loss 2.4612\n",
      "  Iter 251/703: Loss 2.8871\n",
      "  Iter 261/703: Loss 2.5379\n",
      "  Iter 271/703: Loss 3.0054\n",
      "  Iter 281/703: Loss 2.2037\n",
      "  Iter 291/703: Loss 2.3748\n",
      "  Iter 301/703: Loss 2.4765\n",
      "  Iter 311/703: Loss 2.3235\n",
      "  Iter 321/703: Loss 2.3091\n",
      "  Iter 331/703: Loss 2.4580\n",
      "  Iter 341/703: Loss 2.6656\n",
      "  Iter 351/703: Loss 2.6369\n",
      "  Iter 361/703: Loss 2.8448\n",
      "  Iter 371/703: Loss 2.1969\n",
      "  Iter 381/703: Loss 2.6289\n",
      "  Iter 391/703: Loss 2.4540\n",
      "  Iter 401/703: Loss 2.8350\n",
      "  Iter 411/703: Loss 2.4221\n",
      "  Iter 421/703: Loss 2.7536\n",
      "  Iter 431/703: Loss 2.4665\n",
      "  Iter 441/703: Loss 2.7820\n",
      "  Iter 451/703: Loss 2.6300\n",
      "  Iter 461/703: Loss 2.4705\n",
      "  Iter 471/703: Loss 2.4499\n",
      "  Iter 481/703: Loss 2.3343\n",
      "  Iter 491/703: Loss 2.3559\n",
      "  Iter 501/703: Loss 2.3423\n",
      "  Iter 511/703: Loss 2.7233\n",
      "  Iter 521/703: Loss 2.5366\n",
      "  Iter 531/703: Loss 2.8305\n",
      "  Iter 541/703: Loss 2.5079\n",
      "  Iter 551/703: Loss 2.8687\n",
      "  Iter 561/703: Loss 2.8743\n",
      "  Iter 571/703: Loss 2.5313\n",
      "  Iter 581/703: Loss 2.6536\n",
      "  Iter 591/703: Loss 2.7371\n",
      "  Iter 601/703: Loss 2.4420\n",
      "  Iter 611/703: Loss 2.4494\n",
      "  Iter 621/703: Loss 3.1883\n",
      "  Iter 631/703: Loss 2.6507\n",
      "  Iter 641/703: Loss 2.7009\n",
      "  Iter 651/703: Loss 2.5941\n",
      "  Iter 661/703: Loss 2.6727\n",
      "  Iter 671/703: Loss 2.8276\n",
      "  Iter 681/703: Loss 2.3973\n",
      "  Iter 691/703: Loss 2.9873\n",
      "  Iter 701/703: Loss 2.6340\n",
      "  Iter 703/703: Loss 2.6020\n",
      "Fine Train Loss: 2.5779, Fine Train Acc: 0.3960, Val Acc: 0.3550, Val Loss: 2.5463\n",
      "Time: 2928.59s\n",
      "\n",
      "[Epoch 19/100]\n",
      "  Iter   1/703: Loss 2.4786\n",
      "  Iter  11/703: Loss 3.0038\n",
      "  Iter  21/703: Loss 2.5098\n",
      "  Iter  31/703: Loss 2.3789\n",
      "  Iter  41/703: Loss 2.5959\n",
      "  Iter  51/703: Loss 2.2576\n",
      "  Iter  61/703: Loss 2.5030\n",
      "  Iter  71/703: Loss 2.4400\n",
      "  Iter  81/703: Loss 2.5018\n",
      "  Iter  91/703: Loss 2.1916\n",
      "  Iter 101/703: Loss 2.5274\n",
      "  Iter 111/703: Loss 2.7046\n",
      "  Iter 121/703: Loss 2.7396\n",
      "  Iter 131/703: Loss 2.7016\n",
      "  Iter 141/703: Loss 2.4406\n",
      "  Iter 151/703: Loss 2.7034\n",
      "  Iter 161/703: Loss 2.6467\n",
      "  Iter 171/703: Loss 2.4215\n",
      "  Iter 181/703: Loss 2.4455\n",
      "  Iter 191/703: Loss 2.1952\n",
      "  Iter 201/703: Loss 2.9608\n",
      "  Iter 211/703: Loss 2.5814\n",
      "  Iter 221/703: Loss 2.7606\n",
      "  Iter 231/703: Loss 2.8792\n",
      "  Iter 241/703: Loss 2.3977\n",
      "  Iter 251/703: Loss 2.6939\n",
      "  Iter 261/703: Loss 2.4861\n",
      "  Iter 271/703: Loss 2.3504\n",
      "  Iter 281/703: Loss 2.6553\n",
      "  Iter 291/703: Loss 2.3407\n",
      "  Iter 301/703: Loss 2.5033\n",
      "  Iter 311/703: Loss 2.2756\n",
      "  Iter 321/703: Loss 2.7337\n",
      "  Iter 331/703: Loss 2.4445\n",
      "  Iter 341/703: Loss 2.3522\n",
      "  Iter 351/703: Loss 2.5206\n",
      "  Iter 361/703: Loss 2.3414\n",
      "  Iter 371/703: Loss 2.6741\n",
      "  Iter 381/703: Loss 2.4347\n",
      "  Iter 391/703: Loss 2.4297\n",
      "  Iter 401/703: Loss 2.6304\n",
      "  Iter 411/703: Loss 2.4274\n",
      "  Iter 421/703: Loss 2.2075\n",
      "  Iter 431/703: Loss 2.6818\n",
      "  Iter 441/703: Loss 2.5239\n",
      "  Iter 451/703: Loss 2.1450\n",
      "  Iter 461/703: Loss 2.8663\n",
      "  Iter 471/703: Loss 2.5441\n",
      "  Iter 481/703: Loss 2.3040\n",
      "  Iter 491/703: Loss 2.6113\n",
      "  Iter 501/703: Loss 2.7330\n",
      "  Iter 511/703: Loss 2.3585\n",
      "  Iter 521/703: Loss 2.5603\n",
      "  Iter 531/703: Loss 2.6924\n",
      "  Iter 541/703: Loss 2.5448\n",
      "  Iter 551/703: Loss 2.7941\n",
      "  Iter 561/703: Loss 2.0290\n",
      "  Iter 571/703: Loss 2.6106\n",
      "  Iter 581/703: Loss 2.7791\n",
      "  Iter 591/703: Loss 2.7235\n",
      "  Iter 601/703: Loss 2.1989\n",
      "  Iter 611/703: Loss 2.5133\n",
      "  Iter 621/703: Loss 2.5065\n",
      "  Iter 631/703: Loss 2.2426\n",
      "  Iter 641/703: Loss 2.4781\n",
      "  Iter 651/703: Loss 2.5276\n",
      "  Iter 661/703: Loss 2.5425\n",
      "  Iter 671/703: Loss 2.6143\n",
      "  Iter 681/703: Loss 2.9303\n",
      "  Iter 691/703: Loss 2.7193\n",
      "  Iter 701/703: Loss 2.7640\n",
      "  Iter 703/703: Loss 2.4690\n",
      "Fine Train Loss: 2.5440, Fine Train Acc: 0.4050, Val Acc: 0.3554, Val Loss: 2.5133\n",
      "Time: 2941.74s\n",
      "\n",
      "[Epoch 20/100]\n",
      "  Iter   1/703: Loss 2.5152\n",
      "  Iter  11/703: Loss 2.6901\n",
      "  Iter  21/703: Loss 2.2996\n",
      "  Iter  31/703: Loss 2.6912\n",
      "  Iter  41/703: Loss 2.7989\n",
      "  Iter  51/703: Loss 2.3809\n",
      "  Iter  61/703: Loss 3.1016\n",
      "  Iter  71/703: Loss 2.8146\n",
      "  Iter  81/703: Loss 2.0329\n",
      "  Iter  91/703: Loss 2.7467\n",
      "  Iter 101/703: Loss 2.4704\n",
      "  Iter 111/703: Loss 2.6045\n",
      "  Iter 121/703: Loss 2.2627\n",
      "  Iter 131/703: Loss 2.2572\n",
      "  Iter 141/703: Loss 2.2126\n",
      "  Iter 151/703: Loss 2.6964\n",
      "  Iter 161/703: Loss 2.4215\n",
      "  Iter 171/703: Loss 2.4999\n",
      "  Iter 181/703: Loss 2.4531\n",
      "  Iter 191/703: Loss 2.5887\n",
      "  Iter 201/703: Loss 2.3122\n",
      "  Iter 211/703: Loss 2.6666\n",
      "  Iter 221/703: Loss 2.6328\n",
      "  Iter 231/703: Loss 2.8562\n",
      "  Iter 241/703: Loss 2.5507\n",
      "  Iter 251/703: Loss 2.4346\n",
      "  Iter 261/703: Loss 2.3008\n",
      "  Iter 271/703: Loss 2.7362\n",
      "  Iter 281/703: Loss 2.7270\n",
      "  Iter 291/703: Loss 2.7324\n",
      "  Iter 301/703: Loss 2.4812\n",
      "  Iter 311/703: Loss 2.7804\n",
      "  Iter 321/703: Loss 2.2819\n",
      "  Iter 331/703: Loss 1.9742\n",
      "  Iter 341/703: Loss 2.3766\n",
      "  Iter 351/703: Loss 2.6368\n",
      "  Iter 361/703: Loss 2.3750\n",
      "  Iter 371/703: Loss 2.4657\n",
      "  Iter 381/703: Loss 2.6291\n",
      "  Iter 391/703: Loss 2.7303\n",
      "  Iter 401/703: Loss 2.2490\n",
      "  Iter 411/703: Loss 2.8830\n",
      "  Iter 421/703: Loss 2.2900\n",
      "  Iter 431/703: Loss 2.4828\n",
      "  Iter 441/703: Loss 2.3324\n",
      "  Iter 451/703: Loss 2.2985\n",
      "  Iter 461/703: Loss 2.5347\n",
      "  Iter 471/703: Loss 2.3114\n",
      "  Iter 481/703: Loss 2.5465\n",
      "  Iter 491/703: Loss 2.5600\n",
      "  Iter 501/703: Loss 2.3168\n",
      "  Iter 511/703: Loss 2.3665\n",
      "  Iter 521/703: Loss 2.6907\n",
      "  Iter 531/703: Loss 2.5124\n",
      "  Iter 541/703: Loss 2.5065\n",
      "  Iter 551/703: Loss 2.5596\n",
      "  Iter 561/703: Loss 2.5518\n",
      "  Iter 571/703: Loss 2.4735\n",
      "  Iter 581/703: Loss 2.4961\n",
      "  Iter 591/703: Loss 2.4443\n",
      "  Iter 601/703: Loss 2.3927\n",
      "  Iter 611/703: Loss 2.5247\n",
      "  Iter 621/703: Loss 2.3244\n",
      "  Iter 631/703: Loss 2.2468\n",
      "  Iter 641/703: Loss 2.6326\n",
      "  Iter 651/703: Loss 2.3856\n",
      "  Iter 661/703: Loss 2.3182\n",
      "  Iter 671/703: Loss 2.3862\n",
      "  Iter 681/703: Loss 2.7257\n",
      "  Iter 691/703: Loss 2.2843\n",
      "  Iter 701/703: Loss 2.6182\n",
      "  Iter 703/703: Loss 2.9307\n",
      "Fine Train Loss: 2.5058, Fine Train Acc: 0.4030, Val Acc: 0.3562, Val Loss: 2.5045\n",
      "Time: 2951.99s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch20.pkl\n",
      "\n",
      "[Epoch 21/100]\n",
      "  Iter   1/703: Loss 2.6043\n",
      "  Iter  11/703: Loss 2.6303\n",
      "  Iter  21/703: Loss 2.7435\n",
      "  Iter  31/703: Loss 2.6125\n",
      "  Iter  41/703: Loss 2.6580\n",
      "  Iter  51/703: Loss 2.6125\n",
      "  Iter  61/703: Loss 2.4307\n",
      "  Iter  71/703: Loss 2.4977\n",
      "  Iter  81/703: Loss 2.5887\n",
      "  Iter  91/703: Loss 2.3368\n",
      "  Iter 101/703: Loss 2.6786\n",
      "  Iter 111/703: Loss 2.4310\n",
      "  Iter 121/703: Loss 2.7848\n",
      "  Iter 131/703: Loss 2.8809\n",
      "  Iter 141/703: Loss 2.6079\n",
      "  Iter 151/703: Loss 2.5698\n",
      "  Iter 161/703: Loss 2.4979\n",
      "  Iter 171/703: Loss 2.4679\n",
      "  Iter 181/703: Loss 2.4615\n",
      "  Iter 191/703: Loss 2.7945\n",
      "  Iter 201/703: Loss 2.6462\n",
      "  Iter 211/703: Loss 2.1132\n",
      "  Iter 221/703: Loss 2.2256\n",
      "  Iter 231/703: Loss 2.1634\n",
      "  Iter 241/703: Loss 2.8673\n",
      "  Iter 251/703: Loss 2.5602\n",
      "  Iter 261/703: Loss 2.2967\n",
      "  Iter 271/703: Loss 2.3969\n",
      "  Iter 281/703: Loss 2.7093\n",
      "  Iter 291/703: Loss 2.5396\n",
      "  Iter 301/703: Loss 2.3541\n",
      "  Iter 311/703: Loss 2.5873\n",
      "  Iter 321/703: Loss 2.4643\n",
      "  Iter 331/703: Loss 2.5263\n",
      "  Iter 341/703: Loss 1.9961\n",
      "  Iter 351/703: Loss 2.5484\n",
      "  Iter 361/703: Loss 2.2577\n",
      "  Iter 371/703: Loss 2.3342\n",
      "  Iter 381/703: Loss 2.3790\n",
      "  Iter 391/703: Loss 2.4205\n",
      "  Iter 401/703: Loss 2.7162\n",
      "  Iter 411/703: Loss 2.7536\n",
      "  Iter 421/703: Loss 2.3219\n",
      "  Iter 431/703: Loss 2.8856\n",
      "  Iter 441/703: Loss 2.2936\n",
      "  Iter 451/703: Loss 2.6203\n",
      "  Iter 461/703: Loss 2.6261\n",
      "  Iter 471/703: Loss 2.1722\n",
      "  Iter 481/703: Loss 2.5374\n",
      "  Iter 491/703: Loss 2.0535\n",
      "  Iter 501/703: Loss 2.5269\n",
      "  Iter 511/703: Loss 2.1664\n",
      "  Iter 521/703: Loss 2.5799\n",
      "  Iter 531/703: Loss 2.4134\n",
      "  Iter 541/703: Loss 2.3190\n",
      "  Iter 551/703: Loss 2.2611\n",
      "  Iter 561/703: Loss 1.9715\n",
      "  Iter 571/703: Loss 2.5097\n",
      "  Iter 581/703: Loss 2.7320\n",
      "  Iter 591/703: Loss 2.6629\n",
      "  Iter 601/703: Loss 2.3864\n",
      "  Iter 611/703: Loss 2.4499\n",
      "  Iter 621/703: Loss 2.5546\n",
      "  Iter 631/703: Loss 2.7440\n",
      "  Iter 641/703: Loss 2.7362\n",
      "  Iter 651/703: Loss 2.4763\n",
      "  Iter 661/703: Loss 2.3849\n",
      "  Iter 671/703: Loss 2.0472\n",
      "  Iter 681/703: Loss 2.5211\n",
      "  Iter 691/703: Loss 2.5301\n",
      "  Iter 701/703: Loss 2.2010\n",
      "  Iter 703/703: Loss 2.4213\n",
      "Fine Train Loss: 2.4800, Fine Train Acc: 0.3990, Val Acc: 0.3512, Val Loss: 2.4792\n",
      "Time: 2968.82s\n",
      "\n",
      "[Epoch 22/100]\n",
      "  Iter   1/703: Loss 2.2969\n",
      "  Iter  11/703: Loss 2.4175\n",
      "  Iter  21/703: Loss 1.8675\n",
      "  Iter  31/703: Loss 2.4456\n",
      "  Iter  41/703: Loss 2.3250\n",
      "  Iter  51/703: Loss 2.1984\n",
      "  Iter  61/703: Loss 2.7525\n",
      "  Iter  71/703: Loss 3.1057\n",
      "  Iter  81/703: Loss 2.5347\n",
      "  Iter  91/703: Loss 2.2887\n",
      "  Iter 101/703: Loss 2.1293\n",
      "  Iter 111/703: Loss 2.2664\n",
      "  Iter 121/703: Loss 2.4933\n",
      "  Iter 131/703: Loss 2.3129\n",
      "  Iter 141/703: Loss 2.6451\n",
      "  Iter 151/703: Loss 2.6594\n",
      "  Iter 161/703: Loss 2.3338\n",
      "  Iter 171/703: Loss 2.5907\n",
      "  Iter 181/703: Loss 2.3935\n",
      "  Iter 191/703: Loss 2.4790\n",
      "  Iter 201/703: Loss 2.4783\n",
      "  Iter 211/703: Loss 2.3025\n",
      "  Iter 221/703: Loss 2.6161\n",
      "  Iter 231/703: Loss 2.1817\n",
      "  Iter 241/703: Loss 2.2419\n",
      "  Iter 251/703: Loss 2.3900\n",
      "  Iter 261/703: Loss 2.5011\n",
      "  Iter 271/703: Loss 2.6195\n",
      "  Iter 281/703: Loss 2.3858\n",
      "  Iter 291/703: Loss 2.4356\n",
      "  Iter 301/703: Loss 2.6375\n",
      "  Iter 311/703: Loss 2.1733\n",
      "  Iter 321/703: Loss 2.3386\n",
      "  Iter 331/703: Loss 2.4494\n",
      "  Iter 341/703: Loss 2.1620\n",
      "  Iter 351/703: Loss 2.5247\n",
      "  Iter 361/703: Loss 2.3145\n",
      "  Iter 371/703: Loss 2.3155\n",
      "  Iter 381/703: Loss 2.3871\n",
      "  Iter 391/703: Loss 2.3762\n",
      "  Iter 401/703: Loss 2.2109\n",
      "  Iter 411/703: Loss 2.5083\n",
      "  Iter 421/703: Loss 2.5876\n",
      "  Iter 431/703: Loss 2.4549\n",
      "  Iter 441/703: Loss 2.6764\n",
      "  Iter 451/703: Loss 2.2412\n",
      "  Iter 461/703: Loss 2.4878\n",
      "  Iter 471/703: Loss 2.6028\n",
      "  Iter 481/703: Loss 2.2735\n",
      "  Iter 491/703: Loss 2.7161\n",
      "  Iter 501/703: Loss 2.5870\n",
      "  Iter 511/703: Loss 2.6277\n",
      "  Iter 521/703: Loss 2.5818\n",
      "  Iter 531/703: Loss 2.2009\n",
      "  Iter 541/703: Loss 2.4275\n",
      "  Iter 551/703: Loss 2.3193\n",
      "  Iter 561/703: Loss 2.2004\n",
      "  Iter 571/703: Loss 2.6724\n",
      "  Iter 581/703: Loss 2.6550\n",
      "  Iter 591/703: Loss 2.5205\n",
      "  Iter 601/703: Loss 2.6930\n",
      "  Iter 611/703: Loss 2.4834\n",
      "  Iter 621/703: Loss 2.2049\n",
      "  Iter 631/703: Loss 2.1252\n",
      "  Iter 641/703: Loss 2.5685\n",
      "  Iter 651/703: Loss 2.4077\n",
      "  Iter 661/703: Loss 2.3331\n",
      "  Iter 671/703: Loss 2.3647\n",
      "  Iter 681/703: Loss 2.3124\n",
      "  Iter 691/703: Loss 2.0658\n",
      "  Iter 701/703: Loss 2.2523\n",
      "  Iter 703/703: Loss 2.5005\n",
      "Fine Train Loss: 2.4519, Fine Train Acc: 0.4190, Val Acc: 0.3722, Val Loss: 2.4866\n",
      "Time: 2966.62s\n",
      "\n",
      "[Epoch 23/100]\n",
      "  Iter   1/703: Loss 2.6795\n",
      "  Iter  11/703: Loss 2.5168\n",
      "  Iter  21/703: Loss 2.5484\n",
      "  Iter  31/703: Loss 2.6243\n",
      "  Iter  41/703: Loss 2.5053\n",
      "  Iter  51/703: Loss 2.4349\n",
      "  Iter  61/703: Loss 2.7168\n",
      "  Iter  71/703: Loss 2.3580\n",
      "  Iter  81/703: Loss 2.5374\n",
      "  Iter  91/703: Loss 2.2694\n",
      "  Iter 101/703: Loss 2.5755\n",
      "  Iter 111/703: Loss 2.3569\n",
      "  Iter 121/703: Loss 2.5292\n",
      "  Iter 131/703: Loss 3.1907\n",
      "  Iter 141/703: Loss 2.7078\n",
      "  Iter 151/703: Loss 2.2999\n",
      "  Iter 161/703: Loss 2.3556\n",
      "  Iter 171/703: Loss 2.3445\n",
      "  Iter 181/703: Loss 2.3962\n",
      "  Iter 191/703: Loss 2.6349\n",
      "  Iter 201/703: Loss 2.4831\n",
      "  Iter 211/703: Loss 2.3575\n",
      "  Iter 221/703: Loss 2.4756\n",
      "  Iter 231/703: Loss 2.3903\n",
      "  Iter 241/703: Loss 2.1259\n",
      "  Iter 251/703: Loss 2.4525\n",
      "  Iter 261/703: Loss 2.4429\n",
      "  Iter 271/703: Loss 2.6659\n",
      "  Iter 281/703: Loss 2.6641\n",
      "  Iter 291/703: Loss 2.3564\n",
      "  Iter 301/703: Loss 2.6338\n",
      "  Iter 311/703: Loss 2.4783\n",
      "  Iter 321/703: Loss 2.4256\n",
      "  Iter 331/703: Loss 2.0431\n",
      "  Iter 341/703: Loss 2.4793\n",
      "  Iter 351/703: Loss 2.3472\n",
      "  Iter 361/703: Loss 2.5283\n",
      "  Iter 371/703: Loss 2.6784\n",
      "  Iter 381/703: Loss 2.4882\n",
      "  Iter 391/703: Loss 2.3345\n",
      "  Iter 401/703: Loss 2.5626\n",
      "  Iter 411/703: Loss 2.4591\n",
      "  Iter 421/703: Loss 2.5302\n",
      "  Iter 431/703: Loss 2.4888\n",
      "  Iter 441/703: Loss 2.3170\n",
      "  Iter 451/703: Loss 2.5716\n",
      "  Iter 461/703: Loss 2.4270\n",
      "  Iter 471/703: Loss 2.7356\n",
      "  Iter 481/703: Loss 2.5555\n",
      "  Iter 491/703: Loss 2.5649\n",
      "  Iter 501/703: Loss 1.8725\n",
      "  Iter 511/703: Loss 2.4049\n",
      "  Iter 521/703: Loss 2.4271\n",
      "  Iter 531/703: Loss 2.7593\n",
      "  Iter 541/703: Loss 2.4310\n",
      "  Iter 551/703: Loss 2.1025\n",
      "  Iter 561/703: Loss 2.9312\n",
      "  Iter 571/703: Loss 2.5724\n",
      "  Iter 581/703: Loss 2.3196\n",
      "  Iter 591/703: Loss 2.4968\n",
      "  Iter 601/703: Loss 2.0800\n",
      "  Iter 611/703: Loss 2.1202\n",
      "  Iter 621/703: Loss 2.4939\n",
      "  Iter 631/703: Loss 2.6139\n",
      "  Iter 641/703: Loss 2.5737\n",
      "  Iter 651/703: Loss 2.7769\n",
      "  Iter 661/703: Loss 2.3423\n",
      "  Iter 671/703: Loss 2.0835\n",
      "  Iter 681/703: Loss 2.3988\n",
      "  Iter 691/703: Loss 2.3793\n",
      "  Iter 701/703: Loss 2.4844\n",
      "  Iter 703/703: Loss 2.6388\n",
      "Fine Train Loss: 2.4277, Fine Train Acc: 0.4180, Val Acc: 0.3770, Val Loss: 2.4362\n",
      "Time: 2971.17s\n",
      "\n",
      "[Epoch 24/100]\n",
      "  Iter   1/703: Loss 2.1642\n",
      "  Iter  11/703: Loss 2.6396\n",
      "  Iter  21/703: Loss 2.3834\n",
      "  Iter  31/703: Loss 2.2425\n",
      "  Iter  41/703: Loss 1.9836\n",
      "  Iter  51/703: Loss 2.3237\n",
      "  Iter  61/703: Loss 2.4472\n",
      "  Iter  71/703: Loss 2.1396\n",
      "  Iter  81/703: Loss 2.6094\n",
      "  Iter  91/703: Loss 2.4951\n",
      "  Iter 101/703: Loss 2.3473\n",
      "  Iter 111/703: Loss 2.2570\n",
      "  Iter 121/703: Loss 2.3875\n",
      "  Iter 131/703: Loss 2.6285\n",
      "  Iter 141/703: Loss 2.4305\n",
      "  Iter 151/703: Loss 2.2618\n",
      "  Iter 161/703: Loss 2.3263\n",
      "  Iter 171/703: Loss 2.6364\n",
      "  Iter 181/703: Loss 2.4064\n",
      "  Iter 191/703: Loss 2.4809\n",
      "  Iter 201/703: Loss 2.5209\n",
      "  Iter 211/703: Loss 2.7286\n",
      "  Iter 221/703: Loss 2.3329\n",
      "  Iter 231/703: Loss 2.4673\n",
      "  Iter 241/703: Loss 2.6126\n",
      "  Iter 251/703: Loss 2.3318\n",
      "  Iter 261/703: Loss 2.8039\n",
      "  Iter 271/703: Loss 2.6280\n",
      "  Iter 281/703: Loss 2.4845\n",
      "  Iter 291/703: Loss 2.5611\n",
      "  Iter 301/703: Loss 2.3314\n",
      "  Iter 311/703: Loss 2.2966\n",
      "  Iter 321/703: Loss 2.4776\n",
      "  Iter 331/703: Loss 2.3399\n",
      "  Iter 341/703: Loss 2.2434\n",
      "  Iter 351/703: Loss 2.3422\n",
      "  Iter 361/703: Loss 2.0949\n",
      "  Iter 371/703: Loss 2.4650\n",
      "  Iter 381/703: Loss 2.3541\n",
      "  Iter 391/703: Loss 2.6313\n",
      "  Iter 401/703: Loss 2.3197\n",
      "  Iter 411/703: Loss 2.1853\n",
      "  Iter 421/703: Loss 2.3624\n",
      "  Iter 431/703: Loss 2.8950\n",
      "  Iter 441/703: Loss 2.1130\n",
      "  Iter 451/703: Loss 2.2308\n",
      "  Iter 461/703: Loss 2.2014\n",
      "  Iter 471/703: Loss 2.4872\n",
      "  Iter 481/703: Loss 2.1732\n",
      "  Iter 491/703: Loss 2.3172\n",
      "  Iter 501/703: Loss 2.2905\n",
      "  Iter 511/703: Loss 2.2836\n",
      "  Iter 521/703: Loss 2.3388\n",
      "  Iter 531/703: Loss 2.4349\n",
      "  Iter 541/703: Loss 2.2424\n",
      "  Iter 551/703: Loss 2.5277\n",
      "  Iter 561/703: Loss 2.6114\n",
      "  Iter 571/703: Loss 2.3128\n",
      "  Iter 581/703: Loss 2.5197\n",
      "  Iter 591/703: Loss 2.5494\n",
      "  Iter 601/703: Loss 2.1963\n",
      "  Iter 611/703: Loss 2.3405\n",
      "  Iter 621/703: Loss 2.2486\n",
      "  Iter 631/703: Loss 2.7478\n",
      "  Iter 641/703: Loss 2.3895\n",
      "  Iter 651/703: Loss 2.6075\n",
      "  Iter 661/703: Loss 2.5711\n",
      "  Iter 671/703: Loss 2.2360\n",
      "  Iter 681/703: Loss 2.1613\n",
      "  Iter 691/703: Loss 2.1675\n",
      "  Iter 701/703: Loss 2.4583\n",
      "  Iter 703/703: Loss 2.6808\n",
      "Fine Train Loss: 2.3968, Fine Train Acc: 0.4290, Val Acc: 0.3720, Val Loss: 2.4182\n",
      "Time: 2971.24s\n",
      "\n",
      "[Epoch 25/100]\n",
      "  Iter   1/703: Loss 2.4055\n",
      "  Iter  11/703: Loss 2.5022\n",
      "  Iter  21/703: Loss 2.2334\n",
      "  Iter  31/703: Loss 2.3055\n",
      "  Iter  41/703: Loss 2.6007\n",
      "  Iter  51/703: Loss 1.9254\n",
      "  Iter  61/703: Loss 2.2130\n",
      "  Iter  71/703: Loss 2.2841\n",
      "  Iter  81/703: Loss 2.1463\n",
      "  Iter  91/703: Loss 2.3571\n",
      "  Iter 101/703: Loss 2.7810\n",
      "  Iter 111/703: Loss 2.0721\n",
      "  Iter 121/703: Loss 2.6442\n",
      "  Iter 131/703: Loss 2.5977\n",
      "  Iter 141/703: Loss 2.5389\n",
      "  Iter 151/703: Loss 2.2587\n",
      "  Iter 161/703: Loss 2.3288\n",
      "  Iter 171/703: Loss 2.6660\n",
      "  Iter 181/703: Loss 2.4394\n",
      "  Iter 191/703: Loss 2.1600\n",
      "  Iter 201/703: Loss 2.7549\n",
      "  Iter 211/703: Loss 2.4890\n",
      "  Iter 221/703: Loss 2.1374\n",
      "  Iter 231/703: Loss 2.3373\n",
      "  Iter 241/703: Loss 2.5206\n",
      "  Iter 251/703: Loss 2.2273\n",
      "  Iter 261/703: Loss 2.5667\n",
      "  Iter 271/703: Loss 2.5682\n",
      "  Iter 281/703: Loss 2.5085\n",
      "  Iter 291/703: Loss 2.3086\n",
      "  Iter 301/703: Loss 2.9874\n",
      "  Iter 311/703: Loss 2.6665\n",
      "  Iter 321/703: Loss 2.0633\n",
      "  Iter 331/703: Loss 2.2241\n",
      "  Iter 341/703: Loss 2.3158\n",
      "  Iter 351/703: Loss 2.5973\n",
      "  Iter 361/703: Loss 2.3952\n",
      "  Iter 371/703: Loss 2.6607\n",
      "  Iter 381/703: Loss 2.1491\n",
      "  Iter 391/703: Loss 2.6379\n",
      "  Iter 401/703: Loss 2.6049\n",
      "  Iter 411/703: Loss 2.8515\n",
      "  Iter 421/703: Loss 2.2377\n",
      "  Iter 431/703: Loss 2.6202\n",
      "  Iter 441/703: Loss 2.2500\n",
      "  Iter 451/703: Loss 2.0001\n",
      "  Iter 461/703: Loss 2.7412\n",
      "  Iter 471/703: Loss 2.4957\n",
      "  Iter 481/703: Loss 2.4815\n",
      "  Iter 491/703: Loss 2.1612\n",
      "  Iter 501/703: Loss 2.5115\n",
      "  Iter 511/703: Loss 2.2319\n",
      "  Iter 521/703: Loss 2.5808\n",
      "  Iter 531/703: Loss 2.4561\n",
      "  Iter 541/703: Loss 2.3618\n",
      "  Iter 551/703: Loss 2.3748\n",
      "  Iter 561/703: Loss 2.1150\n",
      "  Iter 571/703: Loss 2.2496\n",
      "  Iter 581/703: Loss 2.1813\n",
      "  Iter 591/703: Loss 2.4992\n",
      "  Iter 601/703: Loss 2.3366\n",
      "  Iter 611/703: Loss 1.8486\n",
      "  Iter 621/703: Loss 2.7391\n",
      "  Iter 631/703: Loss 2.2793\n",
      "  Iter 641/703: Loss 2.2249\n",
      "  Iter 651/703: Loss 2.1139\n",
      "  Iter 661/703: Loss 2.3532\n",
      "  Iter 671/703: Loss 2.4749\n",
      "  Iter 681/703: Loss 2.6732\n",
      "  Iter 691/703: Loss 2.4725\n",
      "  Iter 701/703: Loss 2.0006\n",
      "  Iter 703/703: Loss 2.1833\n",
      "Fine Train Loss: 2.3761, Fine Train Acc: 0.4300, Val Acc: 0.3768, Val Loss: 2.4199\n",
      "Time: 2982.17s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch25.pkl\n",
      "\n",
      "[Epoch 26/100]\n",
      "  Iter   1/703: Loss 2.1318\n",
      "  Iter  11/703: Loss 2.6177\n",
      "  Iter  21/703: Loss 2.3678\n",
      "  Iter  31/703: Loss 2.1936\n",
      "  Iter  41/703: Loss 1.9263\n",
      "  Iter  51/703: Loss 2.3210\n",
      "  Iter  61/703: Loss 2.0800\n",
      "  Iter  71/703: Loss 2.5481\n",
      "  Iter  81/703: Loss 2.3767\n",
      "  Iter  91/703: Loss 2.2217\n",
      "  Iter 101/703: Loss 2.6769\n",
      "  Iter 111/703: Loss 2.1419\n",
      "  Iter 121/703: Loss 2.6467\n",
      "  Iter 131/703: Loss 2.4005\n",
      "  Iter 141/703: Loss 2.4316\n",
      "  Iter 151/703: Loss 2.2810\n",
      "  Iter 161/703: Loss 2.4207\n",
      "  Iter 171/703: Loss 2.6896\n",
      "  Iter 181/703: Loss 2.3524\n",
      "  Iter 191/703: Loss 2.1027\n",
      "  Iter 201/703: Loss 2.6183\n",
      "  Iter 211/703: Loss 2.6473\n",
      "  Iter 221/703: Loss 2.2519\n",
      "  Iter 231/703: Loss 2.5948\n",
      "  Iter 241/703: Loss 2.5272\n",
      "  Iter 251/703: Loss 2.5286\n",
      "  Iter 261/703: Loss 2.4556\n",
      "  Iter 271/703: Loss 2.4189\n",
      "  Iter 281/703: Loss 2.3409\n",
      "  Iter 291/703: Loss 1.9758\n",
      "  Iter 301/703: Loss 2.3181\n",
      "  Iter 311/703: Loss 2.2726\n",
      "  Iter 321/703: Loss 2.2674\n",
      "  Iter 331/703: Loss 2.7213\n",
      "  Iter 341/703: Loss 2.2204\n",
      "  Iter 351/703: Loss 1.8761\n",
      "  Iter 361/703: Loss 2.1806\n",
      "  Iter 371/703: Loss 2.7957\n",
      "  Iter 381/703: Loss 2.0965\n",
      "  Iter 391/703: Loss 2.2833\n",
      "  Iter 401/703: Loss 2.3284\n",
      "  Iter 411/703: Loss 2.3296\n",
      "  Iter 421/703: Loss 2.1667\n",
      "  Iter 431/703: Loss 2.2308\n",
      "  Iter 441/703: Loss 2.5427\n",
      "  Iter 451/703: Loss 2.2300\n",
      "  Iter 461/703: Loss 2.3906\n",
      "  Iter 471/703: Loss 1.8884\n",
      "  Iter 481/703: Loss 2.1238\n",
      "  Iter 491/703: Loss 2.2856\n",
      "  Iter 501/703: Loss 2.2251\n",
      "  Iter 511/703: Loss 2.1511\n",
      "  Iter 521/703: Loss 2.5372\n",
      "  Iter 531/703: Loss 2.6926\n",
      "  Iter 541/703: Loss 2.5058\n",
      "  Iter 551/703: Loss 2.4293\n",
      "  Iter 561/703: Loss 2.3125\n",
      "  Iter 571/703: Loss 2.6468\n",
      "  Iter 581/703: Loss 2.1821\n",
      "  Iter 591/703: Loss 2.4130\n",
      "  Iter 601/703: Loss 2.2539\n",
      "  Iter 611/703: Loss 2.1575\n",
      "  Iter 621/703: Loss 2.1888\n",
      "  Iter 631/703: Loss 2.7913\n",
      "  Iter 641/703: Loss 2.4441\n",
      "  Iter 651/703: Loss 2.2916\n",
      "  Iter 661/703: Loss 2.2033\n",
      "  Iter 671/703: Loss 2.3246\n",
      "  Iter 681/703: Loss 2.1278\n",
      "  Iter 691/703: Loss 2.5175\n",
      "  Iter 701/703: Loss 2.3868\n",
      "  Iter 703/703: Loss 2.4869\n",
      "Fine Train Loss: 2.3622, Fine Train Acc: 0.4180, Val Acc: 0.3844, Val Loss: 2.3874\n",
      "Time: 2985.92s\n",
      "\n",
      "[Epoch 27/100]\n",
      "  Iter   1/703: Loss 2.2160\n",
      "  Iter  11/703: Loss 2.2905\n",
      "  Iter  21/703: Loss 2.1503\n",
      "  Iter  31/703: Loss 2.3204\n",
      "  Iter  41/703: Loss 2.1643\n",
      "  Iter  51/703: Loss 2.4559\n",
      "  Iter  61/703: Loss 2.3642\n",
      "  Iter  71/703: Loss 2.4702\n",
      "  Iter  81/703: Loss 2.5465\n",
      "  Iter  91/703: Loss 2.3835\n",
      "  Iter 101/703: Loss 2.3940\n",
      "  Iter 111/703: Loss 2.2718\n",
      "  Iter 121/703: Loss 2.4116\n",
      "  Iter 131/703: Loss 2.1649\n",
      "  Iter 141/703: Loss 1.9683\n",
      "  Iter 151/703: Loss 2.3324\n",
      "  Iter 161/703: Loss 2.2245\n",
      "  Iter 171/703: Loss 2.9217\n",
      "  Iter 181/703: Loss 2.3453\n",
      "  Iter 191/703: Loss 2.4552\n",
      "  Iter 201/703: Loss 2.4404\n",
      "  Iter 211/703: Loss 2.2203\n",
      "  Iter 221/703: Loss 2.2725\n",
      "  Iter 231/703: Loss 2.1770\n",
      "  Iter 241/703: Loss 2.8636\n",
      "  Iter 251/703: Loss 2.5027\n",
      "  Iter 261/703: Loss 2.4963\n",
      "  Iter 271/703: Loss 2.2335\n",
      "  Iter 281/703: Loss 2.1085\n",
      "  Iter 291/703: Loss 2.4013\n",
      "  Iter 301/703: Loss 2.1327\n",
      "  Iter 311/703: Loss 2.3230\n",
      "  Iter 321/703: Loss 2.4347\n",
      "  Iter 331/703: Loss 2.0635\n",
      "  Iter 341/703: Loss 2.1659\n",
      "  Iter 351/703: Loss 2.1528\n",
      "  Iter 361/703: Loss 1.9429\n",
      "  Iter 371/703: Loss 2.4712\n",
      "  Iter 381/703: Loss 2.2205\n",
      "  Iter 391/703: Loss 2.0742\n",
      "  Iter 401/703: Loss 1.9349\n",
      "  Iter 411/703: Loss 2.1805\n",
      "  Iter 421/703: Loss 2.3235\n",
      "  Iter 431/703: Loss 2.5952\n",
      "  Iter 441/703: Loss 2.4045\n",
      "  Iter 451/703: Loss 2.3798\n",
      "  Iter 461/703: Loss 2.1270\n",
      "  Iter 471/703: Loss 2.2116\n",
      "  Iter 481/703: Loss 2.3414\n",
      "  Iter 491/703: Loss 2.3175\n",
      "  Iter 501/703: Loss 2.3476\n",
      "  Iter 511/703: Loss 2.5284\n",
      "  Iter 521/703: Loss 2.5073\n",
      "  Iter 531/703: Loss 2.6279\n",
      "  Iter 541/703: Loss 2.5356\n",
      "  Iter 551/703: Loss 2.1808\n",
      "  Iter 561/703: Loss 2.9867\n",
      "  Iter 571/703: Loss 2.3618\n",
      "  Iter 581/703: Loss 2.4077\n",
      "  Iter 591/703: Loss 2.4834\n",
      "  Iter 601/703: Loss 2.3438\n",
      "  Iter 611/703: Loss 2.4163\n",
      "  Iter 621/703: Loss 2.1912\n",
      "  Iter 631/703: Loss 2.3486\n",
      "  Iter 641/703: Loss 2.4571\n",
      "  Iter 651/703: Loss 2.1205\n",
      "  Iter 661/703: Loss 2.5416\n",
      "  Iter 671/703: Loss 2.0362\n",
      "  Iter 681/703: Loss 1.9977\n",
      "  Iter 691/703: Loss 2.2474\n",
      "  Iter 701/703: Loss 2.3102\n",
      "  Iter 703/703: Loss 2.2103\n",
      "Fine Train Loss: 2.3265, Fine Train Acc: 0.4620, Val Acc: 0.3898, Val Loss: 2.3478\n",
      "Time: 2988.70s\n",
      "\n",
      "[Epoch 28/100]\n",
      "  Iter   1/703: Loss 2.5706\n",
      "  Iter  11/703: Loss 1.8640\n",
      "  Iter  21/703: Loss 2.1543\n",
      "  Iter  31/703: Loss 2.2344\n",
      "  Iter  41/703: Loss 2.6886\n",
      "  Iter  51/703: Loss 2.2389\n",
      "  Iter  61/703: Loss 1.8994\n",
      "  Iter  71/703: Loss 2.5727\n",
      "  Iter  81/703: Loss 2.2497\n",
      "  Iter  91/703: Loss 2.4557\n",
      "  Iter 101/703: Loss 2.2454\n",
      "  Iter 111/703: Loss 2.5401\n",
      "  Iter 121/703: Loss 2.1580\n",
      "  Iter 131/703: Loss 2.4796\n",
      "  Iter 141/703: Loss 2.9818\n",
      "  Iter 151/703: Loss 2.2633\n",
      "  Iter 161/703: Loss 2.5578\n",
      "  Iter 171/703: Loss 2.9894\n",
      "  Iter 181/703: Loss 2.2031\n",
      "  Iter 191/703: Loss 2.1848\n",
      "  Iter 201/703: Loss 2.0668\n",
      "  Iter 211/703: Loss 2.5028\n",
      "  Iter 221/703: Loss 2.1367\n",
      "  Iter 231/703: Loss 2.1909\n",
      "  Iter 241/703: Loss 2.2339\n",
      "  Iter 251/703: Loss 1.8623\n",
      "  Iter 261/703: Loss 2.3714\n",
      "  Iter 271/703: Loss 2.7638\n",
      "  Iter 281/703: Loss 2.3892\n",
      "  Iter 291/703: Loss 2.6429\n",
      "  Iter 301/703: Loss 2.1946\n",
      "  Iter 311/703: Loss 2.0710\n",
      "  Iter 321/703: Loss 2.2235\n",
      "  Iter 331/703: Loss 2.4458\n",
      "  Iter 341/703: Loss 2.1262\n",
      "  Iter 351/703: Loss 2.2248\n",
      "  Iter 361/703: Loss 2.1337\n",
      "  Iter 371/703: Loss 2.4220\n",
      "  Iter 381/703: Loss 2.0859\n",
      "  Iter 391/703: Loss 2.2929\n",
      "  Iter 401/703: Loss 2.3719\n",
      "  Iter 411/703: Loss 2.6653\n",
      "  Iter 421/703: Loss 2.3313\n",
      "  Iter 431/703: Loss 2.1819\n",
      "  Iter 441/703: Loss 2.5160\n",
      "  Iter 451/703: Loss 2.4636\n",
      "  Iter 461/703: Loss 2.3848\n",
      "  Iter 471/703: Loss 2.5044\n",
      "  Iter 481/703: Loss 2.1849\n",
      "  Iter 491/703: Loss 1.9588\n",
      "  Iter 501/703: Loss 2.1607\n",
      "  Iter 511/703: Loss 2.3365\n",
      "  Iter 521/703: Loss 2.1422\n",
      "  Iter 531/703: Loss 2.0900\n",
      "  Iter 541/703: Loss 2.2650\n",
      "  Iter 551/703: Loss 2.2754\n",
      "  Iter 561/703: Loss 2.2463\n",
      "  Iter 571/703: Loss 2.1979\n",
      "  Iter 581/703: Loss 2.5295\n",
      "  Iter 591/703: Loss 2.0812\n",
      "  Iter 601/703: Loss 2.4263\n",
      "  Iter 611/703: Loss 1.9773\n",
      "  Iter 621/703: Loss 2.4410\n",
      "  Iter 631/703: Loss 2.2516\n",
      "  Iter 641/703: Loss 2.3112\n",
      "  Iter 651/703: Loss 2.2330\n",
      "  Iter 661/703: Loss 2.4086\n",
      "  Iter 671/703: Loss 2.3468\n",
      "  Iter 681/703: Loss 2.6221\n",
      "  Iter 691/703: Loss 2.2180\n",
      "  Iter 701/703: Loss 2.6009\n",
      "  Iter 703/703: Loss 2.2033\n",
      "Fine Train Loss: 2.3124, Fine Train Acc: 0.4620, Val Acc: 0.4040, Val Loss: 2.3388\n",
      "Time: 2992.32s\n",
      "\n",
      "[Epoch 29/100]\n",
      "  Iter   1/703: Loss 2.2856\n",
      "  Iter  11/703: Loss 2.4668\n",
      "  Iter  21/703: Loss 2.5346\n",
      "  Iter  31/703: Loss 2.3554\n",
      "  Iter  41/703: Loss 2.5883\n",
      "  Iter  51/703: Loss 2.4635\n",
      "  Iter  61/703: Loss 2.5014\n",
      "  Iter  71/703: Loss 2.4278\n",
      "  Iter  81/703: Loss 2.1010\n",
      "  Iter  91/703: Loss 2.2065\n",
      "  Iter 101/703: Loss 2.2687\n",
      "  Iter 111/703: Loss 2.0585\n",
      "  Iter 121/703: Loss 2.3581\n",
      "  Iter 131/703: Loss 2.2224\n",
      "  Iter 141/703: Loss 2.6098\n",
      "  Iter 151/703: Loss 2.2136\n",
      "  Iter 161/703: Loss 2.6321\n",
      "  Iter 171/703: Loss 2.5430\n",
      "  Iter 181/703: Loss 2.5577\n",
      "  Iter 191/703: Loss 2.4041\n",
      "  Iter 201/703: Loss 2.5337\n",
      "  Iter 211/703: Loss 2.3740\n",
      "  Iter 221/703: Loss 2.6065\n",
      "  Iter 231/703: Loss 2.6767\n",
      "  Iter 241/703: Loss 2.1101\n",
      "  Iter 251/703: Loss 2.5296\n",
      "  Iter 261/703: Loss 2.3234\n",
      "  Iter 271/703: Loss 2.0862\n",
      "  Iter 281/703: Loss 2.2203\n",
      "  Iter 291/703: Loss 2.4428\n",
      "  Iter 301/703: Loss 2.3458\n",
      "  Iter 311/703: Loss 2.0178\n",
      "  Iter 321/703: Loss 2.1985\n",
      "  Iter 331/703: Loss 2.4444\n",
      "  Iter 341/703: Loss 2.1428\n",
      "  Iter 351/703: Loss 1.9318\n",
      "  Iter 361/703: Loss 2.5068\n",
      "  Iter 371/703: Loss 2.2703\n",
      "  Iter 381/703: Loss 2.6121\n",
      "  Iter 391/703: Loss 2.3818\n",
      "  Iter 401/703: Loss 2.6774\n",
      "  Iter 411/703: Loss 2.3546\n",
      "  Iter 421/703: Loss 2.1731\n",
      "  Iter 431/703: Loss 2.0685\n",
      "  Iter 441/703: Loss 2.1218\n",
      "  Iter 451/703: Loss 2.1840\n",
      "  Iter 461/703: Loss 2.1807\n",
      "  Iter 471/703: Loss 2.2172\n",
      "  Iter 481/703: Loss 2.2409\n",
      "  Iter 491/703: Loss 2.1772\n",
      "  Iter 501/703: Loss 2.6992\n",
      "  Iter 511/703: Loss 2.3085\n",
      "  Iter 521/703: Loss 2.5033\n",
      "  Iter 531/703: Loss 2.0641\n",
      "  Iter 541/703: Loss 2.5685\n",
      "  Iter 551/703: Loss 2.6353\n",
      "  Iter 561/703: Loss 2.1270\n",
      "  Iter 571/703: Loss 2.3890\n",
      "  Iter 581/703: Loss 2.3674\n",
      "  Iter 591/703: Loss 2.2001\n",
      "  Iter 601/703: Loss 2.1428\n",
      "  Iter 611/703: Loss 2.1776\n",
      "  Iter 621/703: Loss 2.2633\n",
      "  Iter 631/703: Loss 2.3561\n",
      "  Iter 641/703: Loss 1.9374\n",
      "  Iter 651/703: Loss 2.3180\n",
      "  Iter 661/703: Loss 2.1272\n",
      "  Iter 671/703: Loss 2.1593\n",
      "  Iter 681/703: Loss 2.3634\n",
      "  Iter 691/703: Loss 2.1447\n",
      "  Iter 701/703: Loss 2.5770\n",
      "  Iter 703/703: Loss 2.3585\n",
      "Fine Train Loss: 2.3083, Fine Train Acc: 0.4680, Val Acc: 0.3870, Val Loss: 2.3518\n",
      "Time: 3007.36s\n",
      "\n",
      "[Epoch 30/100]\n",
      "  Iter   1/703: Loss 2.2193\n",
      "  Iter  11/703: Loss 2.4054\n",
      "  Iter  21/703: Loss 2.3346\n",
      "  Iter  31/703: Loss 2.1178\n",
      "  Iter  41/703: Loss 2.6664\n",
      "  Iter  51/703: Loss 2.3525\n",
      "  Iter  61/703: Loss 2.1585\n",
      "  Iter  71/703: Loss 2.0900\n",
      "  Iter  81/703: Loss 2.0403\n",
      "  Iter  91/703: Loss 1.8901\n",
      "  Iter 101/703: Loss 2.2881\n",
      "  Iter 111/703: Loss 1.9048\n",
      "  Iter 121/703: Loss 2.4408\n",
      "  Iter 131/703: Loss 1.8362\n",
      "  Iter 141/703: Loss 2.4030\n",
      "  Iter 151/703: Loss 2.3565\n",
      "  Iter 161/703: Loss 2.3399\n",
      "  Iter 171/703: Loss 2.5860\n",
      "  Iter 181/703: Loss 2.3458\n",
      "  Iter 191/703: Loss 2.9036\n",
      "  Iter 201/703: Loss 2.3767\n",
      "  Iter 211/703: Loss 2.2803\n",
      "  Iter 221/703: Loss 1.9444\n",
      "  Iter 231/703: Loss 1.8569\n",
      "  Iter 241/703: Loss 2.4462\n",
      "  Iter 251/703: Loss 2.7371\n",
      "  Iter 261/703: Loss 2.2486\n",
      "  Iter 271/703: Loss 2.4341\n",
      "  Iter 281/703: Loss 2.1946\n",
      "  Iter 291/703: Loss 2.3840\n",
      "  Iter 301/703: Loss 2.1398\n",
      "  Iter 311/703: Loss 2.1577\n",
      "  Iter 321/703: Loss 2.4536\n",
      "  Iter 331/703: Loss 2.2213\n",
      "  Iter 341/703: Loss 2.6316\n",
      "  Iter 351/703: Loss 2.2257\n",
      "  Iter 361/703: Loss 2.2542\n",
      "  Iter 371/703: Loss 2.1381\n",
      "  Iter 381/703: Loss 1.9775\n",
      "  Iter 391/703: Loss 2.5858\n",
      "  Iter 401/703: Loss 1.9673\n",
      "  Iter 411/703: Loss 2.5952\n",
      "  Iter 421/703: Loss 2.1728\n",
      "  Iter 431/703: Loss 2.2916\n",
      "  Iter 441/703: Loss 2.2677\n",
      "  Iter 451/703: Loss 2.4592\n",
      "  Iter 461/703: Loss 2.4192\n",
      "  Iter 471/703: Loss 2.1900\n",
      "  Iter 481/703: Loss 2.4219\n",
      "  Iter 491/703: Loss 2.4574\n",
      "  Iter 501/703: Loss 2.2973\n",
      "  Iter 511/703: Loss 2.1028\n",
      "  Iter 521/703: Loss 1.8289\n",
      "  Iter 531/703: Loss 2.2881\n",
      "  Iter 541/703: Loss 2.3379\n",
      "  Iter 551/703: Loss 2.2309\n",
      "  Iter 561/703: Loss 2.0563\n",
      "  Iter 571/703: Loss 2.1788\n",
      "  Iter 581/703: Loss 2.3675\n",
      "  Iter 591/703: Loss 2.5561\n",
      "  Iter 601/703: Loss 2.0044\n",
      "  Iter 611/703: Loss 1.9124\n",
      "  Iter 621/703: Loss 2.0586\n",
      "  Iter 631/703: Loss 2.3011\n",
      "  Iter 641/703: Loss 2.2823\n",
      "  Iter 651/703: Loss 2.1940\n",
      "  Iter 661/703: Loss 2.4604\n",
      "  Iter 671/703: Loss 2.1903\n",
      "  Iter 681/703: Loss 2.1731\n",
      "  Iter 691/703: Loss 2.3952\n",
      "  Iter 701/703: Loss 2.2863\n",
      "  Iter 703/703: Loss 2.5490\n",
      "Fine Train Loss: 2.2696, Fine Train Acc: 0.4520, Val Acc: 0.4092, Val Loss: 2.3068\n",
      "Time: 3023.27s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch30.pkl\n",
      "\n",
      "[Epoch 31/100]\n",
      "  Iter   1/703: Loss 2.4209\n",
      "  Iter  11/703: Loss 2.2819\n",
      "  Iter  21/703: Loss 2.2184\n",
      "  Iter  31/703: Loss 2.8201\n",
      "  Iter  41/703: Loss 2.0524\n",
      "  Iter  51/703: Loss 2.1319\n",
      "  Iter  61/703: Loss 2.3687\n",
      "  Iter  71/703: Loss 2.5141\n",
      "  Iter  81/703: Loss 1.8654\n",
      "  Iter  91/703: Loss 2.0695\n",
      "  Iter 101/703: Loss 2.2758\n",
      "  Iter 111/703: Loss 2.2293\n",
      "  Iter 121/703: Loss 2.2108\n",
      "  Iter 131/703: Loss 2.0904\n",
      "  Iter 141/703: Loss 1.9894\n",
      "  Iter 151/703: Loss 1.8555\n",
      "  Iter 161/703: Loss 2.0766\n",
      "  Iter 171/703: Loss 2.4111\n",
      "  Iter 181/703: Loss 2.2424\n",
      "  Iter 191/703: Loss 2.5416\n",
      "  Iter 201/703: Loss 2.2157\n",
      "  Iter 211/703: Loss 2.2120\n",
      "  Iter 221/703: Loss 2.0902\n",
      "  Iter 231/703: Loss 2.2729\n",
      "  Iter 241/703: Loss 2.3266\n",
      "  Iter 251/703: Loss 2.2259\n",
      "  Iter 261/703: Loss 2.2046\n",
      "  Iter 271/703: Loss 2.1947\n",
      "  Iter 281/703: Loss 2.7938\n",
      "  Iter 291/703: Loss 2.2596\n",
      "  Iter 301/703: Loss 1.9790\n",
      "  Iter 311/703: Loss 2.4260\n",
      "  Iter 321/703: Loss 1.8337\n",
      "  Iter 331/703: Loss 2.3320\n",
      "  Iter 341/703: Loss 2.1210\n",
      "  Iter 351/703: Loss 2.1896\n",
      "  Iter 361/703: Loss 2.2681\n",
      "  Iter 371/703: Loss 2.0189\n",
      "  Iter 381/703: Loss 2.4576\n",
      "  Iter 391/703: Loss 2.1549\n",
      "  Iter 401/703: Loss 2.6036\n",
      "  Iter 411/703: Loss 1.8851\n",
      "  Iter 421/703: Loss 2.2891\n",
      "  Iter 431/703: Loss 2.2051\n",
      "  Iter 441/703: Loss 2.2671\n",
      "  Iter 451/703: Loss 2.3121\n",
      "  Iter 461/703: Loss 2.2666\n",
      "  Iter 471/703: Loss 2.3225\n",
      "  Iter 481/703: Loss 2.2824\n",
      "  Iter 491/703: Loss 2.1171\n",
      "  Iter 501/703: Loss 2.1873\n",
      "  Iter 511/703: Loss 2.1434\n",
      "  Iter 521/703: Loss 2.1054\n",
      "  Iter 531/703: Loss 2.2734\n",
      "  Iter 541/703: Loss 2.4721\n",
      "  Iter 551/703: Loss 2.7392\n",
      "  Iter 561/703: Loss 2.1939\n",
      "  Iter 571/703: Loss 2.0320\n",
      "  Iter 581/703: Loss 2.2679\n",
      "  Iter 591/703: Loss 2.1327\n",
      "  Iter 601/703: Loss 2.0625\n",
      "  Iter 611/703: Loss 2.1757\n",
      "  Iter 621/703: Loss 2.3950\n",
      "  Iter 631/703: Loss 2.3860\n",
      "  Iter 641/703: Loss 2.3165\n",
      "  Iter 651/703: Loss 2.1211\n",
      "  Iter 661/703: Loss 2.3062\n",
      "  Iter 671/703: Loss 1.8942\n",
      "  Iter 681/703: Loss 2.2899\n",
      "  Iter 691/703: Loss 2.5395\n",
      "  Iter 701/703: Loss 2.7281\n",
      "  Iter 703/703: Loss 1.9960\n",
      "Fine Train Loss: 2.2615, Fine Train Acc: 0.4700, Val Acc: 0.3964, Val Loss: 2.3060\n",
      "Time: 3013.60s\n",
      "\n",
      "[Epoch 32/100]\n",
      "  Iter   1/703: Loss 2.1482\n",
      "  Iter  11/703: Loss 2.2457\n",
      "  Iter  21/703: Loss 2.4415\n",
      "  Iter  31/703: Loss 2.2133\n",
      "  Iter  41/703: Loss 2.1258\n",
      "  Iter  51/703: Loss 1.9764\n",
      "  Iter  61/703: Loss 2.7237\n",
      "  Iter  71/703: Loss 2.2638\n",
      "  Iter  81/703: Loss 2.1223\n",
      "  Iter  91/703: Loss 2.6493\n",
      "  Iter 101/703: Loss 2.3035\n",
      "  Iter 111/703: Loss 2.2693\n",
      "  Iter 121/703: Loss 1.8966\n",
      "  Iter 131/703: Loss 2.1262\n",
      "  Iter 141/703: Loss 1.9900\n",
      "  Iter 151/703: Loss 2.4031\n",
      "  Iter 161/703: Loss 1.9773\n",
      "  Iter 171/703: Loss 2.0653\n",
      "  Iter 181/703: Loss 2.2534\n",
      "  Iter 191/703: Loss 2.5205\n",
      "  Iter 201/703: Loss 2.3627\n",
      "  Iter 211/703: Loss 2.1812\n",
      "  Iter 221/703: Loss 2.2575\n",
      "  Iter 231/703: Loss 1.9517\n",
      "  Iter 241/703: Loss 2.0972\n",
      "  Iter 251/703: Loss 2.1199\n",
      "  Iter 261/703: Loss 2.3823\n",
      "  Iter 271/703: Loss 2.4153\n",
      "  Iter 281/703: Loss 2.3875\n",
      "  Iter 291/703: Loss 2.2554\n",
      "  Iter 301/703: Loss 1.9985\n",
      "  Iter 311/703: Loss 2.5175\n",
      "  Iter 321/703: Loss 2.2970\n",
      "  Iter 331/703: Loss 2.5002\n",
      "  Iter 341/703: Loss 2.2332\n",
      "  Iter 351/703: Loss 2.0923\n",
      "  Iter 361/703: Loss 2.2229\n",
      "  Iter 371/703: Loss 2.0595\n",
      "  Iter 381/703: Loss 2.3076\n",
      "  Iter 391/703: Loss 2.2251\n",
      "  Iter 401/703: Loss 1.9908\n",
      "  Iter 411/703: Loss 2.2442\n",
      "  Iter 421/703: Loss 2.0489\n",
      "  Iter 431/703: Loss 2.3509\n",
      "  Iter 441/703: Loss 2.2345\n",
      "  Iter 451/703: Loss 2.6934\n",
      "  Iter 461/703: Loss 2.1054\n",
      "  Iter 471/703: Loss 2.2505\n",
      "  Iter 481/703: Loss 1.7405\n",
      "  Iter 491/703: Loss 2.2546\n",
      "  Iter 501/703: Loss 2.4850\n",
      "  Iter 511/703: Loss 2.1649\n",
      "  Iter 521/703: Loss 2.1041\n",
      "  Iter 531/703: Loss 1.8310\n",
      "  Iter 541/703: Loss 2.2444\n",
      "  Iter 551/703: Loss 2.2192\n",
      "  Iter 561/703: Loss 2.2772\n",
      "  Iter 571/703: Loss 2.0781\n",
      "  Iter 581/703: Loss 2.1880\n",
      "  Iter 591/703: Loss 1.9477\n",
      "  Iter 601/703: Loss 2.2825\n",
      "  Iter 611/703: Loss 2.3717\n",
      "  Iter 621/703: Loss 1.9372\n",
      "  Iter 631/703: Loss 2.4384\n",
      "  Iter 641/703: Loss 2.3462\n",
      "  Iter 651/703: Loss 2.3021\n",
      "  Iter 661/703: Loss 2.0774\n",
      "  Iter 671/703: Loss 2.0462\n",
      "  Iter 681/703: Loss 2.1578\n",
      "  Iter 691/703: Loss 2.4338\n",
      "  Iter 701/703: Loss 2.3799\n",
      "  Iter 703/703: Loss 2.4971\n",
      "Fine Train Loss: 2.2344, Fine Train Acc: 0.4740, Val Acc: 0.4018, Val Loss: 2.2981\n",
      "Time: 2999.77s\n",
      "\n",
      "[Epoch 33/100]\n",
      "  Iter   1/703: Loss 1.8628\n",
      "  Iter  11/703: Loss 2.0418\n",
      "  Iter  21/703: Loss 1.6883\n",
      "  Iter  31/703: Loss 2.4251\n",
      "  Iter  41/703: Loss 2.1754\n",
      "  Iter  51/703: Loss 2.5686\n",
      "  Iter  61/703: Loss 2.2470\n",
      "  Iter  71/703: Loss 2.1738\n",
      "  Iter  81/703: Loss 2.5870\n",
      "  Iter  91/703: Loss 2.2246\n",
      "  Iter 101/703: Loss 2.2360\n",
      "  Iter 111/703: Loss 2.3755\n",
      "  Iter 121/703: Loss 2.6840\n",
      "  Iter 131/703: Loss 2.2180\n",
      "  Iter 141/703: Loss 2.0506\n",
      "  Iter 151/703: Loss 2.0101\n",
      "  Iter 161/703: Loss 2.0780\n",
      "  Iter 171/703: Loss 2.5815\n",
      "  Iter 181/703: Loss 2.3312\n",
      "  Iter 191/703: Loss 2.5697\n",
      "  Iter 201/703: Loss 2.1423\n",
      "  Iter 211/703: Loss 2.4083\n",
      "  Iter 221/703: Loss 2.1671\n",
      "  Iter 231/703: Loss 2.4440\n",
      "  Iter 241/703: Loss 2.2009\n",
      "  Iter 251/703: Loss 2.3522\n",
      "  Iter 261/703: Loss 2.5143\n",
      "  Iter 271/703: Loss 2.5248\n",
      "  Iter 281/703: Loss 2.1987\n",
      "  Iter 291/703: Loss 2.2286\n",
      "  Iter 301/703: Loss 2.1432\n",
      "  Iter 311/703: Loss 1.6916\n",
      "  Iter 321/703: Loss 2.2439\n",
      "  Iter 331/703: Loss 2.0124\n",
      "  Iter 341/703: Loss 2.1892\n",
      "  Iter 351/703: Loss 2.2887\n",
      "  Iter 361/703: Loss 2.1407\n",
      "  Iter 371/703: Loss 2.1207\n",
      "  Iter 381/703: Loss 2.0330\n",
      "  Iter 391/703: Loss 2.0414\n",
      "  Iter 401/703: Loss 2.1202\n",
      "  Iter 411/703: Loss 2.1027\n",
      "  Iter 421/703: Loss 1.8582\n",
      "  Iter 431/703: Loss 1.9724\n",
      "  Iter 441/703: Loss 2.1060\n",
      "  Iter 451/703: Loss 2.0088\n",
      "  Iter 461/703: Loss 2.1461\n",
      "  Iter 471/703: Loss 2.4817\n",
      "  Iter 481/703: Loss 2.0745\n",
      "  Iter 491/703: Loss 2.4768\n",
      "  Iter 501/703: Loss 1.9427\n",
      "  Iter 511/703: Loss 2.1425\n",
      "  Iter 521/703: Loss 2.4085\n",
      "  Iter 531/703: Loss 2.4989\n",
      "  Iter 541/703: Loss 1.9304\n",
      "  Iter 551/703: Loss 2.4999\n",
      "  Iter 561/703: Loss 1.9134\n",
      "  Iter 571/703: Loss 2.4235\n",
      "  Iter 581/703: Loss 2.6123\n",
      "  Iter 591/703: Loss 2.0975\n",
      "  Iter 601/703: Loss 2.2367\n",
      "  Iter 611/703: Loss 2.2280\n",
      "  Iter 621/703: Loss 2.3926\n",
      "  Iter 631/703: Loss 2.1047\n",
      "  Iter 641/703: Loss 1.9917\n",
      "  Iter 651/703: Loss 2.2554\n",
      "  Iter 661/703: Loss 2.2810\n",
      "  Iter 671/703: Loss 2.3241\n",
      "  Iter 681/703: Loss 2.0517\n",
      "  Iter 691/703: Loss 1.9613\n",
      "  Iter 701/703: Loss 2.4280\n",
      "  Iter 703/703: Loss 2.2734\n",
      "Fine Train Loss: 2.2233, Fine Train Acc: 0.4710, Val Acc: 0.4068, Val Loss: 2.3018\n",
      "Time: 3006.63s\n",
      "\n",
      "[Epoch 34/100]\n",
      "  Iter   1/703: Loss 2.0751\n",
      "  Iter  11/703: Loss 2.1649\n",
      "  Iter  21/703: Loss 2.4596\n",
      "  Iter  31/703: Loss 2.0969\n",
      "  Iter  41/703: Loss 1.9118\n",
      "  Iter  51/703: Loss 2.3782\n",
      "  Iter  61/703: Loss 1.9958\n",
      "  Iter  71/703: Loss 1.8228\n",
      "  Iter  81/703: Loss 2.4124\n",
      "  Iter  91/703: Loss 2.1816\n",
      "  Iter 101/703: Loss 1.9896\n",
      "  Iter 111/703: Loss 1.8052\n",
      "  Iter 121/703: Loss 2.4424\n",
      "  Iter 131/703: Loss 2.3703\n",
      "  Iter 141/703: Loss 2.1755\n",
      "  Iter 151/703: Loss 2.1103\n",
      "  Iter 161/703: Loss 2.2833\n",
      "  Iter 171/703: Loss 1.8449\n",
      "  Iter 181/703: Loss 2.1808\n",
      "  Iter 191/703: Loss 2.0862\n",
      "  Iter 201/703: Loss 2.1893\n",
      "  Iter 211/703: Loss 2.3423\n",
      "  Iter 221/703: Loss 2.3891\n",
      "  Iter 231/703: Loss 2.2632\n",
      "  Iter 241/703: Loss 1.8650\n",
      "  Iter 251/703: Loss 2.0028\n",
      "  Iter 261/703: Loss 1.9732\n",
      "  Iter 271/703: Loss 2.3666\n",
      "  Iter 281/703: Loss 1.8510\n",
      "  Iter 291/703: Loss 2.1035\n",
      "  Iter 301/703: Loss 1.8659\n",
      "  Iter 311/703: Loss 2.2274\n",
      "  Iter 321/703: Loss 2.4527\n",
      "  Iter 331/703: Loss 1.9962\n",
      "  Iter 341/703: Loss 2.1808\n",
      "  Iter 351/703: Loss 2.1534\n",
      "  Iter 361/703: Loss 2.3786\n",
      "  Iter 371/703: Loss 2.2895\n",
      "  Iter 381/703: Loss 2.4049\n",
      "  Iter 391/703: Loss 2.4668\n",
      "  Iter 401/703: Loss 2.5504\n",
      "  Iter 411/703: Loss 1.9562\n",
      "  Iter 421/703: Loss 2.6011\n",
      "  Iter 431/703: Loss 2.3731\n",
      "  Iter 441/703: Loss 2.1250\n",
      "  Iter 451/703: Loss 2.2375\n",
      "  Iter 461/703: Loss 2.4102\n",
      "  Iter 471/703: Loss 2.2266\n",
      "  Iter 481/703: Loss 2.4737\n",
      "  Iter 491/703: Loss 2.5480\n",
      "  Iter 501/703: Loss 2.3105\n",
      "  Iter 511/703: Loss 2.0447\n",
      "  Iter 521/703: Loss 2.0856\n",
      "  Iter 531/703: Loss 2.4327\n",
      "  Iter 541/703: Loss 2.3737\n",
      "  Iter 551/703: Loss 2.2211\n",
      "  Iter 561/703: Loss 2.2966\n",
      "  Iter 571/703: Loss 1.9451\n",
      "  Iter 581/703: Loss 2.3589\n",
      "  Iter 591/703: Loss 2.1923\n",
      "  Iter 601/703: Loss 2.4326\n",
      "  Iter 611/703: Loss 2.3334\n",
      "  Iter 621/703: Loss 2.0746\n",
      "  Iter 631/703: Loss 2.0783\n",
      "  Iter 641/703: Loss 2.4489\n",
      "  Iter 651/703: Loss 2.0777\n",
      "  Iter 661/703: Loss 1.9927\n",
      "  Iter 671/703: Loss 2.5316\n",
      "  Iter 681/703: Loss 2.2954\n",
      "  Iter 691/703: Loss 2.0351\n",
      "  Iter 701/703: Loss 2.0692\n",
      "  Iter 703/703: Loss 2.4545\n",
      "Fine Train Loss: 2.2041, Fine Train Acc: 0.4890, Val Acc: 0.4064, Val Loss: 2.2929\n",
      "Time: 3011.96s\n",
      "\n",
      "[Epoch 35/100]\n",
      "  Iter   1/703: Loss 2.1852\n",
      "  Iter  11/703: Loss 2.3411\n",
      "  Iter  21/703: Loss 2.2381\n",
      "  Iter  31/703: Loss 2.2312\n",
      "  Iter  41/703: Loss 2.5567\n",
      "  Iter  51/703: Loss 2.1815\n",
      "  Iter  61/703: Loss 2.2068\n",
      "  Iter  71/703: Loss 2.2695\n",
      "  Iter  81/703: Loss 2.3345\n",
      "  Iter  91/703: Loss 2.1138\n",
      "  Iter 101/703: Loss 2.1517\n",
      "  Iter 111/703: Loss 2.2325\n",
      "  Iter 121/703: Loss 2.4021\n",
      "  Iter 131/703: Loss 2.5212\n",
      "  Iter 141/703: Loss 2.2805\n",
      "  Iter 151/703: Loss 2.1044\n",
      "  Iter 161/703: Loss 2.5777\n",
      "  Iter 171/703: Loss 2.3221\n",
      "  Iter 181/703: Loss 2.3311\n",
      "  Iter 191/703: Loss 2.3934\n",
      "  Iter 201/703: Loss 1.8381\n",
      "  Iter 211/703: Loss 2.3419\n",
      "  Iter 221/703: Loss 2.2831\n",
      "  Iter 231/703: Loss 1.9309\n",
      "  Iter 241/703: Loss 2.1428\n",
      "  Iter 251/703: Loss 2.3139\n",
      "  Iter 261/703: Loss 2.4586\n",
      "  Iter 271/703: Loss 2.0773\n",
      "  Iter 281/703: Loss 1.9552\n",
      "  Iter 291/703: Loss 2.4421\n",
      "  Iter 301/703: Loss 2.5761\n",
      "  Iter 311/703: Loss 2.3056\n",
      "  Iter 321/703: Loss 1.9188\n",
      "  Iter 331/703: Loss 2.5086\n",
      "  Iter 341/703: Loss 2.3046\n",
      "  Iter 351/703: Loss 2.1023\n",
      "  Iter 361/703: Loss 2.2082\n",
      "  Iter 371/703: Loss 2.1989\n",
      "  Iter 381/703: Loss 2.2551\n",
      "  Iter 391/703: Loss 2.4683\n",
      "  Iter 401/703: Loss 2.2603\n",
      "  Iter 411/703: Loss 2.1550\n",
      "  Iter 421/703: Loss 2.0425\n",
      "  Iter 431/703: Loss 2.2000\n",
      "  Iter 441/703: Loss 2.1833\n",
      "  Iter 451/703: Loss 2.0745\n",
      "  Iter 461/703: Loss 2.1529\n",
      "  Iter 471/703: Loss 1.8363\n",
      "  Iter 481/703: Loss 1.8327\n",
      "  Iter 491/703: Loss 2.2636\n",
      "  Iter 501/703: Loss 2.2231\n",
      "  Iter 511/703: Loss 2.2306\n",
      "  Iter 521/703: Loss 2.0782\n",
      "  Iter 531/703: Loss 2.3838\n",
      "  Iter 541/703: Loss 2.1310\n",
      "  Iter 551/703: Loss 2.4500\n",
      "  Iter 561/703: Loss 2.0914\n",
      "  Iter 571/703: Loss 2.3415\n",
      "  Iter 581/703: Loss 2.0160\n",
      "  Iter 591/703: Loss 1.8713\n",
      "  Iter 601/703: Loss 1.7638\n",
      "  Iter 611/703: Loss 2.2535\n",
      "  Iter 621/703: Loss 2.3824\n",
      "  Iter 631/703: Loss 2.1258\n",
      "  Iter 641/703: Loss 2.5802\n",
      "  Iter 651/703: Loss 1.8939\n",
      "  Iter 661/703: Loss 1.8737\n",
      "  Iter 671/703: Loss 1.8474\n",
      "  Iter 681/703: Loss 2.2890\n",
      "  Iter 691/703: Loss 2.2188\n",
      "  Iter 701/703: Loss 2.2270\n",
      "  Iter 703/703: Loss 2.0187\n",
      "Fine Train Loss: 2.1958, Fine Train Acc: 0.4900, Val Acc: 0.4198, Val Loss: 2.2543\n",
      "Time: 3011.77s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch35.pkl\n",
      "\n",
      "[Epoch 36/100]\n",
      "  Iter   1/703: Loss 2.1446\n",
      "  Iter  11/703: Loss 2.2708\n",
      "  Iter  21/703: Loss 2.1903\n",
      "  Iter  31/703: Loss 1.8373\n",
      "  Iter  41/703: Loss 2.0017\n",
      "  Iter  51/703: Loss 2.0103\n",
      "  Iter  61/703: Loss 2.2396\n",
      "  Iter  71/703: Loss 1.9793\n",
      "  Iter  81/703: Loss 2.1653\n",
      "  Iter  91/703: Loss 2.3749\n",
      "  Iter 101/703: Loss 2.1088\n",
      "  Iter 111/703: Loss 2.4233\n",
      "  Iter 121/703: Loss 2.0948\n",
      "  Iter 131/703: Loss 2.1809\n",
      "  Iter 141/703: Loss 2.4816\n",
      "  Iter 151/703: Loss 2.3956\n",
      "  Iter 161/703: Loss 2.0505\n",
      "  Iter 171/703: Loss 2.1287\n",
      "  Iter 181/703: Loss 2.1156\n",
      "  Iter 191/703: Loss 2.1130\n",
      "  Iter 201/703: Loss 2.0379\n",
      "  Iter 211/703: Loss 1.9521\n",
      "  Iter 221/703: Loss 2.1872\n",
      "  Iter 231/703: Loss 1.9944\n",
      "  Iter 241/703: Loss 2.4650\n",
      "  Iter 251/703: Loss 2.1758\n",
      "  Iter 261/703: Loss 2.1286\n",
      "  Iter 271/703: Loss 2.1527\n",
      "  Iter 281/703: Loss 2.2407\n",
      "  Iter 291/703: Loss 2.0113\n",
      "  Iter 301/703: Loss 2.4233\n",
      "  Iter 311/703: Loss 2.0831\n",
      "  Iter 321/703: Loss 2.3257\n",
      "  Iter 331/703: Loss 2.4372\n",
      "  Iter 341/703: Loss 1.9805\n",
      "  Iter 351/703: Loss 2.2128\n",
      "  Iter 361/703: Loss 2.0182\n",
      "  Iter 371/703: Loss 2.1864\n",
      "  Iter 381/703: Loss 2.1317\n",
      "  Iter 391/703: Loss 2.2527\n",
      "  Iter 401/703: Loss 2.1954\n",
      "  Iter 411/703: Loss 2.1712\n",
      "  Iter 421/703: Loss 2.3157\n",
      "  Iter 431/703: Loss 2.2704\n",
      "  Iter 441/703: Loss 2.3629\n",
      "  Iter 451/703: Loss 2.1867\n",
      "  Iter 461/703: Loss 2.1202\n",
      "  Iter 471/703: Loss 2.3517\n",
      "  Iter 481/703: Loss 2.4309\n",
      "  Iter 491/703: Loss 2.1246\n",
      "  Iter 501/703: Loss 2.0646\n",
      "  Iter 511/703: Loss 2.1551\n",
      "  Iter 521/703: Loss 2.1927\n",
      "  Iter 531/703: Loss 2.3307\n",
      "  Iter 541/703: Loss 2.0709\n",
      "  Iter 551/703: Loss 1.9080\n",
      "  Iter 561/703: Loss 2.1692\n",
      "  Iter 571/703: Loss 1.8689\n",
      "  Iter 581/703: Loss 1.9436\n",
      "  Iter 591/703: Loss 1.9542\n",
      "  Iter 601/703: Loss 2.2090\n",
      "  Iter 611/703: Loss 2.0125\n",
      "  Iter 621/703: Loss 2.2939\n",
      "  Iter 631/703: Loss 2.0906\n",
      "  Iter 641/703: Loss 2.2323\n",
      "  Iter 651/703: Loss 1.9428\n",
      "  Iter 661/703: Loss 2.2184\n",
      "  Iter 671/703: Loss 2.1450\n",
      "  Iter 681/703: Loss 1.8650\n",
      "  Iter 691/703: Loss 2.1628\n",
      "  Iter 701/703: Loss 1.9568\n",
      "  Iter 703/703: Loss 2.2713\n",
      "Fine Train Loss: 2.1548, Fine Train Acc: 0.4760, Val Acc: 0.4192, Val Loss: 2.2319\n",
      "Time: 3009.20s\n",
      "\n",
      "[Epoch 37/100]\n",
      "  Iter   1/703: Loss 2.1799\n",
      "  Iter  11/703: Loss 2.3141\n",
      "  Iter  21/703: Loss 1.9689\n",
      "  Iter  31/703: Loss 2.1486\n",
      "  Iter  41/703: Loss 1.9334\n",
      "  Iter  51/703: Loss 2.4481\n",
      "  Iter  61/703: Loss 2.2063\n",
      "  Iter  71/703: Loss 2.0226\n",
      "  Iter  81/703: Loss 2.2335\n",
      "  Iter  91/703: Loss 2.3291\n",
      "  Iter 101/703: Loss 2.2780\n",
      "  Iter 111/703: Loss 2.2374\n",
      "  Iter 121/703: Loss 2.2262\n",
      "  Iter 131/703: Loss 2.3682\n",
      "  Iter 141/703: Loss 2.2532\n",
      "  Iter 151/703: Loss 1.9842\n",
      "  Iter 161/703: Loss 2.4948\n",
      "  Iter 171/703: Loss 2.2476\n",
      "  Iter 181/703: Loss 2.2305\n",
      "  Iter 191/703: Loss 2.1059\n",
      "  Iter 201/703: Loss 2.2633\n",
      "  Iter 211/703: Loss 2.1004\n",
      "  Iter 221/703: Loss 2.0144\n",
      "  Iter 231/703: Loss 2.5119\n",
      "  Iter 241/703: Loss 2.2581\n",
      "  Iter 251/703: Loss 2.5421\n",
      "  Iter 261/703: Loss 2.0700\n",
      "  Iter 271/703: Loss 1.9818\n",
      "  Iter 281/703: Loss 2.0638\n",
      "  Iter 291/703: Loss 2.0903\n",
      "  Iter 301/703: Loss 1.7726\n",
      "  Iter 311/703: Loss 2.1766\n",
      "  Iter 321/703: Loss 1.9645\n",
      "  Iter 331/703: Loss 2.1570\n",
      "  Iter 341/703: Loss 2.0930\n",
      "  Iter 351/703: Loss 2.2445\n",
      "  Iter 361/703: Loss 2.1806\n",
      "  Iter 371/703: Loss 2.1971\n",
      "  Iter 381/703: Loss 1.7700\n",
      "  Iter 391/703: Loss 2.4509\n",
      "  Iter 401/703: Loss 2.2307\n",
      "  Iter 411/703: Loss 2.4655\n",
      "  Iter 421/703: Loss 2.6756\n",
      "  Iter 431/703: Loss 1.9283\n",
      "  Iter 441/703: Loss 2.5955\n",
      "  Iter 451/703: Loss 2.5259\n",
      "  Iter 461/703: Loss 2.0013\n",
      "  Iter 471/703: Loss 2.1154\n",
      "  Iter 481/703: Loss 2.4122\n",
      "  Iter 491/703: Loss 2.2568\n",
      "  Iter 501/703: Loss 2.0273\n",
      "  Iter 511/703: Loss 1.9967\n",
      "  Iter 521/703: Loss 2.3541\n",
      "  Iter 531/703: Loss 2.3837\n",
      "  Iter 541/703: Loss 1.9982\n",
      "  Iter 551/703: Loss 2.2478\n",
      "  Iter 561/703: Loss 1.9453\n",
      "  Iter 571/703: Loss 1.8791\n",
      "  Iter 581/703: Loss 1.9427\n",
      "  Iter 591/703: Loss 2.1745\n",
      "  Iter 601/703: Loss 1.9313\n",
      "  Iter 611/703: Loss 2.0532\n",
      "  Iter 621/703: Loss 1.9708\n",
      "  Iter 631/703: Loss 2.1154\n",
      "  Iter 641/703: Loss 2.0507\n",
      "  Iter 651/703: Loss 2.1770\n",
      "  Iter 661/703: Loss 2.2766\n",
      "  Iter 671/703: Loss 2.1272\n",
      "  Iter 681/703: Loss 2.1943\n",
      "  Iter 691/703: Loss 2.0398\n",
      "  Iter 701/703: Loss 1.8872\n",
      "  Iter 703/703: Loss 2.0446\n",
      "Fine Train Loss: 2.1485, Fine Train Acc: 0.4720, Val Acc: 0.4212, Val Loss: 2.2329\n",
      "Time: 3010.31s\n",
      "\n",
      "[Epoch 38/100]\n",
      "  Iter   1/703: Loss 2.1549\n",
      "  Iter  11/703: Loss 2.2205\n",
      "  Iter  21/703: Loss 2.1342\n",
      "  Iter  31/703: Loss 2.0849\n",
      "  Iter  41/703: Loss 2.0801\n",
      "  Iter  51/703: Loss 1.6701\n",
      "  Iter  61/703: Loss 2.0062\n",
      "  Iter  71/703: Loss 2.2080\n",
      "  Iter  81/703: Loss 2.3545\n",
      "  Iter  91/703: Loss 2.2909\n",
      "  Iter 101/703: Loss 2.2274\n",
      "  Iter 111/703: Loss 2.2526\n",
      "  Iter 121/703: Loss 2.1882\n",
      "  Iter 131/703: Loss 1.9445\n",
      "  Iter 141/703: Loss 2.3831\n",
      "  Iter 151/703: Loss 2.0435\n",
      "  Iter 161/703: Loss 2.3751\n",
      "  Iter 171/703: Loss 2.0358\n",
      "  Iter 181/703: Loss 2.0182\n",
      "  Iter 191/703: Loss 2.1981\n",
      "  Iter 201/703: Loss 2.0529\n",
      "  Iter 211/703: Loss 2.6500\n",
      "  Iter 221/703: Loss 2.5027\n",
      "  Iter 231/703: Loss 1.9528\n",
      "  Iter 241/703: Loss 2.2489\n",
      "  Iter 251/703: Loss 1.8789\n",
      "  Iter 261/703: Loss 1.8237\n",
      "  Iter 271/703: Loss 2.3381\n",
      "  Iter 281/703: Loss 2.1354\n",
      "  Iter 291/703: Loss 2.4384\n",
      "  Iter 301/703: Loss 2.2601\n",
      "  Iter 311/703: Loss 2.1088\n",
      "  Iter 321/703: Loss 2.1177\n",
      "  Iter 331/703: Loss 2.1651\n",
      "  Iter 341/703: Loss 2.1421\n",
      "  Iter 351/703: Loss 1.9667\n",
      "  Iter 361/703: Loss 1.8760\n",
      "  Iter 371/703: Loss 2.2834\n",
      "  Iter 381/703: Loss 2.2878\n",
      "  Iter 391/703: Loss 1.7851\n",
      "  Iter 401/703: Loss 2.1886\n",
      "  Iter 411/703: Loss 1.9252\n",
      "  Iter 421/703: Loss 2.0362\n",
      "  Iter 431/703: Loss 2.6941\n",
      "  Iter 441/703: Loss 1.9318\n",
      "  Iter 451/703: Loss 2.1922\n",
      "  Iter 461/703: Loss 2.1748\n",
      "  Iter 471/703: Loss 2.2063\n",
      "  Iter 481/703: Loss 2.0165\n",
      "  Iter 491/703: Loss 2.5074\n",
      "  Iter 501/703: Loss 2.3782\n",
      "  Iter 511/703: Loss 2.0100\n",
      "  Iter 521/703: Loss 2.0773\n",
      "  Iter 531/703: Loss 1.9536\n",
      "  Iter 541/703: Loss 2.7934\n",
      "  Iter 551/703: Loss 2.3957\n",
      "  Iter 561/703: Loss 1.8343\n",
      "  Iter 571/703: Loss 2.2030\n",
      "  Iter 581/703: Loss 2.1690\n",
      "  Iter 591/703: Loss 1.7311\n",
      "  Iter 601/703: Loss 2.6495\n",
      "  Iter 611/703: Loss 2.0167\n",
      "  Iter 621/703: Loss 1.9936\n",
      "  Iter 631/703: Loss 2.0939\n",
      "  Iter 641/703: Loss 2.4663\n",
      "  Iter 651/703: Loss 1.9834\n",
      "  Iter 661/703: Loss 2.2595\n",
      "  Iter 671/703: Loss 2.0330\n",
      "  Iter 681/703: Loss 2.1436\n",
      "  Iter 691/703: Loss 2.2624\n",
      "  Iter 701/703: Loss 2.1896\n",
      "  Iter 703/703: Loss 2.4665\n",
      "Fine Train Loss: 2.1445, Fine Train Acc: 0.5090, Val Acc: 0.4184, Val Loss: 2.2240\n",
      "Time: 3015.91s\n",
      "\n",
      "[Epoch 39/100]\n",
      "  Iter   1/703: Loss 2.3687\n",
      "  Iter  11/703: Loss 2.2452\n",
      "  Iter  21/703: Loss 2.2551\n",
      "  Iter  31/703: Loss 2.0340\n",
      "  Iter  41/703: Loss 2.1861\n",
      "  Iter  51/703: Loss 1.7960\n",
      "  Iter  61/703: Loss 1.9734\n",
      "  Iter  71/703: Loss 1.8759\n",
      "  Iter  81/703: Loss 2.3668\n",
      "  Iter  91/703: Loss 2.1153\n",
      "  Iter 101/703: Loss 2.1814\n",
      "  Iter 111/703: Loss 1.9752\n",
      "  Iter 121/703: Loss 2.1539\n",
      "  Iter 131/703: Loss 2.1461\n",
      "  Iter 141/703: Loss 2.1896\n",
      "  Iter 151/703: Loss 2.3769\n",
      "  Iter 161/703: Loss 2.0348\n",
      "  Iter 171/703: Loss 2.1129\n",
      "  Iter 181/703: Loss 2.3277\n",
      "  Iter 191/703: Loss 1.8742\n",
      "  Iter 201/703: Loss 2.1532\n",
      "  Iter 211/703: Loss 2.0748\n",
      "  Iter 221/703: Loss 2.0952\n",
      "  Iter 231/703: Loss 2.0482\n",
      "  Iter 241/703: Loss 2.1785\n",
      "  Iter 251/703: Loss 2.2735\n",
      "  Iter 261/703: Loss 1.6482\n",
      "  Iter 271/703: Loss 1.9654\n",
      "  Iter 281/703: Loss 1.9711\n",
      "  Iter 291/703: Loss 2.3284\n",
      "  Iter 301/703: Loss 2.0435\n",
      "  Iter 311/703: Loss 2.2881\n",
      "  Iter 321/703: Loss 2.0386\n",
      "  Iter 331/703: Loss 1.6744\n",
      "  Iter 341/703: Loss 1.9534\n",
      "  Iter 351/703: Loss 1.8150\n",
      "  Iter 361/703: Loss 2.0340\n",
      "  Iter 371/703: Loss 2.1341\n",
      "  Iter 381/703: Loss 2.3750\n",
      "  Iter 391/703: Loss 2.1667\n",
      "  Iter 401/703: Loss 2.3802\n",
      "  Iter 411/703: Loss 1.9323\n",
      "  Iter 421/703: Loss 2.1911\n",
      "  Iter 431/703: Loss 1.3843\n",
      "  Iter 441/703: Loss 2.0663\n",
      "  Iter 451/703: Loss 2.2650\n",
      "  Iter 461/703: Loss 2.5454\n",
      "  Iter 471/703: Loss 2.2239\n",
      "  Iter 481/703: Loss 2.4279\n",
      "  Iter 491/703: Loss 2.0566\n",
      "  Iter 501/703: Loss 2.2074\n",
      "  Iter 511/703: Loss 2.2581\n",
      "  Iter 521/703: Loss 2.1044\n",
      "  Iter 531/703: Loss 2.1259\n",
      "  Iter 541/703: Loss 2.2288\n",
      "  Iter 551/703: Loss 2.1956\n",
      "  Iter 561/703: Loss 2.4374\n",
      "  Iter 571/703: Loss 2.0763\n",
      "  Iter 581/703: Loss 2.2016\n",
      "  Iter 591/703: Loss 2.0206\n",
      "  Iter 601/703: Loss 2.3251\n",
      "  Iter 611/703: Loss 2.2973\n",
      "  Iter 621/703: Loss 1.8995\n",
      "  Iter 631/703: Loss 1.9458\n",
      "  Iter 641/703: Loss 2.3592\n",
      "  Iter 651/703: Loss 2.4798\n",
      "  Iter 661/703: Loss 2.3041\n",
      "  Iter 671/703: Loss 2.0111\n",
      "  Iter 681/703: Loss 2.0036\n",
      "  Iter 691/703: Loss 2.2917\n",
      "  Iter 701/703: Loss 1.7379\n",
      "  Iter 703/703: Loss 2.3657\n",
      "Fine Train Loss: 2.1109, Fine Train Acc: 0.4900, Val Acc: 0.4254, Val Loss: 2.2085\n",
      "Time: 3004.93s\n",
      "\n",
      "[Epoch 40/100]\n",
      "  Iter   1/703: Loss 1.9496\n",
      "  Iter  11/703: Loss 2.2321\n",
      "  Iter  21/703: Loss 2.2391\n",
      "  Iter  31/703: Loss 2.1330\n",
      "  Iter  41/703: Loss 1.9299\n",
      "  Iter  51/703: Loss 1.7451\n",
      "  Iter  61/703: Loss 2.2546\n",
      "  Iter  71/703: Loss 2.0696\n",
      "  Iter  81/703: Loss 1.8460\n",
      "  Iter  91/703: Loss 2.1639\n",
      "  Iter 101/703: Loss 2.3040\n",
      "  Iter 111/703: Loss 1.8738\n",
      "  Iter 121/703: Loss 2.4391\n",
      "  Iter 131/703: Loss 1.9033\n",
      "  Iter 141/703: Loss 2.0436\n",
      "  Iter 151/703: Loss 1.8009\n",
      "  Iter 161/703: Loss 2.3263\n",
      "  Iter 171/703: Loss 2.2875\n",
      "  Iter 181/703: Loss 1.9904\n",
      "  Iter 191/703: Loss 2.0952\n",
      "  Iter 201/703: Loss 2.3405\n",
      "  Iter 211/703: Loss 1.9005\n",
      "  Iter 221/703: Loss 2.3929\n",
      "  Iter 231/703: Loss 1.9643\n",
      "  Iter 241/703: Loss 1.8694\n",
      "  Iter 251/703: Loss 2.2303\n",
      "  Iter 261/703: Loss 1.8280\n",
      "  Iter 271/703: Loss 1.9429\n",
      "  Iter 281/703: Loss 2.1942\n",
      "  Iter 291/703: Loss 1.6862\n",
      "  Iter 301/703: Loss 2.0036\n",
      "  Iter 311/703: Loss 1.8927\n",
      "  Iter 321/703: Loss 2.1277\n",
      "  Iter 331/703: Loss 2.1445\n",
      "  Iter 341/703: Loss 2.0229\n",
      "  Iter 351/703: Loss 1.8864\n",
      "  Iter 361/703: Loss 2.0185\n",
      "  Iter 371/703: Loss 2.0762\n",
      "  Iter 381/703: Loss 1.9556\n",
      "  Iter 391/703: Loss 2.1439\n",
      "  Iter 401/703: Loss 2.2198\n",
      "  Iter 411/703: Loss 2.1123\n",
      "  Iter 421/703: Loss 2.0529\n",
      "  Iter 431/703: Loss 1.9595\n",
      "  Iter 441/703: Loss 1.8851\n",
      "  Iter 451/703: Loss 1.8735\n",
      "  Iter 461/703: Loss 2.1015\n",
      "  Iter 471/703: Loss 2.2466\n",
      "  Iter 481/703: Loss 2.0723\n",
      "  Iter 491/703: Loss 1.8132\n",
      "  Iter 501/703: Loss 2.0682\n",
      "  Iter 511/703: Loss 1.7264\n",
      "  Iter 521/703: Loss 1.8295\n",
      "  Iter 531/703: Loss 2.5302\n",
      "  Iter 541/703: Loss 1.9710\n",
      "  Iter 551/703: Loss 1.9723\n",
      "  Iter 561/703: Loss 1.7551\n",
      "  Iter 571/703: Loss 2.1046\n",
      "  Iter 581/703: Loss 2.1710\n",
      "  Iter 591/703: Loss 2.0836\n",
      "  Iter 601/703: Loss 1.9249\n",
      "  Iter 611/703: Loss 2.5021\n",
      "  Iter 621/703: Loss 1.8047\n",
      "  Iter 631/703: Loss 1.9689\n",
      "  Iter 641/703: Loss 2.0185\n",
      "  Iter 651/703: Loss 1.9434\n",
      "  Iter 661/703: Loss 1.7580\n",
      "  Iter 671/703: Loss 2.1802\n",
      "  Iter 681/703: Loss 1.7747\n",
      "  Iter 691/703: Loss 1.8136\n",
      "  Iter 701/703: Loss 1.8329\n",
      "  Iter 703/703: Loss 2.2489\n",
      "Fine Train Loss: 2.0882, Fine Train Acc: 0.5040, Val Acc: 0.4294, Val Loss: 2.1976\n",
      "Time: 3007.76s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch40.pkl\n",
      "\n",
      "[Epoch 41/100]\n",
      "  Iter   1/703: Loss 2.1372\n",
      "  Iter  11/703: Loss 2.3958\n",
      "  Iter  21/703: Loss 2.1262\n",
      "  Iter  31/703: Loss 2.0152\n",
      "  Iter  41/703: Loss 2.1922\n",
      "  Iter  51/703: Loss 2.2142\n",
      "  Iter  61/703: Loss 1.7566\n",
      "  Iter  71/703: Loss 2.2076\n",
      "  Iter  81/703: Loss 1.9979\n",
      "  Iter  91/703: Loss 2.0402\n",
      "  Iter 101/703: Loss 2.0613\n",
      "  Iter 111/703: Loss 2.3207\n",
      "  Iter 121/703: Loss 2.0043\n",
      "  Iter 131/703: Loss 2.2411\n",
      "  Iter 141/703: Loss 1.6367\n",
      "  Iter 151/703: Loss 2.0294\n",
      "  Iter 161/703: Loss 1.9933\n",
      "  Iter 171/703: Loss 1.7168\n",
      "  Iter 181/703: Loss 2.0626\n",
      "  Iter 191/703: Loss 2.3270\n",
      "  Iter 201/703: Loss 2.2962\n",
      "  Iter 211/703: Loss 1.8714\n",
      "  Iter 221/703: Loss 2.1665\n",
      "  Iter 231/703: Loss 1.9761\n",
      "  Iter 241/703: Loss 2.0377\n",
      "  Iter 251/703: Loss 1.8573\n",
      "  Iter 261/703: Loss 1.7051\n",
      "  Iter 271/703: Loss 1.9830\n",
      "  Iter 281/703: Loss 2.0219\n",
      "  Iter 291/703: Loss 2.0488\n",
      "  Iter 301/703: Loss 1.8998\n",
      "  Iter 311/703: Loss 1.9803\n",
      "  Iter 321/703: Loss 2.3327\n",
      "  Iter 331/703: Loss 2.1476\n",
      "  Iter 341/703: Loss 1.9971\n",
      "  Iter 351/703: Loss 1.9244\n",
      "  Iter 361/703: Loss 1.9077\n",
      "  Iter 371/703: Loss 2.5003\n",
      "  Iter 381/703: Loss 2.2410\n",
      "  Iter 391/703: Loss 1.6742\n",
      "  Iter 401/703: Loss 2.0375\n",
      "  Iter 411/703: Loss 2.2047\n",
      "  Iter 421/703: Loss 2.0245\n",
      "  Iter 431/703: Loss 1.9089\n",
      "  Iter 441/703: Loss 2.4235\n",
      "  Iter 451/703: Loss 2.4903\n",
      "  Iter 461/703: Loss 1.8092\n",
      "  Iter 471/703: Loss 2.4572\n",
      "  Iter 481/703: Loss 2.2642\n",
      "  Iter 491/703: Loss 1.9654\n",
      "  Iter 501/703: Loss 2.3576\n",
      "  Iter 511/703: Loss 1.8917\n",
      "  Iter 521/703: Loss 1.6139\n",
      "  Iter 531/703: Loss 2.2393\n",
      "  Iter 541/703: Loss 2.0914\n",
      "  Iter 551/703: Loss 1.6350\n",
      "  Iter 561/703: Loss 1.9250\n",
      "  Iter 571/703: Loss 1.7689\n",
      "  Iter 581/703: Loss 2.0679\n",
      "  Iter 591/703: Loss 2.2547\n",
      "  Iter 601/703: Loss 2.2399\n",
      "  Iter 611/703: Loss 2.3450\n",
      "  Iter 621/703: Loss 2.1981\n",
      "  Iter 631/703: Loss 2.1368\n",
      "  Iter 641/703: Loss 2.1749\n",
      "  Iter 651/703: Loss 2.2939\n",
      "  Iter 661/703: Loss 2.0010\n",
      "  Iter 671/703: Loss 2.0589\n",
      "  Iter 681/703: Loss 1.8407\n",
      "  Iter 691/703: Loss 2.5385\n",
      "  Iter 701/703: Loss 1.9882\n",
      "  Iter 703/703: Loss 1.9985\n",
      "Fine Train Loss: 2.0850, Fine Train Acc: 0.5130, Val Acc: 0.4174, Val Loss: 2.2079\n",
      "Time: 3013.54s\n",
      "\n",
      "[Epoch 42/100]\n",
      "  Iter   1/703: Loss 2.2019\n",
      "  Iter  11/703: Loss 2.1954\n",
      "  Iter  21/703: Loss 2.3872\n",
      "  Iter  31/703: Loss 2.1112\n",
      "  Iter  41/703: Loss 2.0376\n",
      "  Iter  51/703: Loss 2.0983\n",
      "  Iter  61/703: Loss 2.0923\n",
      "  Iter  71/703: Loss 1.6768\n",
      "  Iter  81/703: Loss 2.0529\n",
      "  Iter  91/703: Loss 2.0366\n",
      "  Iter 101/703: Loss 2.0729\n",
      "  Iter 111/703: Loss 1.8673\n",
      "  Iter 121/703: Loss 1.6469\n",
      "  Iter 131/703: Loss 2.0115\n",
      "  Iter 141/703: Loss 1.6974\n",
      "  Iter 151/703: Loss 2.2025\n",
      "  Iter 161/703: Loss 2.0923\n",
      "  Iter 171/703: Loss 2.3167\n",
      "  Iter 181/703: Loss 2.3326\n",
      "  Iter 191/703: Loss 1.8182\n",
      "  Iter 201/703: Loss 1.7520\n",
      "  Iter 211/703: Loss 2.0583\n",
      "  Iter 221/703: Loss 1.8099\n",
      "  Iter 231/703: Loss 2.1168\n",
      "  Iter 241/703: Loss 2.0323\n",
      "  Iter 251/703: Loss 1.9871\n",
      "  Iter 261/703: Loss 2.0710\n",
      "  Iter 271/703: Loss 2.3826\n",
      "  Iter 281/703: Loss 2.4791\n",
      "  Iter 291/703: Loss 1.7520\n",
      "  Iter 301/703: Loss 2.0418\n",
      "  Iter 311/703: Loss 1.9774\n",
      "  Iter 321/703: Loss 1.7508\n",
      "  Iter 331/703: Loss 1.9495\n",
      "  Iter 341/703: Loss 1.7459\n",
      "  Iter 351/703: Loss 1.7025\n",
      "  Iter 361/703: Loss 2.2221\n",
      "  Iter 371/703: Loss 2.1209\n",
      "  Iter 381/703: Loss 2.1644\n",
      "  Iter 391/703: Loss 1.8698\n",
      "  Iter 401/703: Loss 1.9443\n",
      "  Iter 411/703: Loss 2.1879\n",
      "  Iter 421/703: Loss 2.2449\n",
      "  Iter 431/703: Loss 1.8101\n",
      "  Iter 441/703: Loss 1.8232\n",
      "  Iter 451/703: Loss 2.2892\n",
      "  Iter 461/703: Loss 2.2705\n",
      "  Iter 471/703: Loss 2.0108\n",
      "  Iter 481/703: Loss 1.9575\n",
      "  Iter 491/703: Loss 2.1819\n",
      "  Iter 501/703: Loss 1.9477\n",
      "  Iter 511/703: Loss 1.9120\n",
      "  Iter 521/703: Loss 2.2353\n",
      "  Iter 531/703: Loss 1.9925\n",
      "  Iter 541/703: Loss 2.1984\n",
      "  Iter 551/703: Loss 2.0446\n",
      "  Iter 561/703: Loss 2.2161\n",
      "  Iter 571/703: Loss 2.2872\n",
      "  Iter 581/703: Loss 2.1279\n",
      "  Iter 591/703: Loss 1.6738\n",
      "  Iter 601/703: Loss 2.3634\n",
      "  Iter 611/703: Loss 2.1496\n",
      "  Iter 621/703: Loss 2.2304\n",
      "  Iter 631/703: Loss 2.0153\n",
      "  Iter 641/703: Loss 1.7454\n",
      "  Iter 651/703: Loss 1.8155\n",
      "  Iter 661/703: Loss 2.0468\n",
      "  Iter 671/703: Loss 2.0930\n",
      "  Iter 681/703: Loss 2.2316\n",
      "  Iter 691/703: Loss 2.3672\n",
      "  Iter 701/703: Loss 2.2994\n",
      "  Iter 703/703: Loss 1.9977\n",
      "Fine Train Loss: 2.0676, Fine Train Acc: 0.5270, Val Acc: 0.4388, Val Loss: 2.1604\n",
      "Time: 3017.34s\n",
      "\n",
      "[Epoch 43/100]\n",
      "  Iter   1/703: Loss 2.0708\n",
      "  Iter  11/703: Loss 1.9435\n",
      "  Iter  21/703: Loss 2.3546\n",
      "  Iter  31/703: Loss 2.2742\n",
      "  Iter  41/703: Loss 1.8058\n",
      "  Iter  51/703: Loss 1.9147\n",
      "  Iter  61/703: Loss 1.8599\n",
      "  Iter  71/703: Loss 1.9904\n",
      "  Iter  81/703: Loss 2.1050\n",
      "  Iter  91/703: Loss 2.1100\n",
      "  Iter 101/703: Loss 2.1637\n",
      "  Iter 111/703: Loss 2.2553\n",
      "  Iter 121/703: Loss 1.9659\n",
      "  Iter 131/703: Loss 2.0861\n",
      "  Iter 141/703: Loss 2.4049\n",
      "  Iter 151/703: Loss 1.9808\n",
      "  Iter 161/703: Loss 1.9320\n",
      "  Iter 171/703: Loss 2.0840\n",
      "  Iter 181/703: Loss 1.7982\n",
      "  Iter 191/703: Loss 1.7614\n",
      "  Iter 201/703: Loss 2.2407\n",
      "  Iter 211/703: Loss 1.9961\n",
      "  Iter 221/703: Loss 2.1781\n",
      "  Iter 231/703: Loss 1.8876\n",
      "  Iter 241/703: Loss 2.0413\n",
      "  Iter 251/703: Loss 1.7053\n",
      "  Iter 261/703: Loss 1.9486\n",
      "  Iter 271/703: Loss 1.8644\n",
      "  Iter 281/703: Loss 1.9106\n",
      "  Iter 291/703: Loss 2.0644\n",
      "  Iter 301/703: Loss 1.8315\n",
      "  Iter 311/703: Loss 2.2083\n",
      "  Iter 321/703: Loss 1.6296\n",
      "  Iter 331/703: Loss 1.7417\n",
      "  Iter 341/703: Loss 1.9872\n",
      "  Iter 351/703: Loss 2.0081\n",
      "  Iter 361/703: Loss 2.0658\n",
      "  Iter 371/703: Loss 2.3242\n",
      "  Iter 381/703: Loss 1.9062\n",
      "  Iter 391/703: Loss 1.9099\n",
      "  Iter 401/703: Loss 2.1849\n",
      "  Iter 411/703: Loss 1.6712\n",
      "  Iter 421/703: Loss 1.8818\n",
      "  Iter 431/703: Loss 2.2320\n",
      "  Iter 441/703: Loss 2.0731\n",
      "  Iter 451/703: Loss 2.1655\n",
      "  Iter 461/703: Loss 2.3668\n",
      "  Iter 471/703: Loss 1.8594\n",
      "  Iter 481/703: Loss 2.0055\n",
      "  Iter 491/703: Loss 1.8850\n",
      "  Iter 501/703: Loss 2.0669\n",
      "  Iter 511/703: Loss 2.0270\n",
      "  Iter 521/703: Loss 2.5855\n",
      "  Iter 531/703: Loss 1.8613\n",
      "  Iter 541/703: Loss 1.8933\n",
      "  Iter 551/703: Loss 2.1542\n",
      "  Iter 561/703: Loss 2.0017\n",
      "  Iter 571/703: Loss 2.0756\n",
      "  Iter 581/703: Loss 2.3393\n",
      "  Iter 591/703: Loss 2.4092\n",
      "  Iter 601/703: Loss 1.6222\n",
      "  Iter 611/703: Loss 1.9517\n",
      "  Iter 621/703: Loss 2.3243\n",
      "  Iter 631/703: Loss 1.9001\n",
      "  Iter 641/703: Loss 1.9091\n",
      "  Iter 651/703: Loss 2.1487\n",
      "  Iter 661/703: Loss 1.9901\n",
      "  Iter 671/703: Loss 1.8801\n",
      "  Iter 681/703: Loss 1.6339\n",
      "  Iter 691/703: Loss 1.7733\n",
      "  Iter 701/703: Loss 1.9790\n",
      "  Iter 703/703: Loss 2.3699\n",
      "Fine Train Loss: 2.0596, Fine Train Acc: 0.5140, Val Acc: 0.4426, Val Loss: 2.1537\n",
      "Time: 3019.61s\n",
      "\n",
      "[Epoch 44/100]\n",
      "  Iter   1/703: Loss 2.0265\n",
      "  Iter  11/703: Loss 2.0427\n",
      "  Iter  21/703: Loss 1.9807\n",
      "  Iter  31/703: Loss 1.9759\n",
      "  Iter  41/703: Loss 2.1005\n",
      "  Iter  51/703: Loss 2.2513\n",
      "  Iter  61/703: Loss 1.9326\n",
      "  Iter  71/703: Loss 1.9492\n",
      "  Iter  81/703: Loss 1.8266\n",
      "  Iter  91/703: Loss 2.2640\n",
      "  Iter 101/703: Loss 2.1478\n",
      "  Iter 111/703: Loss 1.9891\n",
      "  Iter 121/703: Loss 1.7119\n",
      "  Iter 131/703: Loss 1.8806\n",
      "  Iter 141/703: Loss 2.1508\n",
      "  Iter 151/703: Loss 1.7677\n",
      "  Iter 161/703: Loss 2.0955\n",
      "  Iter 171/703: Loss 2.0901\n",
      "  Iter 181/703: Loss 1.8789\n",
      "  Iter 191/703: Loss 2.5791\n",
      "  Iter 201/703: Loss 2.0754\n",
      "  Iter 211/703: Loss 2.3173\n",
      "  Iter 221/703: Loss 1.8325\n",
      "  Iter 231/703: Loss 1.9599\n",
      "  Iter 241/703: Loss 2.1524\n",
      "  Iter 251/703: Loss 2.0103\n",
      "  Iter 261/703: Loss 2.2571\n",
      "  Iter 271/703: Loss 2.1640\n",
      "  Iter 281/703: Loss 1.8044\n",
      "  Iter 291/703: Loss 1.9367\n",
      "  Iter 301/703: Loss 1.9829\n",
      "  Iter 311/703: Loss 1.7662\n",
      "  Iter 321/703: Loss 2.2689\n",
      "  Iter 331/703: Loss 2.2050\n",
      "  Iter 341/703: Loss 2.0174\n",
      "  Iter 351/703: Loss 2.0248\n",
      "  Iter 361/703: Loss 2.0258\n",
      "  Iter 371/703: Loss 2.4610\n",
      "  Iter 381/703: Loss 1.9431\n",
      "  Iter 391/703: Loss 2.4467\n",
      "  Iter 401/703: Loss 1.9747\n",
      "  Iter 411/703: Loss 2.0173\n",
      "  Iter 421/703: Loss 1.8349\n",
      "  Iter 431/703: Loss 1.8556\n",
      "  Iter 441/703: Loss 1.9275\n",
      "  Iter 451/703: Loss 1.8048\n",
      "  Iter 461/703: Loss 2.2077\n",
      "  Iter 471/703: Loss 1.7561\n",
      "  Iter 481/703: Loss 1.9546\n",
      "  Iter 491/703: Loss 2.1996\n",
      "  Iter 501/703: Loss 1.8774\n",
      "  Iter 511/703: Loss 2.1184\n",
      "  Iter 521/703: Loss 2.0195\n",
      "  Iter 531/703: Loss 1.9815\n",
      "  Iter 541/703: Loss 2.2811\n",
      "  Iter 551/703: Loss 2.2162\n",
      "  Iter 561/703: Loss 1.8816\n",
      "  Iter 571/703: Loss 2.0304\n",
      "  Iter 581/703: Loss 2.0671\n",
      "  Iter 591/703: Loss 2.2870\n",
      "  Iter 601/703: Loss 1.9771\n",
      "  Iter 611/703: Loss 2.1430\n",
      "  Iter 621/703: Loss 1.6225\n",
      "  Iter 631/703: Loss 2.1727\n",
      "  Iter 641/703: Loss 1.8240\n",
      "  Iter 651/703: Loss 1.9455\n",
      "  Iter 661/703: Loss 2.0480\n",
      "  Iter 671/703: Loss 1.8066\n",
      "  Iter 681/703: Loss 2.2346\n",
      "  Iter 691/703: Loss 1.9652\n",
      "  Iter 701/703: Loss 2.2717\n",
      "  Iter 703/703: Loss 1.8421\n",
      "Fine Train Loss: 2.0407, Fine Train Acc: 0.5160, Val Acc: 0.4300, Val Loss: 2.1459\n",
      "Time: 3014.77s\n",
      "\n",
      "[Epoch 45/100]\n",
      "  Iter   1/703: Loss 1.8079\n",
      "  Iter  11/703: Loss 2.0128\n",
      "  Iter  21/703: Loss 2.0780\n",
      "  Iter  31/703: Loss 2.1566\n",
      "  Iter  41/703: Loss 2.1974\n",
      "  Iter  51/703: Loss 2.1304\n",
      "  Iter  61/703: Loss 1.9896\n",
      "  Iter  71/703: Loss 2.0796\n",
      "  Iter  81/703: Loss 2.0682\n",
      "  Iter  91/703: Loss 2.0883\n",
      "  Iter 101/703: Loss 1.7976\n",
      "  Iter 111/703: Loss 2.0068\n",
      "  Iter 121/703: Loss 1.9124\n",
      "  Iter 131/703: Loss 2.2793\n",
      "  Iter 141/703: Loss 2.1272\n",
      "  Iter 151/703: Loss 2.1636\n",
      "  Iter 161/703: Loss 1.8362\n",
      "  Iter 171/703: Loss 2.1431\n",
      "  Iter 181/703: Loss 2.0228\n",
      "  Iter 191/703: Loss 2.0832\n",
      "  Iter 201/703: Loss 1.9183\n",
      "  Iter 211/703: Loss 2.2035\n",
      "  Iter 221/703: Loss 1.9990\n",
      "  Iter 231/703: Loss 1.9707\n",
      "  Iter 241/703: Loss 1.9896\n",
      "  Iter 251/703: Loss 1.9879\n",
      "  Iter 261/703: Loss 2.1949\n",
      "  Iter 271/703: Loss 1.9635\n",
      "  Iter 281/703: Loss 2.4271\n",
      "  Iter 291/703: Loss 2.0806\n",
      "  Iter 301/703: Loss 2.1510\n",
      "  Iter 311/703: Loss 1.9672\n",
      "  Iter 321/703: Loss 2.0529\n",
      "  Iter 331/703: Loss 1.9058\n",
      "  Iter 341/703: Loss 1.8650\n",
      "  Iter 351/703: Loss 1.9905\n",
      "  Iter 361/703: Loss 1.9438\n",
      "  Iter 371/703: Loss 1.8668\n",
      "  Iter 381/703: Loss 2.1249\n",
      "  Iter 391/703: Loss 1.8327\n",
      "  Iter 401/703: Loss 2.2422\n",
      "  Iter 411/703: Loss 2.2476\n",
      "  Iter 421/703: Loss 1.9092\n",
      "  Iter 431/703: Loss 2.2312\n",
      "  Iter 441/703: Loss 2.0874\n",
      "  Iter 451/703: Loss 1.9879\n",
      "  Iter 461/703: Loss 2.0175\n",
      "  Iter 471/703: Loss 2.5158\n",
      "  Iter 481/703: Loss 2.3652\n",
      "  Iter 491/703: Loss 1.8735\n",
      "  Iter 501/703: Loss 2.0315\n",
      "  Iter 511/703: Loss 2.2441\n",
      "  Iter 521/703: Loss 2.2155\n",
      "  Iter 531/703: Loss 1.9650\n",
      "  Iter 541/703: Loss 2.3564\n",
      "  Iter 551/703: Loss 1.8986\n",
      "  Iter 561/703: Loss 1.7824\n",
      "  Iter 571/703: Loss 2.1419\n",
      "  Iter 581/703: Loss 1.9592\n",
      "  Iter 591/703: Loss 1.7783\n",
      "  Iter 601/703: Loss 1.8312\n",
      "  Iter 611/703: Loss 2.0322\n",
      "  Iter 621/703: Loss 1.9746\n",
      "  Iter 631/703: Loss 2.2788\n",
      "  Iter 641/703: Loss 2.0841\n",
      "  Iter 651/703: Loss 2.0033\n",
      "  Iter 661/703: Loss 2.0212\n",
      "  Iter 671/703: Loss 1.9176\n",
      "  Iter 681/703: Loss 2.1336\n",
      "  Iter 691/703: Loss 2.0008\n",
      "  Iter 701/703: Loss 2.0038\n",
      "  Iter 703/703: Loss 2.2591\n",
      "Fine Train Loss: 2.0321, Fine Train Acc: 0.5300, Val Acc: 0.4434, Val Loss: 2.1329\n",
      "Time: 3007.94s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch45.pkl\n",
      "\n",
      "[Epoch 46/100]\n",
      "  Iter   1/703: Loss 2.2571\n",
      "  Iter  11/703: Loss 2.1605\n",
      "  Iter  21/703: Loss 2.1296\n",
      "  Iter  31/703: Loss 1.5370\n",
      "  Iter  41/703: Loss 2.0001\n",
      "  Iter  51/703: Loss 2.0685\n",
      "  Iter  61/703: Loss 1.9030\n",
      "  Iter  71/703: Loss 1.9924\n",
      "  Iter  81/703: Loss 1.7684\n",
      "  Iter  91/703: Loss 1.9410\n",
      "  Iter 101/703: Loss 2.5777\n",
      "  Iter 111/703: Loss 2.0942\n",
      "  Iter 121/703: Loss 1.9314\n",
      "  Iter 131/703: Loss 2.0257\n",
      "  Iter 141/703: Loss 2.0516\n",
      "  Iter 151/703: Loss 1.9358\n",
      "  Iter 161/703: Loss 1.9292\n",
      "  Iter 171/703: Loss 1.5911\n",
      "  Iter 181/703: Loss 1.6527\n",
      "  Iter 191/703: Loss 2.0001\n",
      "  Iter 201/703: Loss 1.7770\n",
      "  Iter 211/703: Loss 1.5825\n",
      "  Iter 221/703: Loss 1.8618\n",
      "  Iter 231/703: Loss 1.9653\n",
      "  Iter 241/703: Loss 2.2747\n",
      "  Iter 251/703: Loss 1.9264\n",
      "  Iter 261/703: Loss 2.1128\n",
      "  Iter 271/703: Loss 2.1059\n",
      "  Iter 281/703: Loss 1.8114\n",
      "  Iter 291/703: Loss 1.6369\n",
      "  Iter 301/703: Loss 1.9546\n",
      "  Iter 311/703: Loss 1.9730\n",
      "  Iter 321/703: Loss 2.1058\n",
      "  Iter 331/703: Loss 2.1180\n",
      "  Iter 341/703: Loss 2.0168\n",
      "  Iter 351/703: Loss 2.1402\n",
      "  Iter 361/703: Loss 2.5207\n",
      "  Iter 371/703: Loss 2.0829\n",
      "  Iter 381/703: Loss 1.9961\n",
      "  Iter 391/703: Loss 2.1022\n",
      "  Iter 401/703: Loss 2.0097\n",
      "  Iter 411/703: Loss 1.8715\n",
      "  Iter 421/703: Loss 2.1088\n",
      "  Iter 431/703: Loss 1.9769\n",
      "  Iter 441/703: Loss 1.7611\n",
      "  Iter 451/703: Loss 2.0136\n",
      "  Iter 461/703: Loss 2.1581\n",
      "  Iter 471/703: Loss 2.1502\n",
      "  Iter 481/703: Loss 2.2517\n",
      "  Iter 491/703: Loss 1.9143\n",
      "  Iter 501/703: Loss 2.5690\n",
      "  Iter 511/703: Loss 2.3144\n",
      "  Iter 521/703: Loss 2.1795\n",
      "  Iter 531/703: Loss 2.0566\n",
      "  Iter 541/703: Loss 1.9540\n",
      "  Iter 551/703: Loss 1.7433\n",
      "  Iter 561/703: Loss 2.2533\n",
      "  Iter 571/703: Loss 2.1393\n",
      "  Iter 581/703: Loss 2.0294\n",
      "  Iter 591/703: Loss 1.9734\n",
      "  Iter 601/703: Loss 2.2200\n",
      "  Iter 611/703: Loss 2.0314\n",
      "  Iter 621/703: Loss 1.9335\n",
      "  Iter 631/703: Loss 1.9697\n",
      "  Iter 641/703: Loss 1.8309\n",
      "  Iter 651/703: Loss 2.4618\n",
      "  Iter 661/703: Loss 2.0522\n",
      "  Iter 671/703: Loss 2.2548\n",
      "  Iter 681/703: Loss 2.0149\n",
      "  Iter 691/703: Loss 1.9504\n",
      "  Iter 701/703: Loss 2.1171\n",
      "  Iter 703/703: Loss 2.0756\n",
      "Fine Train Loss: 2.0032, Fine Train Acc: 0.5540, Val Acc: 0.4398, Val Loss: 2.1656\n",
      "Time: 3012.97s\n",
      "\n",
      "[Epoch 47/100]\n",
      "  Iter   1/703: Loss 1.9043\n",
      "  Iter  11/703: Loss 1.8326\n",
      "  Iter  21/703: Loss 1.5648\n",
      "  Iter  31/703: Loss 2.1514\n",
      "  Iter  41/703: Loss 1.8563\n",
      "  Iter  51/703: Loss 2.0225\n",
      "  Iter  61/703: Loss 1.4537\n",
      "  Iter  71/703: Loss 2.2685\n",
      "  Iter  81/703: Loss 1.8456\n",
      "  Iter  91/703: Loss 2.2450\n",
      "  Iter 101/703: Loss 2.0841\n",
      "  Iter 111/703: Loss 2.1282\n",
      "  Iter 121/703: Loss 2.4560\n",
      "  Iter 131/703: Loss 2.1216\n",
      "  Iter 141/703: Loss 2.1070\n",
      "  Iter 151/703: Loss 2.2532\n",
      "  Iter 161/703: Loss 2.1037\n",
      "  Iter 171/703: Loss 2.0151\n",
      "  Iter 181/703: Loss 2.1961\n",
      "  Iter 191/703: Loss 2.2108\n",
      "  Iter 201/703: Loss 2.0344\n",
      "  Iter 211/703: Loss 1.9169\n",
      "  Iter 221/703: Loss 1.7709\n",
      "  Iter 231/703: Loss 2.2691\n",
      "  Iter 241/703: Loss 2.1948\n",
      "  Iter 251/703: Loss 1.6899\n",
      "  Iter 261/703: Loss 1.9837\n",
      "  Iter 271/703: Loss 2.1451\n",
      "  Iter 281/703: Loss 1.8580\n",
      "  Iter 291/703: Loss 2.0743\n",
      "  Iter 301/703: Loss 1.7269\n",
      "  Iter 311/703: Loss 2.0088\n",
      "  Iter 321/703: Loss 1.8080\n",
      "  Iter 331/703: Loss 2.0839\n",
      "  Iter 341/703: Loss 1.9231\n",
      "  Iter 351/703: Loss 1.9091\n",
      "  Iter 361/703: Loss 1.6758\n",
      "  Iter 371/703: Loss 2.3733\n",
      "  Iter 381/703: Loss 1.7563\n",
      "  Iter 391/703: Loss 1.8775\n",
      "  Iter 401/703: Loss 1.8748\n",
      "  Iter 411/703: Loss 2.0695\n",
      "  Iter 421/703: Loss 1.8510\n",
      "  Iter 431/703: Loss 2.0046\n",
      "  Iter 441/703: Loss 2.2516\n",
      "  Iter 451/703: Loss 2.0759\n",
      "  Iter 461/703: Loss 2.0371\n",
      "  Iter 471/703: Loss 1.8429\n",
      "  Iter 481/703: Loss 2.0159\n",
      "  Iter 491/703: Loss 2.4339\n",
      "  Iter 501/703: Loss 2.3137\n",
      "  Iter 511/703: Loss 2.2663\n",
      "  Iter 521/703: Loss 2.3009\n",
      "  Iter 531/703: Loss 1.7707\n",
      "  Iter 541/703: Loss 1.7303\n",
      "  Iter 551/703: Loss 1.9811\n",
      "  Iter 561/703: Loss 1.9572\n",
      "  Iter 571/703: Loss 1.9914\n",
      "  Iter 581/703: Loss 2.4941\n",
      "  Iter 591/703: Loss 1.8914\n",
      "  Iter 601/703: Loss 1.6787\n",
      "  Iter 611/703: Loss 1.5776\n",
      "  Iter 621/703: Loss 1.6227\n",
      "  Iter 631/703: Loss 2.2604\n",
      "  Iter 641/703: Loss 1.8985\n",
      "  Iter 651/703: Loss 2.5270\n",
      "  Iter 661/703: Loss 2.1483\n",
      "  Iter 671/703: Loss 1.9752\n",
      "  Iter 681/703: Loss 1.9253\n",
      "  Iter 691/703: Loss 1.5666\n",
      "  Iter 701/703: Loss 1.9594\n",
      "  Iter 703/703: Loss 2.1613\n",
      "Fine Train Loss: 1.9850, Fine Train Acc: 0.5470, Val Acc: 0.4408, Val Loss: 2.1426\n",
      "Time: 3014.62s\n",
      "\n",
      "[Epoch 48/100]\n",
      "  Iter   1/703: Loss 2.3433\n",
      "  Iter  11/703: Loss 1.9368\n",
      "  Iter  21/703: Loss 2.2160\n",
      "  Iter  31/703: Loss 1.9054\n",
      "  Iter  41/703: Loss 1.8817\n",
      "  Iter  51/703: Loss 2.2483\n",
      "  Iter  61/703: Loss 1.8294\n",
      "  Iter  71/703: Loss 1.9635\n",
      "  Iter  81/703: Loss 2.0155\n",
      "  Iter  91/703: Loss 2.0799\n",
      "  Iter 101/703: Loss 1.9949\n",
      "  Iter 111/703: Loss 1.9544\n",
      "  Iter 121/703: Loss 1.6492\n",
      "  Iter 131/703: Loss 2.0036\n",
      "  Iter 141/703: Loss 1.9305\n",
      "  Iter 151/703: Loss 2.0943\n",
      "  Iter 161/703: Loss 2.2047\n",
      "  Iter 171/703: Loss 2.0792\n",
      "  Iter 181/703: Loss 2.1398\n",
      "  Iter 191/703: Loss 1.7353\n",
      "  Iter 201/703: Loss 2.2593\n",
      "  Iter 211/703: Loss 2.1950\n",
      "  Iter 221/703: Loss 1.7687\n",
      "  Iter 231/703: Loss 1.9715\n",
      "  Iter 241/703: Loss 2.2657\n",
      "  Iter 251/703: Loss 1.9363\n",
      "  Iter 261/703: Loss 1.8509\n",
      "  Iter 271/703: Loss 1.7397\n",
      "  Iter 281/703: Loss 2.1436\n",
      "  Iter 291/703: Loss 1.9702\n",
      "  Iter 301/703: Loss 1.8774\n",
      "  Iter 311/703: Loss 1.8959\n",
      "  Iter 321/703: Loss 2.2641\n",
      "  Iter 331/703: Loss 1.8421\n",
      "  Iter 341/703: Loss 1.9521\n",
      "  Iter 351/703: Loss 2.0775\n",
      "  Iter 361/703: Loss 1.9017\n",
      "  Iter 371/703: Loss 1.6615\n",
      "  Iter 381/703: Loss 2.0269\n",
      "  Iter 391/703: Loss 2.0025\n",
      "  Iter 401/703: Loss 2.3249\n",
      "  Iter 411/703: Loss 2.2894\n",
      "  Iter 421/703: Loss 1.8843\n",
      "  Iter 431/703: Loss 2.2177\n",
      "  Iter 441/703: Loss 1.9992\n",
      "  Iter 451/703: Loss 1.8773\n",
      "  Iter 461/703: Loss 2.0365\n",
      "  Iter 471/703: Loss 1.5334\n",
      "  Iter 481/703: Loss 2.6074\n",
      "  Iter 491/703: Loss 2.2850\n",
      "  Iter 501/703: Loss 1.7873\n",
      "  Iter 511/703: Loss 1.9633\n",
      "  Iter 521/703: Loss 1.8537\n",
      "  Iter 531/703: Loss 1.9107\n",
      "  Iter 541/703: Loss 1.9469\n",
      "  Iter 551/703: Loss 1.6016\n",
      "  Iter 561/703: Loss 1.7291\n",
      "  Iter 571/703: Loss 2.1742\n",
      "  Iter 581/703: Loss 1.8472\n",
      "  Iter 591/703: Loss 2.2116\n",
      "  Iter 601/703: Loss 2.3701\n",
      "  Iter 611/703: Loss 2.2164\n",
      "  Iter 621/703: Loss 2.0040\n",
      "  Iter 631/703: Loss 2.1909\n",
      "  Iter 641/703: Loss 2.1567\n",
      "  Iter 651/703: Loss 1.7707\n",
      "  Iter 661/703: Loss 2.0123\n",
      "  Iter 671/703: Loss 1.9897\n",
      "  Iter 681/703: Loss 1.9736\n",
      "  Iter 691/703: Loss 1.8826\n",
      "  Iter 701/703: Loss 1.9795\n",
      "  Iter 703/703: Loss 2.1562\n",
      "Fine Train Loss: 1.9844, Fine Train Acc: 0.5320, Val Acc: 0.4406, Val Loss: 2.1332\n",
      "Time: 3009.42s\n",
      "\n",
      "[Epoch 49/100]\n",
      "  Iter   1/703: Loss 1.9858\n",
      "  Iter  11/703: Loss 2.3821\n",
      "  Iter  21/703: Loss 1.9172\n",
      "  Iter  31/703: Loss 1.6590\n",
      "  Iter  41/703: Loss 1.8477\n",
      "  Iter  51/703: Loss 2.1530\n",
      "  Iter  61/703: Loss 2.0444\n",
      "  Iter  71/703: Loss 2.3008\n",
      "  Iter  81/703: Loss 2.1616\n",
      "  Iter  91/703: Loss 1.7254\n",
      "  Iter 101/703: Loss 1.8022\n",
      "  Iter 111/703: Loss 1.9193\n",
      "  Iter 121/703: Loss 1.9384\n",
      "  Iter 131/703: Loss 2.2269\n",
      "  Iter 141/703: Loss 1.7012\n",
      "  Iter 151/703: Loss 2.1529\n",
      "  Iter 161/703: Loss 1.7584\n",
      "  Iter 171/703: Loss 1.9164\n",
      "  Iter 181/703: Loss 2.0844\n",
      "  Iter 191/703: Loss 1.7992\n",
      "  Iter 201/703: Loss 1.9072\n",
      "  Iter 211/703: Loss 1.9144\n",
      "  Iter 221/703: Loss 1.9901\n",
      "  Iter 231/703: Loss 1.6962\n",
      "  Iter 241/703: Loss 2.2503\n",
      "  Iter 251/703: Loss 1.7499\n",
      "  Iter 261/703: Loss 1.8629\n",
      "  Iter 271/703: Loss 1.9783\n",
      "  Iter 281/703: Loss 1.6095\n",
      "  Iter 291/703: Loss 1.6437\n",
      "  Iter 301/703: Loss 2.0081\n",
      "  Iter 311/703: Loss 1.8931\n",
      "  Iter 321/703: Loss 1.8593\n",
      "  Iter 331/703: Loss 2.1115\n",
      "  Iter 341/703: Loss 1.8513\n",
      "  Iter 351/703: Loss 1.9052\n",
      "  Iter 361/703: Loss 2.0580\n",
      "  Iter 371/703: Loss 1.7630\n",
      "  Iter 381/703: Loss 1.8721\n",
      "  Iter 391/703: Loss 2.2522\n",
      "  Iter 401/703: Loss 1.7891\n",
      "  Iter 411/703: Loss 1.9469\n",
      "  Iter 421/703: Loss 1.6050\n",
      "  Iter 431/703: Loss 1.8982\n",
      "  Iter 441/703: Loss 2.1461\n",
      "  Iter 451/703: Loss 2.0180\n",
      "  Iter 461/703: Loss 1.8986\n",
      "  Iter 471/703: Loss 1.7717\n",
      "  Iter 481/703: Loss 2.3809\n",
      "  Iter 491/703: Loss 1.6445\n",
      "  Iter 501/703: Loss 2.3360\n",
      "  Iter 511/703: Loss 2.1461\n",
      "  Iter 521/703: Loss 2.3084\n",
      "  Iter 531/703: Loss 2.0508\n",
      "  Iter 541/703: Loss 1.8895\n",
      "  Iter 551/703: Loss 1.7711\n",
      "  Iter 561/703: Loss 2.0819\n",
      "  Iter 571/703: Loss 1.9387\n",
      "  Iter 581/703: Loss 1.7779\n",
      "  Iter 591/703: Loss 1.9653\n",
      "  Iter 601/703: Loss 2.2923\n",
      "  Iter 611/703: Loss 2.0179\n",
      "  Iter 621/703: Loss 1.9339\n",
      "  Iter 631/703: Loss 1.6682\n",
      "  Iter 641/703: Loss 1.7873\n",
      "  Iter 651/703: Loss 2.1893\n",
      "  Iter 661/703: Loss 2.0193\n",
      "  Iter 671/703: Loss 2.2543\n",
      "  Iter 681/703: Loss 2.0714\n",
      "  Iter 691/703: Loss 1.6731\n",
      "  Iter 701/703: Loss 1.8393\n",
      "  Iter 703/703: Loss 2.2200\n",
      "Fine Train Loss: 1.9650, Fine Train Acc: 0.5410, Val Acc: 0.4502, Val Loss: 2.1122\n",
      "Time: 3018.95s\n",
      "\n",
      "[Epoch 50/100]\n",
      "  Iter   1/703: Loss 2.1906\n",
      "  Iter  11/703: Loss 2.0991\n",
      "  Iter  21/703: Loss 2.0272\n",
      "  Iter  31/703: Loss 1.7376\n",
      "  Iter  41/703: Loss 1.8784\n",
      "  Iter  51/703: Loss 2.3490\n",
      "  Iter  61/703: Loss 1.9538\n",
      "  Iter  71/703: Loss 1.9478\n",
      "  Iter  81/703: Loss 1.5980\n",
      "  Iter  91/703: Loss 2.1413\n",
      "  Iter 101/703: Loss 1.8552\n",
      "  Iter 111/703: Loss 2.0204\n",
      "  Iter 121/703: Loss 2.3254\n",
      "  Iter 131/703: Loss 1.8236\n",
      "  Iter 141/703: Loss 2.0849\n",
      "  Iter 151/703: Loss 1.4671\n",
      "  Iter 161/703: Loss 1.9274\n",
      "  Iter 171/703: Loss 2.6355\n",
      "  Iter 181/703: Loss 1.9663\n",
      "  Iter 191/703: Loss 2.0600\n",
      "  Iter 201/703: Loss 1.9788\n",
      "  Iter 211/703: Loss 1.8758\n",
      "  Iter 221/703: Loss 1.7002\n",
      "  Iter 231/703: Loss 1.7256\n",
      "  Iter 241/703: Loss 1.9435\n",
      "  Iter 251/703: Loss 2.0908\n",
      "  Iter 261/703: Loss 1.7383\n",
      "  Iter 271/703: Loss 1.8538\n",
      "  Iter 281/703: Loss 2.1435\n",
      "  Iter 291/703: Loss 1.5295\n",
      "  Iter 301/703: Loss 2.3181\n",
      "  Iter 311/703: Loss 1.6518\n",
      "  Iter 321/703: Loss 2.0157\n",
      "  Iter 331/703: Loss 2.0046\n",
      "  Iter 341/703: Loss 1.9498\n",
      "  Iter 351/703: Loss 1.3493\n",
      "  Iter 361/703: Loss 1.9737\n",
      "  Iter 371/703: Loss 2.3112\n",
      "  Iter 381/703: Loss 1.9705\n",
      "  Iter 391/703: Loss 2.0207\n",
      "  Iter 401/703: Loss 1.9599\n",
      "  Iter 411/703: Loss 1.8509\n",
      "  Iter 421/703: Loss 2.1087\n",
      "  Iter 431/703: Loss 2.3662\n",
      "  Iter 441/703: Loss 1.8335\n",
      "  Iter 451/703: Loss 1.9712\n",
      "  Iter 461/703: Loss 1.8523\n",
      "  Iter 471/703: Loss 1.8852\n",
      "  Iter 481/703: Loss 1.5304\n",
      "  Iter 491/703: Loss 1.8614\n",
      "  Iter 501/703: Loss 1.9685\n",
      "  Iter 511/703: Loss 1.8574\n",
      "  Iter 521/703: Loss 1.9169\n",
      "  Iter 531/703: Loss 1.9806\n",
      "  Iter 541/703: Loss 1.9524\n",
      "  Iter 551/703: Loss 2.0058\n",
      "  Iter 561/703: Loss 2.2125\n",
      "  Iter 571/703: Loss 2.2373\n",
      "  Iter 581/703: Loss 2.0921\n",
      "  Iter 591/703: Loss 2.2247\n",
      "  Iter 601/703: Loss 2.2786\n",
      "  Iter 611/703: Loss 1.9158\n",
      "  Iter 621/703: Loss 1.8328\n",
      "  Iter 631/703: Loss 2.3422\n",
      "  Iter 641/703: Loss 1.8199\n",
      "  Iter 651/703: Loss 2.2580\n",
      "  Iter 661/703: Loss 2.1179\n",
      "  Iter 671/703: Loss 1.8246\n",
      "  Iter 681/703: Loss 2.2053\n",
      "  Iter 691/703: Loss 1.8115\n",
      "  Iter 701/703: Loss 2.2049\n",
      "  Iter 703/703: Loss 2.0371\n",
      "Fine Train Loss: 1.9756, Fine Train Acc: 0.5590, Val Acc: 0.4524, Val Loss: 2.1212\n",
      "Time: 3009.68s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch50.pkl\n",
      "\n",
      "[Epoch 51/100]\n",
      "  Iter   1/703: Loss 1.6646\n",
      "  Iter  11/703: Loss 1.9791\n",
      "  Iter  21/703: Loss 2.2222\n",
      "  Iter  31/703: Loss 2.0638\n",
      "  Iter  41/703: Loss 2.3728\n",
      "  Iter  51/703: Loss 2.0436\n",
      "  Iter  61/703: Loss 1.7197\n",
      "  Iter  71/703: Loss 2.1341\n",
      "  Iter  81/703: Loss 1.9157\n",
      "  Iter  91/703: Loss 2.0290\n",
      "  Iter 101/703: Loss 1.9642\n",
      "  Iter 111/703: Loss 1.6593\n",
      "  Iter 121/703: Loss 2.0674\n",
      "  Iter 131/703: Loss 1.7146\n",
      "  Iter 141/703: Loss 1.8884\n",
      "  Iter 151/703: Loss 1.8194\n",
      "  Iter 161/703: Loss 1.5817\n",
      "  Iter 171/703: Loss 1.7590\n",
      "  Iter 181/703: Loss 1.9105\n",
      "  Iter 191/703: Loss 2.0422\n",
      "  Iter 201/703: Loss 1.9136\n",
      "  Iter 211/703: Loss 1.8453\n",
      "  Iter 221/703: Loss 2.3176\n",
      "  Iter 231/703: Loss 2.1008\n",
      "  Iter 241/703: Loss 2.0121\n",
      "  Iter 251/703: Loss 2.2726\n",
      "  Iter 261/703: Loss 2.0569\n",
      "  Iter 271/703: Loss 1.6567\n",
      "  Iter 281/703: Loss 1.7484\n",
      "  Iter 291/703: Loss 1.4426\n",
      "  Iter 301/703: Loss 2.1327\n",
      "  Iter 311/703: Loss 1.9944\n",
      "  Iter 321/703: Loss 1.9532\n",
      "  Iter 331/703: Loss 2.2368\n",
      "  Iter 341/703: Loss 1.7411\n",
      "  Iter 351/703: Loss 2.2260\n",
      "  Iter 361/703: Loss 1.7198\n",
      "  Iter 371/703: Loss 2.0492\n",
      "  Iter 381/703: Loss 1.9396\n",
      "  Iter 391/703: Loss 2.0366\n",
      "  Iter 401/703: Loss 2.2403\n",
      "  Iter 411/703: Loss 2.1605\n",
      "  Iter 421/703: Loss 2.1156\n",
      "  Iter 431/703: Loss 1.9441\n",
      "  Iter 441/703: Loss 1.6584\n",
      "  Iter 451/703: Loss 2.0186\n",
      "  Iter 461/703: Loss 2.1515\n",
      "  Iter 471/703: Loss 2.3251\n",
      "  Iter 481/703: Loss 2.1767\n",
      "  Iter 491/703: Loss 1.7749\n",
      "  Iter 501/703: Loss 2.0176\n",
      "  Iter 511/703: Loss 1.8309\n",
      "  Iter 521/703: Loss 1.8293\n",
      "  Iter 531/703: Loss 1.8260\n",
      "  Iter 541/703: Loss 2.2280\n",
      "  Iter 551/703: Loss 2.2628\n",
      "  Iter 561/703: Loss 1.8936\n",
      "  Iter 571/703: Loss 2.1414\n",
      "  Iter 581/703: Loss 2.2528\n",
      "  Iter 591/703: Loss 1.8088\n",
      "  Iter 601/703: Loss 1.8175\n",
      "  Iter 611/703: Loss 1.9549\n",
      "  Iter 621/703: Loss 1.7922\n",
      "  Iter 631/703: Loss 1.8259\n",
      "  Iter 641/703: Loss 1.9805\n",
      "  Iter 651/703: Loss 1.7740\n",
      "  Iter 661/703: Loss 1.8806\n",
      "  Iter 671/703: Loss 2.0751\n",
      "  Iter 681/703: Loss 2.0919\n",
      "  Iter 691/703: Loss 1.8423\n",
      "  Iter 701/703: Loss 1.9859\n",
      "  Iter 703/703: Loss 2.3223\n",
      "Fine Train Loss: 1.9332, Fine Train Acc: 0.5560, Val Acc: 0.4502, Val Loss: 2.0998\n",
      "Time: 3144.68s\n",
      "\n",
      "[Epoch 52/100]\n",
      "  Iter   1/703: Loss 1.8127\n",
      "  Iter  11/703: Loss 2.3022\n",
      "  Iter  21/703: Loss 1.8370\n",
      "  Iter  31/703: Loss 2.2398\n",
      "  Iter  41/703: Loss 2.0347\n",
      "  Iter  51/703: Loss 2.0472\n",
      "  Iter  61/703: Loss 1.9372\n",
      "  Iter  71/703: Loss 1.6590\n",
      "  Iter  81/703: Loss 1.9651\n",
      "  Iter  91/703: Loss 1.8329\n",
      "  Iter 101/703: Loss 1.8037\n",
      "  Iter 111/703: Loss 1.9700\n",
      "  Iter 121/703: Loss 1.9772\n",
      "  Iter 131/703: Loss 2.0182\n",
      "  Iter 141/703: Loss 1.9298\n",
      "  Iter 151/703: Loss 1.7804\n",
      "  Iter 161/703: Loss 1.8700\n",
      "  Iter 171/703: Loss 1.9954\n",
      "  Iter 181/703: Loss 1.8586\n",
      "  Iter 191/703: Loss 1.9425\n",
      "  Iter 201/703: Loss 1.8246\n",
      "  Iter 211/703: Loss 1.7718\n",
      "  Iter 221/703: Loss 1.9400\n",
      "  Iter 231/703: Loss 1.7003\n",
      "  Iter 241/703: Loss 1.9682\n",
      "  Iter 251/703: Loss 1.9675\n",
      "  Iter 261/703: Loss 2.0364\n",
      "  Iter 271/703: Loss 1.9673\n",
      "  Iter 281/703: Loss 1.8874\n",
      "  Iter 291/703: Loss 1.6803\n",
      "  Iter 301/703: Loss 2.0330\n",
      "  Iter 311/703: Loss 2.1046\n",
      "  Iter 321/703: Loss 2.0675\n",
      "  Iter 331/703: Loss 1.7946\n",
      "  Iter 341/703: Loss 2.0934\n",
      "  Iter 351/703: Loss 2.0298\n",
      "  Iter 361/703: Loss 1.6380\n",
      "  Iter 371/703: Loss 1.7722\n",
      "  Iter 381/703: Loss 1.5668\n",
      "  Iter 391/703: Loss 1.6119\n",
      "  Iter 401/703: Loss 1.9192\n",
      "  Iter 411/703: Loss 1.9450\n",
      "  Iter 421/703: Loss 2.0382\n",
      "  Iter 431/703: Loss 2.1312\n",
      "  Iter 441/703: Loss 2.0633\n",
      "  Iter 451/703: Loss 1.8923\n",
      "  Iter 461/703: Loss 2.1615\n",
      "  Iter 471/703: Loss 1.7902\n",
      "  Iter 481/703: Loss 1.9161\n",
      "  Iter 491/703: Loss 2.2598\n",
      "  Iter 501/703: Loss 1.9237\n",
      "  Iter 511/703: Loss 1.8572\n",
      "  Iter 521/703: Loss 1.9133\n",
      "  Iter 531/703: Loss 1.9351\n",
      "  Iter 541/703: Loss 2.0533\n",
      "  Iter 551/703: Loss 2.2909\n",
      "  Iter 561/703: Loss 1.6803\n",
      "  Iter 571/703: Loss 1.8777\n",
      "  Iter 581/703: Loss 1.9748\n",
      "  Iter 591/703: Loss 1.9854\n",
      "  Iter 601/703: Loss 1.7938\n",
      "  Iter 611/703: Loss 1.6906\n",
      "  Iter 621/703: Loss 1.6447\n",
      "  Iter 631/703: Loss 2.1662\n",
      "  Iter 641/703: Loss 1.7155\n",
      "  Iter 651/703: Loss 1.7748\n",
      "  Iter 661/703: Loss 2.0800\n",
      "  Iter 671/703: Loss 1.9776\n",
      "  Iter 681/703: Loss 2.1606\n",
      "  Iter 691/703: Loss 1.9921\n",
      "  Iter 701/703: Loss 1.9398\n",
      "  Iter 703/703: Loss 2.3733\n",
      "Fine Train Loss: 1.9329, Fine Train Acc: 0.5380, Val Acc: 0.4502, Val Loss: 2.1021\n",
      "Time: 3019.57s\n",
      "\n",
      "[Epoch 53/100]\n",
      "  Iter   1/703: Loss 2.0823\n",
      "  Iter  11/703: Loss 1.8072\n",
      "  Iter  21/703: Loss 2.0903\n",
      "  Iter  31/703: Loss 1.6954\n",
      "  Iter  41/703: Loss 1.6685\n",
      "  Iter  51/703: Loss 1.7826\n",
      "  Iter  61/703: Loss 1.6650\n",
      "  Iter  71/703: Loss 1.9840\n",
      "  Iter  81/703: Loss 1.8420\n",
      "  Iter  91/703: Loss 1.8229\n",
      "  Iter 101/703: Loss 1.8711\n",
      "  Iter 111/703: Loss 2.3147\n",
      "  Iter 121/703: Loss 2.3414\n",
      "  Iter 131/703: Loss 1.7886\n",
      "  Iter 141/703: Loss 2.1383\n",
      "  Iter 151/703: Loss 1.7542\n",
      "  Iter 161/703: Loss 1.9253\n",
      "  Iter 171/703: Loss 2.1531\n",
      "  Iter 181/703: Loss 2.1122\n",
      "  Iter 191/703: Loss 2.1429\n",
      "  Iter 201/703: Loss 1.9968\n",
      "  Iter 211/703: Loss 1.6250\n",
      "  Iter 221/703: Loss 2.0103\n",
      "  Iter 231/703: Loss 2.0716\n",
      "  Iter 241/703: Loss 2.0880\n",
      "  Iter 251/703: Loss 1.6980\n",
      "  Iter 261/703: Loss 1.9345\n",
      "  Iter 271/703: Loss 1.8531\n",
      "  Iter 281/703: Loss 1.8349\n",
      "  Iter 291/703: Loss 1.8115\n",
      "  Iter 301/703: Loss 1.7078\n",
      "  Iter 311/703: Loss 2.3410\n",
      "  Iter 321/703: Loss 1.9022\n",
      "  Iter 331/703: Loss 2.1004\n",
      "  Iter 341/703: Loss 2.0124\n",
      "  Iter 351/703: Loss 2.5012\n",
      "  Iter 361/703: Loss 1.9262\n",
      "  Iter 371/703: Loss 1.9067\n",
      "  Iter 381/703: Loss 1.8005\n",
      "  Iter 391/703: Loss 2.7564\n",
      "  Iter 401/703: Loss 2.1071\n",
      "  Iter 411/703: Loss 2.0220\n",
      "  Iter 421/703: Loss 1.9771\n",
      "  Iter 431/703: Loss 1.7850\n",
      "  Iter 441/703: Loss 1.9899\n",
      "  Iter 451/703: Loss 1.8462\n",
      "  Iter 461/703: Loss 1.7101\n",
      "  Iter 471/703: Loss 2.3213\n",
      "  Iter 481/703: Loss 1.9879\n",
      "  Iter 491/703: Loss 1.5605\n",
      "  Iter 501/703: Loss 1.8224\n",
      "  Iter 511/703: Loss 1.9414\n",
      "  Iter 521/703: Loss 1.9729\n",
      "  Iter 531/703: Loss 1.9319\n",
      "  Iter 541/703: Loss 1.4863\n",
      "  Iter 551/703: Loss 2.0484\n",
      "  Iter 561/703: Loss 2.0746\n",
      "  Iter 571/703: Loss 1.7791\n",
      "  Iter 581/703: Loss 2.2852\n",
      "  Iter 591/703: Loss 1.9509\n",
      "  Iter 601/703: Loss 2.2227\n",
      "  Iter 611/703: Loss 2.0095\n",
      "  Iter 621/703: Loss 1.9361\n",
      "  Iter 631/703: Loss 2.1966\n",
      "  Iter 641/703: Loss 2.0980\n",
      "  Iter 651/703: Loss 2.1813\n",
      "  Iter 661/703: Loss 1.7732\n",
      "  Iter 671/703: Loss 2.2505\n",
      "  Iter 681/703: Loss 1.9710\n",
      "  Iter 691/703: Loss 1.8404\n",
      "  Iter 701/703: Loss 1.6910\n",
      "  Iter 703/703: Loss 1.9539\n",
      "Fine Train Loss: 1.9362, Fine Train Acc: 0.5680, Val Acc: 0.4526, Val Loss: 2.1283\n",
      "Time: 3110.60s\n",
      "\n",
      "[Epoch 54/100]\n",
      "  Iter   1/703: Loss 2.0505\n",
      "  Iter  11/703: Loss 2.0883\n",
      "  Iter  21/703: Loss 1.7369\n",
      "  Iter  31/703: Loss 1.8375\n",
      "  Iter  41/703: Loss 1.9460\n",
      "  Iter  51/703: Loss 1.8325\n",
      "  Iter  61/703: Loss 1.6891\n",
      "  Iter  71/703: Loss 1.9841\n",
      "  Iter  81/703: Loss 1.8631\n",
      "  Iter  91/703: Loss 1.7049\n",
      "  Iter 101/703: Loss 2.1415\n",
      "  Iter 111/703: Loss 2.0057\n",
      "  Iter 121/703: Loss 2.1807\n",
      "  Iter 131/703: Loss 2.2543\n",
      "  Iter 141/703: Loss 1.9396\n",
      "  Iter 151/703: Loss 1.9764\n",
      "  Iter 161/703: Loss 1.7565\n",
      "  Iter 171/703: Loss 1.7819\n",
      "  Iter 181/703: Loss 1.7019\n",
      "  Iter 191/703: Loss 1.9237\n",
      "  Iter 201/703: Loss 1.9498\n",
      "  Iter 211/703: Loss 2.0484\n",
      "  Iter 221/703: Loss 1.6825\n",
      "  Iter 231/703: Loss 1.8324\n",
      "  Iter 241/703: Loss 1.9368\n",
      "  Iter 251/703: Loss 2.1605\n",
      "  Iter 261/703: Loss 2.0555\n",
      "  Iter 271/703: Loss 2.1840\n",
      "  Iter 281/703: Loss 1.6320\n",
      "  Iter 291/703: Loss 1.5457\n",
      "  Iter 301/703: Loss 1.7211\n",
      "  Iter 311/703: Loss 1.8140\n",
      "  Iter 321/703: Loss 1.9368\n",
      "  Iter 331/703: Loss 1.9229\n",
      "  Iter 341/703: Loss 1.9605\n",
      "  Iter 351/703: Loss 1.9476\n",
      "  Iter 361/703: Loss 1.6528\n",
      "  Iter 371/703: Loss 2.0276\n",
      "  Iter 381/703: Loss 2.0976\n",
      "  Iter 391/703: Loss 1.8679\n",
      "  Iter 401/703: Loss 1.8651\n",
      "  Iter 411/703: Loss 1.8937\n",
      "  Iter 421/703: Loss 2.2000\n",
      "  Iter 431/703: Loss 2.0394\n",
      "  Iter 441/703: Loss 2.3630\n",
      "  Iter 451/703: Loss 1.9694\n",
      "  Iter 461/703: Loss 2.0097\n",
      "  Iter 471/703: Loss 1.8287\n",
      "  Iter 481/703: Loss 1.7939\n",
      "  Iter 491/703: Loss 1.8804\n",
      "  Iter 501/703: Loss 2.0067\n",
      "  Iter 511/703: Loss 1.7768\n",
      "  Iter 521/703: Loss 1.8042\n",
      "  Iter 531/703: Loss 1.7555\n",
      "  Iter 541/703: Loss 2.0795\n",
      "  Iter 551/703: Loss 2.0245\n",
      "  Iter 561/703: Loss 2.2942\n",
      "  Iter 571/703: Loss 1.8589\n",
      "  Iter 581/703: Loss 2.3249\n",
      "  Iter 591/703: Loss 1.9982\n",
      "  Iter 601/703: Loss 1.6163\n",
      "  Iter 611/703: Loss 1.9315\n",
      "  Iter 621/703: Loss 1.8117\n",
      "  Iter 631/703: Loss 1.7347\n",
      "  Iter 641/703: Loss 1.6546\n",
      "  Iter 651/703: Loss 2.2140\n",
      "  Iter 661/703: Loss 1.8379\n",
      "  Iter 671/703: Loss 1.8160\n",
      "  Iter 681/703: Loss 2.0733\n",
      "  Iter 691/703: Loss 1.6324\n",
      "  Iter 701/703: Loss 1.8420\n",
      "  Iter 703/703: Loss 1.7030\n",
      "Fine Train Loss: 1.9182, Fine Train Acc: 0.5550, Val Acc: 0.4684, Val Loss: 2.0448\n",
      "Time: 3039.44s\n",
      "\n",
      "[Epoch 55/100]\n",
      "  Iter   1/703: Loss 1.7320\n",
      "  Iter  11/703: Loss 2.0134\n",
      "  Iter  21/703: Loss 1.7950\n",
      "  Iter  31/703: Loss 1.9154\n",
      "  Iter  41/703: Loss 1.8771\n",
      "  Iter  51/703: Loss 1.3517\n",
      "  Iter  61/703: Loss 2.0810\n",
      "  Iter  71/703: Loss 2.0617\n",
      "  Iter  81/703: Loss 1.5908\n",
      "  Iter  91/703: Loss 1.7957\n",
      "  Iter 101/703: Loss 1.9853\n",
      "  Iter 111/703: Loss 1.9496\n",
      "  Iter 121/703: Loss 1.9080\n",
      "  Iter 131/703: Loss 1.7938\n",
      "  Iter 141/703: Loss 1.9366\n",
      "  Iter 151/703: Loss 1.8795\n",
      "  Iter 161/703: Loss 2.0377\n",
      "  Iter 171/703: Loss 1.9795\n",
      "  Iter 181/703: Loss 1.9065\n",
      "  Iter 191/703: Loss 1.6758\n",
      "  Iter 201/703: Loss 2.0013\n",
      "  Iter 211/703: Loss 2.0006\n",
      "  Iter 221/703: Loss 2.0822\n",
      "  Iter 231/703: Loss 1.8842\n",
      "  Iter 241/703: Loss 1.8864\n",
      "  Iter 251/703: Loss 1.9698\n",
      "  Iter 261/703: Loss 2.1313\n",
      "  Iter 271/703: Loss 1.9023\n",
      "  Iter 281/703: Loss 1.6743\n",
      "  Iter 291/703: Loss 2.0167\n",
      "  Iter 301/703: Loss 1.7100\n",
      "  Iter 311/703: Loss 2.0715\n",
      "  Iter 321/703: Loss 1.6305\n",
      "  Iter 331/703: Loss 2.0341\n",
      "  Iter 341/703: Loss 1.9254\n",
      "  Iter 351/703: Loss 1.6476\n",
      "  Iter 361/703: Loss 1.8897\n",
      "  Iter 371/703: Loss 1.8762\n",
      "  Iter 381/703: Loss 1.6935\n",
      "  Iter 391/703: Loss 1.5546\n",
      "  Iter 401/703: Loss 2.1042\n",
      "  Iter 411/703: Loss 1.9933\n",
      "  Iter 421/703: Loss 1.9635\n",
      "  Iter 431/703: Loss 2.0487\n",
      "  Iter 441/703: Loss 1.8953\n",
      "  Iter 451/703: Loss 2.0530\n",
      "  Iter 461/703: Loss 2.1675\n",
      "  Iter 471/703: Loss 1.6525\n",
      "  Iter 481/703: Loss 2.0031\n",
      "  Iter 491/703: Loss 1.6019\n",
      "  Iter 501/703: Loss 1.9888\n",
      "  Iter 511/703: Loss 1.7403\n",
      "  Iter 521/703: Loss 1.6613\n",
      "  Iter 531/703: Loss 1.8566\n",
      "  Iter 541/703: Loss 2.0522\n",
      "  Iter 551/703: Loss 1.9700\n",
      "  Iter 561/703: Loss 1.8830\n",
      "  Iter 571/703: Loss 2.3230\n",
      "  Iter 581/703: Loss 2.3187\n",
      "  Iter 591/703: Loss 1.7336\n",
      "  Iter 601/703: Loss 1.8177\n",
      "  Iter 611/703: Loss 1.9849\n",
      "  Iter 621/703: Loss 1.8566\n",
      "  Iter 631/703: Loss 1.5217\n",
      "  Iter 641/703: Loss 1.6675\n",
      "  Iter 651/703: Loss 2.0489\n",
      "  Iter 661/703: Loss 1.8526\n",
      "  Iter 671/703: Loss 2.0876\n",
      "  Iter 681/703: Loss 1.7884\n",
      "  Iter 691/703: Loss 1.9770\n",
      "  Iter 701/703: Loss 1.6108\n",
      "  Iter 703/703: Loss 1.8085\n",
      "Fine Train Loss: 1.8954, Fine Train Acc: 0.5660, Val Acc: 0.4672, Val Loss: 2.0718\n",
      "Time: 3013.59s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch55.pkl\n",
      "\n",
      "[Epoch 56/100]\n",
      "  Iter   1/703: Loss 2.0462\n",
      "  Iter  11/703: Loss 1.7701\n",
      "  Iter  21/703: Loss 2.3764\n",
      "  Iter  31/703: Loss 2.1250\n",
      "  Iter  41/703: Loss 1.8956\n",
      "  Iter  51/703: Loss 1.9795\n",
      "  Iter  61/703: Loss 1.8494\n",
      "  Iter  71/703: Loss 1.9912\n",
      "  Iter  81/703: Loss 1.8237\n",
      "  Iter  91/703: Loss 2.0562\n",
      "  Iter 101/703: Loss 1.7551\n",
      "  Iter 111/703: Loss 2.0768\n",
      "  Iter 121/703: Loss 1.7965\n",
      "  Iter 131/703: Loss 1.6106\n",
      "  Iter 141/703: Loss 1.6318\n",
      "  Iter 151/703: Loss 1.8559\n",
      "  Iter 161/703: Loss 2.0667\n",
      "  Iter 171/703: Loss 1.9897\n",
      "  Iter 181/703: Loss 1.7749\n",
      "  Iter 191/703: Loss 2.0641\n",
      "  Iter 201/703: Loss 1.9190\n",
      "  Iter 211/703: Loss 2.0330\n",
      "  Iter 221/703: Loss 1.8496\n",
      "  Iter 231/703: Loss 1.7801\n",
      "  Iter 241/703: Loss 2.0373\n",
      "  Iter 251/703: Loss 1.7806\n",
      "  Iter 261/703: Loss 2.2383\n",
      "  Iter 271/703: Loss 2.0754\n",
      "  Iter 281/703: Loss 1.7240\n",
      "  Iter 291/703: Loss 1.7902\n",
      "  Iter 301/703: Loss 1.9399\n",
      "  Iter 311/703: Loss 1.9910\n",
      "  Iter 321/703: Loss 1.8817\n",
      "  Iter 331/703: Loss 1.7246\n",
      "  Iter 341/703: Loss 1.9240\n",
      "  Iter 351/703: Loss 1.8911\n",
      "  Iter 361/703: Loss 1.9487\n",
      "  Iter 371/703: Loss 1.9655\n",
      "  Iter 381/703: Loss 1.9599\n",
      "  Iter 391/703: Loss 1.9446\n",
      "  Iter 401/703: Loss 1.7736\n",
      "  Iter 411/703: Loss 1.8830\n",
      "  Iter 421/703: Loss 2.6281\n",
      "  Iter 431/703: Loss 1.8940\n",
      "  Iter 441/703: Loss 2.2063\n",
      "  Iter 451/703: Loss 1.6713\n",
      "  Iter 461/703: Loss 1.9119\n",
      "  Iter 471/703: Loss 1.4025\n",
      "  Iter 481/703: Loss 1.9833\n",
      "  Iter 491/703: Loss 1.9771\n",
      "  Iter 501/703: Loss 1.8467\n",
      "  Iter 511/703: Loss 1.6592\n",
      "  Iter 521/703: Loss 1.9447\n",
      "  Iter 531/703: Loss 2.4408\n",
      "  Iter 541/703: Loss 2.1059\n",
      "  Iter 551/703: Loss 2.0036\n",
      "  Iter 561/703: Loss 1.6278\n",
      "  Iter 571/703: Loss 1.4408\n",
      "  Iter 581/703: Loss 2.0589\n",
      "  Iter 591/703: Loss 1.6336\n",
      "  Iter 601/703: Loss 1.9918\n",
      "  Iter 611/703: Loss 1.8648\n",
      "  Iter 621/703: Loss 1.8464\n",
      "  Iter 631/703: Loss 1.8807\n",
      "  Iter 641/703: Loss 1.5405\n",
      "  Iter 651/703: Loss 2.0662\n",
      "  Iter 661/703: Loss 1.9889\n",
      "  Iter 671/703: Loss 1.9260\n",
      "  Iter 681/703: Loss 1.9996\n",
      "  Iter 691/703: Loss 1.8440\n",
      "  Iter 701/703: Loss 2.1773\n",
      "  Iter 703/703: Loss 1.9024\n",
      "Fine Train Loss: 1.8966, Fine Train Acc: 0.5660, Val Acc: 0.4566, Val Loss: 2.0621\n",
      "Time: 3016.96s\n",
      "\n",
      "[Epoch 57/100]\n",
      "  Iter   1/703: Loss 1.9148\n",
      "  Iter  11/703: Loss 1.9505\n",
      "  Iter  21/703: Loss 2.0529\n",
      "  Iter  31/703: Loss 1.9317\n",
      "  Iter  41/703: Loss 2.1116\n",
      "  Iter  51/703: Loss 1.9086\n",
      "  Iter  61/703: Loss 2.0425\n",
      "  Iter  71/703: Loss 1.6507\n",
      "  Iter  81/703: Loss 1.9515\n",
      "  Iter  91/703: Loss 1.7530\n",
      "  Iter 101/703: Loss 2.0121\n",
      "  Iter 111/703: Loss 2.0718\n",
      "  Iter 121/703: Loss 2.0611\n",
      "  Iter 131/703: Loss 1.9069\n",
      "  Iter 141/703: Loss 1.9822\n",
      "  Iter 151/703: Loss 1.8947\n",
      "  Iter 161/703: Loss 1.5289\n",
      "  Iter 171/703: Loss 2.1446\n",
      "  Iter 181/703: Loss 1.5548\n",
      "  Iter 191/703: Loss 2.1799\n",
      "  Iter 201/703: Loss 2.1093\n",
      "  Iter 211/703: Loss 1.7277\n",
      "  Iter 221/703: Loss 1.7333\n",
      "  Iter 231/703: Loss 1.9952\n",
      "  Iter 241/703: Loss 1.7079\n",
      "  Iter 251/703: Loss 2.1179\n",
      "  Iter 261/703: Loss 1.9072\n",
      "  Iter 271/703: Loss 2.1115\n",
      "  Iter 281/703: Loss 2.1271\n",
      "  Iter 291/703: Loss 1.6484\n",
      "  Iter 301/703: Loss 1.8676\n",
      "  Iter 311/703: Loss 1.6613\n",
      "  Iter 321/703: Loss 1.6062\n",
      "  Iter 331/703: Loss 1.6355\n",
      "  Iter 341/703: Loss 1.7036\n",
      "  Iter 351/703: Loss 1.6403\n",
      "  Iter 361/703: Loss 1.5871\n",
      "  Iter 371/703: Loss 2.0029\n",
      "  Iter 381/703: Loss 1.5289\n",
      "  Iter 391/703: Loss 2.0825\n",
      "  Iter 401/703: Loss 1.7287\n",
      "  Iter 411/703: Loss 1.5053\n",
      "  Iter 421/703: Loss 1.8865\n",
      "  Iter 431/703: Loss 1.9889\n",
      "  Iter 441/703: Loss 1.6842\n",
      "  Iter 451/703: Loss 1.9277\n",
      "  Iter 461/703: Loss 1.7400\n",
      "  Iter 471/703: Loss 2.0294\n",
      "  Iter 481/703: Loss 2.2414\n",
      "  Iter 491/703: Loss 2.0461\n",
      "  Iter 501/703: Loss 1.9462\n",
      "  Iter 511/703: Loss 1.8388\n",
      "  Iter 521/703: Loss 1.8273\n",
      "  Iter 531/703: Loss 1.7490\n",
      "  Iter 541/703: Loss 1.7286\n",
      "  Iter 551/703: Loss 1.9145\n",
      "  Iter 561/703: Loss 2.0444\n",
      "  Iter 571/703: Loss 1.5626\n",
      "  Iter 581/703: Loss 1.7343\n",
      "  Iter 591/703: Loss 2.2506\n",
      "  Iter 601/703: Loss 1.5439\n",
      "  Iter 611/703: Loss 1.5992\n",
      "  Iter 621/703: Loss 1.8620\n",
      "  Iter 631/703: Loss 1.9811\n",
      "  Iter 641/703: Loss 1.6721\n",
      "  Iter 651/703: Loss 2.1358\n",
      "  Iter 661/703: Loss 1.6715\n",
      "  Iter 671/703: Loss 1.9718\n",
      "  Iter 681/703: Loss 2.0085\n",
      "  Iter 691/703: Loss 1.7644\n",
      "  Iter 701/703: Loss 1.8546\n",
      "  Iter 703/703: Loss 1.7675\n",
      "Fine Train Loss: 1.8854, Fine Train Acc: 0.5770, Val Acc: 0.4584, Val Loss: 2.0400\n",
      "Time: 3008.75s\n",
      "\n",
      "[Epoch 58/100]\n",
      "  Iter   1/703: Loss 1.6976\n",
      "  Iter  11/703: Loss 1.7655\n",
      "  Iter  21/703: Loss 1.9861\n",
      "  Iter  31/703: Loss 2.1314\n",
      "  Iter  41/703: Loss 1.6681\n",
      "  Iter  51/703: Loss 1.5320\n",
      "  Iter  61/703: Loss 1.7097\n",
      "  Iter  71/703: Loss 1.6818\n",
      "  Iter  81/703: Loss 2.0124\n",
      "  Iter  91/703: Loss 1.6970\n",
      "  Iter 101/703: Loss 1.5440\n",
      "  Iter 111/703: Loss 1.7180\n",
      "  Iter 121/703: Loss 1.5183\n",
      "  Iter 131/703: Loss 2.0518\n",
      "  Iter 141/703: Loss 2.3704\n",
      "  Iter 151/703: Loss 1.7491\n",
      "  Iter 161/703: Loss 1.8445\n",
      "  Iter 171/703: Loss 1.9528\n",
      "  Iter 181/703: Loss 1.9145\n",
      "  Iter 191/703: Loss 1.5042\n",
      "  Iter 201/703: Loss 2.0316\n",
      "  Iter 211/703: Loss 1.7882\n",
      "  Iter 221/703: Loss 1.5515\n",
      "  Iter 231/703: Loss 1.8255\n",
      "  Iter 241/703: Loss 1.8802\n",
      "  Iter 251/703: Loss 2.0444\n",
      "  Iter 261/703: Loss 1.4994\n",
      "  Iter 271/703: Loss 2.0904\n",
      "  Iter 281/703: Loss 1.9803\n",
      "  Iter 291/703: Loss 1.3025\n",
      "  Iter 301/703: Loss 1.8750\n",
      "  Iter 311/703: Loss 1.8186\n",
      "  Iter 321/703: Loss 1.8982\n",
      "  Iter 331/703: Loss 1.9294\n",
      "  Iter 341/703: Loss 1.7121\n",
      "  Iter 351/703: Loss 1.9868\n",
      "  Iter 361/703: Loss 1.7170\n",
      "  Iter 371/703: Loss 1.8478\n",
      "  Iter 381/703: Loss 1.6576\n",
      "  Iter 391/703: Loss 1.7006\n",
      "  Iter 401/703: Loss 1.7180\n",
      "  Iter 411/703: Loss 1.8760\n",
      "  Iter 421/703: Loss 1.7430\n",
      "  Iter 431/703: Loss 1.9500\n",
      "  Iter 441/703: Loss 1.6124\n",
      "  Iter 451/703: Loss 1.9206\n",
      "  Iter 461/703: Loss 1.8566\n",
      "  Iter 471/703: Loss 1.9040\n",
      "  Iter 481/703: Loss 2.1106\n",
      "  Iter 491/703: Loss 1.7199\n",
      "  Iter 501/703: Loss 1.7867\n",
      "  Iter 511/703: Loss 1.4406\n",
      "  Iter 521/703: Loss 1.6198\n",
      "  Iter 531/703: Loss 1.5076\n",
      "  Iter 541/703: Loss 1.7518\n",
      "  Iter 551/703: Loss 1.6984\n",
      "  Iter 561/703: Loss 1.5729\n",
      "  Iter 571/703: Loss 2.0429\n",
      "  Iter 581/703: Loss 1.9859\n",
      "  Iter 591/703: Loss 1.9529\n",
      "  Iter 601/703: Loss 2.1274\n",
      "  Iter 611/703: Loss 1.8924\n",
      "  Iter 621/703: Loss 1.9886\n",
      "  Iter 631/703: Loss 1.7938\n",
      "  Iter 641/703: Loss 1.6686\n",
      "  Iter 651/703: Loss 1.6704\n",
      "  Iter 661/703: Loss 1.7966\n",
      "  Iter 671/703: Loss 1.7779\n",
      "  Iter 681/703: Loss 1.7158\n",
      "  Iter 691/703: Loss 1.6997\n",
      "  Iter 701/703: Loss 1.6749\n",
      "  Iter 703/703: Loss 1.9200\n",
      "Fine Train Loss: 1.8641, Fine Train Acc: 0.5670, Val Acc: 0.4638, Val Loss: 2.0364\n",
      "Time: 3024.91s\n",
      "\n",
      "[Epoch 59/100]\n",
      "  Iter   1/703: Loss 1.8177\n",
      "  Iter  11/703: Loss 1.6746\n",
      "  Iter  21/703: Loss 1.8824\n",
      "  Iter  31/703: Loss 1.6051\n",
      "  Iter  41/703: Loss 1.5809\n",
      "  Iter  51/703: Loss 1.5400\n",
      "  Iter  61/703: Loss 1.7897\n",
      "  Iter  71/703: Loss 2.0961\n",
      "  Iter  81/703: Loss 1.6810\n",
      "  Iter  91/703: Loss 1.9648\n",
      "  Iter 101/703: Loss 1.8165\n",
      "  Iter 111/703: Loss 1.8026\n",
      "  Iter 121/703: Loss 1.8766\n",
      "  Iter 131/703: Loss 1.7399\n",
      "  Iter 141/703: Loss 1.4429\n",
      "  Iter 151/703: Loss 1.5382\n",
      "  Iter 161/703: Loss 1.2543\n",
      "  Iter 171/703: Loss 1.9179\n",
      "  Iter 181/703: Loss 1.6901\n",
      "  Iter 191/703: Loss 1.6700\n",
      "  Iter 201/703: Loss 1.5639\n",
      "  Iter 211/703: Loss 2.0220\n",
      "  Iter 221/703: Loss 1.9706\n",
      "  Iter 231/703: Loss 1.7804\n",
      "  Iter 241/703: Loss 1.9928\n",
      "  Iter 251/703: Loss 1.8230\n",
      "  Iter 261/703: Loss 1.8664\n",
      "  Iter 271/703: Loss 2.0631\n",
      "  Iter 281/703: Loss 2.0333\n",
      "  Iter 291/703: Loss 1.9557\n",
      "  Iter 301/703: Loss 1.9113\n",
      "  Iter 311/703: Loss 1.8424\n",
      "  Iter 321/703: Loss 2.1117\n",
      "  Iter 331/703: Loss 1.9893\n",
      "  Iter 341/703: Loss 2.0190\n",
      "  Iter 351/703: Loss 2.1293\n",
      "  Iter 361/703: Loss 1.6419\n",
      "  Iter 371/703: Loss 1.8506\n",
      "  Iter 381/703: Loss 1.8315\n",
      "  Iter 391/703: Loss 2.1813\n",
      "  Iter 401/703: Loss 1.5823\n",
      "  Iter 411/703: Loss 1.7244\n",
      "  Iter 421/703: Loss 1.8559\n",
      "  Iter 431/703: Loss 1.5362\n",
      "  Iter 441/703: Loss 2.0260\n",
      "  Iter 451/703: Loss 1.9981\n",
      "  Iter 461/703: Loss 1.6742\n",
      "  Iter 471/703: Loss 1.6804\n",
      "  Iter 481/703: Loss 1.7246\n",
      "  Iter 491/703: Loss 2.0455\n",
      "  Iter 501/703: Loss 1.8755\n",
      "  Iter 511/703: Loss 1.5003\n",
      "  Iter 521/703: Loss 1.7772\n",
      "  Iter 531/703: Loss 1.8070\n",
      "  Iter 541/703: Loss 2.1034\n",
      "  Iter 551/703: Loss 1.5453\n",
      "  Iter 561/703: Loss 1.8369\n",
      "  Iter 571/703: Loss 1.7554\n",
      "  Iter 581/703: Loss 1.7365\n",
      "  Iter 591/703: Loss 2.1951\n",
      "  Iter 601/703: Loss 1.8153\n",
      "  Iter 611/703: Loss 2.1604\n",
      "  Iter 621/703: Loss 1.5977\n",
      "  Iter 631/703: Loss 1.7726\n",
      "  Iter 641/703: Loss 1.9134\n",
      "  Iter 651/703: Loss 1.7949\n",
      "  Iter 661/703: Loss 1.9638\n",
      "  Iter 671/703: Loss 1.8045\n",
      "  Iter 681/703: Loss 1.7601\n",
      "  Iter 691/703: Loss 1.7522\n",
      "  Iter 701/703: Loss 2.1384\n",
      "  Iter 703/703: Loss 2.0578\n",
      "Fine Train Loss: 1.8595, Fine Train Acc: 0.5650, Val Acc: 0.4698, Val Loss: 2.0679\n",
      "Time: 3016.28s\n",
      "\n",
      "[Epoch 60/100]\n",
      "  Iter   1/703: Loss 1.9683\n",
      "  Iter  11/703: Loss 1.9099\n",
      "  Iter  21/703: Loss 1.6360\n",
      "  Iter  31/703: Loss 2.2164\n",
      "  Iter  41/703: Loss 1.9360\n",
      "  Iter  51/703: Loss 1.8045\n",
      "  Iter  61/703: Loss 1.8665\n",
      "  Iter  71/703: Loss 1.6362\n",
      "  Iter  81/703: Loss 1.9208\n",
      "  Iter  91/703: Loss 1.8883\n",
      "  Iter 101/703: Loss 1.8847\n",
      "  Iter 111/703: Loss 1.8704\n",
      "  Iter 121/703: Loss 1.8145\n",
      "  Iter 131/703: Loss 1.8088\n",
      "  Iter 141/703: Loss 1.8609\n",
      "  Iter 151/703: Loss 1.5388\n",
      "  Iter 161/703: Loss 1.7047\n",
      "  Iter 171/703: Loss 1.9661\n",
      "  Iter 181/703: Loss 1.6423\n",
      "  Iter 191/703: Loss 1.9763\n",
      "  Iter 201/703: Loss 1.8527\n",
      "  Iter 211/703: Loss 1.7650\n",
      "  Iter 221/703: Loss 1.9105\n",
      "  Iter 231/703: Loss 1.9573\n",
      "  Iter 241/703: Loss 2.0156\n",
      "  Iter 251/703: Loss 2.0384\n",
      "  Iter 261/703: Loss 1.9182\n",
      "  Iter 271/703: Loss 1.4011\n",
      "  Iter 281/703: Loss 1.8694\n",
      "  Iter 291/703: Loss 1.5893\n",
      "  Iter 301/703: Loss 1.8745\n",
      "  Iter 311/703: Loss 1.5694\n",
      "  Iter 321/703: Loss 1.6067\n",
      "  Iter 331/703: Loss 2.0614\n",
      "  Iter 341/703: Loss 2.0120\n",
      "  Iter 351/703: Loss 2.1208\n",
      "  Iter 361/703: Loss 1.6976\n",
      "  Iter 371/703: Loss 2.1510\n",
      "  Iter 381/703: Loss 1.7921\n",
      "  Iter 391/703: Loss 1.5531\n",
      "  Iter 401/703: Loss 1.8759\n",
      "  Iter 411/703: Loss 1.7225\n",
      "  Iter 421/703: Loss 1.7235\n",
      "  Iter 431/703: Loss 1.8066\n",
      "  Iter 441/703: Loss 1.8248\n",
      "  Iter 451/703: Loss 1.7587\n",
      "  Iter 461/703: Loss 1.9802\n",
      "  Iter 471/703: Loss 2.0985\n",
      "  Iter 481/703: Loss 1.7939\n",
      "  Iter 491/703: Loss 1.7620\n",
      "  Iter 501/703: Loss 1.7506\n",
      "  Iter 511/703: Loss 1.9398\n",
      "  Iter 521/703: Loss 2.0534\n",
      "  Iter 531/703: Loss 1.7563\n",
      "  Iter 541/703: Loss 1.8372\n",
      "  Iter 551/703: Loss 1.8377\n",
      "  Iter 561/703: Loss 2.2240\n",
      "  Iter 571/703: Loss 1.8644\n",
      "  Iter 581/703: Loss 1.8774\n",
      "  Iter 591/703: Loss 1.7275\n",
      "  Iter 601/703: Loss 2.2125\n",
      "  Iter 611/703: Loss 1.7174\n",
      "  Iter 621/703: Loss 2.1948\n",
      "  Iter 631/703: Loss 1.9375\n",
      "  Iter 641/703: Loss 1.8146\n",
      "  Iter 651/703: Loss 1.9350\n",
      "  Iter 661/703: Loss 2.0075\n",
      "  Iter 671/703: Loss 1.5505\n",
      "  Iter 681/703: Loss 1.6086\n",
      "  Iter 691/703: Loss 2.1335\n",
      "  Iter 701/703: Loss 1.9320\n",
      "  Iter 703/703: Loss 1.9490\n",
      "Fine Train Loss: 1.8528, Fine Train Acc: 0.5750, Val Acc: 0.4756, Val Loss: 2.0138\n",
      "Time: 3012.76s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch60.pkl\n",
      "\n",
      "[Epoch 61/100]\n",
      "  Iter   1/703: Loss 2.0819\n",
      "  Iter  11/703: Loss 2.0634\n",
      "  Iter  21/703: Loss 2.2324\n",
      "  Iter  31/703: Loss 1.6968\n",
      "  Iter  41/703: Loss 1.8836\n",
      "  Iter  51/703: Loss 1.4332\n",
      "  Iter  61/703: Loss 2.0201\n",
      "  Iter  71/703: Loss 1.6929\n",
      "  Iter  81/703: Loss 1.7298\n",
      "  Iter  91/703: Loss 1.5571\n",
      "  Iter 101/703: Loss 1.6184\n",
      "  Iter 111/703: Loss 1.8249\n",
      "  Iter 121/703: Loss 1.8977\n",
      "  Iter 131/703: Loss 1.8598\n",
      "  Iter 141/703: Loss 1.5689\n",
      "  Iter 151/703: Loss 1.5394\n",
      "  Iter 161/703: Loss 1.7797\n",
      "  Iter 171/703: Loss 1.6806\n",
      "  Iter 181/703: Loss 1.7200\n",
      "  Iter 191/703: Loss 2.0911\n",
      "  Iter 201/703: Loss 2.0120\n",
      "  Iter 211/703: Loss 2.0476\n",
      "  Iter 221/703: Loss 1.8092\n",
      "  Iter 231/703: Loss 2.3886\n",
      "  Iter 241/703: Loss 1.6087\n",
      "  Iter 251/703: Loss 1.7559\n",
      "  Iter 261/703: Loss 2.1759\n",
      "  Iter 271/703: Loss 2.3614\n",
      "  Iter 281/703: Loss 1.9289\n",
      "  Iter 291/703: Loss 1.6910\n",
      "  Iter 301/703: Loss 1.9433\n",
      "  Iter 311/703: Loss 1.8738\n",
      "  Iter 321/703: Loss 1.4132\n",
      "  Iter 331/703: Loss 2.1771\n",
      "  Iter 341/703: Loss 1.8338\n",
      "  Iter 351/703: Loss 1.4185\n",
      "  Iter 361/703: Loss 1.7307\n",
      "  Iter 371/703: Loss 1.5526\n",
      "  Iter 381/703: Loss 1.9787\n",
      "  Iter 391/703: Loss 2.0517\n",
      "  Iter 401/703: Loss 1.6351\n",
      "  Iter 411/703: Loss 1.6985\n",
      "  Iter 421/703: Loss 1.7537\n",
      "  Iter 431/703: Loss 1.6517\n",
      "  Iter 441/703: Loss 1.6188\n",
      "  Iter 451/703: Loss 2.1065\n",
      "  Iter 461/703: Loss 2.1589\n",
      "  Iter 471/703: Loss 1.8504\n",
      "  Iter 481/703: Loss 2.1889\n",
      "  Iter 491/703: Loss 1.8914\n",
      "  Iter 501/703: Loss 2.2651\n",
      "  Iter 511/703: Loss 1.3254\n",
      "  Iter 521/703: Loss 2.0121\n",
      "  Iter 531/703: Loss 1.8009\n",
      "  Iter 541/703: Loss 1.7441\n",
      "  Iter 551/703: Loss 2.0507\n",
      "  Iter 561/703: Loss 1.6816\n",
      "  Iter 571/703: Loss 1.9655\n",
      "  Iter 581/703: Loss 2.1130\n",
      "  Iter 591/703: Loss 1.6849\n",
      "  Iter 601/703: Loss 1.7100\n",
      "  Iter 611/703: Loss 2.0125\n",
      "  Iter 621/703: Loss 2.0258\n",
      "  Iter 631/703: Loss 1.6717\n",
      "  Iter 641/703: Loss 2.1962\n",
      "  Iter 651/703: Loss 1.3398\n",
      "  Iter 661/703: Loss 1.9048\n",
      "  Iter 671/703: Loss 1.8812\n",
      "  Iter 681/703: Loss 2.0568\n",
      "  Iter 691/703: Loss 2.3302\n",
      "  Iter 701/703: Loss 1.9149\n",
      "  Iter 703/703: Loss 1.6139\n",
      "Fine Train Loss: 1.8413, Fine Train Acc: 0.5880, Val Acc: 0.4644, Val Loss: 2.0384\n",
      "Time: 3011.48s\n",
      "\n",
      "[Epoch 62/100]\n",
      "  Iter   1/703: Loss 2.0466\n",
      "  Iter  11/703: Loss 1.8823\n",
      "  Iter  21/703: Loss 1.5639\n",
      "  Iter  31/703: Loss 1.5419\n",
      "  Iter  41/703: Loss 1.9954\n",
      "  Iter  51/703: Loss 1.6403\n",
      "  Iter  61/703: Loss 1.9569\n",
      "  Iter  71/703: Loss 1.5660\n",
      "  Iter  81/703: Loss 1.4205\n",
      "  Iter  91/703: Loss 1.5207\n",
      "  Iter 101/703: Loss 1.4581\n",
      "  Iter 111/703: Loss 1.4354\n",
      "  Iter 121/703: Loss 1.8351\n",
      "  Iter 131/703: Loss 1.5858\n",
      "  Iter 141/703: Loss 1.8054\n",
      "  Iter 151/703: Loss 2.0734\n",
      "  Iter 161/703: Loss 1.3205\n",
      "  Iter 171/703: Loss 2.2394\n",
      "  Iter 181/703: Loss 1.8432\n",
      "  Iter 191/703: Loss 2.0094\n",
      "  Iter 201/703: Loss 1.3194\n",
      "  Iter 211/703: Loss 1.5549\n",
      "  Iter 221/703: Loss 1.6289\n",
      "  Iter 231/703: Loss 1.8453\n",
      "  Iter 241/703: Loss 1.7500\n",
      "  Iter 251/703: Loss 1.8442\n",
      "  Iter 261/703: Loss 1.5925\n",
      "  Iter 271/703: Loss 1.7889\n",
      "  Iter 281/703: Loss 1.9079\n",
      "  Iter 291/703: Loss 1.5148\n",
      "  Iter 301/703: Loss 1.7706\n",
      "  Iter 311/703: Loss 1.7299\n",
      "  Iter 321/703: Loss 1.7313\n",
      "  Iter 331/703: Loss 1.5193\n",
      "  Iter 341/703: Loss 1.7578\n",
      "  Iter 351/703: Loss 1.8659\n",
      "  Iter 361/703: Loss 1.8536\n",
      "  Iter 371/703: Loss 1.6700\n",
      "  Iter 381/703: Loss 1.6437\n",
      "  Iter 391/703: Loss 2.0066\n",
      "  Iter 401/703: Loss 1.8568\n",
      "  Iter 411/703: Loss 2.1388\n",
      "  Iter 421/703: Loss 1.9838\n",
      "  Iter 431/703: Loss 1.8420\n",
      "  Iter 441/703: Loss 1.7888\n",
      "  Iter 451/703: Loss 1.9642\n",
      "  Iter 461/703: Loss 1.7776\n",
      "  Iter 471/703: Loss 1.8950\n",
      "  Iter 481/703: Loss 1.8564\n",
      "  Iter 491/703: Loss 2.0733\n",
      "  Iter 501/703: Loss 1.5463\n",
      "  Iter 511/703: Loss 1.7865\n",
      "  Iter 521/703: Loss 1.9036\n",
      "  Iter 531/703: Loss 1.7896\n",
      "  Iter 541/703: Loss 2.0210\n",
      "  Iter 551/703: Loss 1.7318\n",
      "  Iter 561/703: Loss 1.7764\n",
      "  Iter 571/703: Loss 1.8425\n",
      "  Iter 581/703: Loss 1.8986\n",
      "  Iter 591/703: Loss 2.3734\n",
      "  Iter 601/703: Loss 1.8498\n",
      "  Iter 611/703: Loss 2.6344\n",
      "  Iter 621/703: Loss 1.8333\n",
      "  Iter 631/703: Loss 1.7870\n",
      "  Iter 641/703: Loss 1.8134\n",
      "  Iter 651/703: Loss 1.9992\n",
      "  Iter 661/703: Loss 1.9522\n",
      "  Iter 671/703: Loss 1.4498\n",
      "  Iter 681/703: Loss 1.8564\n",
      "  Iter 691/703: Loss 2.0298\n",
      "  Iter 701/703: Loss 1.5805\n",
      "  Iter 703/703: Loss 1.8469\n",
      "Fine Train Loss: 1.8249, Fine Train Acc: 0.5850, Val Acc: 0.4622, Val Loss: 2.0362\n",
      "Time: 3017.00s\n",
      "\n",
      "[Epoch 63/100]\n",
      "  Iter   1/703: Loss 1.6804\n",
      "  Iter  11/703: Loss 1.8905\n",
      "  Iter  21/703: Loss 1.8949\n",
      "  Iter  31/703: Loss 1.8683\n",
      "  Iter  41/703: Loss 1.6738\n",
      "  Iter  51/703: Loss 1.8868\n",
      "  Iter  61/703: Loss 1.6750\n",
      "  Iter  71/703: Loss 1.7124\n",
      "  Iter  81/703: Loss 1.9104\n",
      "  Iter  91/703: Loss 1.7820\n",
      "  Iter 101/703: Loss 1.6453\n",
      "  Iter 111/703: Loss 1.8137\n",
      "  Iter 121/703: Loss 1.8000\n",
      "  Iter 131/703: Loss 1.5812\n",
      "  Iter 141/703: Loss 2.0498\n",
      "  Iter 151/703: Loss 1.9640\n",
      "  Iter 161/703: Loss 1.3193\n",
      "  Iter 171/703: Loss 2.0058\n",
      "  Iter 181/703: Loss 2.1748\n",
      "  Iter 191/703: Loss 1.3424\n",
      "  Iter 201/703: Loss 1.7829\n",
      "  Iter 211/703: Loss 2.0337\n",
      "  Iter 221/703: Loss 1.5729\n",
      "  Iter 231/703: Loss 1.9609\n",
      "  Iter 241/703: Loss 1.9142\n",
      "  Iter 251/703: Loss 1.9575\n",
      "  Iter 261/703: Loss 2.2263\n",
      "  Iter 271/703: Loss 1.8455\n",
      "  Iter 281/703: Loss 1.7605\n",
      "  Iter 291/703: Loss 2.0922\n",
      "  Iter 301/703: Loss 1.8340\n",
      "  Iter 311/703: Loss 1.8666\n",
      "  Iter 321/703: Loss 1.3240\n",
      "  Iter 331/703: Loss 1.8467\n",
      "  Iter 341/703: Loss 1.9332\n",
      "  Iter 351/703: Loss 1.4757\n",
      "  Iter 361/703: Loss 1.6705\n",
      "  Iter 371/703: Loss 1.7583\n",
      "  Iter 381/703: Loss 1.9689\n",
      "  Iter 391/703: Loss 1.5497\n",
      "  Iter 401/703: Loss 2.0759\n",
      "  Iter 411/703: Loss 1.8949\n",
      "  Iter 421/703: Loss 2.1154\n",
      "  Iter 431/703: Loss 1.8006\n",
      "  Iter 441/703: Loss 1.9119\n",
      "  Iter 451/703: Loss 1.6648\n",
      "  Iter 461/703: Loss 1.5753\n",
      "  Iter 471/703: Loss 1.8207\n",
      "  Iter 481/703: Loss 1.4026\n",
      "  Iter 491/703: Loss 1.6668\n",
      "  Iter 501/703: Loss 1.7931\n",
      "  Iter 511/703: Loss 1.7550\n",
      "  Iter 521/703: Loss 2.0985\n",
      "  Iter 531/703: Loss 1.6608\n",
      "  Iter 541/703: Loss 1.5062\n",
      "  Iter 551/703: Loss 1.9545\n",
      "  Iter 561/703: Loss 1.8517\n",
      "  Iter 571/703: Loss 1.5937\n",
      "  Iter 581/703: Loss 1.4413\n",
      "  Iter 591/703: Loss 1.9117\n",
      "  Iter 601/703: Loss 1.9803\n",
      "  Iter 611/703: Loss 2.1582\n",
      "  Iter 621/703: Loss 1.9565\n",
      "  Iter 631/703: Loss 1.9445\n",
      "  Iter 641/703: Loss 1.6906\n",
      "  Iter 651/703: Loss 1.7929\n",
      "  Iter 661/703: Loss 1.8527\n",
      "  Iter 671/703: Loss 1.7699\n",
      "  Iter 681/703: Loss 1.8459\n",
      "  Iter 691/703: Loss 1.8975\n",
      "  Iter 701/703: Loss 2.0598\n",
      "  Iter 703/703: Loss 1.8392\n",
      "Fine Train Loss: 1.8331, Fine Train Acc: 0.5810, Val Acc: 0.4690, Val Loss: 2.0309\n",
      "Time: 3012.63s\n",
      "\n",
      "[Epoch 64/100]\n",
      "  Iter   1/703: Loss 1.8649\n",
      "  Iter  11/703: Loss 1.9321\n",
      "  Iter  21/703: Loss 2.1117\n",
      "  Iter  31/703: Loss 1.6841\n",
      "  Iter  41/703: Loss 1.9009\n",
      "  Iter  51/703: Loss 1.8447\n",
      "  Iter  61/703: Loss 1.9240\n",
      "  Iter  71/703: Loss 1.9698\n",
      "  Iter  81/703: Loss 1.6870\n",
      "  Iter  91/703: Loss 1.7794\n",
      "  Iter 101/703: Loss 1.7579\n",
      "  Iter 111/703: Loss 1.8399\n",
      "  Iter 121/703: Loss 1.8860\n",
      "  Iter 131/703: Loss 1.7981\n",
      "  Iter 141/703: Loss 1.6242\n",
      "  Iter 151/703: Loss 1.7830\n",
      "  Iter 161/703: Loss 1.6342\n",
      "  Iter 171/703: Loss 2.0135\n",
      "  Iter 181/703: Loss 1.8561\n",
      "  Iter 191/703: Loss 1.9712\n",
      "  Iter 201/703: Loss 1.8830\n",
      "  Iter 211/703: Loss 1.7519\n",
      "  Iter 221/703: Loss 1.4220\n",
      "  Iter 231/703: Loss 1.6392\n",
      "  Iter 241/703: Loss 1.5550\n",
      "  Iter 251/703: Loss 1.7565\n",
      "  Iter 261/703: Loss 1.7489\n",
      "  Iter 271/703: Loss 1.7748\n",
      "  Iter 281/703: Loss 1.8175\n",
      "  Iter 291/703: Loss 2.0685\n",
      "  Iter 301/703: Loss 2.1384\n",
      "  Iter 311/703: Loss 2.0429\n",
      "  Iter 321/703: Loss 2.1333\n",
      "  Iter 331/703: Loss 1.7639\n",
      "  Iter 341/703: Loss 1.7621\n",
      "  Iter 351/703: Loss 1.8344\n",
      "  Iter 361/703: Loss 1.8457\n",
      "  Iter 371/703: Loss 1.9567\n",
      "  Iter 381/703: Loss 2.0455\n",
      "  Iter 391/703: Loss 1.8724\n",
      "  Iter 401/703: Loss 1.3275\n",
      "  Iter 411/703: Loss 2.0481\n",
      "  Iter 421/703: Loss 1.6166\n",
      "  Iter 431/703: Loss 1.6373\n",
      "  Iter 441/703: Loss 1.7654\n",
      "  Iter 451/703: Loss 1.9050\n",
      "  Iter 461/703: Loss 1.7455\n",
      "  Iter 471/703: Loss 1.8960\n",
      "  Iter 481/703: Loss 2.1272\n",
      "  Iter 491/703: Loss 1.7743\n",
      "  Iter 501/703: Loss 1.8212\n",
      "  Iter 511/703: Loss 1.6236\n",
      "  Iter 521/703: Loss 1.7055\n",
      "  Iter 531/703: Loss 1.9190\n",
      "  Iter 541/703: Loss 1.8079\n",
      "  Iter 551/703: Loss 1.9716\n",
      "  Iter 561/703: Loss 1.9793\n",
      "  Iter 571/703: Loss 1.6173\n",
      "  Iter 581/703: Loss 1.7840\n",
      "  Iter 591/703: Loss 1.7807\n",
      "  Iter 601/703: Loss 1.7594\n",
      "  Iter 611/703: Loss 1.5264\n",
      "  Iter 621/703: Loss 1.5189\n",
      "  Iter 631/703: Loss 1.8698\n",
      "  Iter 641/703: Loss 1.9554\n",
      "  Iter 651/703: Loss 1.9799\n",
      "  Iter 661/703: Loss 2.1205\n",
      "  Iter 671/703: Loss 1.7600\n",
      "  Iter 681/703: Loss 2.0610\n",
      "  Iter 691/703: Loss 1.7976\n",
      "  Iter 701/703: Loss 1.8505\n",
      "  Iter 703/703: Loss 1.6556\n",
      "Fine Train Loss: 1.8059, Fine Train Acc: 0.5770, Val Acc: 0.4674, Val Loss: 2.0197\n",
      "Time: 3009.87s\n",
      "\n",
      "[Epoch 65/100]\n",
      "  Iter   1/703: Loss 1.7981\n",
      "  Iter  11/703: Loss 2.3076\n",
      "  Iter  21/703: Loss 2.1494\n",
      "  Iter  31/703: Loss 1.7948\n",
      "  Iter  41/703: Loss 1.8570\n",
      "  Iter  51/703: Loss 1.7733\n",
      "  Iter  61/703: Loss 2.0138\n",
      "  Iter  71/703: Loss 1.8380\n",
      "  Iter  81/703: Loss 1.5950\n",
      "  Iter  91/703: Loss 1.9176\n",
      "  Iter 101/703: Loss 2.2136\n",
      "  Iter 111/703: Loss 1.7645\n",
      "  Iter 121/703: Loss 1.6297\n",
      "  Iter 131/703: Loss 1.8117\n",
      "  Iter 141/703: Loss 1.5234\n",
      "  Iter 151/703: Loss 1.7421\n",
      "  Iter 161/703: Loss 1.8732\n",
      "  Iter 171/703: Loss 1.9787\n",
      "  Iter 181/703: Loss 1.7030\n",
      "  Iter 191/703: Loss 1.8568\n",
      "  Iter 201/703: Loss 2.0462\n",
      "  Iter 211/703: Loss 1.7538\n",
      "  Iter 221/703: Loss 1.3487\n",
      "  Iter 231/703: Loss 1.9181\n",
      "  Iter 241/703: Loss 1.5564\n",
      "  Iter 251/703: Loss 1.4292\n",
      "  Iter 261/703: Loss 1.7746\n",
      "  Iter 271/703: Loss 1.3740\n",
      "  Iter 281/703: Loss 1.6974\n",
      "  Iter 291/703: Loss 1.6906\n",
      "  Iter 301/703: Loss 1.9058\n",
      "  Iter 311/703: Loss 2.2798\n",
      "  Iter 321/703: Loss 1.8480\n",
      "  Iter 331/703: Loss 1.8333\n",
      "  Iter 341/703: Loss 1.8444\n",
      "  Iter 351/703: Loss 1.7895\n",
      "  Iter 361/703: Loss 1.6101\n",
      "  Iter 371/703: Loss 1.9860\n",
      "  Iter 381/703: Loss 1.3343\n",
      "  Iter 391/703: Loss 1.8918\n",
      "  Iter 401/703: Loss 1.8630\n",
      "  Iter 411/703: Loss 1.9183\n",
      "  Iter 421/703: Loss 1.8333\n",
      "  Iter 431/703: Loss 2.0634\n",
      "  Iter 441/703: Loss 1.8291\n",
      "  Iter 451/703: Loss 2.2504\n",
      "  Iter 461/703: Loss 1.6363\n",
      "  Iter 471/703: Loss 1.8877\n",
      "  Iter 481/703: Loss 1.8524\n",
      "  Iter 491/703: Loss 1.8241\n",
      "  Iter 501/703: Loss 2.0796\n",
      "  Iter 511/703: Loss 1.8498\n",
      "  Iter 521/703: Loss 1.7928\n",
      "  Iter 531/703: Loss 1.9928\n",
      "  Iter 541/703: Loss 1.6785\n",
      "  Iter 551/703: Loss 1.7040\n",
      "  Iter 561/703: Loss 1.8092\n",
      "  Iter 571/703: Loss 1.7872\n",
      "  Iter 581/703: Loss 1.6682\n",
      "  Iter 591/703: Loss 2.1428\n",
      "  Iter 601/703: Loss 1.7310\n",
      "  Iter 611/703: Loss 1.9381\n",
      "  Iter 621/703: Loss 1.7717\n",
      "  Iter 631/703: Loss 1.5213\n",
      "  Iter 641/703: Loss 1.4840\n",
      "  Iter 651/703: Loss 1.8748\n",
      "  Iter 661/703: Loss 1.7099\n",
      "  Iter 671/703: Loss 1.4784\n",
      "  Iter 681/703: Loss 2.0236\n",
      "  Iter 691/703: Loss 1.8662\n",
      "  Iter 701/703: Loss 1.8152\n",
      "  Iter 703/703: Loss 2.0215\n",
      "Fine Train Loss: 1.7961, Fine Train Acc: 0.5920, Val Acc: 0.4688, Val Loss: 2.0153\n",
      "Time: 3018.53s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch65.pkl\n",
      "\n",
      "[Epoch 66/100]\n",
      "  Iter   1/703: Loss 1.9531\n",
      "  Iter  11/703: Loss 1.7292\n",
      "  Iter  21/703: Loss 1.5970\n",
      "  Iter  31/703: Loss 1.6540\n",
      "  Iter  41/703: Loss 2.0078\n",
      "  Iter  51/703: Loss 1.8146\n",
      "  Iter  61/703: Loss 1.5114\n",
      "  Iter  71/703: Loss 2.0857\n",
      "  Iter  81/703: Loss 1.5651\n",
      "  Iter  91/703: Loss 1.7342\n",
      "  Iter 101/703: Loss 1.6698\n",
      "  Iter 111/703: Loss 2.1296\n",
      "  Iter 121/703: Loss 1.4434\n",
      "  Iter 131/703: Loss 2.0500\n",
      "  Iter 141/703: Loss 1.6690\n",
      "  Iter 151/703: Loss 1.6718\n",
      "  Iter 161/703: Loss 2.0093\n",
      "  Iter 171/703: Loss 2.0338\n",
      "  Iter 181/703: Loss 1.7375\n",
      "  Iter 191/703: Loss 1.8023\n",
      "  Iter 201/703: Loss 2.1816\n",
      "  Iter 211/703: Loss 1.5621\n",
      "  Iter 221/703: Loss 1.5141\n",
      "  Iter 231/703: Loss 1.8976\n",
      "  Iter 241/703: Loss 1.7746\n",
      "  Iter 251/703: Loss 1.6177\n",
      "  Iter 261/703: Loss 1.6851\n",
      "  Iter 271/703: Loss 1.9654\n",
      "  Iter 281/703: Loss 1.7377\n",
      "  Iter 291/703: Loss 1.7870\n",
      "  Iter 301/703: Loss 1.8138\n",
      "  Iter 311/703: Loss 1.5398\n",
      "  Iter 321/703: Loss 1.8049\n",
      "  Iter 331/703: Loss 1.3714\n",
      "  Iter 341/703: Loss 1.7198\n",
      "  Iter 351/703: Loss 1.7295\n",
      "  Iter 361/703: Loss 1.6532\n",
      "  Iter 371/703: Loss 2.0537\n",
      "  Iter 381/703: Loss 1.7542\n",
      "  Iter 391/703: Loss 1.7194\n",
      "  Iter 401/703: Loss 1.7087\n",
      "  Iter 411/703: Loss 2.0903\n",
      "  Iter 421/703: Loss 1.6113\n",
      "  Iter 431/703: Loss 1.5479\n",
      "  Iter 441/703: Loss 1.9079\n",
      "  Iter 451/703: Loss 1.7995\n",
      "  Iter 461/703: Loss 1.3361\n",
      "  Iter 471/703: Loss 1.7510\n",
      "  Iter 481/703: Loss 1.3645\n",
      "  Iter 491/703: Loss 1.7340\n",
      "  Iter 501/703: Loss 1.9548\n",
      "  Iter 511/703: Loss 1.8902\n",
      "  Iter 521/703: Loss 1.6121\n",
      "  Iter 531/703: Loss 1.7943\n",
      "  Iter 541/703: Loss 1.5004\n",
      "  Iter 551/703: Loss 1.6725\n",
      "  Iter 561/703: Loss 1.6883\n",
      "  Iter 571/703: Loss 1.4206\n",
      "  Iter 581/703: Loss 1.4690\n",
      "  Iter 591/703: Loss 1.5946\n",
      "  Iter 601/703: Loss 1.9647\n",
      "  Iter 611/703: Loss 1.8592\n",
      "  Iter 621/703: Loss 1.8972\n",
      "  Iter 631/703: Loss 1.4272\n",
      "  Iter 641/703: Loss 1.7637\n",
      "  Iter 651/703: Loss 1.9602\n",
      "  Iter 661/703: Loss 1.7032\n",
      "  Iter 671/703: Loss 1.8238\n",
      "  Iter 681/703: Loss 1.7543\n",
      "  Iter 691/703: Loss 1.4527\n",
      "  Iter 701/703: Loss 1.5785\n",
      "  Iter 703/703: Loss 1.6868\n",
      "Fine Train Loss: 1.7805, Fine Train Acc: 0.5990, Val Acc: 0.4734, Val Loss: 1.9915\n",
      "Time: 3007.34s\n",
      "\n",
      "[Epoch 67/100]\n",
      "  Iter   1/703: Loss 2.0144\n",
      "  Iter  11/703: Loss 1.8648\n",
      "  Iter  21/703: Loss 1.5907\n",
      "  Iter  31/703: Loss 1.7193\n",
      "  Iter  41/703: Loss 1.8463\n",
      "  Iter  51/703: Loss 1.7343\n",
      "  Iter  61/703: Loss 1.5397\n",
      "  Iter  71/703: Loss 1.7833\n",
      "  Iter  81/703: Loss 1.8224\n",
      "  Iter  91/703: Loss 1.4128\n",
      "  Iter 101/703: Loss 1.7955\n",
      "  Iter 111/703: Loss 2.0810\n",
      "  Iter 121/703: Loss 2.0169\n",
      "  Iter 131/703: Loss 1.7688\n",
      "  Iter 141/703: Loss 1.7576\n",
      "  Iter 151/703: Loss 1.5793\n",
      "  Iter 161/703: Loss 2.0123\n",
      "  Iter 171/703: Loss 1.9425\n",
      "  Iter 181/703: Loss 1.7580\n",
      "  Iter 191/703: Loss 1.7504\n",
      "  Iter 201/703: Loss 2.0397\n",
      "  Iter 211/703: Loss 1.8983\n",
      "  Iter 221/703: Loss 1.4799\n",
      "  Iter 231/703: Loss 2.0524\n",
      "  Iter 241/703: Loss 1.9666\n",
      "  Iter 251/703: Loss 1.5002\n",
      "  Iter 261/703: Loss 1.4753\n",
      "  Iter 271/703: Loss 1.7325\n",
      "  Iter 281/703: Loss 1.8121\n",
      "  Iter 291/703: Loss 2.0304\n",
      "  Iter 301/703: Loss 1.9571\n",
      "  Iter 311/703: Loss 1.9525\n",
      "  Iter 321/703: Loss 1.7300\n",
      "  Iter 331/703: Loss 1.8227\n",
      "  Iter 341/703: Loss 1.5372\n",
      "  Iter 351/703: Loss 1.7562\n",
      "  Iter 361/703: Loss 1.6059\n",
      "  Iter 371/703: Loss 1.7276\n",
      "  Iter 381/703: Loss 1.8657\n",
      "  Iter 391/703: Loss 1.6275\n",
      "  Iter 401/703: Loss 1.8659\n",
      "  Iter 411/703: Loss 1.7622\n",
      "  Iter 421/703: Loss 1.7079\n",
      "  Iter 431/703: Loss 2.1525\n",
      "  Iter 441/703: Loss 2.0680\n",
      "  Iter 451/703: Loss 1.8481\n",
      "  Iter 461/703: Loss 2.2926\n",
      "  Iter 471/703: Loss 1.8844\n",
      "  Iter 481/703: Loss 1.8643\n",
      "  Iter 491/703: Loss 1.9307\n",
      "  Iter 501/703: Loss 1.3377\n",
      "  Iter 511/703: Loss 1.9090\n",
      "  Iter 521/703: Loss 1.6016\n",
      "  Iter 531/703: Loss 1.8648\n",
      "  Iter 541/703: Loss 1.8069\n",
      "  Iter 551/703: Loss 1.5350\n",
      "  Iter 561/703: Loss 1.5323\n",
      "  Iter 571/703: Loss 1.9237\n",
      "  Iter 581/703: Loss 2.0708\n",
      "  Iter 591/703: Loss 1.6708\n",
      "  Iter 601/703: Loss 1.8946\n",
      "  Iter 611/703: Loss 1.6324\n",
      "  Iter 621/703: Loss 1.9576\n",
      "  Iter 631/703: Loss 1.7706\n",
      "  Iter 641/703: Loss 1.6535\n",
      "  Iter 651/703: Loss 1.7297\n",
      "  Iter 661/703: Loss 1.8677\n",
      "  Iter 671/703: Loss 1.7344\n",
      "  Iter 681/703: Loss 1.3843\n",
      "  Iter 691/703: Loss 1.6131\n",
      "  Iter 701/703: Loss 1.7993\n",
      "  Iter 703/703: Loss 1.8427\n",
      "Fine Train Loss: 1.7827, Fine Train Acc: 0.5900, Val Acc: 0.4812, Val Loss: 1.9715\n",
      "Time: 3007.87s\n",
      "\n",
      "[Epoch 68/100]\n",
      "  Iter   1/703: Loss 1.6081\n",
      "  Iter  11/703: Loss 1.7029\n",
      "  Iter  21/703: Loss 1.9035\n",
      "  Iter  31/703: Loss 1.8434\n",
      "  Iter  41/703: Loss 1.8083\n",
      "  Iter  51/703: Loss 1.8633\n",
      "  Iter  61/703: Loss 1.9019\n",
      "  Iter  71/703: Loss 1.9755\n",
      "  Iter  81/703: Loss 1.8873\n",
      "  Iter  91/703: Loss 1.6939\n",
      "  Iter 101/703: Loss 1.9302\n",
      "  Iter 111/703: Loss 1.6966\n",
      "  Iter 121/703: Loss 1.5515\n",
      "  Iter 131/703: Loss 1.8130\n",
      "  Iter 141/703: Loss 1.6219\n",
      "  Iter 151/703: Loss 2.0157\n",
      "  Iter 161/703: Loss 1.6049\n",
      "  Iter 171/703: Loss 1.7063\n",
      "  Iter 181/703: Loss 2.1043\n",
      "  Iter 191/703: Loss 1.3824\n",
      "  Iter 201/703: Loss 1.7819\n",
      "  Iter 211/703: Loss 1.7293\n",
      "  Iter 221/703: Loss 1.4980\n",
      "  Iter 231/703: Loss 1.6129\n",
      "  Iter 241/703: Loss 1.7398\n",
      "  Iter 251/703: Loss 1.7992\n",
      "  Iter 261/703: Loss 1.8654\n",
      "  Iter 271/703: Loss 1.6447\n",
      "  Iter 281/703: Loss 1.8069\n",
      "  Iter 291/703: Loss 1.8705\n",
      "  Iter 301/703: Loss 1.8201\n",
      "  Iter 311/703: Loss 1.6460\n",
      "  Iter 321/703: Loss 1.6173\n",
      "  Iter 331/703: Loss 2.0458\n",
      "  Iter 341/703: Loss 1.7029\n",
      "  Iter 351/703: Loss 1.6560\n",
      "  Iter 361/703: Loss 1.8911\n",
      "  Iter 371/703: Loss 1.5591\n",
      "  Iter 381/703: Loss 1.6028\n",
      "  Iter 391/703: Loss 1.7697\n",
      "  Iter 401/703: Loss 2.0970\n",
      "  Iter 411/703: Loss 1.8697\n",
      "  Iter 421/703: Loss 1.7213\n",
      "  Iter 431/703: Loss 1.9840\n",
      "  Iter 441/703: Loss 1.8410\n",
      "  Iter 451/703: Loss 1.8581\n",
      "  Iter 461/703: Loss 1.4968\n",
      "  Iter 471/703: Loss 1.9814\n",
      "  Iter 481/703: Loss 1.7937\n",
      "  Iter 491/703: Loss 1.6496\n",
      "  Iter 501/703: Loss 1.6135\n",
      "  Iter 511/703: Loss 1.8996\n",
      "  Iter 521/703: Loss 1.7359\n",
      "  Iter 531/703: Loss 2.2097\n",
      "  Iter 541/703: Loss 2.0821\n",
      "  Iter 551/703: Loss 1.6874\n",
      "  Iter 561/703: Loss 1.8299\n",
      "  Iter 571/703: Loss 1.9422\n",
      "  Iter 581/703: Loss 1.8075\n",
      "  Iter 591/703: Loss 1.8154\n",
      "  Iter 601/703: Loss 1.6994\n",
      "  Iter 611/703: Loss 1.4595\n",
      "  Iter 621/703: Loss 1.7240\n",
      "  Iter 631/703: Loss 1.4939\n",
      "  Iter 641/703: Loss 1.6508\n",
      "  Iter 651/703: Loss 2.1188\n",
      "  Iter 661/703: Loss 1.6148\n",
      "  Iter 671/703: Loss 1.6783\n",
      "  Iter 681/703: Loss 1.5443\n",
      "  Iter 691/703: Loss 1.7693\n",
      "  Iter 701/703: Loss 1.8912\n",
      "  Iter 703/703: Loss 1.5347\n",
      "Fine Train Loss: 1.7688, Fine Train Acc: 0.5950, Val Acc: 0.4742, Val Loss: 1.9794\n",
      "Time: 3020.64s\n",
      "\n",
      "[Epoch 69/100]\n",
      "  Iter   1/703: Loss 1.8517\n",
      "  Iter  11/703: Loss 1.6916\n",
      "  Iter  21/703: Loss 1.8315\n",
      "  Iter  31/703: Loss 1.9737\n",
      "  Iter  41/703: Loss 1.7134\n",
      "  Iter  51/703: Loss 1.6909\n",
      "  Iter  61/703: Loss 1.8223\n",
      "  Iter  71/703: Loss 1.9547\n",
      "  Iter  81/703: Loss 1.6050\n",
      "  Iter  91/703: Loss 1.6988\n",
      "  Iter 101/703: Loss 1.8122\n",
      "  Iter 111/703: Loss 1.8236\n",
      "  Iter 121/703: Loss 2.1569\n",
      "  Iter 131/703: Loss 1.8432\n",
      "  Iter 141/703: Loss 1.7465\n",
      "  Iter 151/703: Loss 1.5723\n",
      "  Iter 161/703: Loss 1.7464\n",
      "  Iter 171/703: Loss 1.4744\n",
      "  Iter 181/703: Loss 1.5170\n",
      "  Iter 191/703: Loss 2.1020\n",
      "  Iter 201/703: Loss 1.7468\n",
      "  Iter 211/703: Loss 1.9471\n",
      "  Iter 221/703: Loss 1.7124\n",
      "  Iter 231/703: Loss 1.6780\n",
      "  Iter 241/703: Loss 1.6015\n",
      "  Iter 251/703: Loss 1.7402\n",
      "  Iter 261/703: Loss 1.5999\n",
      "  Iter 271/703: Loss 1.6450\n",
      "  Iter 281/703: Loss 1.8417\n",
      "  Iter 291/703: Loss 1.7201\n",
      "  Iter 301/703: Loss 2.0552\n",
      "  Iter 311/703: Loss 1.8521\n",
      "  Iter 321/703: Loss 1.6474\n",
      "  Iter 331/703: Loss 1.7843\n",
      "  Iter 341/703: Loss 1.9464\n",
      "  Iter 351/703: Loss 2.0170\n",
      "  Iter 361/703: Loss 1.5408\n",
      "  Iter 371/703: Loss 1.8253\n",
      "  Iter 381/703: Loss 1.6015\n",
      "  Iter 391/703: Loss 1.6687\n",
      "  Iter 401/703: Loss 1.5770\n",
      "  Iter 411/703: Loss 1.6143\n",
      "  Iter 421/703: Loss 1.5391\n",
      "  Iter 431/703: Loss 1.3461\n",
      "  Iter 441/703: Loss 1.5706\n",
      "  Iter 451/703: Loss 1.6616\n",
      "  Iter 461/703: Loss 1.8913\n",
      "  Iter 471/703: Loss 1.6269\n",
      "  Iter 481/703: Loss 1.8153\n",
      "  Iter 491/703: Loss 1.7770\n",
      "  Iter 501/703: Loss 1.7774\n",
      "  Iter 511/703: Loss 1.5590\n",
      "  Iter 521/703: Loss 1.5677\n",
      "  Iter 531/703: Loss 1.8840\n",
      "  Iter 541/703: Loss 1.9721\n",
      "  Iter 551/703: Loss 2.0873\n",
      "  Iter 561/703: Loss 1.6470\n",
      "  Iter 571/703: Loss 1.6614\n",
      "  Iter 581/703: Loss 1.6849\n",
      "  Iter 591/703: Loss 1.6642\n",
      "  Iter 601/703: Loss 1.8532\n",
      "  Iter 611/703: Loss 1.6459\n",
      "  Iter 621/703: Loss 1.7107\n",
      "  Iter 631/703: Loss 1.6701\n",
      "  Iter 641/703: Loss 1.7985\n",
      "  Iter 651/703: Loss 1.7631\n",
      "  Iter 661/703: Loss 1.9646\n",
      "  Iter 671/703: Loss 1.5694\n",
      "  Iter 681/703: Loss 2.3103\n",
      "  Iter 691/703: Loss 1.9363\n",
      "  Iter 701/703: Loss 2.2948\n",
      "  Iter 703/703: Loss 1.6999\n",
      "Fine Train Loss: 1.7656, Fine Train Acc: 0.5900, Val Acc: 0.4742, Val Loss: 1.9870\n",
      "Time: 3019.66s\n",
      "\n",
      "[Epoch 70/100]\n",
      "  Iter   1/703: Loss 1.7421\n",
      "  Iter  11/703: Loss 1.9281\n",
      "  Iter  21/703: Loss 1.9378\n",
      "  Iter  31/703: Loss 1.6869\n",
      "  Iter  41/703: Loss 1.6634\n",
      "  Iter  51/703: Loss 1.4619\n",
      "  Iter  61/703: Loss 1.9110\n",
      "  Iter  71/703: Loss 1.8963\n",
      "  Iter  81/703: Loss 1.7066\n",
      "  Iter  91/703: Loss 1.8004\n",
      "  Iter 101/703: Loss 1.5241\n",
      "  Iter 111/703: Loss 1.7266\n",
      "  Iter 121/703: Loss 1.9521\n",
      "  Iter 131/703: Loss 1.5072\n",
      "  Iter 141/703: Loss 1.8587\n",
      "  Iter 151/703: Loss 1.7650\n",
      "  Iter 161/703: Loss 1.6524\n",
      "  Iter 171/703: Loss 1.8895\n",
      "  Iter 181/703: Loss 1.9337\n",
      "  Iter 191/703: Loss 1.7547\n",
      "  Iter 201/703: Loss 1.6078\n",
      "  Iter 211/703: Loss 1.7682\n",
      "  Iter 221/703: Loss 1.8100\n",
      "  Iter 231/703: Loss 1.6295\n",
      "  Iter 241/703: Loss 2.1140\n",
      "  Iter 251/703: Loss 1.9081\n",
      "  Iter 261/703: Loss 1.5069\n",
      "  Iter 271/703: Loss 2.0235\n",
      "  Iter 281/703: Loss 1.5994\n",
      "  Iter 291/703: Loss 1.9888\n",
      "  Iter 301/703: Loss 2.0893\n",
      "  Iter 311/703: Loss 1.5135\n",
      "  Iter 321/703: Loss 1.8033\n",
      "  Iter 331/703: Loss 1.8606\n",
      "  Iter 341/703: Loss 1.4578\n",
      "  Iter 351/703: Loss 1.6375\n",
      "  Iter 361/703: Loss 1.9325\n",
      "  Iter 371/703: Loss 1.7762\n",
      "  Iter 381/703: Loss 1.5671\n",
      "  Iter 391/703: Loss 1.9011\n",
      "  Iter 401/703: Loss 1.8649\n",
      "  Iter 411/703: Loss 1.6339\n",
      "  Iter 421/703: Loss 1.8267\n",
      "  Iter 431/703: Loss 1.4356\n",
      "  Iter 441/703: Loss 1.6600\n",
      "  Iter 451/703: Loss 1.7587\n",
      "  Iter 461/703: Loss 1.6419\n",
      "  Iter 471/703: Loss 1.6309\n",
      "  Iter 481/703: Loss 1.5104\n",
      "  Iter 491/703: Loss 1.5172\n",
      "  Iter 501/703: Loss 1.6764\n",
      "  Iter 511/703: Loss 1.4303\n",
      "  Iter 521/703: Loss 2.0378\n",
      "  Iter 531/703: Loss 1.8680\n",
      "  Iter 541/703: Loss 1.6478\n",
      "  Iter 551/703: Loss 1.9367\n",
      "  Iter 561/703: Loss 1.7812\n",
      "  Iter 571/703: Loss 1.7146\n",
      "  Iter 581/703: Loss 2.1182\n",
      "  Iter 591/703: Loss 1.8436\n",
      "  Iter 601/703: Loss 1.8943\n",
      "  Iter 611/703: Loss 1.4359\n",
      "  Iter 621/703: Loss 1.7775\n",
      "  Iter 631/703: Loss 1.6197\n",
      "  Iter 641/703: Loss 1.6371\n",
      "  Iter 651/703: Loss 1.6993\n",
      "  Iter 661/703: Loss 1.8843\n",
      "  Iter 671/703: Loss 1.8415\n",
      "  Iter 681/703: Loss 1.8512\n",
      "  Iter 691/703: Loss 1.9158\n",
      "  Iter 701/703: Loss 1.7917\n",
      "  Iter 703/703: Loss 1.5679\n",
      "Fine Train Loss: 1.7571, Fine Train Acc: 0.5910, Val Acc: 0.4774, Val Loss: 1.9973\n",
      "Time: 3020.20s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch70.pkl\n",
      "\n",
      "[Epoch 71/100]\n",
      "  Iter   1/703: Loss 1.4672\n",
      "  Iter  11/703: Loss 1.9119\n",
      "  Iter  21/703: Loss 1.8441\n",
      "  Iter  31/703: Loss 1.7842\n",
      "  Iter  41/703: Loss 1.7692\n",
      "  Iter  51/703: Loss 1.7764\n",
      "  Iter  61/703: Loss 1.6453\n",
      "  Iter  71/703: Loss 1.6723\n",
      "  Iter  81/703: Loss 1.6276\n",
      "  Iter  91/703: Loss 1.6137\n",
      "  Iter 101/703: Loss 1.8085\n",
      "  Iter 111/703: Loss 1.4916\n",
      "  Iter 121/703: Loss 1.8926\n",
      "  Iter 131/703: Loss 1.9350\n",
      "  Iter 141/703: Loss 1.6630\n",
      "  Iter 151/703: Loss 1.5669\n",
      "  Iter 161/703: Loss 1.6317\n",
      "  Iter 171/703: Loss 1.5949\n",
      "  Iter 181/703: Loss 1.4323\n",
      "  Iter 191/703: Loss 2.0478\n",
      "  Iter 201/703: Loss 2.0391\n",
      "  Iter 211/703: Loss 1.5952\n",
      "  Iter 221/703: Loss 1.6501\n",
      "  Iter 231/703: Loss 1.8233\n",
      "  Iter 241/703: Loss 1.8685\n",
      "  Iter 251/703: Loss 1.7584\n",
      "  Iter 261/703: Loss 1.6494\n",
      "  Iter 271/703: Loss 1.4439\n",
      "  Iter 281/703: Loss 1.5707\n",
      "  Iter 291/703: Loss 2.0090\n",
      "  Iter 301/703: Loss 1.5272\n",
      "  Iter 311/703: Loss 1.7623\n",
      "  Iter 321/703: Loss 1.9727\n",
      "  Iter 331/703: Loss 1.6223\n",
      "  Iter 341/703: Loss 2.0508\n",
      "  Iter 351/703: Loss 1.8148\n",
      "  Iter 361/703: Loss 1.7812\n",
      "  Iter 371/703: Loss 1.8594\n",
      "  Iter 381/703: Loss 1.9315\n",
      "  Iter 391/703: Loss 1.6037\n",
      "  Iter 401/703: Loss 2.1045\n",
      "  Iter 411/703: Loss 1.8464\n",
      "  Iter 421/703: Loss 1.7720\n",
      "  Iter 431/703: Loss 1.5894\n",
      "  Iter 441/703: Loss 1.6534\n",
      "  Iter 451/703: Loss 1.7742\n",
      "  Iter 461/703: Loss 1.8760\n",
      "  Iter 471/703: Loss 1.5791\n",
      "  Iter 481/703: Loss 1.9188\n",
      "  Iter 491/703: Loss 1.4622\n",
      "  Iter 501/703: Loss 1.8649\n",
      "  Iter 511/703: Loss 1.8165\n",
      "  Iter 521/703: Loss 1.5806\n",
      "  Iter 531/703: Loss 1.7379\n",
      "  Iter 541/703: Loss 1.6641\n",
      "  Iter 551/703: Loss 1.4656\n",
      "  Iter 561/703: Loss 1.8027\n",
      "  Iter 571/703: Loss 1.7063\n",
      "  Iter 581/703: Loss 2.1897\n",
      "  Iter 591/703: Loss 2.1094\n",
      "  Iter 601/703: Loss 1.6445\n",
      "  Iter 611/703: Loss 1.7268\n",
      "  Iter 621/703: Loss 1.3967\n",
      "  Iter 631/703: Loss 1.6463\n",
      "  Iter 641/703: Loss 1.9585\n",
      "  Iter 651/703: Loss 2.0138\n",
      "  Iter 661/703: Loss 1.6613\n",
      "  Iter 671/703: Loss 1.4932\n",
      "  Iter 681/703: Loss 1.6820\n",
      "  Iter 691/703: Loss 2.2719\n",
      "  Iter 701/703: Loss 1.5981\n",
      "  Iter 703/703: Loss 1.3516\n",
      "Fine Train Loss: 1.7421, Fine Train Acc: 0.6040, Val Acc: 0.4838, Val Loss: 1.9665\n",
      "Time: 3041.46s\n",
      "\n",
      "[Epoch 72/100]\n",
      "  Iter   1/703: Loss 2.0173\n",
      "  Iter  11/703: Loss 1.8946\n",
      "  Iter  21/703: Loss 1.5965\n",
      "  Iter  31/703: Loss 1.4856\n",
      "  Iter  41/703: Loss 1.9630\n",
      "  Iter  51/703: Loss 1.6622\n",
      "  Iter  61/703: Loss 1.5351\n",
      "  Iter  71/703: Loss 1.4538\n",
      "  Iter  81/703: Loss 1.9298\n",
      "  Iter  91/703: Loss 1.4399\n",
      "  Iter 101/703: Loss 1.5815\n",
      "  Iter 111/703: Loss 1.4943\n",
      "  Iter 121/703: Loss 1.5171\n",
      "  Iter 131/703: Loss 1.7745\n",
      "  Iter 141/703: Loss 1.6435\n",
      "  Iter 151/703: Loss 1.9661\n",
      "  Iter 161/703: Loss 1.6291\n",
      "  Iter 171/703: Loss 1.5075\n",
      "  Iter 181/703: Loss 2.0199\n",
      "  Iter 191/703: Loss 1.8354\n",
      "  Iter 201/703: Loss 1.6344\n",
      "  Iter 211/703: Loss 1.8600\n",
      "  Iter 221/703: Loss 1.9608\n",
      "  Iter 231/703: Loss 1.8728\n",
      "  Iter 241/703: Loss 1.7224\n",
      "  Iter 251/703: Loss 1.5387\n",
      "  Iter 261/703: Loss 1.7119\n",
      "  Iter 271/703: Loss 1.7625\n",
      "  Iter 281/703: Loss 1.3703\n",
      "  Iter 291/703: Loss 1.6071\n",
      "  Iter 301/703: Loss 1.9027\n",
      "  Iter 311/703: Loss 1.7719\n",
      "  Iter 321/703: Loss 1.6298\n",
      "  Iter 331/703: Loss 1.7815\n",
      "  Iter 341/703: Loss 1.6853\n",
      "  Iter 351/703: Loss 1.8446\n",
      "  Iter 361/703: Loss 1.6375\n",
      "  Iter 371/703: Loss 1.6799\n",
      "  Iter 381/703: Loss 1.9540\n",
      "  Iter 391/703: Loss 1.9316\n",
      "  Iter 401/703: Loss 1.7475\n",
      "  Iter 411/703: Loss 1.7516\n",
      "  Iter 421/703: Loss 1.7060\n",
      "  Iter 431/703: Loss 1.7204\n",
      "  Iter 441/703: Loss 2.0320\n",
      "  Iter 451/703: Loss 1.3649\n",
      "  Iter 461/703: Loss 1.4939\n",
      "  Iter 471/703: Loss 1.6992\n",
      "  Iter 481/703: Loss 1.3054\n",
      "  Iter 491/703: Loss 1.8915\n",
      "  Iter 501/703: Loss 1.4512\n",
      "  Iter 511/703: Loss 1.6540\n",
      "  Iter 521/703: Loss 1.6207\n",
      "  Iter 531/703: Loss 1.8024\n",
      "  Iter 541/703: Loss 1.8776\n",
      "  Iter 551/703: Loss 1.8527\n",
      "  Iter 561/703: Loss 1.9151\n",
      "  Iter 571/703: Loss 1.4864\n",
      "  Iter 581/703: Loss 1.4409\n",
      "  Iter 591/703: Loss 1.9026\n",
      "  Iter 601/703: Loss 1.6721\n",
      "  Iter 611/703: Loss 2.3248\n",
      "  Iter 621/703: Loss 1.7111\n",
      "  Iter 631/703: Loss 1.9553\n",
      "  Iter 641/703: Loss 1.8724\n",
      "  Iter 651/703: Loss 1.4608\n",
      "  Iter 661/703: Loss 1.9098\n",
      "  Iter 671/703: Loss 1.5854\n",
      "  Iter 681/703: Loss 1.6822\n",
      "  Iter 691/703: Loss 1.8957\n",
      "  Iter 701/703: Loss 1.9717\n",
      "  Iter 703/703: Loss 1.7176\n",
      "Fine Train Loss: 1.7454, Fine Train Acc: 0.6100, Val Acc: 0.4822, Val Loss: 2.0036\n",
      "Time: 3061.57s\n",
      "\n",
      "[Epoch 73/100]\n",
      "  Iter   1/703: Loss 1.6530\n",
      "  Iter  11/703: Loss 1.5682\n",
      "  Iter  21/703: Loss 1.6047\n",
      "  Iter  31/703: Loss 2.0015\n",
      "  Iter  41/703: Loss 1.7992\n",
      "  Iter  51/703: Loss 1.8654\n",
      "  Iter  61/703: Loss 1.7996\n",
      "  Iter  71/703: Loss 1.6919\n",
      "  Iter  81/703: Loss 1.5391\n",
      "  Iter  91/703: Loss 1.8869\n",
      "  Iter 101/703: Loss 1.8667\n",
      "  Iter 111/703: Loss 1.7948\n",
      "  Iter 121/703: Loss 1.5914\n",
      "  Iter 131/703: Loss 1.4005\n",
      "  Iter 141/703: Loss 1.6793\n",
      "  Iter 151/703: Loss 1.5084\n",
      "  Iter 161/703: Loss 1.9049\n",
      "  Iter 171/703: Loss 1.3802\n",
      "  Iter 181/703: Loss 1.7869\n",
      "  Iter 191/703: Loss 2.0261\n",
      "  Iter 201/703: Loss 1.8194\n",
      "  Iter 211/703: Loss 1.7295\n",
      "  Iter 221/703: Loss 1.7486\n",
      "  Iter 231/703: Loss 1.8504\n",
      "  Iter 241/703: Loss 1.3505\n",
      "  Iter 251/703: Loss 1.6351\n",
      "  Iter 261/703: Loss 1.6999\n",
      "  Iter 271/703: Loss 1.5564\n",
      "  Iter 281/703: Loss 1.5189\n",
      "  Iter 291/703: Loss 1.8135\n",
      "  Iter 301/703: Loss 1.3768\n",
      "  Iter 311/703: Loss 1.5182\n",
      "  Iter 321/703: Loss 1.9656\n",
      "  Iter 331/703: Loss 1.5563\n",
      "  Iter 341/703: Loss 1.6325\n",
      "  Iter 351/703: Loss 1.8657\n",
      "  Iter 361/703: Loss 1.8077\n",
      "  Iter 371/703: Loss 1.9131\n",
      "  Iter 381/703: Loss 1.7544\n",
      "  Iter 391/703: Loss 1.3885\n",
      "  Iter 401/703: Loss 1.6316\n",
      "  Iter 411/703: Loss 1.4524\n",
      "  Iter 421/703: Loss 1.3021\n",
      "  Iter 431/703: Loss 1.6156\n",
      "  Iter 441/703: Loss 1.3589\n",
      "  Iter 451/703: Loss 1.7724\n",
      "  Iter 461/703: Loss 1.7983\n",
      "  Iter 471/703: Loss 1.7726\n",
      "  Iter 481/703: Loss 1.3306\n",
      "  Iter 491/703: Loss 1.5734\n",
      "  Iter 501/703: Loss 1.8881\n",
      "  Iter 511/703: Loss 2.0075\n",
      "  Iter 521/703: Loss 1.7856\n",
      "  Iter 531/703: Loss 1.6399\n",
      "  Iter 541/703: Loss 1.9856\n",
      "  Iter 551/703: Loss 1.6645\n",
      "  Iter 561/703: Loss 1.8659\n",
      "  Iter 571/703: Loss 1.7654\n",
      "  Iter 581/703: Loss 1.4905\n",
      "  Iter 591/703: Loss 1.5945\n",
      "  Iter 601/703: Loss 1.8802\n",
      "  Iter 611/703: Loss 1.6302\n",
      "  Iter 621/703: Loss 1.6856\n",
      "  Iter 631/703: Loss 1.4487\n",
      "  Iter 641/703: Loss 2.2642\n",
      "  Iter 651/703: Loss 1.4280\n",
      "  Iter 661/703: Loss 1.5451\n",
      "  Iter 671/703: Loss 1.5852\n",
      "  Iter 681/703: Loss 1.8873\n",
      "  Iter 691/703: Loss 1.8686\n",
      "  Iter 701/703: Loss 2.3241\n",
      "  Iter 703/703: Loss 1.4096\n",
      "Fine Train Loss: 1.7211, Fine Train Acc: 0.6190, Val Acc: 0.4866, Val Loss: 1.9791\n",
      "Time: 3043.18s\n",
      "\n",
      "[Epoch 74/100]\n",
      "  Iter   1/703: Loss 1.4332\n",
      "  Iter  11/703: Loss 1.6622\n",
      "  Iter  21/703: Loss 1.5606\n",
      "  Iter  31/703: Loss 1.6376\n",
      "  Iter  41/703: Loss 2.2743\n",
      "  Iter  51/703: Loss 1.7287\n",
      "  Iter  61/703: Loss 1.5614\n",
      "  Iter  71/703: Loss 1.8774\n",
      "  Iter  81/703: Loss 1.8456\n",
      "  Iter  91/703: Loss 1.6087\n",
      "  Iter 101/703: Loss 1.7456\n",
      "  Iter 111/703: Loss 1.6925\n",
      "  Iter 121/703: Loss 1.8467\n",
      "  Iter 131/703: Loss 1.3210\n",
      "  Iter 141/703: Loss 1.7367\n",
      "  Iter 151/703: Loss 1.5979\n",
      "  Iter 161/703: Loss 1.8728\n",
      "  Iter 171/703: Loss 1.5838\n",
      "  Iter 181/703: Loss 1.6678\n",
      "  Iter 191/703: Loss 1.7364\n",
      "  Iter 201/703: Loss 1.4775\n",
      "  Iter 211/703: Loss 2.1952\n",
      "  Iter 221/703: Loss 2.0403\n",
      "  Iter 231/703: Loss 1.7943\n",
      "  Iter 241/703: Loss 1.3258\n",
      "  Iter 251/703: Loss 2.0250\n",
      "  Iter 261/703: Loss 1.8810\n",
      "  Iter 271/703: Loss 1.6952\n",
      "  Iter 281/703: Loss 1.9641\n",
      "  Iter 291/703: Loss 1.7735\n",
      "  Iter 301/703: Loss 1.7165\n",
      "  Iter 311/703: Loss 1.5826\n",
      "  Iter 321/703: Loss 1.4021\n",
      "  Iter 331/703: Loss 1.8794\n",
      "  Iter 341/703: Loss 1.6185\n",
      "  Iter 351/703: Loss 1.8787\n",
      "  Iter 361/703: Loss 2.0560\n",
      "  Iter 371/703: Loss 1.9143\n",
      "  Iter 381/703: Loss 1.8163\n",
      "  Iter 391/703: Loss 1.6134\n",
      "  Iter 401/703: Loss 1.7970\n",
      "  Iter 411/703: Loss 2.2131\n",
      "  Iter 421/703: Loss 1.5928\n",
      "  Iter 431/703: Loss 1.9814\n",
      "  Iter 441/703: Loss 1.5963\n",
      "  Iter 451/703: Loss 1.6846\n",
      "  Iter 461/703: Loss 1.9177\n",
      "  Iter 471/703: Loss 1.7140\n",
      "  Iter 481/703: Loss 1.5864\n",
      "  Iter 491/703: Loss 1.8483\n",
      "  Iter 501/703: Loss 1.5961\n",
      "  Iter 511/703: Loss 1.8756\n",
      "  Iter 521/703: Loss 1.4883\n",
      "  Iter 531/703: Loss 1.3940\n",
      "  Iter 541/703: Loss 1.8590\n",
      "  Iter 551/703: Loss 2.0143\n",
      "  Iter 561/703: Loss 1.3051\n",
      "  Iter 571/703: Loss 2.0049\n",
      "  Iter 581/703: Loss 1.6032\n",
      "  Iter 591/703: Loss 1.8288\n",
      "  Iter 601/703: Loss 1.7464\n",
      "  Iter 611/703: Loss 1.8225\n",
      "  Iter 621/703: Loss 1.8863\n",
      "  Iter 631/703: Loss 1.7535\n",
      "  Iter 641/703: Loss 1.8883\n",
      "  Iter 651/703: Loss 1.8370\n",
      "  Iter 661/703: Loss 1.6062\n",
      "  Iter 671/703: Loss 1.6995\n",
      "  Iter 681/703: Loss 1.9216\n",
      "  Iter 691/703: Loss 1.5564\n",
      "  Iter 701/703: Loss 1.2424\n",
      "  Iter 703/703: Loss 1.7746\n",
      "Fine Train Loss: 1.7254, Fine Train Acc: 0.6260, Val Acc: 0.4866, Val Loss: 1.9673\n",
      "Time: 3036.82s\n",
      "\n",
      "[Epoch 75/100]\n",
      "  Iter   1/703: Loss 1.7928\n",
      "  Iter  11/703: Loss 1.7363\n",
      "  Iter  21/703: Loss 1.6544\n",
      "  Iter  31/703: Loss 1.4661\n",
      "  Iter  41/703: Loss 1.9726\n",
      "  Iter  51/703: Loss 1.7292\n",
      "  Iter  61/703: Loss 1.6729\n",
      "  Iter  71/703: Loss 1.7064\n",
      "  Iter  81/703: Loss 1.5344\n",
      "  Iter  91/703: Loss 1.9402\n",
      "  Iter 101/703: Loss 1.6671\n",
      "  Iter 111/703: Loss 1.6505\n",
      "  Iter 121/703: Loss 1.9091\n",
      "  Iter 131/703: Loss 1.8931\n",
      "  Iter 141/703: Loss 1.5724\n",
      "  Iter 151/703: Loss 2.0758\n",
      "  Iter 161/703: Loss 1.9228\n",
      "  Iter 171/703: Loss 1.7291\n",
      "  Iter 181/703: Loss 1.7833\n",
      "  Iter 191/703: Loss 1.7625\n",
      "  Iter 201/703: Loss 1.8111\n",
      "  Iter 211/703: Loss 1.7283\n",
      "  Iter 221/703: Loss 1.7625\n",
      "  Iter 231/703: Loss 1.5875\n",
      "  Iter 241/703: Loss 2.0211\n",
      "  Iter 251/703: Loss 2.0942\n",
      "  Iter 261/703: Loss 1.5163\n",
      "  Iter 271/703: Loss 1.7891\n",
      "  Iter 281/703: Loss 1.6921\n",
      "  Iter 291/703: Loss 1.6680\n",
      "  Iter 301/703: Loss 2.1346\n",
      "  Iter 311/703: Loss 2.1551\n",
      "  Iter 321/703: Loss 1.7592\n",
      "  Iter 331/703: Loss 1.6801\n",
      "  Iter 341/703: Loss 1.5923\n",
      "  Iter 351/703: Loss 1.5910\n",
      "  Iter 361/703: Loss 1.8089\n",
      "  Iter 371/703: Loss 1.7068\n",
      "  Iter 381/703: Loss 1.4437\n",
      "  Iter 391/703: Loss 1.5768\n",
      "  Iter 401/703: Loss 1.7347\n",
      "  Iter 411/703: Loss 1.8525\n",
      "  Iter 421/703: Loss 1.9312\n",
      "  Iter 431/703: Loss 1.6941\n",
      "  Iter 441/703: Loss 1.6001\n",
      "  Iter 451/703: Loss 1.4836\n",
      "  Iter 461/703: Loss 1.3618\n",
      "  Iter 471/703: Loss 1.5786\n",
      "  Iter 481/703: Loss 1.6380\n",
      "  Iter 491/703: Loss 1.9714\n",
      "  Iter 501/703: Loss 1.7955\n",
      "  Iter 511/703: Loss 1.8838\n",
      "  Iter 521/703: Loss 1.7960\n",
      "  Iter 531/703: Loss 1.6747\n",
      "  Iter 541/703: Loss 1.7381\n",
      "  Iter 551/703: Loss 1.8223\n",
      "  Iter 561/703: Loss 1.5860\n",
      "  Iter 571/703: Loss 2.0129\n",
      "  Iter 581/703: Loss 1.4943\n",
      "  Iter 591/703: Loss 1.3645\n",
      "  Iter 601/703: Loss 1.6237\n",
      "  Iter 611/703: Loss 1.5567\n",
      "  Iter 621/703: Loss 1.3975\n",
      "  Iter 631/703: Loss 1.5854\n",
      "  Iter 641/703: Loss 1.7697\n",
      "  Iter 651/703: Loss 1.7931\n",
      "  Iter 661/703: Loss 1.3528\n",
      "  Iter 671/703: Loss 1.4373\n",
      "  Iter 681/703: Loss 1.9442\n",
      "  Iter 691/703: Loss 1.5661\n",
      "  Iter 701/703: Loss 1.8281\n",
      "  Iter 703/703: Loss 1.7890\n",
      "Fine Train Loss: 1.7200, Fine Train Acc: 0.6170, Val Acc: 0.4978, Val Loss: 1.9305\n",
      "Time: 3031.52s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch75.pkl\n",
      "\n",
      "[Epoch 76/100]\n",
      "  Iter   1/703: Loss 1.5321\n",
      "  Iter  11/703: Loss 1.6858\n",
      "  Iter  21/703: Loss 1.9521\n",
      "  Iter  31/703: Loss 1.5128\n",
      "  Iter  41/703: Loss 1.9503\n",
      "  Iter  51/703: Loss 1.9839\n",
      "  Iter  61/703: Loss 1.5480\n",
      "  Iter  71/703: Loss 1.4047\n",
      "  Iter  81/703: Loss 1.7230\n",
      "  Iter  91/703: Loss 1.5695\n",
      "  Iter 101/703: Loss 1.6366\n",
      "  Iter 111/703: Loss 1.4647\n",
      "  Iter 121/703: Loss 1.7273\n",
      "  Iter 131/703: Loss 1.7733\n",
      "  Iter 141/703: Loss 1.8391\n",
      "  Iter 151/703: Loss 1.7817\n",
      "  Iter 161/703: Loss 1.5768\n",
      "  Iter 171/703: Loss 1.6327\n",
      "  Iter 181/703: Loss 1.3538\n",
      "  Iter 191/703: Loss 1.7841\n",
      "  Iter 201/703: Loss 1.7669\n",
      "  Iter 211/703: Loss 1.8625\n",
      "  Iter 221/703: Loss 1.5701\n",
      "  Iter 231/703: Loss 1.8887\n",
      "  Iter 241/703: Loss 1.8114\n",
      "  Iter 251/703: Loss 1.6800\n",
      "  Iter 261/703: Loss 2.0448\n",
      "  Iter 271/703: Loss 1.6536\n",
      "  Iter 281/703: Loss 1.7874\n",
      "  Iter 291/703: Loss 1.8693\n",
      "  Iter 301/703: Loss 1.6972\n",
      "  Iter 311/703: Loss 1.8146\n",
      "  Iter 321/703: Loss 1.7281\n",
      "  Iter 331/703: Loss 1.5422\n",
      "  Iter 341/703: Loss 1.5397\n",
      "  Iter 351/703: Loss 1.9492\n",
      "  Iter 361/703: Loss 1.7508\n",
      "  Iter 371/703: Loss 1.7516\n",
      "  Iter 381/703: Loss 1.3631\n",
      "  Iter 391/703: Loss 1.5601\n",
      "  Iter 401/703: Loss 1.5205\n",
      "  Iter 411/703: Loss 1.5529\n",
      "  Iter 421/703: Loss 1.8484\n",
      "  Iter 431/703: Loss 1.7071\n",
      "  Iter 441/703: Loss 1.4000\n",
      "  Iter 451/703: Loss 1.7450\n",
      "  Iter 461/703: Loss 1.8333\n",
      "  Iter 471/703: Loss 1.8339\n",
      "  Iter 481/703: Loss 1.7654\n",
      "  Iter 491/703: Loss 2.1787\n",
      "  Iter 501/703: Loss 1.5659\n",
      "  Iter 511/703: Loss 1.9301\n",
      "  Iter 521/703: Loss 1.8241\n",
      "  Iter 531/703: Loss 1.6937\n",
      "  Iter 541/703: Loss 2.0635\n",
      "  Iter 551/703: Loss 1.7267\n",
      "  Iter 561/703: Loss 1.6310\n",
      "  Iter 571/703: Loss 1.6388\n",
      "  Iter 581/703: Loss 1.4087\n",
      "  Iter 591/703: Loss 1.9272\n",
      "  Iter 601/703: Loss 1.7666\n",
      "  Iter 611/703: Loss 1.7483\n",
      "  Iter 621/703: Loss 1.8901\n",
      "  Iter 631/703: Loss 1.9377\n",
      "  Iter 641/703: Loss 1.3567\n",
      "  Iter 651/703: Loss 1.0563\n",
      "  Iter 661/703: Loss 1.4271\n",
      "  Iter 671/703: Loss 1.4821\n",
      "  Iter 681/703: Loss 1.9058\n",
      "  Iter 691/703: Loss 1.8881\n",
      "  Iter 701/703: Loss 1.1782\n",
      "  Iter 703/703: Loss 1.9742\n",
      "Fine Train Loss: 1.6981, Fine Train Acc: 0.6210, Val Acc: 0.4930, Val Loss: 1.9412\n",
      "Time: 3007.90s\n",
      "\n",
      "[Epoch 77/100]\n",
      "  Iter   1/703: Loss 1.4804\n",
      "  Iter  11/703: Loss 1.6228\n",
      "  Iter  21/703: Loss 1.7218\n",
      "  Iter  31/703: Loss 1.9039\n",
      "  Iter  41/703: Loss 1.8130\n",
      "  Iter  51/703: Loss 1.8162\n",
      "  Iter  61/703: Loss 1.7514\n",
      "  Iter  71/703: Loss 1.6604\n",
      "  Iter  81/703: Loss 1.8398\n",
      "  Iter  91/703: Loss 1.6305\n",
      "  Iter 101/703: Loss 2.1905\n",
      "  Iter 111/703: Loss 1.3862\n",
      "  Iter 121/703: Loss 1.5279\n",
      "  Iter 131/703: Loss 1.8113\n",
      "  Iter 141/703: Loss 1.8785\n",
      "  Iter 151/703: Loss 1.8262\n",
      "  Iter 161/703: Loss 1.7400\n",
      "  Iter 171/703: Loss 1.8690\n",
      "  Iter 181/703: Loss 1.6356\n",
      "  Iter 191/703: Loss 1.5674\n",
      "  Iter 201/703: Loss 1.4858\n",
      "  Iter 211/703: Loss 1.6016\n",
      "  Iter 221/703: Loss 1.6506\n",
      "  Iter 231/703: Loss 1.8964\n",
      "  Iter 241/703: Loss 1.6485\n",
      "  Iter 251/703: Loss 1.6528\n",
      "  Iter 261/703: Loss 1.9903\n",
      "  Iter 271/703: Loss 1.6208\n",
      "  Iter 281/703: Loss 1.6949\n",
      "  Iter 291/703: Loss 1.5360\n",
      "  Iter 301/703: Loss 1.6455\n",
      "  Iter 311/703: Loss 1.9234\n",
      "  Iter 321/703: Loss 1.7473\n",
      "  Iter 331/703: Loss 1.6779\n",
      "  Iter 341/703: Loss 1.6837\n",
      "  Iter 351/703: Loss 1.8023\n",
      "  Iter 361/703: Loss 1.5693\n",
      "  Iter 371/703: Loss 1.8460\n",
      "  Iter 381/703: Loss 1.6102\n",
      "  Iter 391/703: Loss 1.8938\n",
      "  Iter 401/703: Loss 1.6254\n",
      "  Iter 411/703: Loss 1.6563\n",
      "  Iter 421/703: Loss 2.0028\n",
      "  Iter 431/703: Loss 1.7409\n",
      "  Iter 441/703: Loss 1.5705\n",
      "  Iter 451/703: Loss 1.6891\n",
      "  Iter 461/703: Loss 1.8146\n",
      "  Iter 471/703: Loss 1.5331\n",
      "  Iter 481/703: Loss 1.7781\n",
      "  Iter 491/703: Loss 1.4941\n",
      "  Iter 501/703: Loss 1.8619\n",
      "  Iter 511/703: Loss 1.5707\n",
      "  Iter 521/703: Loss 1.7538\n",
      "  Iter 531/703: Loss 1.6048\n",
      "  Iter 541/703: Loss 1.8296\n",
      "  Iter 551/703: Loss 1.9111\n",
      "  Iter 561/703: Loss 1.5489\n",
      "  Iter 571/703: Loss 1.8771\n",
      "  Iter 581/703: Loss 1.6414\n",
      "  Iter 591/703: Loss 1.9089\n",
      "  Iter 601/703: Loss 1.7180\n",
      "  Iter 611/703: Loss 1.7815\n",
      "  Iter 621/703: Loss 1.6865\n",
      "  Iter 631/703: Loss 1.5650\n",
      "  Iter 641/703: Loss 1.6465\n",
      "  Iter 651/703: Loss 1.7389\n",
      "  Iter 661/703: Loss 1.9579\n",
      "  Iter 671/703: Loss 1.3755\n",
      "  Iter 681/703: Loss 1.5970\n",
      "  Iter 691/703: Loss 1.4075\n",
      "  Iter 701/703: Loss 1.9308\n",
      "  Iter 703/703: Loss 1.6328\n",
      "Fine Train Loss: 1.6982, Fine Train Acc: 0.6160, Val Acc: 0.4872, Val Loss: 1.9460\n",
      "Time: 3009.22s\n",
      "\n",
      "[Epoch 78/100]\n",
      "  Iter   1/703: Loss 1.4272\n",
      "  Iter  11/703: Loss 1.4307\n",
      "  Iter  21/703: Loss 1.7630\n",
      "  Iter  31/703: Loss 1.8376\n",
      "  Iter  41/703: Loss 1.3489\n",
      "  Iter  51/703: Loss 1.4288\n",
      "  Iter  61/703: Loss 1.6514\n",
      "  Iter  71/703: Loss 1.5004\n",
      "  Iter  81/703: Loss 1.6915\n",
      "  Iter  91/703: Loss 1.9269\n",
      "  Iter 101/703: Loss 1.6689\n",
      "  Iter 111/703: Loss 1.8031\n",
      "  Iter 121/703: Loss 1.5928\n",
      "  Iter 131/703: Loss 1.5213\n",
      "  Iter 141/703: Loss 1.3006\n",
      "  Iter 151/703: Loss 1.6617\n",
      "  Iter 161/703: Loss 2.1102\n",
      "  Iter 171/703: Loss 1.4150\n",
      "  Iter 181/703: Loss 1.7923\n",
      "  Iter 191/703: Loss 1.5524\n",
      "  Iter 201/703: Loss 1.4581\n",
      "  Iter 211/703: Loss 2.2464\n",
      "  Iter 221/703: Loss 1.5938\n",
      "  Iter 231/703: Loss 1.8576\n",
      "  Iter 241/703: Loss 1.9385\n",
      "  Iter 251/703: Loss 1.6877\n",
      "  Iter 261/703: Loss 1.9549\n",
      "  Iter 271/703: Loss 2.0125\n",
      "  Iter 281/703: Loss 1.6942\n",
      "  Iter 291/703: Loss 1.4772\n",
      "  Iter 301/703: Loss 1.6457\n",
      "  Iter 311/703: Loss 1.4897\n",
      "  Iter 321/703: Loss 1.9098\n",
      "  Iter 331/703: Loss 1.6119\n",
      "  Iter 341/703: Loss 2.0656\n",
      "  Iter 351/703: Loss 1.6764\n",
      "  Iter 361/703: Loss 1.5825\n",
      "  Iter 371/703: Loss 1.4907\n",
      "  Iter 381/703: Loss 2.0683\n",
      "  Iter 391/703: Loss 1.8077\n",
      "  Iter 401/703: Loss 1.5309\n",
      "  Iter 411/703: Loss 1.7402\n",
      "  Iter 421/703: Loss 1.9467\n",
      "  Iter 431/703: Loss 1.8627\n",
      "  Iter 441/703: Loss 2.0541\n",
      "  Iter 451/703: Loss 1.6132\n",
      "  Iter 461/703: Loss 1.7252\n",
      "  Iter 471/703: Loss 1.6219\n",
      "  Iter 481/703: Loss 1.6414\n",
      "  Iter 491/703: Loss 1.5471\n",
      "  Iter 501/703: Loss 1.9280\n",
      "  Iter 511/703: Loss 1.4326\n",
      "  Iter 521/703: Loss 1.6328\n",
      "  Iter 531/703: Loss 1.6677\n",
      "  Iter 541/703: Loss 2.0086\n",
      "  Iter 551/703: Loss 1.8042\n",
      "  Iter 561/703: Loss 1.5870\n",
      "  Iter 571/703: Loss 1.5171\n",
      "  Iter 581/703: Loss 1.7936\n",
      "  Iter 591/703: Loss 1.8682\n",
      "  Iter 601/703: Loss 2.2643\n",
      "  Iter 611/703: Loss 1.4242\n",
      "  Iter 621/703: Loss 1.8360\n",
      "  Iter 631/703: Loss 1.4996\n",
      "  Iter 641/703: Loss 1.8142\n",
      "  Iter 651/703: Loss 1.7277\n",
      "  Iter 661/703: Loss 1.7675\n",
      "  Iter 671/703: Loss 1.6684\n",
      "  Iter 681/703: Loss 1.5880\n",
      "  Iter 691/703: Loss 1.8058\n",
      "  Iter 701/703: Loss 1.8367\n",
      "  Iter 703/703: Loss 1.7034\n",
      "Fine Train Loss: 1.6869, Fine Train Acc: 0.6040, Val Acc: 0.4970, Val Loss: 1.9331\n",
      "Time: 3018.85s\n",
      "\n",
      "[Epoch 79/100]\n",
      "  Iter   1/703: Loss 1.6254\n",
      "  Iter  11/703: Loss 1.6834\n",
      "  Iter  21/703: Loss 1.4975\n",
      "  Iter  31/703: Loss 1.8588\n",
      "  Iter  41/703: Loss 1.7899\n",
      "  Iter  51/703: Loss 1.2106\n",
      "  Iter  61/703: Loss 1.7954\n",
      "  Iter  71/703: Loss 1.3614\n",
      "  Iter  81/703: Loss 1.6722\n",
      "  Iter  91/703: Loss 1.8500\n",
      "  Iter 101/703: Loss 1.6661\n",
      "  Iter 111/703: Loss 1.6372\n",
      "  Iter 121/703: Loss 2.1739\n",
      "  Iter 131/703: Loss 1.4400\n",
      "  Iter 141/703: Loss 1.5126\n",
      "  Iter 151/703: Loss 1.7993\n",
      "  Iter 161/703: Loss 1.8083\n",
      "  Iter 171/703: Loss 1.9037\n",
      "  Iter 181/703: Loss 1.4401\n",
      "  Iter 191/703: Loss 1.4967\n",
      "  Iter 201/703: Loss 1.9472\n",
      "  Iter 211/703: Loss 1.2683\n",
      "  Iter 221/703: Loss 1.4955\n",
      "  Iter 231/703: Loss 1.5412\n",
      "  Iter 241/703: Loss 1.9860\n",
      "  Iter 251/703: Loss 1.7169\n",
      "  Iter 261/703: Loss 1.6446\n",
      "  Iter 271/703: Loss 1.9037\n",
      "  Iter 281/703: Loss 1.9462\n",
      "  Iter 291/703: Loss 1.5799\n",
      "  Iter 301/703: Loss 2.0551\n",
      "  Iter 311/703: Loss 1.4639\n",
      "  Iter 321/703: Loss 1.7662\n",
      "  Iter 331/703: Loss 1.8907\n",
      "  Iter 341/703: Loss 1.6774\n",
      "  Iter 351/703: Loss 1.5504\n",
      "  Iter 361/703: Loss 1.1893\n",
      "  Iter 371/703: Loss 1.4533\n",
      "  Iter 381/703: Loss 1.4679\n",
      "  Iter 391/703: Loss 1.3885\n",
      "  Iter 401/703: Loss 1.6634\n",
      "  Iter 411/703: Loss 1.7352\n",
      "  Iter 421/703: Loss 1.3648\n",
      "  Iter 431/703: Loss 1.5035\n",
      "  Iter 441/703: Loss 1.7546\n",
      "  Iter 451/703: Loss 1.1031\n",
      "  Iter 461/703: Loss 1.7059\n",
      "  Iter 471/703: Loss 1.8940\n",
      "  Iter 481/703: Loss 1.6861\n",
      "  Iter 491/703: Loss 1.6334\n",
      "  Iter 501/703: Loss 1.6717\n",
      "  Iter 511/703: Loss 1.7800\n",
      "  Iter 521/703: Loss 1.8484\n",
      "  Iter 531/703: Loss 1.5089\n",
      "  Iter 541/703: Loss 1.7104\n",
      "  Iter 551/703: Loss 1.3978\n",
      "  Iter 561/703: Loss 1.4407\n",
      "  Iter 571/703: Loss 1.3417\n",
      "  Iter 581/703: Loss 1.3473\n",
      "  Iter 591/703: Loss 1.6484\n",
      "  Iter 601/703: Loss 1.8319\n",
      "  Iter 611/703: Loss 1.8553\n",
      "  Iter 621/703: Loss 1.8668\n",
      "  Iter 631/703: Loss 1.3446\n",
      "  Iter 641/703: Loss 1.4246\n",
      "  Iter 651/703: Loss 1.5482\n",
      "  Iter 661/703: Loss 1.6967\n",
      "  Iter 671/703: Loss 1.7099\n",
      "  Iter 681/703: Loss 1.5845\n",
      "  Iter 691/703: Loss 1.8380\n",
      "  Iter 701/703: Loss 1.4619\n",
      "  Iter 703/703: Loss 1.5405\n",
      "Fine Train Loss: 1.6870, Fine Train Acc: 0.6230, Val Acc: 0.4904, Val Loss: 1.9552\n",
      "Time: 3017.01s\n",
      "\n",
      "[Epoch 80/100]\n",
      "  Iter   1/703: Loss 1.6963\n",
      "  Iter  11/703: Loss 1.7147\n",
      "  Iter  21/703: Loss 1.8467\n",
      "  Iter  31/703: Loss 1.4766\n",
      "  Iter  41/703: Loss 1.5535\n",
      "  Iter  51/703: Loss 1.6419\n",
      "  Iter  61/703: Loss 1.4979\n",
      "  Iter  71/703: Loss 1.5568\n",
      "  Iter  81/703: Loss 1.6699\n",
      "  Iter  91/703: Loss 1.5167\n",
      "  Iter 101/703: Loss 1.9290\n",
      "  Iter 111/703: Loss 1.6023\n",
      "  Iter 121/703: Loss 1.5951\n",
      "  Iter 131/703: Loss 1.4318\n",
      "  Iter 141/703: Loss 1.6667\n",
      "  Iter 151/703: Loss 1.7437\n",
      "  Iter 161/703: Loss 1.6201\n",
      "  Iter 171/703: Loss 1.4705\n",
      "  Iter 181/703: Loss 1.5746\n",
      "  Iter 191/703: Loss 1.6579\n",
      "  Iter 201/703: Loss 1.2472\n",
      "  Iter 211/703: Loss 1.9259\n",
      "  Iter 221/703: Loss 1.7625\n",
      "  Iter 231/703: Loss 1.7602\n",
      "  Iter 241/703: Loss 1.4502\n",
      "  Iter 251/703: Loss 1.9607\n",
      "  Iter 261/703: Loss 1.6680\n",
      "  Iter 271/703: Loss 1.7324\n",
      "  Iter 281/703: Loss 1.5174\n",
      "  Iter 291/703: Loss 1.8447\n",
      "  Iter 301/703: Loss 1.6730\n",
      "  Iter 311/703: Loss 1.6822\n",
      "  Iter 321/703: Loss 1.6598\n",
      "  Iter 331/703: Loss 1.6289\n",
      "  Iter 341/703: Loss 1.4405\n",
      "  Iter 351/703: Loss 1.4799\n",
      "  Iter 361/703: Loss 1.6228\n",
      "  Iter 371/703: Loss 1.6176\n",
      "  Iter 381/703: Loss 1.5882\n",
      "  Iter 391/703: Loss 1.6833\n",
      "  Iter 401/703: Loss 1.6898\n",
      "  Iter 411/703: Loss 1.5848\n",
      "  Iter 421/703: Loss 1.8368\n",
      "  Iter 431/703: Loss 1.6241\n",
      "  Iter 441/703: Loss 1.5160\n",
      "  Iter 451/703: Loss 1.6018\n",
      "  Iter 461/703: Loss 1.5340\n",
      "  Iter 471/703: Loss 1.5796\n",
      "  Iter 481/703: Loss 1.9278\n",
      "  Iter 491/703: Loss 1.6486\n",
      "  Iter 501/703: Loss 1.5528\n",
      "  Iter 511/703: Loss 1.8253\n",
      "  Iter 521/703: Loss 1.4365\n",
      "  Iter 531/703: Loss 1.7248\n",
      "  Iter 541/703: Loss 1.3568\n",
      "  Iter 551/703: Loss 1.8067\n",
      "  Iter 561/703: Loss 1.8044\n",
      "  Iter 571/703: Loss 1.8493\n",
      "  Iter 581/703: Loss 1.4145\n",
      "  Iter 591/703: Loss 1.4513\n",
      "  Iter 601/703: Loss 1.7227\n",
      "  Iter 611/703: Loss 1.5547\n",
      "  Iter 621/703: Loss 1.6659\n",
      "  Iter 631/703: Loss 1.4847\n",
      "  Iter 641/703: Loss 1.5244\n",
      "  Iter 651/703: Loss 1.8625\n",
      "  Iter 661/703: Loss 1.4779\n",
      "  Iter 671/703: Loss 1.8494\n",
      "  Iter 681/703: Loss 1.8214\n",
      "  Iter 691/703: Loss 1.7912\n",
      "  Iter 701/703: Loss 1.6388\n",
      "  Iter 703/703: Loss 1.5576\n",
      "Fine Train Loss: 1.6771, Fine Train Acc: 0.6260, Val Acc: 0.4886, Val Loss: 1.9497\n",
      "Time: 3021.17s\n",
      ">>> Model saved to MiniVGGNet_final_ex2_epoch80.pkl\n",
      "\n",
      "[Epoch 81/100]\n",
      "  Iter   1/703: Loss 1.7048\n",
      "  Iter  11/703: Loss 2.0095\n",
      "  Iter  21/703: Loss 1.6604\n",
      "  Iter  31/703: Loss 1.8617\n",
      "  Iter  41/703: Loss 1.6824\n",
      "  Iter  51/703: Loss 1.5313\n",
      "  Iter  61/703: Loss 2.0666\n",
      "  Iter  71/703: Loss 1.5183\n",
      "  Iter  81/703: Loss 1.4560\n",
      "  Iter  91/703: Loss 1.5691\n",
      "  Iter 101/703: Loss 1.4510\n",
      "  Iter 111/703: Loss 1.6865\n",
      "  Iter 121/703: Loss 1.9971\n",
      "  Iter 131/703: Loss 1.7501\n",
      "  Iter 141/703: Loss 1.4946\n",
      "  Iter 151/703: Loss 1.6466\n",
      "  Iter 161/703: Loss 1.4665\n",
      "  Iter 171/703: Loss 1.5148\n",
      "  Iter 181/703: Loss 1.5398\n",
      "  Iter 191/703: Loss 1.8754\n",
      "  Iter 201/703: Loss 1.7194\n",
      "  Iter 211/703: Loss 1.3577\n",
      "  Iter 221/703: Loss 1.8455\n",
      "  Iter 231/703: Loss 1.9877\n",
      "  Iter 241/703: Loss 1.7467\n",
      "  Iter 251/703: Loss 1.7431\n",
      "  Iter 261/703: Loss 2.0500\n",
      "  Iter 271/703: Loss 1.8503\n",
      "  Iter 281/703: Loss 1.5597\n",
      "  Iter 291/703: Loss 1.4692\n",
      "  Iter 301/703: Loss 1.8585\n",
      "  Iter 311/703: Loss 1.6572\n",
      "  Iter 321/703: Loss 1.8047\n",
      "  Iter 331/703: Loss 1.7456\n",
      "  Iter 341/703: Loss 1.6834\n",
      "  Iter 351/703: Loss 1.8025\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==== Running ex2 : train dataset 50,000 = original + on-the-fly (random crop + horizontal flip) ====\")\n",
    "model = MiniVGGNet()\n",
    "\n",
    "x_train, y_train = data['train']\n",
    "x_val, y_val = data['val']\n",
    "x_test, y_test = data['test']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    model_name='MiniVGGNet_final_ex2',\n",
    "    train_data=(x_train, y_train),\n",
    "    val_data=(x_val, y_val),\n",
    "    test_data=(x_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    lr=0.001,\n",
    "    smoothing=0.15\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_log(\"MiniVGGNet_final_ex2_log.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"MiniVGGNet_final_ex2_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"MiniVGGNet_final_ex2_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"learning_curve_loss.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"learning_curve_accuracy.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

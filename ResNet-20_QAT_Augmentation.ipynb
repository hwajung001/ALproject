{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from common.util import shuffle_dataset\n",
    "\n",
    "# CIFAR-100 다운로드 및 압축 해제\n",
    "def download_cifar100(dest=\"./cifar-100-python\"):\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "\n",
    "    def is_within_directory(directory, target):\n",
    "        abs_directory = os.path.abspath(directory)\n",
    "        abs_target = os.path.abspath(target)\n",
    "        return os.path.commonprefix([abs_directory, abs_target]) == abs_directory\n",
    "\n",
    "    def safe_extract(tar, path=\".\", members=None):\n",
    "        for member in tar.getmembers():\n",
    "            member_path = os.path.join(path, member.name)\n",
    "            if not is_within_directory(path, member_path):\n",
    "                raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "        tar.extractall(path, members)\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest, exist_ok=True)\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        with tarfile.open(filename, \"r:gz\") as tar:\n",
    "            safe_extract(tar, path=\"./\")\n",
    "        print(\"CIFAR-100 downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"CIFAR-100 already downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 배치 로딩\n",
    "def load_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "        data = data_dict[b'data']\n",
    "        fine_labels = np.array(data_dict[b'fine_labels'])\n",
    "        coarse_labels = np.array(data_dict[b'coarse_labels'])\n",
    "        return data, fine_labels, coarse_labels\n",
    "\n",
    "# 메타데이터 로딩\n",
    "def load_meta(data_dir=\"./cifar-100-python\"):\n",
    "    with open(os.path.join(data_dir, \"meta\"), 'rb') as f:\n",
    "        meta_dict = pickle.load(f, encoding='bytes')\n",
    "        fine_label_names = [name.decode('utf-8') for name in meta_dict[b'fine_label_names']]\n",
    "        coarse_label_names = [name.decode('utf-8') for name in meta_dict[b'coarse_label_names']]\n",
    "        return {\"fine_label_names\": fine_label_names, \"coarse_label_names\": coarse_label_names}\n",
    "\n",
    "# 정규화 함수\n",
    "def normalize(x):\n",
    "    mean = np.array([0.507, 0.487, 0.441]).reshape(1, 3, 1, 1)\n",
    "    std = np.array([0.267, 0.256, 0.276]).reshape(1, 3, 1, 1)\n",
    "    return (x - mean) / std\n",
    "\n",
    "# 전체 데이터 로딩\n",
    "def load_cifar100(data_dir=\"./cifar-100-python\"):\n",
    "    x_train, y_train_fine, y_train_coarse = load_batch(os.path.join(data_dir, \"train\"))\n",
    "    x_test, y_test_fine, y_test_coarse = load_batch(os.path.join(data_dir, \"test\"))\n",
    "\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "\n",
    "    x_train = normalize(x_train)\n",
    "    x_test = normalize(x_test)\n",
    "\n",
    "    val_size = int(0.1 * len(x_train))\n",
    "    x_val, y_val_fine, y_val_coarse = (\n",
    "        x_train[:val_size], y_train_fine[:val_size], y_train_coarse[:val_size]\n",
    "    )\n",
    "    x_train, y_train_fine, y_train_coarse = (\n",
    "        x_train[val_size:], y_train_fine[val_size:], y_train_coarse[val_size:]\n",
    "    )\n",
    "\n",
    "    x_train, y_train_fine = shuffle_dataset(x_train, y_train_fine)\n",
    "    x_train, y_train_coarse = shuffle_dataset(x_train, y_train_coarse)\n",
    "\n",
    "    return (x_train, y_train_fine, y_train_coarse), (x_val, y_val_fine, y_val_coarse), (x_test, y_test_fine, y_test_coarse)\n",
    "\n",
    "download_cifar100()\n",
    "(x_train, y_train_fine, y_train_coarse), (x_val, y_val_fine, y_val_coarse), (x_test, y_test_fine, y_test_coarse) = load_cifar100()\n",
    "meta = load_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강 함수\n",
    "def random_crop(x, crop_size=32, padding=4):\n",
    "    n, c, h, w = x.shape\n",
    "    padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='reflect')\n",
    "    cropped = np.empty((n, c, crop_size, crop_size), dtype=x.dtype)\n",
    "    for i in range(n):\n",
    "        top = np.random.randint(0, padding * 2 + 1)\n",
    "        left = np.random.randint(0, padding * 2 + 1)\n",
    "        cropped[i] = padded[i, :, top:top+crop_size, left:left+crop_size]\n",
    "    return cropped\n",
    "\n",
    "def horizontal_flip(x):\n",
    "    return x[:, :, :, ::-1]\n",
    "\n",
    "def cutout(x, size=16):  \n",
    "    x_cut = x.copy()\n",
    "    n, c, h, w = x.shape\n",
    "    for i in range(n):\n",
    "        cy, cx = np.random.randint(h), np.random.randint(w)\n",
    "        y1 = np.clip(cy - size // 2, 0, h)\n",
    "        y2 = np.clip(cy + size // 2, 0, h)\n",
    "        x1 = np.clip(cx - size // 2, 0, w)\n",
    "        x2 = np.clip(cx + size // 2, 0, w)\n",
    "        x_cut[i, :, y1:y2, x1:x2] = 0\n",
    "    return x_cut\n",
    "\n",
    "def color_jitter(x, brightness=0.3, contrast=0.3):\n",
    "    x_jittered = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        b = 1 + np.random.uniform(-brightness, brightness)\n",
    "        c = 1 + np.random.uniform(-contrast, contrast)\n",
    "        mean = x_jittered[i].mean(axis=(1, 2), keepdims=True)\n",
    "        x_jittered[i] = (x_jittered[i] - mean) * c + mean\n",
    "        x_jittered[i] = np.clip(x_jittered[i] * b, 0, 1)\n",
    "    return x_jittered\n",
    "\n",
    "# 증강 저장 함수\n",
    "def save_augmented_dataset(x, y, aug_func, name):\n",
    "    x_aug = aug_func(x)\n",
    "    x_combined = np.concatenate([x, x_aug], axis=0)\n",
    "    y_combined = np.concatenate([y, y], axis=0)\n",
    "    np.savez(f\"cifar100_train_{name}.npz\", x=x_combined, y=y_combined)\n",
    "    print(f\"> Saved: cifar100_train_{name}.npz\")\n",
    "\n",
    "# 전체 CIFAR-100 로딩 및 증강 저장\n",
    "def prepare_and_save_all(data_dir=\"./cifar-100-python\"):\n",
    "    x_train, y_train_fine, _ = load_batch(os.path.join(data_dir, \"train\"))\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "    x_train = normalize(x_train)\n",
    "\n",
    "    save_augmented_dataset(x_train, y_train_fine, random_crop, \"random_crop\")\n",
    "    save_augmented_dataset(x_train, y_train_fine, horizontal_flip, \"horizontal_flip\")\n",
    "    save_augmented_dataset(x_train, y_train_fine, cutout, \"cutout\")\n",
    "    save_augmented_dataset(x_train, y_train_fine, color_jitter, \"color_jitter\")\n",
    "\n",
    "prepare_and_save_all()\n",
    "\n",
    "# 저장된 .npz 증강 데이터셋 로드\n",
    "def load_augmented_dataset(filename):\n",
    "    data = np.load(filename)\n",
    "    x = data['x']\n",
    "    y = data['y']\n",
    "    return x, y\n",
    "\n",
    "# 증강된 데이터셋 불러오기\n",
    "x_crop, y_crop = load_augmented_dataset(\"cifar100_train_random_crop.npz\")\n",
    "x_flip, y_flip = load_augmented_dataset(\"cifar100_train_horizontal_flip.npz\")\n",
    "x_cutout, y_cutout = load_augmented_dataset(\"cifar100_train_cutout.npz\")\n",
    "x_jitter, y_jitter = load_augmented_dataset(\"cifar100_train_color_jitter.npz\")\n",
    "\n",
    "# 데이터셋 정보 출력\n",
    "print(\"Augmented CIFAR-100 Dataset Loaded!\")\n",
    "print(f\"[Random Crop]     X: {x_crop.shape}, Y: {y_crop.shape}\")\n",
    "print(f\"[Horizontal Flip] X: {x_flip.shape}, Y: {y_flip.shape}\")\n",
    "print(f\"[Cutout]          X: {x_cutout.shape}, Y: {y_cutout.shape}\")\n",
    "print(f\"[Color Jitter]    X: {x_jitter.shape}, Y: {y_jitter.shape}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0~1 범위의 이미지를 [0, 255] 정수값으로 변환\n",
    "def denormalize(img):\n",
    "    mean = np.array([0.507, 0.487, 0.441]).reshape(3, 1, 1)\n",
    "    std = np.array([0.267, 0.256, 0.276]).reshape(3, 1, 1)\n",
    "    img = img * std + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "# 시각화 함수\n",
    "def show_augmented_comparison(x_orig, x_aug_combined, y_combined, label_names, aug_name, num_samples=5, save_dir=\"./aug_vis\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    n = len(x_aug_combined) // 2 \n",
    "    idxs = np.random.choice(n, num_samples, replace=False)\n",
    "\n",
    "    plt.figure(figsize=(num_samples * 2, 4))\n",
    "    for i, idx in enumerate(idxs):\n",
    "        label = label_names[y_combined[idx]]\n",
    "\n",
    "        # 원본\n",
    "        orig = denormalize(x_orig[idx])\n",
    "        orig = np.transpose(orig, (1, 2, 0))\n",
    "        plt.subplot(2, num_samples, i + 1)\n",
    "        plt.imshow(orig)\n",
    "        plt.title(f\"Orig: {label}\", fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # 증강 이미지: x_aug의 후반부\n",
    "        aug = denormalize(x_aug_combined[idx + n])\n",
    "        aug = np.transpose(aug, (1, 2, 0))\n",
    "        plt.subplot(2, num_samples, i + 1 + num_samples)\n",
    "        plt.imshow(aug)\n",
    "        plt.title(f\"{aug_name}: {label}\", fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"{aug_name} Comparison\", fontsize=12)\n",
    "    save_path = os.path.join(save_dir, f\"compare_{aug_name.lower().replace(' ', '_')}.png\")\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"> Saved: {save_path}\")\n",
    "\n",
    "meta = load_meta()\n",
    "label_names = meta['fine_label_names']\n",
    "\n",
    "# 원본 x_train 로드\n",
    "x_train, y_train_fine, _ = load_batch(\"./cifar-100-python/train\")\n",
    "x_train = x_train.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "x_train = normalize(x_train)\n",
    "\n",
    "# 증강 비교 시각화 실행\n",
    "show_augmented_comparison(x_train, x_crop, y_crop, label_names, \"Random Crop\")\n",
    "show_augmented_comparison(x_train, x_flip, y_flip, label_names, \"Horizontal Flip\")\n",
    "show_augmented_comparison(x_train, x_cutout, y_cutout, label_names, \"Cutout\")\n",
    "show_augmented_comparison(x_train, x_jitter, y_jitter, label_names, \"Color Jitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake Quantization 함수\n",
    "def fake_quantize(x, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    \n",
    "    if x_max == x_min:\n",
    "        return x  # avoid divide by zero\n",
    "    \n",
    "    scale = (x_max - x_min) / (qmax - qmin)\n",
    "    zero_point = qmin - x_min / scale\n",
    "    zero_point = np.clip(np.round(zero_point), qmin, qmax)\n",
    "\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x = np.clip(np.round(q_x), qmin, qmax)\n",
    "    fq_x = scale * (q_x - zero_point)\n",
    "    return fq_x\n",
    "\n",
    "\n",
    "# 모델 레이어 및 ResNet-20 정의\n",
    "from common.layers import Convolution, Affine, Relu, BatchNormalization\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.util import im2col, col2im\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        # Fake Quantization\n",
    "        W_q = fake_quantize(self.W)\n",
    "        b_q = fake_quantize(self.b)\n",
    "        x_q = fake_quantize(self.x)\n",
    "\n",
    "        out = np.dot(x_q, W_q) + b_q\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)\n",
    "        return dx\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, _, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        # Fake Quantization\n",
    "        W_q = fake_quantize(self.W)\n",
    "        b_q = fake_quantize(self.b)\n",
    "        x_q = fake_quantize(x)\n",
    "\n",
    "        col = im2col(x_q, FH, FW, self.stride, self.pad)\n",
    "        col_W = W_q.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + b_q\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout).transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class ResidualBlock:\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        self.stride = stride\n",
    "        self.equal_in_out = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        self.conv1 = Convolution(\n",
    "            W=np.random.randn(out_channels, in_channels, 3, 3) * np.sqrt(2. / in_channels),\n",
    "            b=np.zeros(out_channels),\n",
    "            stride=stride,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn1 = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.conv2 = Convolution(\n",
    "            W=np.random.randn(out_channels, out_channels, 3, 3) * np.sqrt(2. / out_channels),\n",
    "            b=np.zeros(out_channels),\n",
    "            stride=1,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn2 = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "        self.relu2 = Relu()\n",
    "\n",
    "        if not self.equal_in_out:\n",
    "            self.shortcut = Convolution(\n",
    "                W=np.random.randn(out_channels, in_channels, 1, 1) * np.sqrt(2. / in_channels),\n",
    "                b=np.zeros(out_channels),\n",
    "                stride=stride,\n",
    "                pad=0\n",
    "            )\n",
    "            self.bn_shortcut = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.x = x\n",
    "\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out, train_flg)\n",
    "        out = self.relu1.forward(out)\n",
    "\n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out, train_flg)\n",
    "        self.out_main = out\n",
    "\n",
    "        if self.equal_in_out:\n",
    "            shortcut = x\n",
    "        else:\n",
    "            shortcut = self.shortcut.forward(x)\n",
    "            shortcut = self.bn_shortcut.forward(shortcut, train_flg)\n",
    "        self.out_shortcut = shortcut\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu2.forward(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.relu2.backward(dout)\n",
    "\n",
    "        dshortcut = dout.copy()\n",
    "        dmain = dout.copy()\n",
    "\n",
    "        dmain = self.bn2.backward(dmain)\n",
    "        dmain = self.conv2.backward(dmain)\n",
    "\n",
    "        dmain = self.relu1.backward(dmain)\n",
    "        dmain = self.bn1.backward(dmain)\n",
    "        dmain = self.conv1.backward(dmain)\n",
    "\n",
    "        if not self.equal_in_out:\n",
    "            dshortcut = self.bn_shortcut.backward(dshortcut)\n",
    "            dshortcut = self.shortcut.backward(dshortcut)\n",
    "\n",
    "        dx = dmain + dshortcut\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet20:\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_classes=100):\n",
    "        self.params = []\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        self.conv1 = Convolution(\n",
    "            W=np.random.randn(16, 3, 3, 3) * np.sqrt(2. / 3),\n",
    "            b=np.zeros(16),\n",
    "            stride=1,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn1 = BatchNormalization(gamma=np.ones(16), beta=np.zeros(16))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.layer1 = [ResidualBlock(16, 16, stride=1) for _ in range(3)]\n",
    "        self.layer2 = [ResidualBlock(16 if i == 0 else 32, 32, stride=2 if i == 0 else 1) for i in range(3)]\n",
    "        self.layer3 = [ResidualBlock(32 if i == 0 else 64, 64, stride=2 if i == 0 else 1) for i in range(3)]\n",
    "\n",
    "        self.fc = Affine(W=np.random.randn(64, num_classes) * np.sqrt(2. / 64), b=np.zeros(num_classes))\n",
    "\n",
    "    def clip_weights(self, clip_value=1.0):\n",
    "    # 개별 레이어의 weight들을 [-clip_value, clip_value]로 제한\n",
    "        self.conv1.W = np.clip(self.conv1.W, -clip_value, clip_value)\n",
    "        self.fc.W = np.clip(self.fc.W, -clip_value, clip_value)\n",
    "\n",
    "        for block in self.layer1 + self.layer2 + self.layer3:\n",
    "            block.conv1.W = np.clip(block.conv1.W, -clip_value, clip_value)\n",
    "            block.conv2.W = np.clip(block.conv2.W, -clip_value, clip_value)\n",
    "            if not block.equal_in_out:\n",
    "                block.shortcut.W = np.clip(block.shortcut.W, -clip_value, clip_value)\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input = x\n",
    "\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out, train_flg)\n",
    "        out = self.relu1.forward(out)\n",
    "\n",
    "        for block in self.layer1:\n",
    "            out = block.forward(out, train_flg)\n",
    "        for block in self.layer2:\n",
    "            out = block.forward(out, train_flg)\n",
    "        for block in self.layer3:\n",
    "            out = block.forward(out, train_flg)\n",
    "\n",
    "        self.feature_map = out\n",
    "\n",
    "        N, C, H, W = out.shape\n",
    "        out = out.mean(axis=(2, 3))\n",
    "\n",
    "        self.pooled = out\n",
    "        out = self.fc.forward(out)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x, batch_size=100):\n",
    "        y_list = []\n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            y_batch = self.forward(x_batch, train_flg=False)\n",
    "            y_list.append(y_batch)\n",
    "        return np.concatenate(y_list, axis=0)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.forward(x, train_flg=True)\n",
    "        return cross_entropy_error(softmax(y), t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        acc = 0.0\n",
    "        total = x.shape[0]\n",
    "        for i in range(0, total, batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "\n",
    "            y = self.predict(x_batch)\n",
    "            y = np.argmax(y, axis=1)\n",
    "\n",
    "            if t.ndim != 1:\n",
    "                t_batch = np.argmax(t_batch, axis=1)\n",
    "\n",
    "            acc += np.sum(y == t_batch)\n",
    "\n",
    "        return acc / total\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.fc.backward(dout)\n",
    "        dout = dout.reshape(self.feature_map.shape[0], self.feature_map.shape[1], 1, 1)\n",
    "        dout = dout.repeat(self.feature_map.shape[2], axis=2).repeat(self.feature_map.shape[3], axis=3)\n",
    "\n",
    "        for block in reversed(self.layer3):\n",
    "            dout = block.backward(dout)\n",
    "        for block in reversed(self.layer2):\n",
    "            dout = block.backward(dout)\n",
    "        for block in reversed(self.layer1):\n",
    "            dout = block.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        return dout\n",
    "\n",
    "# 모델 구조 출력\n",
    "\n",
    "def count_params(layer):\n",
    "    count = 0\n",
    "    if hasattr(layer, 'W'):\n",
    "        count += np.prod(layer.W.shape)\n",
    "    if hasattr(layer, 'b'):\n",
    "        count += np.prod(layer.b.shape)\n",
    "    return count\n",
    "\n",
    "def print_resnet20_summary(model, input_shape=(1, 3, 32, 32)):\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Layer (type)':<35}{'Output Shape':<25}{'Param #':>10}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "    x = np.zeros(input_shape)\n",
    "    total_params = 0\n",
    "    layer_idx = 1\n",
    "\n",
    "    # Conv1\n",
    "    x = model.conv1.forward(x)\n",
    "    p = count_params(model.conv1)\n",
    "    print(f\"{layer_idx:>2}. {'Conv1':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "    layer_idx += 1\n",
    "\n",
    "    x = model.bn1.forward(x, train_flg=False)\n",
    "    x = model.relu1.forward(x)\n",
    "\n",
    "    # Residual Blocks\n",
    "    for i, layer_block in enumerate([model.layer1, model.layer2, model.layer3]):\n",
    "        for j, block in enumerate(layer_block):\n",
    "            residual = x.copy()\n",
    "\n",
    "            # Conv1\n",
    "            x = block.conv1.forward(x)\n",
    "            p = count_params(block.conv1)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv1\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn1.forward(x, train_flg=False)\n",
    "            x = block.relu1.forward(x)\n",
    "\n",
    "            # Conv2\n",
    "            x = block.conv2.forward(x)\n",
    "            p = count_params(block.conv2)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv2\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn2.forward(x, train_flg=False)\n",
    "\n",
    "            # Shortcut (optional)\n",
    "            if not block.equal_in_out:\n",
    "                x_sc = block.shortcut.forward(residual)\n",
    "                p = count_params(block.shortcut)\n",
    "                name = f\"└─ Shortcut[{i+1}-{j+1}]\"\n",
    "                print(f\"{'':>3} {name:<32}{str(x_sc.shape):<25}{p:>10,}\", flush=True)\n",
    "                total_params += p\n",
    "                x = x + x_sc\n",
    "                x = block.bn_shortcut.forward(x, train_flg=False)\n",
    "            else:\n",
    "                x = x + residual\n",
    "\n",
    "            x = block.relu2.forward(x)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = x.mean(axis=(2, 3))\n",
    "    print(f\"{'':>3} {'GlobalAvgPool':<32}{str(x.shape):<25}{'0':>10}\", flush=True)\n",
    "\n",
    "    # FC\n",
    "    x = model.fc.forward(x)\n",
    "    p = count_params(model.fc)\n",
    "    print(f\"{layer_idx:>2}. {'FC':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Total weight layers:':<60}{'20'}\", flush=True)\n",
    "    print(f\"{'Total params:':<60}{total_params:,}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "model = ResNet20()\n",
    "print_resnet20_summary(model, input_shape=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from common.optimizer import Adam\n",
    "from common.functions import softmax\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name, train_data, val_data, test_data, epochs=20, batch_size=64, optimizer_name='sgd', lr=0.01):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.val_x, self.val_t = val_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "        self.max_iter = self.epochs * self.iter_per_epoch\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        if optimizer_name == 'adam':\n",
    "            self.optimizer = Adam(lr=lr)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "        if hasattr(self.model.fc, 'W'):\n",
    "            param_dict['fc_W'] = self.model.fc.W\n",
    "            param_dict['fc_b'] = self.model.fc.b\n",
    "            grad_dict['fc_W'] = self.model.fc.dW\n",
    "            grad_dict['fc_b'] = self.model.fc.db\n",
    "\n",
    "        idx = 0\n",
    "        for layer in self.model.layer1 + self.model.layer2 + self.model.layer3:\n",
    "            for attr in ['conv1', 'conv2', 'shortcut']:\n",
    "                if hasattr(layer, attr):\n",
    "                    conv = getattr(layer, attr)\n",
    "                    param_dict[f'{idx}_W'] = conv.W\n",
    "                    param_dict[f'{idx}_b'] = conv.b\n",
    "                    grad_dict[f'{idx}_W'] = conv.dW\n",
    "                    grad_dict[f'{idx}_b'] = conv.db\n",
    "                    idx += 1\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            return (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            return dx / batch_size\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        self.model.backward(self.loss_grad(x_batch, t_batch))\n",
    "\n",
    "        if hasattr(self.model, 'clip_weights'):\n",
    "            self.model.clip_weights(clip_value=1.0)\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"[Epoch {epoch + 1}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"  Iter {i:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            test_acc = self.model.accuracy(self.test_x, self.test_t)\n",
    "            val_loss = self.batched_loss(self.val_x, self.val_t, batch_size=128)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            self.val_loss_list.append(val_loss)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Train acc: {train_acc:.4f}, Val loss: {val_loss:.4f}, Test acc: {test_acc:.4f} (Time: {elapsed:.2f}s)\\n\", flush=True)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                model_filename = f\"{self.model_name}_epoch_{epoch+1}.pkl\"\n",
    "                self.save_model(model_filename)\n",
    "                print(f\">>> Saved model to {model_filename}\", flush=True)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "\n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': getattr(self.optimizer, 'beta1', None),\n",
    "            'beta2': getattr(self.optimizer, 'beta2', None),\n",
    "            'eps': getattr(self.optimizer, 'eps', None),\n",
    "            'm': getattr(self.optimizer, 'm', {}),\n",
    "            'v': getattr(self.optimizer, 'v', {}),\n",
    "            't': getattr(self.optimizer, 't', 0),\n",
    "        }\n",
    "\n",
    "        save_data = {\n",
    "            'model': model_state,\n",
    "            'optimizer': optimizer_state,\n",
    "            'train_loss_list': self.train_loss_list,\n",
    "            'train_acc_list': self.train_acc_list,\n",
    "            'test_acc_list': self.test_acc_list,\n",
    "            'val_loss_list': self.val_loss_list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename,\n",
    "                 loss=np.array(self.train_loss_list),\n",
    "                 train_acc=np.array(self.train_acc_list),\n",
    "                 test_acc=np.array(self.test_acc_list),\n",
    "                 val_loss=np.array(self.val_loss_list))\n",
    "        print(f\"Log saved to {filename}\", flush=True)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        for k in params:\n",
    "            if k in state['model']:\n",
    "                params[k][...] = state['model'][k]\n",
    "            else:\n",
    "                print(f\"[WARN] Key {k} not found in checkpoint!\", flush=True)\n",
    "\n",
    "        opt = state['optimizer']\n",
    "        self.optimizer.lr = opt['lr']\n",
    "        self.optimizer.beta1 = opt['beta1']\n",
    "        self.optimizer.beta2 = opt['beta2']\n",
    "        self.optimizer.eps = opt['eps']\n",
    "        self.optimizer.m = opt['m']\n",
    "        self.optimizer.v = opt['v']\n",
    "        self.optimizer.t = opt['t']\n",
    "\n",
    "        # 복원된 로그\n",
    "        self.train_loss_list = state.get('train_loss_list', [])\n",
    "        self.train_acc_list = state.get('train_acc_list', [])\n",
    "        self.test_acc_list = state.get('test_acc_list', [])\n",
    "        self.val_loss_list = state.get('val_loss_list', [])\n",
    "\n",
    "    def batched_loss(self, x, t, batch_size=128):\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "            loss = self.model.loss(x_batch, t_batch)\n",
    "            total_loss += loss * len(x_batch)\n",
    "            total_count += len(x_batch)\n",
    "        return total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_augmented_comparison():\n",
    "    augmentations = [\"random_crop\", \"horizontal_flip\", \"cutout\", \"color_jitter\"]\n",
    "    configs = [\n",
    "        {\"lr\": 0.01, \"bs\": 64, \"opt\": \"adam\"},\n",
    "        {\"lr\": 0.001, \"bs\": 32, \"opt\": \"adam\"},\n",
    "    ]\n",
    "    results = []\n",
    "\n",
    "    (x_train_base, y_train_base, _), (x_val, y_val_fine, _), (x_test, y_test_fine, _) = load_cifar100()\n",
    "\n",
    "    for aug in augmentations:\n",
    "        x_aug, y_aug = load_augmented_dataset(f\"cifar100_train_{aug}.npz\")\n",
    "        print(f\"\\n>>> [Data] Augmentation: {aug}, Shape: {x_aug.shape}, Labels: {y_aug.shape}\", flush=True)\n",
    "\n",
    "        for cfg in configs:\n",
    "            model_name = f\"ResNet-20_{aug}_lr{cfg['lr']}_bs{cfg['bs']}_{cfg['opt']}\"\n",
    "            print(f\"\\n[Training {model_name}]\", flush=True)\n",
    "\n",
    "            model = ResNet20()\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                train_data=(x_aug, y_aug),\n",
    "                val_data=(x_val, y_val_fine),\n",
    "                test_data=(x_test, y_test_fine),\n",
    "                epochs=10,\n",
    "                batch_size=cfg[\"bs\"],\n",
    "                optimizer_name=cfg[\"opt\"],\n",
    "                lr=cfg[\"lr\"]\n",
    "            )\n",
    "            trainer.train()\n",
    "            trainer.save_log(f\"{model_name}_log.npz\")\n",
    "            trainer.save_model(f\"{model_name}_epoch10.pkl\")\n",
    "\n",
    "            final_acc = trainer.test_acc_list[-1]\n",
    "            results.append((aug, cfg[\"lr\"], cfg[\"bs\"], cfg[\"opt\"], final_acc))\n",
    "\n",
    "    print(\"\\n[증강별 튜닝 결과 요약]\")\n",
    "    for aug, lr, bs, opt, acc in results:\n",
    "        print(f\"{aug} | lr={lr}, bs={bs}, opt={opt} → test_acc={acc:.4f}\", flush=True)\n",
    "\n",
    "main_augmented_comparison()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671d4c08-9429-4e57-b57f-069620231916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c2ddd91-4dcd-420c-915f-b6d38c11be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.util import shuffle_dataset\n",
    "from common.optimizer import SGD\n",
    "from common.functions import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fcbc600-4a8e-4809-a6a8-8e3823901028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 불러오기\n",
    "#나중에 .py 파일로 뺄것.\n",
    "def load_cifar100(data_dir='./cifar-100-python', validation_rate=0.2):\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            return pickle.load(fo, encoding='bytes')\n",
    "\n",
    "    # 데이터 로딩\n",
    "    train = unpickle(os.path.join(data_dir, 'train'))\n",
    "    test = unpickle(os.path.join(data_dir, 'test'))\n",
    "    meta = unpickle(os.path.join(data_dir, 'meta'))\n",
    "\n",
    "    # 이미지 정규화 및 reshape\n",
    "    x_train = train[b'data'].reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "    x_test = test[b'data'].reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "\n",
    "    # 라벨 분리 (fine, coarse)\n",
    "    y_train_fine = np.array(train[b'fine_labels'])\n",
    "    y_test_fine = np.array(test[b'fine_labels'])\n",
    "\n",
    "    y_train_coarse = np.array(train[b'coarse_labels'])\n",
    "    y_test_coarse = np.array(test[b'coarse_labels'])\n",
    "\n",
    "    # --- 셔플 후 validation 분할 ---\n",
    "    from common.util import shuffle_dataset\n",
    "    x_train, y_train_fine = shuffle_dataset(x_train, y_train_fine)\n",
    "    _, y_train_coarse = shuffle_dataset(x_train, y_train_coarse)\n",
    "\n",
    "    validation_num = int(x_train.shape[0] * validation_rate)\n",
    "\n",
    "    x_val = x_train[:validation_num]\n",
    "    y_val = y_train_fine[:validation_num]\n",
    "\n",
    "    x_train = x_train[validation_num:]\n",
    "    y_train = y_train_fine[validation_num:]\n",
    "    y_train_coarse = y_train_coarse[validation_num:]\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test_fine), (y_train_coarse, y_test_coarse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9716d3a-4a72-4224-b8a3-d910d1e3e1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine label 예시 (y_train): [31 79 74 40 12 71 58  2 10 88]\n",
      "Coarse label 예시 (y_train_coarse): [ 7  2 13 17  8  5  9  0  9  3]\n"
     ]
    }
   ],
   "source": [
    "#데이터셋 사용\n",
    "data_dir = './cifar-100-python'  # 이 위치에 압축 푼 폴더가 있어야 해\n",
    "\n",
    "# 데이터 로딩 및 분할\n",
    "(x_train, y_train), (x_valid, y_valid), (x_test, y_test), (y_train_coarse, y_test_coarse) = load_cifar100()\n",
    "\n",
    "#예시 출력\n",
    "print(\"Fine label 예시 (y_train):\", y_train[:10])\n",
    "print(\"Coarse label 예시 (y_train_coarse):\", y_train_coarse[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a72735a1-a185-4ec8-8beb-fcaf7f8a4a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 training data 수: 4000\n"
     ]
    }
   ],
   "source": [
    "#데이터셋 크기 설정\n",
    "print(\"전체 training data 수:\", x_train.shape[0])\n",
    "desired_num=40000\n",
    "x_train = x_train[:desired_num]\n",
    "y_train = y_train[:desired_num]\n",
    "y_train_coarse = y_train_coarse[:desired_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "10eaf1c2-b311-4e03-bc67-6ac4d512b960",
   "metadata": {},
   "outputs": [],
   "source": [
    "##데이터셋 셔플\n",
    "# 총 학습 데이터 개수 계산\n",
    "num_train = x_train.shape[0]\n",
    "num_valid = x_valid.shape[0]\n",
    "num_test = x_test.shape[0]\n",
    "\n",
    "#1. 섞을 순서 만들기.\n",
    "shuffle_idx = np.random.permutation(num_train)\n",
    "\n",
    "#2. 이미지와 fine label, coars label을 같은 순서로 섞기.\n",
    "x_train = x_train[shuffle_idx]\n",
    "y_train = y_train[shuffle_idx]\n",
    "y_train_coarse = y_train_coarse[shuffle_idx]\n",
    "\n",
    "# 검증 비율 설정\n",
    "validation_rate = 0.2\n",
    "validation_num = int(num_train * validation_rate)\n",
    "\n",
    "# 검증 데이터 분리\n",
    "x_val = x_train[:validation_num]\n",
    "y_val = y_train[:validation_num]\n",
    "y_val_coarse = y_train_coarse[:validation_num]\n",
    "\n",
    "# 학습 데이터 재설정\n",
    "x_train = x_train[validation_num:]\n",
    "y_train = y_train[validation_num:]\n",
    "y_train_coarse = y_train_coarse[validation_num:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6e9ab457-fb01-4a20-bdbf-a403514ef15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 데이터 추출\n",
    "num_train = x_train.shape[0]\n",
    "idx_train = np.arange(num_train)\n",
    "\n",
    "# 검증용 데이터 추출\n",
    "num_valid = x_val.shape[0]\n",
    "idx_valid = np.arange(num_valid)\n",
    "\n",
    "# 평가용 데이터 추출\n",
    "num_test = x_test.shape[0]\n",
    "idx_test = np.arange(num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5168177a-6ee4-45a5-9adc-63bec47fca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망 하이퍼파라미터 설정\n",
    "input_size = 3072                   # 입력 차원 (32X32X3) #RGB\n",
    "output_size = 100                   # 출력 차원 (100 클래스) #fine label\n",
    "hideen_size_list = [100, 50]        # 은닉층 2개\n",
    "\n",
    "max_epochs = 1000                     # Epoch 횟수\n",
    "batch_size = 100                    # mini-batch 개수\n",
    "learning_rate = 0.01                # 학습률\n",
    "weight_decay_lambda = 0.01           # 가중치 감소 계수 (정규화 항)\n",
    "activation = 'relu'\n",
    "weight_init_std = 'he'              # 가중치 초기화 방식 ('xavier', 'he', 1.0)\n",
    "use_batchnorm = True                # 배치 정규화 사용 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b2df32d-d329-4af1-b295-5cc138e3dbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 다층 신경망 인스턴스 생성 ---\n",
    "# --- MLP---\n",
    "network = MultiLayerNetExtend(\n",
    "    input_size=input_size,\n",
    "    hidden_size_list=hideen_size_list,\n",
    "    output_size=output_size,\n",
    "    activation=activation,\n",
    "    weight_init_std=weight_init_std,\n",
    "    weight_decay_lambda=weight_decay_lambda,\n",
    "    use_batchnorm=use_batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e23c4779-51d4-492b-8c43-1cf58dd1e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 옵티마이저 설정 ---\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "# optimizer = Momentum(lr=learning_rate)\n",
    "# optimizer = AdaGrad(lr=learning_rate)\n",
    "# optimizer = Adam(lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "82ac73ba-26f2-4c1f-90f8-305b24af4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 학습 로그 저장용 리스트 초기화 ---\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "train_loss_per_epoch_list = []\n",
    "train_acc_per_epoch_list = []\n",
    "\n",
    "valid_loss_list = []\n",
    "valid_acc_list = []\n",
    "valid_loss_per_epoch_list = []\n",
    "valid_acc_per_epoch_list = []\n",
    "\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "test_loss_per_epoch_list = []\n",
    "test_acc_per_epoch_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "afe21a18-7674-48b1-9887-1484abc30eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1에폭당 반복 수 계산 ---\n",
    "train_per_epoch = max(num_train / batch_size, 1)\n",
    "valid_per_epoch = max(num_valid / batch_size, 1)\n",
    "test_per_epoch = max(num_test / batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "658e6a1b-bcac-424b-a434-05e335df6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 로그 데이터 저장 함수 ---\n",
    "# --- 나중에 .py 파일로 뺄것.\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def save_logs_npy(path_prefix='logs/', logs_dict={}):\n",
    "    os.makedirs(path_prefix, exist_ok=True)\n",
    "    for name, values in logs_dict.items():\n",
    "        np.save(os.path.join(path_prefix, f'{name}.npy'), np.array(values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b6bead3c-4249-4f5d-ad3d-b660f2beb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예: 실험 이름마다 별도 폴더로 로그 저장\n",
    "experiment_name = \"exp1_MLP_mapping\"\n",
    "log_path = os.path.join(\"logs\", experiment_name)\n",
    "\n",
    "save_logs_npy(log_path, {\n",
    "    'train_loss': train_loss_per_epoch_list,\n",
    "    'train_acc': train_acc_per_epoch_list,\n",
    "    'valid_loss': valid_loss_per_epoch_list,\n",
    "    'valid_acc': valid_acc_per_epoch_list\n",
    "})\n",
    "\n",
    "#나중에 저장된 파일 불러올 때\n",
    "#train_loss = np.load('logs/exp1_MLP_mapping/train_loss.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "117af80d-8c54-49c7-84e8-2a647361d3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nimport numpy as np\\nfrom datetime import datetime\\n\\ndef save_logs_npy_with_timestamp(logs_dict, base_dir=\\'logs\\', exp_name=\\'exp\\'):\\n    # 현재 시간 기반 타임스탬프 생성\\n    timestamp = datetime.now().strftime(\\'%Y%m%d_%H%M%S\\')\\n    full_path = os.path.join(base_dir, f\"{exp_name}_{timestamp}\")\\n\\n    os.makedirs(full_path, exist_ok=True)\\n\\n    for name, values in logs_dict.items():\\n        np.save(os.path.join(full_path, f\\'{name}.npy\\'), np.array(values))\\n    \\n    print(f\"로그 저장 완료: {full_path}\")\\n    return full_path  # 경로 반환해줘도 좋음\\n\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#timestamp 추가된 로그 데이터 저장 함수\n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def save_logs_npy_with_timestamp(logs_dict, base_dir='logs', exp_name='exp'):\n",
    "    # 현재 시간 기반 타임스탬프 생성\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    full_path = os.path.join(base_dir, f\"{exp_name}_{timestamp}\")\n",
    "\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "    for name, values in logs_dict.items():\n",
    "        np.save(os.path.join(full_path, f'{name}.npy'), np.array(values))\n",
    "    \n",
    "    print(f\"로그 저장 완료: {full_path}\")\n",
    "    return full_path  # 경로 반환해줘도 좋음\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "129806f5-e3e8-4555-aecf-e63004327293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch] 0 / 1000 | [Train] loss:618.2089, acc:0.9984 | [Valid] loss:623.2672, acc:0.0638\n",
      "[epoch] 1 / 1000 | [Train] loss:618.2712, acc:0.9978 | [Valid] loss:623.2317, acc:0.0725\n",
      "[epoch] 2 / 1000 | [Train] loss:618.1394, acc:0.9981 | [Valid] loss:623.1446, acc:0.0688\n",
      "[epoch] 3 / 1000 | [Train] loss:618.2080, acc:0.9978 | [Valid] loss:623.2760, acc:0.0675\n",
      "[epoch] 4 / 1000 | [Train] loss:618.2640, acc:0.9978 | [Valid] loss:623.3024, acc:0.0725\n",
      "[epoch] 5 / 1000 | [Train] loss:618.2083, acc:0.9981 | [Valid] loss:623.3266, acc:0.0688\n",
      "[epoch] 6 / 1000 | [Train] loss:618.2916, acc:0.9981 | [Valid] loss:623.3460, acc:0.0700\n",
      "[epoch] 7 / 1000 | [Train] loss:618.3909, acc:0.9981 | [Valid] loss:623.3869, acc:0.0737\n",
      "[epoch] 8 / 1000 | [Train] loss:618.3558, acc:0.9981 | [Valid] loss:623.3323, acc:0.0675\n",
      "[epoch] 9 / 1000 | [Train] loss:618.3224, acc:0.9981 | [Valid] loss:623.4115, acc:0.0713\n",
      "[epoch] 10 / 1000 | [Train] loss:618.3748, acc:0.9981 | [Valid] loss:623.4962, acc:0.0738\n",
      "[epoch] 11 / 1000 | [Train] loss:618.4878, acc:0.9981 | [Valid] loss:623.4993, acc:0.0687\n",
      "[epoch] 12 / 1000 | [Train] loss:618.4034, acc:0.9981 | [Valid] loss:623.4547, acc:0.0688\n",
      "[epoch] 13 / 1000 | [Train] loss:618.4538, acc:0.9981 | [Valid] loss:623.4564, acc:0.0700\n",
      "[epoch] 14 / 1000 | [Train] loss:618.4791, acc:0.9981 | [Valid] loss:623.4719, acc:0.0675\n",
      "[epoch] 15 / 1000 | [Train] loss:618.4540, acc:0.9978 | [Valid] loss:623.5257, acc:0.0675\n",
      "[epoch] 16 / 1000 | [Train] loss:618.6076, acc:0.9981 | [Valid] loss:623.6058, acc:0.0700\n",
      "[epoch] 17 / 1000 | [Train] loss:618.5540, acc:0.9978 | [Valid] loss:623.5993, acc:0.0713\n",
      "[epoch] 18 / 1000 | [Train] loss:618.5885, acc:0.9984 | [Valid] loss:623.5800, acc:0.0700\n",
      "[epoch] 19 / 1000 | [Train] loss:618.4885, acc:0.9981 | [Valid] loss:623.5875, acc:0.0725\n",
      "[epoch] 20 / 1000 | [Train] loss:618.5799, acc:0.9978 | [Valid] loss:623.6496, acc:0.0638\n",
      "[epoch] 21 / 1000 | [Train] loss:618.6160, acc:0.9981 | [Valid] loss:623.6062, acc:0.0688\n",
      "[epoch] 22 / 1000 | [Train] loss:618.6189, acc:0.9981 | [Valid] loss:623.6122, acc:0.0688\n",
      "[epoch] 23 / 1000 | [Train] loss:618.6700, acc:0.9981 | [Valid] loss:623.6590, acc:0.0662\n",
      "[epoch] 24 / 1000 | [Train] loss:618.6868, acc:0.9981 | [Valid] loss:623.7135, acc:0.0700\n",
      "[epoch] 25 / 1000 | [Train] loss:618.6985, acc:0.9981 | [Valid] loss:623.6714, acc:0.0663\n",
      "[epoch] 26 / 1000 | [Train] loss:618.6618, acc:0.9984 | [Valid] loss:623.6631, acc:0.0688\n",
      "[epoch] 27 / 1000 | [Train] loss:618.6622, acc:0.9978 | [Valid] loss:623.7221, acc:0.0750\n",
      "[epoch] 28 / 1000 | [Train] loss:618.6944, acc:0.9984 | [Valid] loss:623.7009, acc:0.0662\n",
      "[epoch] 29 / 1000 | [Train] loss:618.7336, acc:0.9984 | [Valid] loss:623.7451, acc:0.0725\n",
      "[epoch] 30 / 1000 | [Train] loss:618.6268, acc:0.9981 | [Valid] loss:623.7021, acc:0.0663\n",
      "[epoch] 31 / 1000 | [Train] loss:618.7045, acc:0.9984 | [Valid] loss:623.7176, acc:0.0725\n",
      "[epoch] 32 / 1000 | [Train] loss:618.7198, acc:0.9984 | [Valid] loss:623.7324, acc:0.0700\n",
      "[epoch] 33 / 1000 | [Train] loss:618.7493, acc:0.9981 | [Valid] loss:623.6662, acc:0.0688\n",
      "[epoch] 34 / 1000 | [Train] loss:618.5927, acc:0.9981 | [Valid] loss:623.6535, acc:0.0663\n",
      "[epoch] 35 / 1000 | [Train] loss:618.7420, acc:0.9981 | [Valid] loss:623.7724, acc:0.0675\n",
      "[epoch] 36 / 1000 | [Train] loss:618.7451, acc:0.9978 | [Valid] loss:623.7624, acc:0.0675\n",
      "[epoch] 37 / 1000 | [Train] loss:618.6409, acc:0.9984 | [Valid] loss:623.7330, acc:0.0687\n",
      "[epoch] 38 / 1000 | [Train] loss:618.6111, acc:0.9984 | [Valid] loss:623.6169, acc:0.0713\n",
      "[epoch] 39 / 1000 | [Train] loss:618.6013, acc:0.9978 | [Valid] loss:623.5496, acc:0.0675\n",
      "[epoch] 40 / 1000 | [Train] loss:618.5134, acc:0.9981 | [Valid] loss:623.6118, acc:0.0650\n",
      "[epoch] 41 / 1000 | [Train] loss:618.5306, acc:0.9981 | [Valid] loss:623.5743, acc:0.0700\n",
      "[epoch] 42 / 1000 | [Train] loss:618.5604, acc:0.9981 | [Valid] loss:623.5628, acc:0.0675\n",
      "[epoch] 43 / 1000 | [Train] loss:618.5285, acc:0.9984 | [Valid] loss:623.5848, acc:0.0675\n",
      "[epoch] 44 / 1000 | [Train] loss:618.6761, acc:0.9981 | [Valid] loss:623.7234, acc:0.0700\n",
      "[epoch] 45 / 1000 | [Train] loss:618.6194, acc:0.9984 | [Valid] loss:623.6254, acc:0.0650\n",
      "[epoch] 46 / 1000 | [Train] loss:618.5674, acc:0.9984 | [Valid] loss:623.6648, acc:0.0688\n",
      "[epoch] 47 / 1000 | [Train] loss:618.6374, acc:0.9984 | [Valid] loss:623.6520, acc:0.0725\n",
      "[epoch] 48 / 1000 | [Train] loss:618.5624, acc:0.9984 | [Valid] loss:623.6013, acc:0.0700\n",
      "[epoch] 49 / 1000 | [Train] loss:618.6880, acc:0.9981 | [Valid] loss:623.7446, acc:0.0700\n",
      "[epoch] 50 / 1000 | [Train] loss:618.7585, acc:0.9981 | [Valid] loss:623.7658, acc:0.0687\n",
      "[epoch] 51 / 1000 | [Train] loss:618.7645, acc:0.9984 | [Valid] loss:623.8086, acc:0.0712\n",
      "[epoch] 52 / 1000 | [Train] loss:618.6997, acc:0.9981 | [Valid] loss:623.7344, acc:0.0663\n",
      "[epoch] 53 / 1000 | [Train] loss:618.7074, acc:0.9984 | [Valid] loss:623.7047, acc:0.0650\n",
      "[epoch] 54 / 1000 | [Train] loss:618.7695, acc:0.9981 | [Valid] loss:623.7656, acc:0.0650\n",
      "[epoch] 55 / 1000 | [Train] loss:618.7775, acc:0.9984 | [Valid] loss:623.8344, acc:0.0713\n",
      "[epoch] 56 / 1000 | [Train] loss:618.8410, acc:0.9984 | [Valid] loss:623.8006, acc:0.0675\n",
      "[epoch] 57 / 1000 | [Train] loss:618.6870, acc:0.9978 | [Valid] loss:623.7106, acc:0.0650\n",
      "[epoch] 58 / 1000 | [Train] loss:618.7129, acc:0.9981 | [Valid] loss:623.8400, acc:0.0687\n",
      "[epoch] 59 / 1000 | [Train] loss:618.7200, acc:0.9981 | [Valid] loss:623.7744, acc:0.0675\n",
      "[epoch] 60 / 1000 | [Train] loss:618.6954, acc:0.9981 | [Valid] loss:623.7490, acc:0.0687\n",
      "[epoch] 61 / 1000 | [Train] loss:618.6980, acc:0.9978 | [Valid] loss:623.7484, acc:0.0725\n",
      "[epoch] 62 / 1000 | [Train] loss:618.7227, acc:0.9981 | [Valid] loss:623.7728, acc:0.0688\n",
      "[epoch] 63 / 1000 | [Train] loss:618.7085, acc:0.9978 | [Valid] loss:623.7866, acc:0.0638\n",
      "[epoch] 64 / 1000 | [Train] loss:618.9118, acc:0.9984 | [Valid] loss:623.8707, acc:0.0700\n",
      "[epoch] 65 / 1000 | [Train] loss:618.8862, acc:0.9984 | [Valid] loss:623.8592, acc:0.0687\n",
      "[epoch] 66 / 1000 | [Train] loss:618.9040, acc:0.9978 | [Valid] loss:623.9027, acc:0.0663\n",
      "[epoch] 67 / 1000 | [Train] loss:618.8333, acc:0.9981 | [Valid] loss:623.9124, acc:0.0688\n",
      "[epoch] 68 / 1000 | [Train] loss:618.9196, acc:0.9987 | [Valid] loss:623.8611, acc:0.0688\n",
      "[epoch] 69 / 1000 | [Train] loss:618.8211, acc:0.9987 | [Valid] loss:623.7808, acc:0.0700\n",
      "[epoch] 70 / 1000 | [Train] loss:618.6810, acc:0.9978 | [Valid] loss:623.7061, acc:0.0688\n",
      "[epoch] 71 / 1000 | [Train] loss:618.7401, acc:0.9984 | [Valid] loss:623.7071, acc:0.0663\n",
      "[epoch] 72 / 1000 | [Train] loss:618.7237, acc:0.9984 | [Valid] loss:623.7212, acc:0.0725\n",
      "[epoch] 73 / 1000 | [Train] loss:618.7901, acc:0.9984 | [Valid] loss:623.7506, acc:0.0688\n",
      "[epoch] 74 / 1000 | [Train] loss:618.6253, acc:0.9981 | [Valid] loss:623.7828, acc:0.0688\n",
      "[epoch] 75 / 1000 | [Train] loss:618.7468, acc:0.9981 | [Valid] loss:623.6734, acc:0.0650\n",
      "[epoch] 76 / 1000 | [Train] loss:618.6513, acc:0.9978 | [Valid] loss:623.7290, acc:0.0675\n",
      "[epoch] 77 / 1000 | [Train] loss:618.7242, acc:0.9981 | [Valid] loss:623.7336, acc:0.0700\n",
      "[epoch] 78 / 1000 | [Train] loss:618.7560, acc:0.9981 | [Valid] loss:623.7530, acc:0.0688\n",
      "[epoch] 79 / 1000 | [Train] loss:618.7021, acc:0.9984 | [Valid] loss:623.7460, acc:0.0675\n",
      "[epoch] 80 / 1000 | [Train] loss:618.7070, acc:0.9978 | [Valid] loss:623.7510, acc:0.0688\n",
      "[epoch] 81 / 1000 | [Train] loss:618.8550, acc:0.9981 | [Valid] loss:623.7718, acc:0.0688\n",
      "[epoch] 82 / 1000 | [Train] loss:618.7068, acc:0.9984 | [Valid] loss:623.6956, acc:0.0713\n",
      "[epoch] 83 / 1000 | [Train] loss:618.6587, acc:0.9984 | [Valid] loss:623.6714, acc:0.0700\n",
      "[epoch] 84 / 1000 | [Train] loss:618.5894, acc:0.9978 | [Valid] loss:623.6354, acc:0.0675\n",
      "[epoch] 85 / 1000 | [Train] loss:618.6355, acc:0.9981 | [Valid] loss:623.6525, acc:0.0713\n",
      "[epoch] 86 / 1000 | [Train] loss:618.5562, acc:0.9978 | [Valid] loss:623.6971, acc:0.0713\n",
      "[epoch] 87 / 1000 | [Train] loss:618.6678, acc:0.9978 | [Valid] loss:623.6245, acc:0.0687\n",
      "[epoch] 88 / 1000 | [Train] loss:618.6173, acc:0.9978 | [Valid] loss:623.6394, acc:0.0675\n",
      "[epoch] 89 / 1000 | [Train] loss:618.5926, acc:0.9981 | [Valid] loss:623.6588, acc:0.0675\n",
      "[epoch] 90 / 1000 | [Train] loss:618.6054, acc:0.9984 | [Valid] loss:623.6837, acc:0.0725\n",
      "[epoch] 91 / 1000 | [Train] loss:618.6106, acc:0.9978 | [Valid] loss:623.6663, acc:0.0675\n",
      "[epoch] 92 / 1000 | [Train] loss:618.6679, acc:0.9981 | [Valid] loss:623.6991, acc:0.0700\n",
      "[epoch] 93 / 1000 | [Train] loss:618.7217, acc:0.9978 | [Valid] loss:623.7342, acc:0.0688\n",
      "[epoch] 94 / 1000 | [Train] loss:618.7128, acc:0.9981 | [Valid] loss:623.7240, acc:0.0688\n",
      "[epoch] 95 / 1000 | [Train] loss:618.6199, acc:0.9984 | [Valid] loss:623.7454, acc:0.0688\n",
      "[epoch] 96 / 1000 | [Train] loss:618.6428, acc:0.9981 | [Valid] loss:623.7258, acc:0.0700\n",
      "[epoch] 97 / 1000 | [Train] loss:618.7744, acc:0.9975 | [Valid] loss:623.7402, acc:0.0725\n",
      "[epoch] 98 / 1000 | [Train] loss:618.6741, acc:0.9981 | [Valid] loss:623.8173, acc:0.0688\n",
      "[epoch] 99 / 1000 | [Train] loss:618.8511, acc:0.9981 | [Valid] loss:623.8878, acc:0.0663\n",
      "[epoch] 100 / 1000 | [Train] loss:618.8197, acc:0.9981 | [Valid] loss:623.8599, acc:0.0637\n",
      "[epoch] 101 / 1000 | [Train] loss:618.8100, acc:0.9981 | [Valid] loss:623.7996, acc:0.0663\n",
      "[epoch] 102 / 1000 | [Train] loss:618.7240, acc:0.9981 | [Valid] loss:623.7076, acc:0.0737\n",
      "[epoch] 103 / 1000 | [Train] loss:618.6758, acc:0.9981 | [Valid] loss:623.7093, acc:0.0712\n",
      "[epoch] 104 / 1000 | [Train] loss:618.7251, acc:0.9981 | [Valid] loss:623.7550, acc:0.0738\n",
      "[epoch] 105 / 1000 | [Train] loss:618.7060, acc:0.9981 | [Valid] loss:623.6943, acc:0.0688\n",
      "[epoch] 106 / 1000 | [Train] loss:618.6686, acc:0.9981 | [Valid] loss:623.6569, acc:0.0700\n",
      "[epoch] 107 / 1000 | [Train] loss:618.5814, acc:0.9984 | [Valid] loss:623.6477, acc:0.0725\n",
      "[epoch] 108 / 1000 | [Train] loss:618.5162, acc:0.9978 | [Valid] loss:623.6565, acc:0.0688\n",
      "[epoch] 109 / 1000 | [Train] loss:618.7050, acc:0.9981 | [Valid] loss:623.7029, acc:0.0700\n",
      "[epoch] 110 / 1000 | [Train] loss:618.7667, acc:0.9981 | [Valid] loss:623.7433, acc:0.0675\n",
      "[epoch] 111 / 1000 | [Train] loss:618.7762, acc:0.9981 | [Valid] loss:623.7911, acc:0.0688\n",
      "[epoch] 112 / 1000 | [Train] loss:618.7538, acc:0.9981 | [Valid] loss:623.7983, acc:0.0688\n",
      "[epoch] 113 / 1000 | [Train] loss:618.8191, acc:0.9981 | [Valid] loss:623.8274, acc:0.0650\n",
      "[epoch] 114 / 1000 | [Train] loss:618.7290, acc:0.9981 | [Valid] loss:623.7101, acc:0.0737\n",
      "[epoch] 115 / 1000 | [Train] loss:618.7028, acc:0.9981 | [Valid] loss:623.7176, acc:0.0700\n",
      "[epoch] 116 / 1000 | [Train] loss:618.7214, acc:0.9981 | [Valid] loss:623.6975, acc:0.0663\n",
      "[epoch] 117 / 1000 | [Train] loss:618.7264, acc:0.9981 | [Valid] loss:623.6962, acc:0.0700\n",
      "[epoch] 118 / 1000 | [Train] loss:618.6159, acc:0.9981 | [Valid] loss:623.7006, acc:0.0688\n",
      "[epoch] 119 / 1000 | [Train] loss:618.7152, acc:0.9981 | [Valid] loss:623.7308, acc:0.0700\n",
      "[epoch] 120 / 1000 | [Train] loss:618.7212, acc:0.9981 | [Valid] loss:623.7537, acc:0.0688\n",
      "[epoch] 121 / 1000 | [Train] loss:618.8016, acc:0.9978 | [Valid] loss:623.7451, acc:0.0675\n",
      "[epoch] 122 / 1000 | [Train] loss:618.7012, acc:0.9984 | [Valid] loss:623.7507, acc:0.0675\n",
      "[epoch] 123 / 1000 | [Train] loss:618.7187, acc:0.9981 | [Valid] loss:623.7948, acc:0.0700\n",
      "[epoch] 124 / 1000 | [Train] loss:618.7789, acc:0.9981 | [Valid] loss:623.7695, acc:0.0700\n",
      "[epoch] 125 / 1000 | [Train] loss:618.7432, acc:0.9981 | [Valid] loss:623.6859, acc:0.0650\n",
      "[epoch] 126 / 1000 | [Train] loss:618.6937, acc:0.9984 | [Valid] loss:623.6181, acc:0.0662\n",
      "[epoch] 127 / 1000 | [Train] loss:618.6295, acc:0.9981 | [Valid] loss:623.6607, acc:0.0713\n",
      "[epoch] 128 / 1000 | [Train] loss:618.7117, acc:0.9981 | [Valid] loss:623.7302, acc:0.0712\n",
      "[epoch] 129 / 1000 | [Train] loss:618.7021, acc:0.9984 | [Valid] loss:623.7316, acc:0.0713\n",
      "[epoch] 130 / 1000 | [Train] loss:618.7109, acc:0.9978 | [Valid] loss:623.8036, acc:0.0625\n",
      "[epoch] 131 / 1000 | [Train] loss:618.7232, acc:0.9981 | [Valid] loss:623.7247, acc:0.0688\n",
      "[epoch] 132 / 1000 | [Train] loss:618.6980, acc:0.9978 | [Valid] loss:623.7129, acc:0.0700\n",
      "[epoch] 133 / 1000 | [Train] loss:618.6548, acc:0.9984 | [Valid] loss:623.7247, acc:0.0688\n",
      "[epoch] 134 / 1000 | [Train] loss:618.6674, acc:0.9981 | [Valid] loss:623.7614, acc:0.0725\n",
      "[epoch] 135 / 1000 | [Train] loss:618.7200, acc:0.9987 | [Valid] loss:623.8154, acc:0.0687\n",
      "[epoch] 136 / 1000 | [Train] loss:618.7780, acc:0.9984 | [Valid] loss:623.8074, acc:0.0688\n",
      "[epoch] 137 / 1000 | [Train] loss:618.7647, acc:0.9978 | [Valid] loss:623.8318, acc:0.0675\n",
      "[epoch] 138 / 1000 | [Train] loss:618.9354, acc:0.9981 | [Valid] loss:623.7858, acc:0.0688\n",
      "[epoch] 139 / 1000 | [Train] loss:618.7632, acc:0.9981 | [Valid] loss:623.8548, acc:0.0688\n",
      "[epoch] 140 / 1000 | [Train] loss:618.7496, acc:0.9984 | [Valid] loss:623.8679, acc:0.0663\n",
      "[epoch] 141 / 1000 | [Train] loss:618.8373, acc:0.9981 | [Valid] loss:623.8796, acc:0.0700\n",
      "[epoch] 142 / 1000 | [Train] loss:618.8417, acc:0.9984 | [Valid] loss:623.8266, acc:0.0663\n",
      "[epoch] 143 / 1000 | [Train] loss:618.8839, acc:0.9981 | [Valid] loss:623.8101, acc:0.0725\n",
      "[epoch] 144 / 1000 | [Train] loss:618.8455, acc:0.9981 | [Valid] loss:623.8443, acc:0.0700\n",
      "[epoch] 145 / 1000 | [Train] loss:618.7592, acc:0.9984 | [Valid] loss:623.8036, acc:0.0663\n",
      "[epoch] 146 / 1000 | [Train] loss:618.7591, acc:0.9981 | [Valid] loss:623.8213, acc:0.0700\n",
      "[epoch] 147 / 1000 | [Train] loss:618.8344, acc:0.9984 | [Valid] loss:623.7895, acc:0.0675\n",
      "[epoch] 148 / 1000 | [Train] loss:618.7625, acc:0.9984 | [Valid] loss:623.7596, acc:0.0650\n",
      "[epoch] 149 / 1000 | [Train] loss:618.7639, acc:0.9981 | [Valid] loss:623.7711, acc:0.0700\n",
      "[epoch] 150 / 1000 | [Train] loss:618.6932, acc:0.9978 | [Valid] loss:623.7830, acc:0.0713\n",
      "[epoch] 151 / 1000 | [Train] loss:618.6931, acc:0.9978 | [Valid] loss:623.7356, acc:0.0675\n",
      "[epoch] 152 / 1000 | [Train] loss:618.6970, acc:0.9984 | [Valid] loss:623.7352, acc:0.0650\n",
      "[epoch] 153 / 1000 | [Train] loss:618.7066, acc:0.9984 | [Valid] loss:623.7580, acc:0.0700\n",
      "[epoch] 154 / 1000 | [Train] loss:618.7349, acc:0.9978 | [Valid] loss:623.7804, acc:0.0725\n",
      "[epoch] 155 / 1000 | [Train] loss:618.7298, acc:0.9981 | [Valid] loss:623.7455, acc:0.0688\n",
      "[epoch] 156 / 1000 | [Train] loss:618.6820, acc:0.9981 | [Valid] loss:623.7560, acc:0.0675\n",
      "[epoch] 157 / 1000 | [Train] loss:618.7549, acc:0.9987 | [Valid] loss:623.7971, acc:0.0650\n",
      "[epoch] 158 / 1000 | [Train] loss:618.8286, acc:0.9984 | [Valid] loss:623.7789, acc:0.0663\n",
      "[epoch] 159 / 1000 | [Train] loss:618.6886, acc:0.9978 | [Valid] loss:623.7206, acc:0.0663\n",
      "[epoch] 160 / 1000 | [Train] loss:618.6583, acc:0.9984 | [Valid] loss:623.7699, acc:0.0675\n",
      "[epoch] 161 / 1000 | [Train] loss:618.8425, acc:0.9981 | [Valid] loss:623.8312, acc:0.0700\n",
      "[epoch] 162 / 1000 | [Train] loss:618.7742, acc:0.9981 | [Valid] loss:623.8872, acc:0.0663\n",
      "[epoch] 163 / 1000 | [Train] loss:618.7880, acc:0.9981 | [Valid] loss:623.7927, acc:0.0688\n",
      "[epoch] 164 / 1000 | [Train] loss:618.7648, acc:0.9984 | [Valid] loss:623.7914, acc:0.0700\n",
      "[epoch] 165 / 1000 | [Train] loss:618.7033, acc:0.9984 | [Valid] loss:623.7356, acc:0.0675\n",
      "[epoch] 166 / 1000 | [Train] loss:618.7044, acc:0.9981 | [Valid] loss:623.7948, acc:0.0688\n",
      "[epoch] 167 / 1000 | [Train] loss:618.7304, acc:0.9975 | [Valid] loss:623.8312, acc:0.0688\n",
      "[epoch] 168 / 1000 | [Train] loss:618.7760, acc:0.9978 | [Valid] loss:623.7872, acc:0.0712\n",
      "[epoch] 169 / 1000 | [Train] loss:618.7581, acc:0.9981 | [Valid] loss:623.7619, acc:0.0650\n",
      "[epoch] 170 / 1000 | [Train] loss:618.7961, acc:0.9984 | [Valid] loss:623.7947, acc:0.0700\n",
      "[epoch] 171 / 1000 | [Train] loss:618.8351, acc:0.9984 | [Valid] loss:623.8227, acc:0.0700\n",
      "[epoch] 172 / 1000 | [Train] loss:618.7485, acc:0.9978 | [Valid] loss:623.7607, acc:0.0700\n",
      "[epoch] 173 / 1000 | [Train] loss:618.6570, acc:0.9981 | [Valid] loss:623.7548, acc:0.0713\n",
      "[epoch] 174 / 1000 | [Train] loss:618.6734, acc:0.9984 | [Valid] loss:623.7820, acc:0.0688\n",
      "[epoch] 175 / 1000 | [Train] loss:618.7678, acc:0.9984 | [Valid] loss:623.7349, acc:0.0675\n",
      "[epoch] 176 / 1000 | [Train] loss:618.7550, acc:0.9981 | [Valid] loss:623.8037, acc:0.0713\n",
      "[epoch] 177 / 1000 | [Train] loss:618.8177, acc:0.9984 | [Valid] loss:623.9292, acc:0.0713\n",
      "[epoch] 178 / 1000 | [Train] loss:618.9119, acc:0.9984 | [Valid] loss:623.9998, acc:0.0675\n",
      "[epoch] 179 / 1000 | [Train] loss:618.9737, acc:0.9984 | [Valid] loss:623.9897, acc:0.0688\n",
      "[epoch] 180 / 1000 | [Train] loss:618.9801, acc:0.9978 | [Valid] loss:623.9530, acc:0.0713\n",
      "[epoch] 181 / 1000 | [Train] loss:618.9851, acc:0.9981 | [Valid] loss:623.9680, acc:0.0688\n",
      "[epoch] 182 / 1000 | [Train] loss:618.9341, acc:0.9978 | [Valid] loss:624.0377, acc:0.0663\n",
      "[epoch] 183 / 1000 | [Train] loss:619.0695, acc:0.9978 | [Valid] loss:624.0594, acc:0.0675\n",
      "[epoch] 184 / 1000 | [Train] loss:619.0774, acc:0.9981 | [Valid] loss:623.9550, acc:0.0662\n",
      "[epoch] 185 / 1000 | [Train] loss:618.8517, acc:0.9981 | [Valid] loss:623.9661, acc:0.0675\n",
      "[epoch] 186 / 1000 | [Train] loss:618.9873, acc:0.9978 | [Valid] loss:624.0310, acc:0.0700\n",
      "[epoch] 187 / 1000 | [Train] loss:619.0135, acc:0.9981 | [Valid] loss:624.0844, acc:0.0650\n",
      "[epoch] 188 / 1000 | [Train] loss:619.0835, acc:0.9984 | [Valid] loss:624.0544, acc:0.0712\n",
      "[epoch] 189 / 1000 | [Train] loss:619.0208, acc:0.9981 | [Valid] loss:624.0440, acc:0.0663\n",
      "[epoch] 190 / 1000 | [Train] loss:618.9210, acc:0.9981 | [Valid] loss:624.0736, acc:0.0725\n",
      "[epoch] 191 / 1000 | [Train] loss:619.0804, acc:0.9984 | [Valid] loss:624.2208, acc:0.0688\n",
      "[epoch] 192 / 1000 | [Train] loss:619.3076, acc:0.9981 | [Valid] loss:624.3087, acc:0.0713\n",
      "[epoch] 193 / 1000 | [Train] loss:619.2947, acc:0.9984 | [Valid] loss:624.3105, acc:0.0700\n",
      "[epoch] 194 / 1000 | [Train] loss:619.2418, acc:0.9975 | [Valid] loss:624.2183, acc:0.0675\n",
      "[epoch] 195 / 1000 | [Train] loss:619.2465, acc:0.9981 | [Valid] loss:624.2403, acc:0.0650\n",
      "[epoch] 196 / 1000 | [Train] loss:619.1050, acc:0.9984 | [Valid] loss:624.2164, acc:0.0700\n",
      "[epoch] 197 / 1000 | [Train] loss:619.1639, acc:0.9981 | [Valid] loss:624.1811, acc:0.0712\n",
      "[epoch] 198 / 1000 | [Train] loss:619.2251, acc:0.9987 | [Valid] loss:624.2181, acc:0.0688\n",
      "[epoch] 199 / 1000 | [Train] loss:619.2198, acc:0.9978 | [Valid] loss:624.1131, acc:0.0650\n",
      "[epoch] 200 / 1000 | [Train] loss:619.0747, acc:0.9981 | [Valid] loss:624.1028, acc:0.0638\n",
      "[epoch] 201 / 1000 | [Train] loss:619.0684, acc:0.9981 | [Valid] loss:624.0345, acc:0.0688\n",
      "[epoch] 202 / 1000 | [Train] loss:618.9872, acc:0.9981 | [Valid] loss:624.0859, acc:0.0688\n",
      "[epoch] 203 / 1000 | [Train] loss:619.0188, acc:0.9972 | [Valid] loss:624.0591, acc:0.0663\n",
      "[epoch] 204 / 1000 | [Train] loss:619.0731, acc:0.9984 | [Valid] loss:623.9826, acc:0.0675\n",
      "[epoch] 205 / 1000 | [Train] loss:618.9720, acc:0.9981 | [Valid] loss:623.9492, acc:0.0675\n",
      "[epoch] 206 / 1000 | [Train] loss:618.8934, acc:0.9981 | [Valid] loss:623.9420, acc:0.0700\n",
      "[epoch] 207 / 1000 | [Train] loss:618.9142, acc:0.9981 | [Valid] loss:623.9526, acc:0.0663\n",
      "[epoch] 208 / 1000 | [Train] loss:618.9226, acc:0.9984 | [Valid] loss:623.9322, acc:0.0700\n",
      "[epoch] 209 / 1000 | [Train] loss:618.9193, acc:0.9984 | [Valid] loss:623.8797, acc:0.0650\n",
      "[epoch] 210 / 1000 | [Train] loss:618.8454, acc:0.9984 | [Valid] loss:623.8897, acc:0.0688\n",
      "[epoch] 211 / 1000 | [Train] loss:618.9320, acc:0.9981 | [Valid] loss:623.8761, acc:0.0688\n",
      "[epoch] 212 / 1000 | [Train] loss:618.7948, acc:0.9978 | [Valid] loss:623.8413, acc:0.0688\n",
      "[epoch] 213 / 1000 | [Train] loss:618.8823, acc:0.9981 | [Valid] loss:623.8910, acc:0.0700\n",
      "[epoch] 214 / 1000 | [Train] loss:618.9209, acc:0.9981 | [Valid] loss:623.9854, acc:0.0700\n",
      "[epoch] 215 / 1000 | [Train] loss:618.9207, acc:0.9988 | [Valid] loss:624.0069, acc:0.0713\n",
      "[epoch] 216 / 1000 | [Train] loss:619.0006, acc:0.9984 | [Valid] loss:623.9461, acc:0.0688\n",
      "[epoch] 217 / 1000 | [Train] loss:618.9330, acc:0.9981 | [Valid] loss:623.9495, acc:0.0688\n",
      "[epoch] 218 / 1000 | [Train] loss:618.8823, acc:0.9978 | [Valid] loss:624.1109, acc:0.0675\n",
      "[epoch] 219 / 1000 | [Train] loss:619.0983, acc:0.9984 | [Valid] loss:624.0902, acc:0.0700\n",
      "[epoch] 220 / 1000 | [Train] loss:619.0074, acc:0.9984 | [Valid] loss:624.1016, acc:0.0713\n",
      "[epoch] 221 / 1000 | [Train] loss:619.0583, acc:0.9981 | [Valid] loss:624.1169, acc:0.0688\n",
      "[epoch] 222 / 1000 | [Train] loss:619.1491, acc:0.9981 | [Valid] loss:624.2023, acc:0.0663\n",
      "[epoch] 223 / 1000 | [Train] loss:619.0548, acc:0.9984 | [Valid] loss:624.0849, acc:0.0713\n",
      "[epoch] 224 / 1000 | [Train] loss:619.1245, acc:0.9981 | [Valid] loss:624.1556, acc:0.0688\n",
      "[epoch] 225 / 1000 | [Train] loss:619.0163, acc:0.9978 | [Valid] loss:624.0629, acc:0.0675\n",
      "[epoch] 226 / 1000 | [Train] loss:618.9444, acc:0.9978 | [Valid] loss:623.9918, acc:0.0700\n",
      "[epoch] 227 / 1000 | [Train] loss:619.0166, acc:0.9981 | [Valid] loss:624.0877, acc:0.0738\n",
      "[epoch] 228 / 1000 | [Train] loss:619.0369, acc:0.9984 | [Valid] loss:624.1401, acc:0.0713\n",
      "[epoch] 229 / 1000 | [Train] loss:619.0724, acc:0.9984 | [Valid] loss:624.1241, acc:0.0725\n",
      "[epoch] 230 / 1000 | [Train] loss:619.1259, acc:0.9984 | [Valid] loss:624.1398, acc:0.0700\n",
      "[epoch] 231 / 1000 | [Train] loss:619.1296, acc:0.9981 | [Valid] loss:624.2348, acc:0.0687\n",
      "[epoch] 232 / 1000 | [Train] loss:619.1896, acc:0.9981 | [Valid] loss:624.1533, acc:0.0675\n",
      "[epoch] 233 / 1000 | [Train] loss:619.1693, acc:0.9984 | [Valid] loss:624.2263, acc:0.0663\n",
      "[epoch] 234 / 1000 | [Train] loss:619.2728, acc:0.9981 | [Valid] loss:624.1818, acc:0.0700\n",
      "[epoch] 235 / 1000 | [Train] loss:619.1806, acc:0.9978 | [Valid] loss:624.1364, acc:0.0738\n",
      "[epoch] 236 / 1000 | [Train] loss:619.0994, acc:0.9987 | [Valid] loss:624.1078, acc:0.0675\n",
      "[epoch] 237 / 1000 | [Train] loss:619.1006, acc:0.9978 | [Valid] loss:624.0965, acc:0.0663\n",
      "[epoch] 238 / 1000 | [Train] loss:619.0927, acc:0.9987 | [Valid] loss:624.0954, acc:0.0675\n",
      "[epoch] 239 / 1000 | [Train] loss:619.0328, acc:0.9981 | [Valid] loss:624.0979, acc:0.0738\n",
      "[epoch] 240 / 1000 | [Train] loss:619.1136, acc:0.9984 | [Valid] loss:624.0508, acc:0.0688\n",
      "[epoch] 241 / 1000 | [Train] loss:618.9413, acc:0.9978 | [Valid] loss:624.0369, acc:0.0688\n",
      "[epoch] 242 / 1000 | [Train] loss:619.1091, acc:0.9978 | [Valid] loss:624.1894, acc:0.0650\n",
      "[epoch] 243 / 1000 | [Train] loss:619.1264, acc:0.9981 | [Valid] loss:624.1386, acc:0.0675\n",
      "[epoch] 244 / 1000 | [Train] loss:619.0527, acc:0.9981 | [Valid] loss:624.1456, acc:0.0725\n",
      "[epoch] 245 / 1000 | [Train] loss:619.0916, acc:0.9981 | [Valid] loss:624.1074, acc:0.0725\n",
      "[epoch] 246 / 1000 | [Train] loss:619.1490, acc:0.9981 | [Valid] loss:624.1352, acc:0.0700\n",
      "[epoch] 247 / 1000 | [Train] loss:619.1341, acc:0.9984 | [Valid] loss:624.1176, acc:0.0663\n",
      "[epoch] 248 / 1000 | [Train] loss:619.1325, acc:0.9981 | [Valid] loss:624.1293, acc:0.0650\n",
      "[epoch] 249 / 1000 | [Train] loss:619.0607, acc:0.9981 | [Valid] loss:624.0565, acc:0.0675\n",
      "[epoch] 250 / 1000 | [Train] loss:618.9706, acc:0.9984 | [Valid] loss:624.0367, acc:0.0687\n",
      "[epoch] 251 / 1000 | [Train] loss:619.0522, acc:0.9981 | [Valid] loss:624.1417, acc:0.0663\n",
      "[epoch] 252 / 1000 | [Train] loss:619.0881, acc:0.9984 | [Valid] loss:624.1426, acc:0.0675\n",
      "[epoch] 253 / 1000 | [Train] loss:619.1018, acc:0.9984 | [Valid] loss:624.1799, acc:0.0675\n",
      "[epoch] 254 / 1000 | [Train] loss:619.1020, acc:0.9978 | [Valid] loss:624.1375, acc:0.0688\n",
      "[epoch] 255 / 1000 | [Train] loss:619.1622, acc:0.9984 | [Valid] loss:624.1244, acc:0.0688\n",
      "[epoch] 256 / 1000 | [Train] loss:619.0452, acc:0.9978 | [Valid] loss:624.0642, acc:0.0700\n",
      "[epoch] 257 / 1000 | [Train] loss:618.9932, acc:0.9984 | [Valid] loss:624.0790, acc:0.0663\n",
      "[epoch] 258 / 1000 | [Train] loss:619.0425, acc:0.9981 | [Valid] loss:624.0288, acc:0.0700\n",
      "[epoch] 259 / 1000 | [Train] loss:619.0389, acc:0.9981 | [Valid] loss:624.0448, acc:0.0688\n",
      "[epoch] 260 / 1000 | [Train] loss:618.9559, acc:0.9984 | [Valid] loss:624.0651, acc:0.0675\n",
      "[epoch] 261 / 1000 | [Train] loss:619.0712, acc:0.9984 | [Valid] loss:624.2140, acc:0.0675\n",
      "[epoch] 262 / 1000 | [Train] loss:619.0853, acc:0.9981 | [Valid] loss:624.1253, acc:0.0675\n",
      "[epoch] 263 / 1000 | [Train] loss:619.1190, acc:0.9981 | [Valid] loss:624.0985, acc:0.0700\n",
      "[epoch] 264 / 1000 | [Train] loss:619.0957, acc:0.9978 | [Valid] loss:624.1872, acc:0.0750\n",
      "[epoch] 265 / 1000 | [Train] loss:619.1240, acc:0.9978 | [Valid] loss:624.1827, acc:0.0688\n",
      "[epoch] 266 / 1000 | [Train] loss:619.1266, acc:0.9981 | [Valid] loss:624.2244, acc:0.0688\n",
      "[epoch] 267 / 1000 | [Train] loss:619.1626, acc:0.9981 | [Valid] loss:624.2915, acc:0.0700\n",
      "[epoch] 268 / 1000 | [Train] loss:619.3889, acc:0.9978 | [Valid] loss:624.3060, acc:0.0700\n",
      "[epoch] 269 / 1000 | [Train] loss:619.1359, acc:0.9981 | [Valid] loss:624.2229, acc:0.0688\n",
      "[epoch] 270 / 1000 | [Train] loss:619.1855, acc:0.9984 | [Valid] loss:624.1561, acc:0.0675\n",
      "[epoch] 271 / 1000 | [Train] loss:619.1184, acc:0.9981 | [Valid] loss:624.1139, acc:0.0700\n",
      "[epoch] 272 / 1000 | [Train] loss:619.1267, acc:0.9981 | [Valid] loss:624.1700, acc:0.0675\n",
      "[epoch] 273 / 1000 | [Train] loss:619.1957, acc:0.9981 | [Valid] loss:624.2482, acc:0.0675\n",
      "[epoch] 274 / 1000 | [Train] loss:619.2374, acc:0.9978 | [Valid] loss:624.2184, acc:0.0713\n",
      "[epoch] 275 / 1000 | [Train] loss:619.2521, acc:0.9981 | [Valid] loss:624.2808, acc:0.0675\n",
      "[epoch] 276 / 1000 | [Train] loss:619.2522, acc:0.9981 | [Valid] loss:624.2201, acc:0.0700\n",
      "[epoch] 277 / 1000 | [Train] loss:619.2016, acc:0.9981 | [Valid] loss:624.2866, acc:0.0688\n",
      "[epoch] 278 / 1000 | [Train] loss:619.3363, acc:0.9981 | [Valid] loss:624.3977, acc:0.0738\n",
      "[epoch] 279 / 1000 | [Train] loss:619.4998, acc:0.9984 | [Valid] loss:624.4754, acc:0.0700\n",
      "[epoch] 280 / 1000 | [Train] loss:619.4820, acc:0.9978 | [Valid] loss:624.5198, acc:0.0675\n",
      "[epoch] 281 / 1000 | [Train] loss:619.5523, acc:0.9984 | [Valid] loss:624.5456, acc:0.0675\n",
      "[epoch] 282 / 1000 | [Train] loss:619.6286, acc:0.9981 | [Valid] loss:624.5358, acc:0.0688\n",
      "[epoch] 283 / 1000 | [Train] loss:619.5917, acc:0.9981 | [Valid] loss:624.6044, acc:0.0663\n",
      "[epoch] 284 / 1000 | [Train] loss:619.5947, acc:0.9984 | [Valid] loss:624.6708, acc:0.0625\n",
      "[epoch] 285 / 1000 | [Train] loss:619.6325, acc:0.9981 | [Valid] loss:624.7420, acc:0.0688\n",
      "[epoch] 286 / 1000 | [Train] loss:619.6658, acc:0.9975 | [Valid] loss:624.6245, acc:0.0688\n",
      "[epoch] 287 / 1000 | [Train] loss:619.6509, acc:0.9981 | [Valid] loss:624.6677, acc:0.0675\n",
      "[epoch] 288 / 1000 | [Train] loss:619.7161, acc:0.9978 | [Valid] loss:624.7525, acc:0.0675\n",
      "[epoch] 289 / 1000 | [Train] loss:619.7049, acc:0.9981 | [Valid] loss:624.6785, acc:0.0688\n",
      "[epoch] 290 / 1000 | [Train] loss:619.6573, acc:0.9981 | [Valid] loss:624.6930, acc:0.0687\n",
      "[epoch] 291 / 1000 | [Train] loss:619.6162, acc:0.9984 | [Valid] loss:624.6305, acc:0.0663\n",
      "[epoch] 292 / 1000 | [Train] loss:619.5589, acc:0.9975 | [Valid] loss:624.5273, acc:0.0700\n",
      "[epoch] 293 / 1000 | [Train] loss:619.4612, acc:0.9984 | [Valid] loss:624.5801, acc:0.0662\n",
      "[epoch] 294 / 1000 | [Train] loss:619.6171, acc:0.9981 | [Valid] loss:624.6401, acc:0.0663\n",
      "[epoch] 295 / 1000 | [Train] loss:619.5832, acc:0.9978 | [Valid] loss:624.5966, acc:0.0725\n",
      "[epoch] 296 / 1000 | [Train] loss:619.6440, acc:0.9981 | [Valid] loss:624.6287, acc:0.0688\n",
      "[epoch] 297 / 1000 | [Train] loss:619.5727, acc:0.9981 | [Valid] loss:624.6754, acc:0.0688\n",
      "[epoch] 298 / 1000 | [Train] loss:619.6262, acc:0.9984 | [Valid] loss:624.6434, acc:0.0700\n",
      "[epoch] 299 / 1000 | [Train] loss:619.6184, acc:0.9981 | [Valid] loss:624.6682, acc:0.0712\n",
      "[epoch] 300 / 1000 | [Train] loss:619.6540, acc:0.9981 | [Valid] loss:624.6991, acc:0.0688\n",
      "[epoch] 301 / 1000 | [Train] loss:619.7243, acc:0.9981 | [Valid] loss:624.8218, acc:0.0737\n",
      "[epoch] 302 / 1000 | [Train] loss:619.7080, acc:0.9981 | [Valid] loss:624.7836, acc:0.0713\n",
      "[epoch] 303 / 1000 | [Train] loss:619.8033, acc:0.9978 | [Valid] loss:624.8508, acc:0.0675\n",
      "[epoch] 304 / 1000 | [Train] loss:619.8753, acc:0.9981 | [Valid] loss:624.7518, acc:0.0725\n",
      "[epoch] 305 / 1000 | [Train] loss:619.7429, acc:0.9984 | [Valid] loss:624.6959, acc:0.0700\n",
      "[epoch] 306 / 1000 | [Train] loss:619.6734, acc:0.9984 | [Valid] loss:624.6566, acc:0.0663\n",
      "[epoch] 307 / 1000 | [Train] loss:619.5745, acc:0.9981 | [Valid] loss:624.6254, acc:0.0712\n",
      "[epoch] 308 / 1000 | [Train] loss:619.6443, acc:0.9984 | [Valid] loss:624.7351, acc:0.0687\n",
      "[epoch] 309 / 1000 | [Train] loss:619.8144, acc:0.9981 | [Valid] loss:624.8970, acc:0.0675\n",
      "[epoch] 310 / 1000 | [Train] loss:619.9504, acc:0.9978 | [Valid] loss:624.9317, acc:0.0663\n",
      "[epoch] 311 / 1000 | [Train] loss:619.8521, acc:0.9984 | [Valid] loss:624.8738, acc:0.0688\n",
      "[epoch] 312 / 1000 | [Train] loss:619.8382, acc:0.9978 | [Valid] loss:624.8336, acc:0.0688\n",
      "[epoch] 313 / 1000 | [Train] loss:619.8639, acc:0.9984 | [Valid] loss:624.8816, acc:0.0713\n",
      "[epoch] 314 / 1000 | [Train] loss:619.9834, acc:0.9984 | [Valid] loss:624.9530, acc:0.0675\n",
      "[epoch] 315 / 1000 | [Train] loss:619.8814, acc:0.9981 | [Valid] loss:624.8848, acc:0.0688\n",
      "[epoch] 316 / 1000 | [Train] loss:619.8869, acc:0.9981 | [Valid] loss:624.9118, acc:0.0688\n",
      "[epoch] 317 / 1000 | [Train] loss:619.8844, acc:0.9981 | [Valid] loss:624.9560, acc:0.0700\n",
      "[epoch] 318 / 1000 | [Train] loss:619.8748, acc:0.9981 | [Valid] loss:624.8720, acc:0.0713\n",
      "[epoch] 319 / 1000 | [Train] loss:619.8564, acc:0.9984 | [Valid] loss:624.9240, acc:0.0700\n",
      "[epoch] 320 / 1000 | [Train] loss:619.9770, acc:0.9984 | [Valid] loss:624.9243, acc:0.0675\n",
      "[epoch] 321 / 1000 | [Train] loss:619.9142, acc:0.9975 | [Valid] loss:624.9599, acc:0.0713\n",
      "[epoch] 322 / 1000 | [Train] loss:619.9678, acc:0.9984 | [Valid] loss:625.0270, acc:0.0650\n",
      "[epoch] 323 / 1000 | [Train] loss:619.9489, acc:0.9984 | [Valid] loss:625.0338, acc:0.0688\n",
      "[epoch] 324 / 1000 | [Train] loss:620.0600, acc:0.9981 | [Valid] loss:625.0965, acc:0.0675\n",
      "[epoch] 325 / 1000 | [Train] loss:620.0526, acc:0.9981 | [Valid] loss:625.0464, acc:0.0675\n",
      "[epoch] 326 / 1000 | [Train] loss:619.9651, acc:0.9975 | [Valid] loss:624.9958, acc:0.0713\n",
      "[epoch] 327 / 1000 | [Train] loss:619.9066, acc:0.9981 | [Valid] loss:624.9347, acc:0.0688\n",
      "[epoch] 328 / 1000 | [Train] loss:619.8075, acc:0.9981 | [Valid] loss:624.9034, acc:0.0650\n",
      "[epoch] 329 / 1000 | [Train] loss:619.9524, acc:0.9984 | [Valid] loss:624.9634, acc:0.0675\n",
      "[epoch] 330 / 1000 | [Train] loss:619.9263, acc:0.9978 | [Valid] loss:624.9973, acc:0.0687\n",
      "[epoch] 331 / 1000 | [Train] loss:619.9978, acc:0.9981 | [Valid] loss:625.0210, acc:0.0700\n",
      "[epoch] 332 / 1000 | [Train] loss:620.0478, acc:0.9981 | [Valid] loss:625.1111, acc:0.0700\n",
      "[epoch] 333 / 1000 | [Train] loss:620.1029, acc:0.9978 | [Valid] loss:625.0592, acc:0.0675\n",
      "[epoch] 334 / 1000 | [Train] loss:620.0331, acc:0.9981 | [Valid] loss:625.0706, acc:0.0663\n",
      "[epoch] 335 / 1000 | [Train] loss:620.1518, acc:0.9984 | [Valid] loss:625.0494, acc:0.0688\n",
      "[epoch] 336 / 1000 | [Train] loss:620.0237, acc:0.9975 | [Valid] loss:625.0820, acc:0.0713\n",
      "[epoch] 337 / 1000 | [Train] loss:620.0807, acc:0.9981 | [Valid] loss:625.0785, acc:0.0700\n",
      "[epoch] 338 / 1000 | [Train] loss:620.1555, acc:0.9981 | [Valid] loss:625.1417, acc:0.0700\n",
      "[epoch] 339 / 1000 | [Train] loss:620.1583, acc:0.9984 | [Valid] loss:625.2280, acc:0.0675\n",
      "[epoch] 340 / 1000 | [Train] loss:620.1363, acc:0.9978 | [Valid] loss:625.2050, acc:0.0700\n",
      "[epoch] 341 / 1000 | [Train] loss:620.1524, acc:0.9984 | [Valid] loss:625.0806, acc:0.0700\n",
      "[epoch] 342 / 1000 | [Train] loss:619.9811, acc:0.9981 | [Valid] loss:624.9660, acc:0.0663\n",
      "[epoch] 343 / 1000 | [Train] loss:619.8986, acc:0.9981 | [Valid] loss:624.9635, acc:0.0675\n",
      "[epoch] 344 / 1000 | [Train] loss:619.8991, acc:0.9981 | [Valid] loss:624.9574, acc:0.0700\n",
      "[epoch] 345 / 1000 | [Train] loss:619.9504, acc:0.9978 | [Valid] loss:624.8959, acc:0.0688\n",
      "[epoch] 346 / 1000 | [Train] loss:619.9204, acc:0.9978 | [Valid] loss:624.9671, acc:0.0700\n",
      "[epoch] 347 / 1000 | [Train] loss:619.9553, acc:0.9978 | [Valid] loss:625.0071, acc:0.0687\n",
      "[epoch] 348 / 1000 | [Train] loss:619.9612, acc:0.9978 | [Valid] loss:625.0776, acc:0.0700\n",
      "[epoch] 349 / 1000 | [Train] loss:620.1164, acc:0.9981 | [Valid] loss:625.1559, acc:0.0713\n",
      "[epoch] 350 / 1000 | [Train] loss:620.1244, acc:0.9981 | [Valid] loss:625.1218, acc:0.0650\n",
      "[epoch] 351 / 1000 | [Train] loss:620.0147, acc:0.9978 | [Valid] loss:625.1963, acc:0.0700\n",
      "[epoch] 352 / 1000 | [Train] loss:620.0879, acc:0.9984 | [Valid] loss:625.1015, acc:0.0700\n",
      "[epoch] 353 / 1000 | [Train] loss:620.0121, acc:0.9984 | [Valid] loss:625.0674, acc:0.0725\n",
      "[epoch] 354 / 1000 | [Train] loss:620.1558, acc:0.9981 | [Valid] loss:625.1311, acc:0.0700\n",
      "[epoch] 355 / 1000 | [Train] loss:620.1494, acc:0.9981 | [Valid] loss:625.1832, acc:0.0688\n",
      "[epoch] 356 / 1000 | [Train] loss:620.0777, acc:0.9981 | [Valid] loss:625.1318, acc:0.0725\n",
      "[epoch] 357 / 1000 | [Train] loss:620.0760, acc:0.9981 | [Valid] loss:625.1449, acc:0.0675\n",
      "[epoch] 358 / 1000 | [Train] loss:620.1543, acc:0.9981 | [Valid] loss:625.0961, acc:0.0713\n",
      "[epoch] 359 / 1000 | [Train] loss:619.9568, acc:0.9981 | [Valid] loss:625.0337, acc:0.0713\n",
      "[epoch] 360 / 1000 | [Train] loss:619.9194, acc:0.9978 | [Valid] loss:624.9603, acc:0.0688\n",
      "[epoch] 361 / 1000 | [Train] loss:619.9655, acc:0.9978 | [Valid] loss:625.0464, acc:0.0675\n",
      "[epoch] 362 / 1000 | [Train] loss:620.0375, acc:0.9978 | [Valid] loss:625.0918, acc:0.0688\n",
      "[epoch] 363 / 1000 | [Train] loss:620.0986, acc:0.9984 | [Valid] loss:625.0615, acc:0.0687\n",
      "[epoch] 364 / 1000 | [Train] loss:620.0014, acc:0.9978 | [Valid] loss:625.0276, acc:0.0663\n",
      "[epoch] 365 / 1000 | [Train] loss:619.8874, acc:0.9984 | [Valid] loss:625.0132, acc:0.0663\n",
      "[epoch] 366 / 1000 | [Train] loss:619.9490, acc:0.9984 | [Valid] loss:625.0248, acc:0.0650\n",
      "[epoch] 367 / 1000 | [Train] loss:620.0632, acc:0.9981 | [Valid] loss:625.0561, acc:0.0700\n",
      "[epoch] 368 / 1000 | [Train] loss:620.0287, acc:0.9981 | [Valid] loss:625.0715, acc:0.0675\n",
      "[epoch] 369 / 1000 | [Train] loss:620.0882, acc:0.9981 | [Valid] loss:625.0383, acc:0.0725\n",
      "[epoch] 370 / 1000 | [Train] loss:619.9525, acc:0.9984 | [Valid] loss:625.0297, acc:0.0688\n",
      "[epoch] 371 / 1000 | [Train] loss:620.0689, acc:0.9984 | [Valid] loss:624.9883, acc:0.0675\n",
      "[epoch] 372 / 1000 | [Train] loss:619.9508, acc:0.9978 | [Valid] loss:624.9813, acc:0.0688\n",
      "[epoch] 373 / 1000 | [Train] loss:619.9695, acc:0.9978 | [Valid] loss:625.0440, acc:0.0675\n",
      "[epoch] 374 / 1000 | [Train] loss:619.9460, acc:0.9981 | [Valid] loss:624.9903, acc:0.0688\n",
      "[epoch] 375 / 1000 | [Train] loss:620.0531, acc:0.9975 | [Valid] loss:625.0296, acc:0.0725\n",
      "[epoch] 376 / 1000 | [Train] loss:620.0719, acc:0.9984 | [Valid] loss:625.0756, acc:0.0675\n",
      "[epoch] 377 / 1000 | [Train] loss:620.1220, acc:0.9984 | [Valid] loss:625.0911, acc:0.0675\n",
      "[epoch] 378 / 1000 | [Train] loss:620.1454, acc:0.9978 | [Valid] loss:625.1755, acc:0.0663\n",
      "[epoch] 379 / 1000 | [Train] loss:620.0840, acc:0.9981 | [Valid] loss:625.1606, acc:0.0688\n",
      "[epoch] 380 / 1000 | [Train] loss:620.1189, acc:0.9984 | [Valid] loss:625.1398, acc:0.0663\n",
      "[epoch] 381 / 1000 | [Train] loss:620.2119, acc:0.9978 | [Valid] loss:625.2249, acc:0.0700\n",
      "[epoch] 382 / 1000 | [Train] loss:620.2680, acc:0.9981 | [Valid] loss:625.3068, acc:0.0712\n",
      "[epoch] 383 / 1000 | [Train] loss:620.2958, acc:0.9978 | [Valid] loss:625.2736, acc:0.0688\n",
      "[epoch] 384 / 1000 | [Train] loss:620.2597, acc:0.9981 | [Valid] loss:625.2261, acc:0.0688\n",
      "[epoch] 385 / 1000 | [Train] loss:620.3563, acc:0.9984 | [Valid] loss:625.2897, acc:0.0713\n",
      "[epoch] 386 / 1000 | [Train] loss:620.3076, acc:0.9978 | [Valid] loss:625.2183, acc:0.0675\n",
      "[epoch] 387 / 1000 | [Train] loss:620.1362, acc:0.9981 | [Valid] loss:625.2069, acc:0.0700\n",
      "[epoch] 388 / 1000 | [Train] loss:620.2174, acc:0.9981 | [Valid] loss:625.1737, acc:0.0737\n",
      "[epoch] 389 / 1000 | [Train] loss:620.1158, acc:0.9981 | [Valid] loss:625.2197, acc:0.0688\n",
      "[epoch] 390 / 1000 | [Train] loss:620.2310, acc:0.9978 | [Valid] loss:625.2088, acc:0.0712\n",
      "[epoch] 391 / 1000 | [Train] loss:620.1865, acc:0.9978 | [Valid] loss:625.1978, acc:0.0688\n",
      "[epoch] 392 / 1000 | [Train] loss:620.1600, acc:0.9981 | [Valid] loss:625.1246, acc:0.0713\n",
      "[epoch] 393 / 1000 | [Train] loss:619.9707, acc:0.9984 | [Valid] loss:625.0921, acc:0.0725\n",
      "[epoch] 394 / 1000 | [Train] loss:620.1073, acc:0.9981 | [Valid] loss:625.1516, acc:0.0650\n",
      "[epoch] 395 / 1000 | [Train] loss:620.1344, acc:0.9978 | [Valid] loss:625.0787, acc:0.0687\n",
      "[epoch] 396 / 1000 | [Train] loss:620.0912, acc:0.9981 | [Valid] loss:625.0530, acc:0.0675\n",
      "[epoch] 397 / 1000 | [Train] loss:620.0123, acc:0.9981 | [Valid] loss:625.0563, acc:0.0625\n",
      "[epoch] 398 / 1000 | [Train] loss:620.0393, acc:0.9981 | [Valid] loss:625.1355, acc:0.0688\n",
      "[epoch] 399 / 1000 | [Train] loss:620.1172, acc:0.9981 | [Valid] loss:625.2253, acc:0.0625\n",
      "[epoch] 400 / 1000 | [Train] loss:620.1740, acc:0.9981 | [Valid] loss:625.2673, acc:0.0700\n",
      "[epoch] 401 / 1000 | [Train] loss:620.2109, acc:0.9981 | [Valid] loss:625.2337, acc:0.0675\n",
      "[epoch] 402 / 1000 | [Train] loss:620.2139, acc:0.9981 | [Valid] loss:625.2666, acc:0.0712\n",
      "[epoch] 403 / 1000 | [Train] loss:620.2269, acc:0.9978 | [Valid] loss:625.1981, acc:0.0675\n",
      "[epoch] 404 / 1000 | [Train] loss:620.2383, acc:0.9978 | [Valid] loss:625.3064, acc:0.0700\n",
      "[epoch] 405 / 1000 | [Train] loss:620.2716, acc:0.9975 | [Valid] loss:625.2452, acc:0.0675\n",
      "[epoch] 406 / 1000 | [Train] loss:620.2360, acc:0.9981 | [Valid] loss:625.2242, acc:0.0700\n",
      "[epoch] 407 / 1000 | [Train] loss:620.2525, acc:0.9981 | [Valid] loss:625.2967, acc:0.0688\n",
      "[epoch] 408 / 1000 | [Train] loss:620.3369, acc:0.9981 | [Valid] loss:625.3093, acc:0.0688\n",
      "[epoch] 409 / 1000 | [Train] loss:620.1722, acc:0.9978 | [Valid] loss:625.3496, acc:0.0688\n",
      "[epoch] 410 / 1000 | [Train] loss:620.2612, acc:0.9981 | [Valid] loss:625.2913, acc:0.0675\n",
      "[epoch] 411 / 1000 | [Train] loss:620.3338, acc:0.9984 | [Valid] loss:625.3653, acc:0.0688\n",
      "[epoch] 412 / 1000 | [Train] loss:620.3785, acc:0.9981 | [Valid] loss:625.4094, acc:0.0713\n",
      "[epoch] 413 / 1000 | [Train] loss:620.4018, acc:0.9981 | [Valid] loss:625.3218, acc:0.0650\n",
      "[epoch] 414 / 1000 | [Train] loss:620.3758, acc:0.9984 | [Valid] loss:625.3662, acc:0.0675\n",
      "[epoch] 415 / 1000 | [Train] loss:620.2274, acc:0.9981 | [Valid] loss:625.2641, acc:0.0675\n",
      "[epoch] 416 / 1000 | [Train] loss:620.2453, acc:0.9984 | [Valid] loss:625.2392, acc:0.0687\n",
      "[epoch] 417 / 1000 | [Train] loss:620.1569, acc:0.9978 | [Valid] loss:625.2234, acc:0.0688\n",
      "[epoch] 418 / 1000 | [Train] loss:620.1802, acc:0.9981 | [Valid] loss:625.2336, acc:0.0713\n",
      "[epoch] 419 / 1000 | [Train] loss:620.1946, acc:0.9981 | [Valid] loss:625.2435, acc:0.0663\n",
      "[epoch] 420 / 1000 | [Train] loss:620.2140, acc:0.9981 | [Valid] loss:625.2603, acc:0.0700\n",
      "[epoch] 421 / 1000 | [Train] loss:620.2682, acc:0.9981 | [Valid] loss:625.2559, acc:0.0700\n",
      "[epoch] 422 / 1000 | [Train] loss:620.2087, acc:0.9981 | [Valid] loss:625.1594, acc:0.0688\n",
      "[epoch] 423 / 1000 | [Train] loss:620.1055, acc:0.9981 | [Valid] loss:625.1094, acc:0.0650\n",
      "[epoch] 424 / 1000 | [Train] loss:620.1704, acc:0.9981 | [Valid] loss:625.1148, acc:0.0688\n",
      "[epoch] 425 / 1000 | [Train] loss:620.2089, acc:0.9984 | [Valid] loss:625.1188, acc:0.0663\n",
      "[epoch] 426 / 1000 | [Train] loss:620.0073, acc:0.9981 | [Valid] loss:625.1073, acc:0.0675\n",
      "[epoch] 427 / 1000 | [Train] loss:619.9692, acc:0.9981 | [Valid] loss:624.9970, acc:0.0675\n",
      "[epoch] 428 / 1000 | [Train] loss:620.0075, acc:0.9981 | [Valid] loss:625.0963, acc:0.0725\n",
      "[epoch] 429 / 1000 | [Train] loss:620.0475, acc:0.9981 | [Valid] loss:625.0354, acc:0.0687\n",
      "[epoch] 430 / 1000 | [Train] loss:620.0138, acc:0.9978 | [Valid] loss:625.0622, acc:0.0688\n",
      "[epoch] 431 / 1000 | [Train] loss:620.1431, acc:0.9981 | [Valid] loss:625.1076, acc:0.0638\n",
      "[epoch] 432 / 1000 | [Train] loss:620.1092, acc:0.9978 | [Valid] loss:625.1809, acc:0.0688\n",
      "[epoch] 433 / 1000 | [Train] loss:620.0618, acc:0.9981 | [Valid] loss:625.1133, acc:0.0700\n",
      "[epoch] 434 / 1000 | [Train] loss:619.9728, acc:0.9981 | [Valid] loss:625.0963, acc:0.0700\n",
      "[epoch] 435 / 1000 | [Train] loss:620.0072, acc:0.9984 | [Valid] loss:625.1154, acc:0.0663\n",
      "[epoch] 436 / 1000 | [Train] loss:620.0422, acc:0.9981 | [Valid] loss:625.1669, acc:0.0688\n",
      "[epoch] 437 / 1000 | [Train] loss:620.2126, acc:0.9981 | [Valid] loss:625.1566, acc:0.0688\n",
      "[epoch] 438 / 1000 | [Train] loss:620.0679, acc:0.9981 | [Valid] loss:625.0639, acc:0.0663\n",
      "[epoch] 439 / 1000 | [Train] loss:620.0291, acc:0.9978 | [Valid] loss:625.1158, acc:0.0700\n",
      "[epoch] 440 / 1000 | [Train] loss:620.1535, acc:0.9981 | [Valid] loss:625.1202, acc:0.0737\n",
      "[epoch] 441 / 1000 | [Train] loss:620.0009, acc:0.9981 | [Valid] loss:625.0233, acc:0.0688\n",
      "[epoch] 442 / 1000 | [Train] loss:620.0433, acc:0.9984 | [Valid] loss:625.0248, acc:0.0675\n",
      "[epoch] 443 / 1000 | [Train] loss:620.0809, acc:0.9981 | [Valid] loss:625.0461, acc:0.0700\n",
      "[epoch] 444 / 1000 | [Train] loss:620.0347, acc:0.9978 | [Valid] loss:625.0753, acc:0.0638\n",
      "[epoch] 445 / 1000 | [Train] loss:620.0578, acc:0.9978 | [Valid] loss:625.0961, acc:0.0688\n",
      "[epoch] 446 / 1000 | [Train] loss:619.9953, acc:0.9981 | [Valid] loss:625.0470, acc:0.0700\n",
      "[epoch] 447 / 1000 | [Train] loss:620.0864, acc:0.9978 | [Valid] loss:625.0443, acc:0.0688\n",
      "[epoch] 448 / 1000 | [Train] loss:620.1259, acc:0.9984 | [Valid] loss:625.1158, acc:0.0688\n",
      "[epoch] 449 / 1000 | [Train] loss:620.1403, acc:0.9984 | [Valid] loss:625.1894, acc:0.0688\n",
      "[epoch] 450 / 1000 | [Train] loss:620.1581, acc:0.9984 | [Valid] loss:625.2003, acc:0.0713\n",
      "[epoch] 451 / 1000 | [Train] loss:620.2262, acc:0.9984 | [Valid] loss:625.3226, acc:0.0700\n",
      "[epoch] 452 / 1000 | [Train] loss:620.1226, acc:0.9981 | [Valid] loss:625.1929, acc:0.0713\n",
      "[epoch] 453 / 1000 | [Train] loss:620.2246, acc:0.9981 | [Valid] loss:625.2785, acc:0.0650\n",
      "[epoch] 454 / 1000 | [Train] loss:620.3626, acc:0.9981 | [Valid] loss:625.3162, acc:0.0688\n",
      "[epoch] 455 / 1000 | [Train] loss:620.2728, acc:0.9981 | [Valid] loss:625.3630, acc:0.0663\n",
      "[epoch] 456 / 1000 | [Train] loss:620.4156, acc:0.9981 | [Valid] loss:625.3861, acc:0.0712\n",
      "[epoch] 457 / 1000 | [Train] loss:620.2337, acc:0.9981 | [Valid] loss:625.2378, acc:0.0725\n",
      "[epoch] 458 / 1000 | [Train] loss:620.2449, acc:0.9984 | [Valid] loss:625.3151, acc:0.0637\n",
      "[epoch] 459 / 1000 | [Train] loss:620.3882, acc:0.9984 | [Valid] loss:625.3265, acc:0.0688\n",
      "[epoch] 460 / 1000 | [Train] loss:620.2106, acc:0.9981 | [Valid] loss:625.3064, acc:0.0737\n",
      "[epoch] 461 / 1000 | [Train] loss:620.2906, acc:0.9981 | [Valid] loss:625.3833, acc:0.0650\n",
      "[epoch] 462 / 1000 | [Train] loss:620.3447, acc:0.9975 | [Valid] loss:625.3145, acc:0.0700\n",
      "[epoch] 463 / 1000 | [Train] loss:620.3413, acc:0.9981 | [Valid] loss:625.2588, acc:0.0700\n",
      "[epoch] 464 / 1000 | [Train] loss:620.2851, acc:0.9984 | [Valid] loss:625.3037, acc:0.0663\n",
      "[epoch] 465 / 1000 | [Train] loss:620.3220, acc:0.9984 | [Valid] loss:625.3082, acc:0.0663\n",
      "[epoch] 466 / 1000 | [Train] loss:620.2138, acc:0.9978 | [Valid] loss:625.2949, acc:0.0700\n",
      "[epoch] 467 / 1000 | [Train] loss:620.2124, acc:0.9981 | [Valid] loss:625.2910, acc:0.0663\n",
      "[epoch] 468 / 1000 | [Train] loss:620.2829, acc:0.9981 | [Valid] loss:625.4221, acc:0.0713\n",
      "[epoch] 469 / 1000 | [Train] loss:620.4026, acc:0.9981 | [Valid] loss:625.5324, acc:0.0700\n",
      "[epoch] 470 / 1000 | [Train] loss:620.4892, acc:0.9978 | [Valid] loss:625.6165, acc:0.0675\n",
      "[epoch] 471 / 1000 | [Train] loss:620.5656, acc:0.9981 | [Valid] loss:625.6650, acc:0.0713\n",
      "[epoch] 472 / 1000 | [Train] loss:620.5963, acc:0.9984 | [Valid] loss:625.6066, acc:0.0688\n",
      "[epoch] 473 / 1000 | [Train] loss:620.5283, acc:0.9978 | [Valid] loss:625.5780, acc:0.0750\n",
      "[epoch] 474 / 1000 | [Train] loss:620.4895, acc:0.9978 | [Valid] loss:625.5983, acc:0.0638\n",
      "[epoch] 475 / 1000 | [Train] loss:620.5938, acc:0.9978 | [Valid] loss:625.6121, acc:0.0688\n",
      "[epoch] 476 / 1000 | [Train] loss:620.6348, acc:0.9978 | [Valid] loss:625.6569, acc:0.0675\n",
      "[epoch] 477 / 1000 | [Train] loss:620.5579, acc:0.9978 | [Valid] loss:625.6656, acc:0.0688\n",
      "[epoch] 478 / 1000 | [Train] loss:620.5195, acc:0.9981 | [Valid] loss:625.6333, acc:0.0713\n",
      "[epoch] 479 / 1000 | [Train] loss:620.6406, acc:0.9978 | [Valid] loss:625.5752, acc:0.0700\n",
      "[epoch] 480 / 1000 | [Train] loss:620.6026, acc:0.9981 | [Valid] loss:625.6236, acc:0.0688\n",
      "[epoch] 481 / 1000 | [Train] loss:620.6508, acc:0.9981 | [Valid] loss:625.6584, acc:0.0650\n",
      "[epoch] 482 / 1000 | [Train] loss:620.6116, acc:0.9984 | [Valid] loss:625.6041, acc:0.0675\n",
      "[epoch] 483 / 1000 | [Train] loss:620.5456, acc:0.9981 | [Valid] loss:625.5831, acc:0.0687\n",
      "[epoch] 484 / 1000 | [Train] loss:620.6208, acc:0.9981 | [Valid] loss:625.6624, acc:0.0688\n",
      "[epoch] 485 / 1000 | [Train] loss:620.6706, acc:0.9981 | [Valid] loss:625.7074, acc:0.0700\n",
      "[epoch] 486 / 1000 | [Train] loss:620.6935, acc:0.9981 | [Valid] loss:625.6839, acc:0.0675\n",
      "[epoch] 487 / 1000 | [Train] loss:620.6887, acc:0.9975 | [Valid] loss:625.7152, acc:0.0675\n",
      "[epoch] 488 / 1000 | [Train] loss:620.6291, acc:0.9981 | [Valid] loss:625.6331, acc:0.0713\n",
      "[epoch] 489 / 1000 | [Train] loss:620.5760, acc:0.9978 | [Valid] loss:625.7078, acc:0.0688\n",
      "[epoch] 490 / 1000 | [Train] loss:620.6742, acc:0.9981 | [Valid] loss:625.6954, acc:0.0688\n",
      "[epoch] 491 / 1000 | [Train] loss:620.6256, acc:0.9984 | [Valid] loss:625.5780, acc:0.0675\n",
      "[epoch] 492 / 1000 | [Train] loss:620.5972, acc:0.9981 | [Valid] loss:625.5004, acc:0.0700\n",
      "[epoch] 493 / 1000 | [Train] loss:620.4571, acc:0.9981 | [Valid] loss:625.4600, acc:0.0675\n",
      "[epoch] 494 / 1000 | [Train] loss:620.4323, acc:0.9978 | [Valid] loss:625.5225, acc:0.0688\n",
      "[epoch] 495 / 1000 | [Train] loss:620.4899, acc:0.9981 | [Valid] loss:625.6125, acc:0.0688\n",
      "[epoch] 496 / 1000 | [Train] loss:620.5527, acc:0.9978 | [Valid] loss:625.5861, acc:0.0688\n",
      "[epoch] 497 / 1000 | [Train] loss:620.6127, acc:0.9981 | [Valid] loss:625.5574, acc:0.0638\n",
      "[epoch] 498 / 1000 | [Train] loss:620.4911, acc:0.9981 | [Valid] loss:625.4792, acc:0.0688\n",
      "[epoch] 499 / 1000 | [Train] loss:620.4657, acc:0.9978 | [Valid] loss:625.4358, acc:0.0713\n",
      "[epoch] 500 / 1000 | [Train] loss:620.4611, acc:0.9984 | [Valid] loss:625.4629, acc:0.0700\n",
      "[epoch] 501 / 1000 | [Train] loss:620.4040, acc:0.9981 | [Valid] loss:625.4883, acc:0.0713\n",
      "[epoch] 502 / 1000 | [Train] loss:620.3828, acc:0.9981 | [Valid] loss:625.5202, acc:0.0700\n",
      "[epoch] 503 / 1000 | [Train] loss:620.4254, acc:0.9975 | [Valid] loss:625.4975, acc:0.0712\n",
      "[epoch] 504 / 1000 | [Train] loss:620.5144, acc:0.9981 | [Valid] loss:625.4145, acc:0.0687\n",
      "[epoch] 505 / 1000 | [Train] loss:620.4277, acc:0.9984 | [Valid] loss:625.3001, acc:0.0687\n",
      "[epoch] 506 / 1000 | [Train] loss:620.3717, acc:0.9984 | [Valid] loss:625.3688, acc:0.0688\n",
      "[epoch] 507 / 1000 | [Train] loss:620.2660, acc:0.9984 | [Valid] loss:625.2669, acc:0.0700\n",
      "[epoch] 508 / 1000 | [Train] loss:620.1276, acc:0.9981 | [Valid] loss:625.2042, acc:0.0737\n",
      "[epoch] 509 / 1000 | [Train] loss:620.2869, acc:0.9981 | [Valid] loss:625.2895, acc:0.0688\n",
      "[epoch] 510 / 1000 | [Train] loss:620.2224, acc:0.9981 | [Valid] loss:625.2653, acc:0.0712\n",
      "[epoch] 511 / 1000 | [Train] loss:620.3025, acc:0.9978 | [Valid] loss:625.2635, acc:0.0713\n",
      "[epoch] 512 / 1000 | [Train] loss:620.2238, acc:0.9984 | [Valid] loss:625.2604, acc:0.0675\n",
      "[epoch] 513 / 1000 | [Train] loss:620.2062, acc:0.9981 | [Valid] loss:625.2185, acc:0.0700\n",
      "[epoch] 514 / 1000 | [Train] loss:620.1408, acc:0.9984 | [Valid] loss:625.1776, acc:0.0663\n",
      "[epoch] 515 / 1000 | [Train] loss:620.2010, acc:0.9981 | [Valid] loss:625.2149, acc:0.0700\n",
      "[epoch] 516 / 1000 | [Train] loss:620.2799, acc:0.9984 | [Valid] loss:625.2142, acc:0.0700\n",
      "[epoch] 517 / 1000 | [Train] loss:620.1761, acc:0.9978 | [Valid] loss:625.2135, acc:0.0675\n",
      "[epoch] 518 / 1000 | [Train] loss:620.2130, acc:0.9984 | [Valid] loss:625.3150, acc:0.0688\n",
      "[epoch] 519 / 1000 | [Train] loss:620.2981, acc:0.9978 | [Valid] loss:625.3281, acc:0.0688\n",
      "[epoch] 520 / 1000 | [Train] loss:620.3600, acc:0.9984 | [Valid] loss:625.2900, acc:0.0700\n",
      "[epoch] 521 / 1000 | [Train] loss:620.1852, acc:0.9978 | [Valid] loss:625.1917, acc:0.0700\n",
      "[epoch] 522 / 1000 | [Train] loss:620.1164, acc:0.9978 | [Valid] loss:625.2148, acc:0.0663\n",
      "[epoch] 523 / 1000 | [Train] loss:620.1715, acc:0.9981 | [Valid] loss:625.3288, acc:0.0675\n",
      "[epoch] 524 / 1000 | [Train] loss:620.2636, acc:0.9984 | [Valid] loss:625.3476, acc:0.0688\n",
      "[epoch] 525 / 1000 | [Train] loss:620.2658, acc:0.9981 | [Valid] loss:625.3236, acc:0.0688\n",
      "[epoch] 526 / 1000 | [Train] loss:620.2050, acc:0.9981 | [Valid] loss:625.2501, acc:0.0725\n",
      "[epoch] 527 / 1000 | [Train] loss:620.2320, acc:0.9978 | [Valid] loss:625.3220, acc:0.0700\n",
      "[epoch] 528 / 1000 | [Train] loss:620.2913, acc:0.9984 | [Valid] loss:625.3074, acc:0.0688\n",
      "[epoch] 529 / 1000 | [Train] loss:620.2473, acc:0.9981 | [Valid] loss:625.2244, acc:0.0675\n",
      "[epoch] 530 / 1000 | [Train] loss:620.1098, acc:0.9978 | [Valid] loss:625.1427, acc:0.0725\n",
      "[epoch] 531 / 1000 | [Train] loss:620.1117, acc:0.9984 | [Valid] loss:625.1282, acc:0.0700\n",
      "[epoch] 532 / 1000 | [Train] loss:620.1427, acc:0.9978 | [Valid] loss:625.1760, acc:0.0688\n",
      "[epoch] 533 / 1000 | [Train] loss:620.1389, acc:0.9981 | [Valid] loss:625.1520, acc:0.0663\n",
      "[epoch] 534 / 1000 | [Train] loss:620.1219, acc:0.9984 | [Valid] loss:625.1002, acc:0.0688\n",
      "[epoch] 535 / 1000 | [Train] loss:620.1005, acc:0.9981 | [Valid] loss:625.0764, acc:0.0700\n",
      "[epoch] 536 / 1000 | [Train] loss:620.0967, acc:0.9978 | [Valid] loss:625.0906, acc:0.0663\n",
      "[epoch] 537 / 1000 | [Train] loss:620.1073, acc:0.9981 | [Valid] loss:625.0694, acc:0.0713\n",
      "[epoch] 538 / 1000 | [Train] loss:620.0787, acc:0.9987 | [Valid] loss:625.0490, acc:0.0700\n",
      "[epoch] 539 / 1000 | [Train] loss:619.8889, acc:0.9981 | [Valid] loss:624.9360, acc:0.0713\n",
      "[epoch] 540 / 1000 | [Train] loss:619.9761, acc:0.9981 | [Valid] loss:624.9374, acc:0.0712\n",
      "[epoch] 541 / 1000 | [Train] loss:619.8671, acc:0.9981 | [Valid] loss:624.9382, acc:0.0675\n",
      "[epoch] 542 / 1000 | [Train] loss:619.9077, acc:0.9984 | [Valid] loss:625.0007, acc:0.0663\n",
      "[epoch] 543 / 1000 | [Train] loss:619.9499, acc:0.9978 | [Valid] loss:625.0388, acc:0.0675\n",
      "[epoch] 544 / 1000 | [Train] loss:620.0596, acc:0.9978 | [Valid] loss:625.0987, acc:0.0688\n",
      "[epoch] 545 / 1000 | [Train] loss:619.9912, acc:0.9981 | [Valid] loss:625.0531, acc:0.0675\n",
      "[epoch] 546 / 1000 | [Train] loss:620.0353, acc:0.9978 | [Valid] loss:625.0761, acc:0.0675\n",
      "[epoch] 547 / 1000 | [Train] loss:620.0361, acc:0.9981 | [Valid] loss:625.0916, acc:0.0725\n",
      "[epoch] 548 / 1000 | [Train] loss:620.0227, acc:0.9984 | [Valid] loss:625.0499, acc:0.0688\n",
      "[epoch] 549 / 1000 | [Train] loss:620.1089, acc:0.9984 | [Valid] loss:625.1350, acc:0.0700\n",
      "[epoch] 550 / 1000 | [Train] loss:620.1532, acc:0.9981 | [Valid] loss:625.1439, acc:0.0675\n",
      "[epoch] 551 / 1000 | [Train] loss:620.1483, acc:0.9981 | [Valid] loss:625.1362, acc:0.0713\n",
      "[epoch] 552 / 1000 | [Train] loss:620.1339, acc:0.9984 | [Valid] loss:625.0973, acc:0.0700\n",
      "[epoch] 553 / 1000 | [Train] loss:620.0609, acc:0.9975 | [Valid] loss:625.0476, acc:0.0700\n",
      "[epoch] 554 / 1000 | [Train] loss:620.0601, acc:0.9984 | [Valid] loss:625.1236, acc:0.0687\n",
      "[epoch] 555 / 1000 | [Train] loss:620.0549, acc:0.9984 | [Valid] loss:625.0776, acc:0.0650\n",
      "[epoch] 556 / 1000 | [Train] loss:619.9648, acc:0.9981 | [Valid] loss:625.0397, acc:0.0688\n",
      "[epoch] 557 / 1000 | [Train] loss:619.9759, acc:0.9981 | [Valid] loss:625.0438, acc:0.0713\n",
      "[epoch] 558 / 1000 | [Train] loss:620.1070, acc:0.9978 | [Valid] loss:625.1123, acc:0.0700\n",
      "[epoch] 559 / 1000 | [Train] loss:619.9530, acc:0.9981 | [Valid] loss:624.9900, acc:0.0688\n",
      "[epoch] 560 / 1000 | [Train] loss:619.9549, acc:0.9981 | [Valid] loss:624.9761, acc:0.0663\n",
      "[epoch] 561 / 1000 | [Train] loss:619.9190, acc:0.9978 | [Valid] loss:625.0555, acc:0.0663\n",
      "[epoch] 562 / 1000 | [Train] loss:620.0482, acc:0.9984 | [Valid] loss:625.0074, acc:0.0713\n",
      "[epoch] 563 / 1000 | [Train] loss:619.9592, acc:0.9984 | [Valid] loss:624.9966, acc:0.0675\n",
      "[epoch] 564 / 1000 | [Train] loss:619.9271, acc:0.9984 | [Valid] loss:624.8970, acc:0.0700\n",
      "[epoch] 565 / 1000 | [Train] loss:619.9247, acc:0.9981 | [Valid] loss:624.8684, acc:0.0738\n",
      "[epoch] 566 / 1000 | [Train] loss:619.8263, acc:0.9981 | [Valid] loss:624.9203, acc:0.0725\n",
      "[epoch] 567 / 1000 | [Train] loss:619.8369, acc:0.9984 | [Valid] loss:624.9095, acc:0.0700\n",
      "[epoch] 568 / 1000 | [Train] loss:619.8181, acc:0.9978 | [Valid] loss:624.9238, acc:0.0675\n",
      "[epoch] 569 / 1000 | [Train] loss:619.9754, acc:0.9984 | [Valid] loss:624.9711, acc:0.0675\n",
      "[epoch] 570 / 1000 | [Train] loss:619.9171, acc:0.9981 | [Valid] loss:625.0367, acc:0.0688\n",
      "[epoch] 571 / 1000 | [Train] loss:620.0261, acc:0.9978 | [Valid] loss:624.9675, acc:0.0675\n",
      "[epoch] 572 / 1000 | [Train] loss:619.9486, acc:0.9981 | [Valid] loss:624.9354, acc:0.0675\n",
      "[epoch] 573 / 1000 | [Train] loss:619.8661, acc:0.9981 | [Valid] loss:624.9542, acc:0.0688\n",
      "[epoch] 574 / 1000 | [Train] loss:619.9289, acc:0.9984 | [Valid] loss:625.0059, acc:0.0700\n",
      "[epoch] 575 / 1000 | [Train] loss:619.9908, acc:0.9984 | [Valid] loss:624.9581, acc:0.0663\n",
      "[epoch] 576 / 1000 | [Train] loss:620.0384, acc:0.9981 | [Valid] loss:625.0289, acc:0.0700\n",
      "[epoch] 577 / 1000 | [Train] loss:619.9399, acc:0.9978 | [Valid] loss:624.9967, acc:0.0687\n",
      "[epoch] 578 / 1000 | [Train] loss:619.9466, acc:0.9984 | [Valid] loss:624.9771, acc:0.0663\n",
      "[epoch] 579 / 1000 | [Train] loss:619.9277, acc:0.9981 | [Valid] loss:625.0213, acc:0.0663\n",
      "[epoch] 580 / 1000 | [Train] loss:620.0666, acc:0.9978 | [Valid] loss:625.0574, acc:0.0675\n",
      "[epoch] 581 / 1000 | [Train] loss:619.8873, acc:0.9975 | [Valid] loss:625.0120, acc:0.0688\n",
      "[epoch] 582 / 1000 | [Train] loss:619.8652, acc:0.9975 | [Valid] loss:625.0094, acc:0.0688\n",
      "[epoch] 583 / 1000 | [Train] loss:620.0823, acc:0.9975 | [Valid] loss:624.9647, acc:0.0675\n",
      "[epoch] 584 / 1000 | [Train] loss:619.9098, acc:0.9975 | [Valid] loss:625.0131, acc:0.0675\n",
      "[epoch] 585 / 1000 | [Train] loss:619.9452, acc:0.9978 | [Valid] loss:624.9866, acc:0.0675\n",
      "[epoch] 586 / 1000 | [Train] loss:619.9544, acc:0.9978 | [Valid] loss:624.9300, acc:0.0688\n",
      "[epoch] 587 / 1000 | [Train] loss:619.9019, acc:0.9981 | [Valid] loss:624.9002, acc:0.0725\n",
      "[epoch] 588 / 1000 | [Train] loss:619.8081, acc:0.9981 | [Valid] loss:624.8337, acc:0.0713\n",
      "[epoch] 589 / 1000 | [Train] loss:619.8059, acc:0.9978 | [Valid] loss:624.9004, acc:0.0700\n",
      "[epoch] 590 / 1000 | [Train] loss:619.8564, acc:0.9975 | [Valid] loss:624.8592, acc:0.0700\n",
      "[epoch] 591 / 1000 | [Train] loss:619.8640, acc:0.9981 | [Valid] loss:624.7996, acc:0.0675\n",
      "[epoch] 592 / 1000 | [Train] loss:619.8193, acc:0.9984 | [Valid] loss:624.8529, acc:0.0700\n",
      "[epoch] 593 / 1000 | [Train] loss:619.8076, acc:0.9981 | [Valid] loss:624.8197, acc:0.0675\n",
      "[epoch] 594 / 1000 | [Train] loss:619.8854, acc:0.9978 | [Valid] loss:624.9085, acc:0.0688\n",
      "[epoch] 595 / 1000 | [Train] loss:619.9113, acc:0.9981 | [Valid] loss:624.9360, acc:0.0638\n",
      "[epoch] 596 / 1000 | [Train] loss:619.9220, acc:0.9981 | [Valid] loss:624.9105, acc:0.0675\n",
      "[epoch] 597 / 1000 | [Train] loss:619.8401, acc:0.9978 | [Valid] loss:624.8172, acc:0.0725\n",
      "[epoch] 598 / 1000 | [Train] loss:619.7738, acc:0.9984 | [Valid] loss:624.6862, acc:0.0700\n",
      "[epoch] 599 / 1000 | [Train] loss:619.6225, acc:0.9978 | [Valid] loss:624.6767, acc:0.0675\n",
      "[epoch] 600 / 1000 | [Train] loss:619.5976, acc:0.9981 | [Valid] loss:624.6252, acc:0.0737\n",
      "[epoch] 601 / 1000 | [Train] loss:619.5768, acc:0.9984 | [Valid] loss:624.6319, acc:0.0688\n",
      "[epoch] 602 / 1000 | [Train] loss:619.6224, acc:0.9984 | [Valid] loss:624.5982, acc:0.0663\n",
      "[epoch] 603 / 1000 | [Train] loss:619.4927, acc:0.9978 | [Valid] loss:624.5100, acc:0.0725\n",
      "[epoch] 604 / 1000 | [Train] loss:619.5882, acc:0.9981 | [Valid] loss:624.6186, acc:0.0700\n",
      "[epoch] 605 / 1000 | [Train] loss:619.5714, acc:0.9978 | [Valid] loss:624.6746, acc:0.0688\n",
      "[epoch] 606 / 1000 | [Train] loss:619.6939, acc:0.9984 | [Valid] loss:624.8130, acc:0.0700\n",
      "[epoch] 607 / 1000 | [Train] loss:619.8261, acc:0.9981 | [Valid] loss:624.7928, acc:0.0650\n",
      "[epoch] 608 / 1000 | [Train] loss:619.7991, acc:0.9978 | [Valid] loss:624.8080, acc:0.0625\n",
      "[epoch] 609 / 1000 | [Train] loss:619.6787, acc:0.9984 | [Valid] loss:624.7624, acc:0.0675\n",
      "[epoch] 610 / 1000 | [Train] loss:619.7991, acc:0.9984 | [Valid] loss:624.7441, acc:0.0675\n",
      "[epoch] 611 / 1000 | [Train] loss:619.6485, acc:0.9981 | [Valid] loss:624.7668, acc:0.0688\n",
      "[epoch] 612 / 1000 | [Train] loss:619.7300, acc:0.9978 | [Valid] loss:624.7692, acc:0.0700\n",
      "[epoch] 613 / 1000 | [Train] loss:619.6515, acc:0.9978 | [Valid] loss:624.6931, acc:0.0725\n",
      "[epoch] 614 / 1000 | [Train] loss:619.6715, acc:0.9981 | [Valid] loss:624.5896, acc:0.0713\n",
      "[epoch] 615 / 1000 | [Train] loss:619.5532, acc:0.9984 | [Valid] loss:624.6174, acc:0.0663\n",
      "[epoch] 616 / 1000 | [Train] loss:619.6245, acc:0.9984 | [Valid] loss:624.6178, acc:0.0688\n",
      "[epoch] 617 / 1000 | [Train] loss:619.5721, acc:0.9984 | [Valid] loss:624.5273, acc:0.0700\n",
      "[epoch] 618 / 1000 | [Train] loss:619.4626, acc:0.9981 | [Valid] loss:624.5080, acc:0.0650\n",
      "[epoch] 619 / 1000 | [Train] loss:619.5343, acc:0.9981 | [Valid] loss:624.5647, acc:0.0712\n",
      "[epoch] 620 / 1000 | [Train] loss:619.4536, acc:0.9978 | [Valid] loss:624.4673, acc:0.0700\n",
      "[epoch] 621 / 1000 | [Train] loss:619.3948, acc:0.9978 | [Valid] loss:624.5512, acc:0.0700\n",
      "[epoch] 622 / 1000 | [Train] loss:619.5351, acc:0.9984 | [Valid] loss:624.6530, acc:0.0688\n",
      "[epoch] 623 / 1000 | [Train] loss:619.5513, acc:0.9978 | [Valid] loss:624.6407, acc:0.0700\n",
      "[epoch] 624 / 1000 | [Train] loss:619.6266, acc:0.9981 | [Valid] loss:624.6793, acc:0.0713\n",
      "[epoch] 625 / 1000 | [Train] loss:619.7215, acc:0.9981 | [Valid] loss:624.7691, acc:0.0688\n",
      "[epoch] 626 / 1000 | [Train] loss:619.6915, acc:0.9981 | [Valid] loss:624.7733, acc:0.0663\n",
      "[epoch] 627 / 1000 | [Train] loss:619.6517, acc:0.9981 | [Valid] loss:624.7158, acc:0.0700\n",
      "[epoch] 628 / 1000 | [Train] loss:619.7174, acc:0.9981 | [Valid] loss:624.7629, acc:0.0650\n",
      "[epoch] 629 / 1000 | [Train] loss:619.6880, acc:0.9981 | [Valid] loss:624.8002, acc:0.0688\n",
      "[epoch] 630 / 1000 | [Train] loss:619.7612, acc:0.9984 | [Valid] loss:624.7288, acc:0.0700\n",
      "[epoch] 631 / 1000 | [Train] loss:619.6697, acc:0.9984 | [Valid] loss:624.6624, acc:0.0700\n",
      "[epoch] 632 / 1000 | [Train] loss:619.5783, acc:0.9981 | [Valid] loss:624.7313, acc:0.0700\n",
      "[epoch] 633 / 1000 | [Train] loss:619.7144, acc:0.9981 | [Valid] loss:624.7300, acc:0.0687\n",
      "[epoch] 634 / 1000 | [Train] loss:619.6268, acc:0.9978 | [Valid] loss:624.7313, acc:0.0712\n",
      "[epoch] 635 / 1000 | [Train] loss:619.7789, acc:0.9978 | [Valid] loss:624.7930, acc:0.0663\n",
      "[epoch] 636 / 1000 | [Train] loss:619.7405, acc:0.9978 | [Valid] loss:624.7862, acc:0.0688\n",
      "[epoch] 637 / 1000 | [Train] loss:619.7515, acc:0.9981 | [Valid] loss:624.7913, acc:0.0688\n",
      "[epoch] 638 / 1000 | [Train] loss:619.7558, acc:0.9984 | [Valid] loss:624.7916, acc:0.0688\n",
      "[epoch] 639 / 1000 | [Train] loss:619.8100, acc:0.9984 | [Valid] loss:624.8075, acc:0.0688\n",
      "[epoch] 640 / 1000 | [Train] loss:619.7211, acc:0.9981 | [Valid] loss:624.6943, acc:0.0700\n",
      "[epoch] 641 / 1000 | [Train] loss:619.6551, acc:0.9981 | [Valid] loss:624.7683, acc:0.0675\n",
      "[epoch] 642 / 1000 | [Train] loss:619.7984, acc:0.9981 | [Valid] loss:624.9436, acc:0.0725\n",
      "[epoch] 643 / 1000 | [Train] loss:620.0035, acc:0.9981 | [Valid] loss:624.9720, acc:0.0675\n",
      "[epoch] 644 / 1000 | [Train] loss:619.9277, acc:0.9978 | [Valid] loss:625.0604, acc:0.0675\n",
      "[epoch] 645 / 1000 | [Train] loss:620.0414, acc:0.9981 | [Valid] loss:625.0396, acc:0.0663\n",
      "[epoch] 646 / 1000 | [Train] loss:620.0509, acc:0.9981 | [Valid] loss:624.9803, acc:0.0675\n",
      "[epoch] 647 / 1000 | [Train] loss:619.9316, acc:0.9981 | [Valid] loss:624.9916, acc:0.0675\n",
      "[epoch] 648 / 1000 | [Train] loss:620.0500, acc:0.9981 | [Valid] loss:625.0428, acc:0.0700\n",
      "[epoch] 649 / 1000 | [Train] loss:620.0727, acc:0.9984 | [Valid] loss:625.0449, acc:0.0650\n",
      "[epoch] 650 / 1000 | [Train] loss:620.0486, acc:0.9978 | [Valid] loss:625.0420, acc:0.0700\n",
      "[epoch] 651 / 1000 | [Train] loss:619.9042, acc:0.9984 | [Valid] loss:624.9717, acc:0.0725\n",
      "[epoch] 652 / 1000 | [Train] loss:619.9189, acc:0.9981 | [Valid] loss:625.0112, acc:0.0688\n",
      "[epoch] 653 / 1000 | [Train] loss:620.0090, acc:0.9984 | [Valid] loss:625.0248, acc:0.0675\n",
      "[epoch] 654 / 1000 | [Train] loss:619.9666, acc:0.9981 | [Valid] loss:625.0688, acc:0.0687\n",
      "[epoch] 655 / 1000 | [Train] loss:620.0329, acc:0.9975 | [Valid] loss:624.9880, acc:0.0675\n",
      "[epoch] 656 / 1000 | [Train] loss:619.9752, acc:0.9984 | [Valid] loss:624.9058, acc:0.0663\n",
      "[epoch] 657 / 1000 | [Train] loss:619.8387, acc:0.9984 | [Valid] loss:624.8316, acc:0.0675\n",
      "[epoch] 658 / 1000 | [Train] loss:619.8233, acc:0.9981 | [Valid] loss:624.8304, acc:0.0663\n",
      "[epoch] 659 / 1000 | [Train] loss:619.9308, acc:0.9978 | [Valid] loss:624.8936, acc:0.0675\n",
      "[epoch] 660 / 1000 | [Train] loss:619.9122, acc:0.9975 | [Valid] loss:624.9071, acc:0.0725\n",
      "[epoch] 661 / 1000 | [Train] loss:619.9429, acc:0.9981 | [Valid] loss:624.9234, acc:0.0713\n",
      "[epoch] 662 / 1000 | [Train] loss:619.8684, acc:0.9981 | [Valid] loss:624.9572, acc:0.0713\n",
      "[epoch] 663 / 1000 | [Train] loss:619.9890, acc:0.9981 | [Valid] loss:624.9122, acc:0.0700\n",
      "[epoch] 664 / 1000 | [Train] loss:619.8414, acc:0.9978 | [Valid] loss:624.8737, acc:0.0700\n",
      "[epoch] 665 / 1000 | [Train] loss:619.7350, acc:0.9984 | [Valid] loss:624.8714, acc:0.0700\n",
      "[epoch] 666 / 1000 | [Train] loss:619.7646, acc:0.9984 | [Valid] loss:624.8580, acc:0.0700\n",
      "[epoch] 667 / 1000 | [Train] loss:619.8042, acc:0.9984 | [Valid] loss:624.8828, acc:0.0700\n",
      "[epoch] 668 / 1000 | [Train] loss:619.8718, acc:0.9978 | [Valid] loss:624.7779, acc:0.0700\n",
      "[epoch] 669 / 1000 | [Train] loss:619.8077, acc:0.9981 | [Valid] loss:624.7358, acc:0.0650\n",
      "[epoch] 670 / 1000 | [Train] loss:619.7537, acc:0.9984 | [Valid] loss:624.6902, acc:0.0675\n",
      "[epoch] 671 / 1000 | [Train] loss:619.5737, acc:0.9981 | [Valid] loss:624.6755, acc:0.0725\n",
      "[epoch] 672 / 1000 | [Train] loss:619.6473, acc:0.9984 | [Valid] loss:624.5365, acc:0.0700\n",
      "[epoch] 673 / 1000 | [Train] loss:619.4477, acc:0.9981 | [Valid] loss:624.5224, acc:0.0737\n",
      "[epoch] 674 / 1000 | [Train] loss:619.4034, acc:0.9984 | [Valid] loss:624.4587, acc:0.0688\n",
      "[epoch] 675 / 1000 | [Train] loss:619.4656, acc:0.9984 | [Valid] loss:624.4576, acc:0.0713\n",
      "[epoch] 676 / 1000 | [Train] loss:619.4374, acc:0.9981 | [Valid] loss:624.3999, acc:0.0663\n",
      "[epoch] 677 / 1000 | [Train] loss:619.3402, acc:0.9981 | [Valid] loss:624.3505, acc:0.0663\n",
      "[epoch] 678 / 1000 | [Train] loss:619.3072, acc:0.9981 | [Valid] loss:624.3626, acc:0.0663\n",
      "[epoch] 679 / 1000 | [Train] loss:619.2929, acc:0.9981 | [Valid] loss:624.2870, acc:0.0700\n",
      "[epoch] 680 / 1000 | [Train] loss:619.2523, acc:0.9981 | [Valid] loss:624.3451, acc:0.0688\n",
      "[epoch] 681 / 1000 | [Train] loss:619.2807, acc:0.9981 | [Valid] loss:624.3490, acc:0.0688\n",
      "[epoch] 682 / 1000 | [Train] loss:619.2845, acc:0.9981 | [Valid] loss:624.2838, acc:0.0675\n",
      "[epoch] 683 / 1000 | [Train] loss:619.3458, acc:0.9981 | [Valid] loss:624.2835, acc:0.0675\n",
      "[epoch] 684 / 1000 | [Train] loss:619.2273, acc:0.9981 | [Valid] loss:624.2912, acc:0.0713\n",
      "[epoch] 685 / 1000 | [Train] loss:619.2390, acc:0.9978 | [Valid] loss:624.2717, acc:0.0713\n",
      "[epoch] 686 / 1000 | [Train] loss:619.2873, acc:0.9981 | [Valid] loss:624.3510, acc:0.0688\n",
      "[epoch] 687 / 1000 | [Train] loss:619.2880, acc:0.9981 | [Valid] loss:624.2755, acc:0.0700\n",
      "[epoch] 688 / 1000 | [Train] loss:619.2782, acc:0.9981 | [Valid] loss:624.2577, acc:0.0675\n",
      "[epoch] 689 / 1000 | [Train] loss:619.3431, acc:0.9984 | [Valid] loss:624.2657, acc:0.0688\n",
      "[epoch] 690 / 1000 | [Train] loss:619.2300, acc:0.9984 | [Valid] loss:624.2548, acc:0.0650\n",
      "[epoch] 691 / 1000 | [Train] loss:619.1898, acc:0.9981 | [Valid] loss:624.3150, acc:0.0688\n",
      "[epoch] 692 / 1000 | [Train] loss:619.3931, acc:0.9975 | [Valid] loss:624.4094, acc:0.0663\n",
      "[epoch] 693 / 1000 | [Train] loss:619.3495, acc:0.9981 | [Valid] loss:624.4091, acc:0.0638\n",
      "[epoch] 694 / 1000 | [Train] loss:619.3296, acc:0.9978 | [Valid] loss:624.4689, acc:0.0675\n",
      "[epoch] 695 / 1000 | [Train] loss:619.4029, acc:0.9981 | [Valid] loss:624.4681, acc:0.0675\n",
      "[epoch] 696 / 1000 | [Train] loss:619.3673, acc:0.9984 | [Valid] loss:624.3762, acc:0.0675\n",
      "[epoch] 697 / 1000 | [Train] loss:619.3552, acc:0.9981 | [Valid] loss:624.4143, acc:0.0663\n",
      "[epoch] 698 / 1000 | [Train] loss:619.4291, acc:0.9981 | [Valid] loss:624.4194, acc:0.0662\n",
      "[epoch] 699 / 1000 | [Train] loss:619.4121, acc:0.9981 | [Valid] loss:624.4510, acc:0.0713\n",
      "[epoch] 700 / 1000 | [Train] loss:619.5084, acc:0.9984 | [Valid] loss:624.4691, acc:0.0713\n",
      "[epoch] 701 / 1000 | [Train] loss:619.4896, acc:0.9981 | [Valid] loss:624.5407, acc:0.0700\n",
      "[epoch] 702 / 1000 | [Train] loss:619.5629, acc:0.9981 | [Valid] loss:624.5682, acc:0.0738\n",
      "[epoch] 703 / 1000 | [Train] loss:619.5491, acc:0.9978 | [Valid] loss:624.5763, acc:0.0675\n",
      "[epoch] 704 / 1000 | [Train] loss:619.5757, acc:0.9981 | [Valid] loss:624.5385, acc:0.0675\n",
      "[epoch] 705 / 1000 | [Train] loss:619.5864, acc:0.9984 | [Valid] loss:624.5370, acc:0.0688\n",
      "[epoch] 706 / 1000 | [Train] loss:619.5328, acc:0.9978 | [Valid] loss:624.5367, acc:0.0700\n",
      "[epoch] 707 / 1000 | [Train] loss:619.4621, acc:0.9984 | [Valid] loss:624.4874, acc:0.0700\n",
      "[epoch] 708 / 1000 | [Train] loss:619.4306, acc:0.9984 | [Valid] loss:624.3922, acc:0.0675\n",
      "[epoch] 709 / 1000 | [Train] loss:619.3903, acc:0.9978 | [Valid] loss:624.4345, acc:0.0713\n",
      "[epoch] 710 / 1000 | [Train] loss:619.3843, acc:0.9984 | [Valid] loss:624.4368, acc:0.0687\n",
      "[epoch] 711 / 1000 | [Train] loss:619.3869, acc:0.9984 | [Valid] loss:624.3802, acc:0.0662\n",
      "[epoch] 712 / 1000 | [Train] loss:619.4582, acc:0.9984 | [Valid] loss:624.4533, acc:0.0663\n",
      "[epoch] 713 / 1000 | [Train] loss:619.4434, acc:0.9984 | [Valid] loss:624.4423, acc:0.0675\n",
      "[epoch] 714 / 1000 | [Train] loss:619.4622, acc:0.9984 | [Valid] loss:624.3838, acc:0.0688\n",
      "[epoch] 715 / 1000 | [Train] loss:619.3644, acc:0.9981 | [Valid] loss:624.3388, acc:0.0688\n",
      "[epoch] 716 / 1000 | [Train] loss:619.4053, acc:0.9981 | [Valid] loss:624.4480, acc:0.0650\n",
      "[epoch] 717 / 1000 | [Train] loss:619.4630, acc:0.9978 | [Valid] loss:624.4571, acc:0.0650\n",
      "[epoch] 718 / 1000 | [Train] loss:619.3838, acc:0.9981 | [Valid] loss:624.4167, acc:0.0700\n",
      "[epoch] 719 / 1000 | [Train] loss:619.3666, acc:0.9981 | [Valid] loss:624.3483, acc:0.0700\n",
      "[epoch] 720 / 1000 | [Train] loss:619.4222, acc:0.9984 | [Valid] loss:624.3251, acc:0.0675\n",
      "[epoch] 721 / 1000 | [Train] loss:619.3180, acc:0.9981 | [Valid] loss:624.3421, acc:0.0688\n",
      "[epoch] 722 / 1000 | [Train] loss:619.3646, acc:0.9978 | [Valid] loss:624.3267, acc:0.0712\n",
      "[epoch] 723 / 1000 | [Train] loss:619.3820, acc:0.9984 | [Valid] loss:624.3094, acc:0.0688\n",
      "[epoch] 724 / 1000 | [Train] loss:619.2361, acc:0.9981 | [Valid] loss:624.3477, acc:0.0688\n",
      "[epoch] 725 / 1000 | [Train] loss:619.2336, acc:0.9984 | [Valid] loss:624.3328, acc:0.0713\n",
      "[epoch] 726 / 1000 | [Train] loss:619.2805, acc:0.9981 | [Valid] loss:624.3116, acc:0.0713\n",
      "[epoch] 727 / 1000 | [Train] loss:619.1797, acc:0.9978 | [Valid] loss:624.2434, acc:0.0700\n",
      "[epoch] 728 / 1000 | [Train] loss:619.1091, acc:0.9981 | [Valid] loss:624.2318, acc:0.0700\n",
      "[epoch] 729 / 1000 | [Train] loss:619.1675, acc:0.9981 | [Valid] loss:624.2771, acc:0.0663\n",
      "[epoch] 730 / 1000 | [Train] loss:619.2972, acc:0.9981 | [Valid] loss:624.3434, acc:0.0713\n",
      "[epoch] 731 / 1000 | [Train] loss:619.3147, acc:0.9981 | [Valid] loss:624.3107, acc:0.0638\n",
      "[epoch] 732 / 1000 | [Train] loss:619.3613, acc:0.9984 | [Valid] loss:624.3627, acc:0.0663\n",
      "[epoch] 733 / 1000 | [Train] loss:619.4228, acc:0.9981 | [Valid] loss:624.3440, acc:0.0675\n",
      "[epoch] 734 / 1000 | [Train] loss:619.3704, acc:0.9984 | [Valid] loss:624.4361, acc:0.0688\n",
      "[epoch] 735 / 1000 | [Train] loss:619.4039, acc:0.9981 | [Valid] loss:624.4246, acc:0.0675\n",
      "[epoch] 736 / 1000 | [Train] loss:619.4063, acc:0.9981 | [Valid] loss:624.5060, acc:0.0650\n",
      "[epoch] 737 / 1000 | [Train] loss:619.4658, acc:0.9978 | [Valid] loss:624.5043, acc:0.0713\n",
      "[epoch] 738 / 1000 | [Train] loss:619.4558, acc:0.9984 | [Valid] loss:624.5460, acc:0.0700\n",
      "[epoch] 739 / 1000 | [Train] loss:619.4668, acc:0.9981 | [Valid] loss:624.4959, acc:0.0688\n",
      "[epoch] 740 / 1000 | [Train] loss:619.4865, acc:0.9981 | [Valid] loss:624.5103, acc:0.0700\n",
      "[epoch] 741 / 1000 | [Train] loss:619.4082, acc:0.9978 | [Valid] loss:624.4895, acc:0.0688\n",
      "[epoch] 742 / 1000 | [Train] loss:619.4026, acc:0.9981 | [Valid] loss:624.4495, acc:0.0675\n",
      "[epoch] 743 / 1000 | [Train] loss:619.3858, acc:0.9978 | [Valid] loss:624.4137, acc:0.0675\n",
      "[epoch] 744 / 1000 | [Train] loss:619.3827, acc:0.9975 | [Valid] loss:624.3821, acc:0.0687\n",
      "[epoch] 745 / 1000 | [Train] loss:619.4185, acc:0.9984 | [Valid] loss:624.4172, acc:0.0663\n",
      "[epoch] 746 / 1000 | [Train] loss:619.3435, acc:0.9981 | [Valid] loss:624.3592, acc:0.0687\n",
      "[epoch] 747 / 1000 | [Train] loss:619.3665, acc:0.9984 | [Valid] loss:624.4293, acc:0.0713\n",
      "[epoch] 748 / 1000 | [Train] loss:619.4488, acc:0.9984 | [Valid] loss:624.4671, acc:0.0663\n",
      "[epoch] 749 / 1000 | [Train] loss:619.3960, acc:0.9978 | [Valid] loss:624.3646, acc:0.0663\n",
      "[epoch] 750 / 1000 | [Train] loss:619.3380, acc:0.9978 | [Valid] loss:624.3422, acc:0.0700\n",
      "[epoch] 751 / 1000 | [Train] loss:619.2900, acc:0.9981 | [Valid] loss:624.3680, acc:0.0700\n",
      "[epoch] 752 / 1000 | [Train] loss:619.4183, acc:0.9981 | [Valid] loss:624.4381, acc:0.0713\n",
      "[epoch] 753 / 1000 | [Train] loss:619.4271, acc:0.9981 | [Valid] loss:624.4374, acc:0.0700\n",
      "[epoch] 754 / 1000 | [Train] loss:619.3909, acc:0.9981 | [Valid] loss:624.4510, acc:0.0688\n",
      "[epoch] 755 / 1000 | [Train] loss:619.4756, acc:0.9981 | [Valid] loss:624.5079, acc:0.0688\n",
      "[epoch] 756 / 1000 | [Train] loss:619.5682, acc:0.9984 | [Valid] loss:624.5754, acc:0.0687\n",
      "[epoch] 757 / 1000 | [Train] loss:619.6189, acc:0.9981 | [Valid] loss:624.6303, acc:0.0675\n",
      "[epoch] 758 / 1000 | [Train] loss:619.5781, acc:0.9984 | [Valid] loss:624.6789, acc:0.0700\n",
      "[epoch] 759 / 1000 | [Train] loss:619.7245, acc:0.9981 | [Valid] loss:624.7585, acc:0.0650\n",
      "[epoch] 760 / 1000 | [Train] loss:619.7232, acc:0.9981 | [Valid] loss:624.7221, acc:0.0687\n",
      "[epoch] 761 / 1000 | [Train] loss:619.7822, acc:0.9981 | [Valid] loss:624.8926, acc:0.0675\n",
      "[epoch] 762 / 1000 | [Train] loss:619.8378, acc:0.9981 | [Valid] loss:624.8455, acc:0.0675\n",
      "[epoch] 763 / 1000 | [Train] loss:619.9276, acc:0.9975 | [Valid] loss:624.8604, acc:0.0688\n",
      "[epoch] 764 / 1000 | [Train] loss:619.8184, acc:0.9981 | [Valid] loss:624.8778, acc:0.0662\n",
      "[epoch] 765 / 1000 | [Train] loss:619.8054, acc:0.9981 | [Valid] loss:624.8449, acc:0.0663\n",
      "[epoch] 766 / 1000 | [Train] loss:619.8911, acc:0.9978 | [Valid] loss:624.8732, acc:0.0663\n",
      "[epoch] 767 / 1000 | [Train] loss:619.9137, acc:0.9981 | [Valid] loss:624.8519, acc:0.0700\n",
      "[epoch] 768 / 1000 | [Train] loss:619.8759, acc:0.9981 | [Valid] loss:624.9369, acc:0.0663\n",
      "[epoch] 769 / 1000 | [Train] loss:619.9216, acc:0.9978 | [Valid] loss:624.9159, acc:0.0650\n",
      "[epoch] 770 / 1000 | [Train] loss:619.9108, acc:0.9975 | [Valid] loss:624.9258, acc:0.0675\n",
      "[epoch] 771 / 1000 | [Train] loss:619.8853, acc:0.9978 | [Valid] loss:624.9332, acc:0.0650\n",
      "[epoch] 772 / 1000 | [Train] loss:619.9102, acc:0.9981 | [Valid] loss:624.8849, acc:0.0687\n",
      "[epoch] 773 / 1000 | [Train] loss:619.8314, acc:0.9981 | [Valid] loss:624.8111, acc:0.0700\n",
      "[epoch] 774 / 1000 | [Train] loss:619.7515, acc:0.9981 | [Valid] loss:624.8494, acc:0.0713\n",
      "[epoch] 775 / 1000 | [Train] loss:619.8996, acc:0.9984 | [Valid] loss:624.8862, acc:0.0650\n",
      "[epoch] 776 / 1000 | [Train] loss:619.8454, acc:0.9978 | [Valid] loss:624.8283, acc:0.0700\n",
      "[epoch] 777 / 1000 | [Train] loss:619.7638, acc:0.9984 | [Valid] loss:624.7550, acc:0.0675\n",
      "[epoch] 778 / 1000 | [Train] loss:619.7434, acc:0.9984 | [Valid] loss:624.8291, acc:0.0675\n",
      "[epoch] 779 / 1000 | [Train] loss:619.8080, acc:0.9981 | [Valid] loss:624.7655, acc:0.0688\n",
      "[epoch] 780 / 1000 | [Train] loss:619.7159, acc:0.9981 | [Valid] loss:624.7822, acc:0.0688\n",
      "[epoch] 781 / 1000 | [Train] loss:619.7339, acc:0.9978 | [Valid] loss:624.7848, acc:0.0650\n",
      "[epoch] 782 / 1000 | [Train] loss:619.8183, acc:0.9978 | [Valid] loss:624.8052, acc:0.0687\n",
      "[epoch] 783 / 1000 | [Train] loss:619.7845, acc:0.9978 | [Valid] loss:624.7951, acc:0.0675\n",
      "[epoch] 784 / 1000 | [Train] loss:619.8171, acc:0.9984 | [Valid] loss:624.8164, acc:0.0700\n",
      "[epoch] 785 / 1000 | [Train] loss:619.7991, acc:0.9981 | [Valid] loss:624.8291, acc:0.0713\n",
      "[epoch] 786 / 1000 | [Train] loss:619.7520, acc:0.9981 | [Valid] loss:624.7420, acc:0.0737\n",
      "[epoch] 787 / 1000 | [Train] loss:619.7028, acc:0.9978 | [Valid] loss:624.8107, acc:0.0700\n",
      "[epoch] 788 / 1000 | [Train] loss:619.7180, acc:0.9978 | [Valid] loss:624.8487, acc:0.0688\n",
      "[epoch] 789 / 1000 | [Train] loss:619.8208, acc:0.9981 | [Valid] loss:624.8639, acc:0.0700\n",
      "[epoch] 790 / 1000 | [Train] loss:619.8610, acc:0.9981 | [Valid] loss:624.8990, acc:0.0675\n",
      "[epoch] 791 / 1000 | [Train] loss:619.8731, acc:0.9981 | [Valid] loss:624.9124, acc:0.0688\n",
      "[epoch] 792 / 1000 | [Train] loss:619.8555, acc:0.9978 | [Valid] loss:624.8910, acc:0.0713\n",
      "[epoch] 793 / 1000 | [Train] loss:619.8001, acc:0.9978 | [Valid] loss:624.8612, acc:0.0638\n",
      "[epoch] 794 / 1000 | [Train] loss:619.7745, acc:0.9981 | [Valid] loss:624.8227, acc:0.0675\n",
      "[epoch] 795 / 1000 | [Train] loss:619.7400, acc:0.9984 | [Valid] loss:624.8253, acc:0.0688\n",
      "[epoch] 796 / 1000 | [Train] loss:619.8342, acc:0.9981 | [Valid] loss:624.8081, acc:0.0688\n",
      "[epoch] 797 / 1000 | [Train] loss:619.7203, acc:0.9984 | [Valid] loss:624.8345, acc:0.0700\n",
      "[epoch] 798 / 1000 | [Train] loss:619.8196, acc:0.9978 | [Valid] loss:624.7162, acc:0.0725\n",
      "[epoch] 799 / 1000 | [Train] loss:619.6891, acc:0.9984 | [Valid] loss:624.6892, acc:0.0663\n",
      "[epoch] 800 / 1000 | [Train] loss:619.7228, acc:0.9984 | [Valid] loss:624.7464, acc:0.0663\n",
      "[epoch] 801 / 1000 | [Train] loss:619.7275, acc:0.9981 | [Valid] loss:624.8020, acc:0.0688\n",
      "[epoch] 802 / 1000 | [Train] loss:619.7643, acc:0.9984 | [Valid] loss:624.8320, acc:0.0725\n",
      "[epoch] 803 / 1000 | [Train] loss:619.8203, acc:0.9984 | [Valid] loss:624.7839, acc:0.0688\n",
      "[epoch] 804 / 1000 | [Train] loss:619.6612, acc:0.9981 | [Valid] loss:624.7432, acc:0.0700\n",
      "[epoch] 805 / 1000 | [Train] loss:619.6529, acc:0.9981 | [Valid] loss:624.6936, acc:0.0713\n",
      "[epoch] 806 / 1000 | [Train] loss:619.7077, acc:0.9984 | [Valid] loss:624.6818, acc:0.0675\n",
      "[epoch] 807 / 1000 | [Train] loss:619.7471, acc:0.9984 | [Valid] loss:624.7582, acc:0.0675\n",
      "[epoch] 808 / 1000 | [Train] loss:619.7211, acc:0.9978 | [Valid] loss:624.7655, acc:0.0688\n",
      "[epoch] 809 / 1000 | [Train] loss:619.7175, acc:0.9978 | [Valid] loss:624.8112, acc:0.0700\n",
      "[epoch] 810 / 1000 | [Train] loss:619.7253, acc:0.9981 | [Valid] loss:624.6607, acc:0.0700\n",
      "[epoch] 811 / 1000 | [Train] loss:619.6575, acc:0.9981 | [Valid] loss:624.5995, acc:0.0700\n",
      "[epoch] 812 / 1000 | [Train] loss:619.6863, acc:0.9984 | [Valid] loss:624.6005, acc:0.0712\n",
      "[epoch] 813 / 1000 | [Train] loss:619.4783, acc:0.9981 | [Valid] loss:624.5360, acc:0.0700\n",
      "[epoch] 814 / 1000 | [Train] loss:619.5117, acc:0.9981 | [Valid] loss:624.6225, acc:0.0675\n",
      "[epoch] 815 / 1000 | [Train] loss:619.5952, acc:0.9978 | [Valid] loss:624.6409, acc:0.0700\n",
      "[epoch] 816 / 1000 | [Train] loss:619.4900, acc:0.9984 | [Valid] loss:624.6129, acc:0.0713\n",
      "[epoch] 817 / 1000 | [Train] loss:619.6963, acc:0.9984 | [Valid] loss:624.6739, acc:0.0713\n",
      "[epoch] 818 / 1000 | [Train] loss:619.6333, acc:0.9984 | [Valid] loss:624.6371, acc:0.0700\n",
      "[epoch] 819 / 1000 | [Train] loss:619.7040, acc:0.9984 | [Valid] loss:624.7429, acc:0.0700\n",
      "[epoch] 820 / 1000 | [Train] loss:619.6605, acc:0.9975 | [Valid] loss:624.6874, acc:0.0700\n",
      "[epoch] 821 / 1000 | [Train] loss:619.6249, acc:0.9984 | [Valid] loss:624.6795, acc:0.0675\n",
      "[epoch] 822 / 1000 | [Train] loss:619.7077, acc:0.9984 | [Valid] loss:624.6855, acc:0.0675\n",
      "[epoch] 823 / 1000 | [Train] loss:619.5562, acc:0.9978 | [Valid] loss:624.5436, acc:0.0688\n",
      "[epoch] 824 / 1000 | [Train] loss:619.5086, acc:0.9981 | [Valid] loss:624.4768, acc:0.0712\n",
      "[epoch] 825 / 1000 | [Train] loss:619.4218, acc:0.9984 | [Valid] loss:624.5320, acc:0.0663\n",
      "[epoch] 826 / 1000 | [Train] loss:619.5749, acc:0.9975 | [Valid] loss:624.5043, acc:0.0700\n",
      "[epoch] 827 / 1000 | [Train] loss:619.4458, acc:0.9981 | [Valid] loss:624.4569, acc:0.0712\n",
      "[epoch] 828 / 1000 | [Train] loss:619.4917, acc:0.9978 | [Valid] loss:624.5260, acc:0.0688\n",
      "[epoch] 829 / 1000 | [Train] loss:619.4706, acc:0.9981 | [Valid] loss:624.6435, acc:0.0675\n",
      "[epoch] 830 / 1000 | [Train] loss:619.6338, acc:0.9981 | [Valid] loss:624.6468, acc:0.0700\n",
      "[epoch] 831 / 1000 | [Train] loss:619.6442, acc:0.9984 | [Valid] loss:624.7145, acc:0.0700\n",
      "[epoch] 832 / 1000 | [Train] loss:619.7837, acc:0.9981 | [Valid] loss:624.8022, acc:0.0688\n",
      "[epoch] 833 / 1000 | [Train] loss:619.7274, acc:0.9978 | [Valid] loss:624.7122, acc:0.0675\n",
      "[epoch] 834 / 1000 | [Train] loss:619.6751, acc:0.9984 | [Valid] loss:624.5913, acc:0.0687\n",
      "[epoch] 835 / 1000 | [Train] loss:619.6246, acc:0.9975 | [Valid] loss:624.6749, acc:0.0725\n",
      "[epoch] 836 / 1000 | [Train] loss:619.6448, acc:0.9978 | [Valid] loss:624.6738, acc:0.0663\n",
      "[epoch] 837 / 1000 | [Train] loss:619.5714, acc:0.9978 | [Valid] loss:624.6067, acc:0.0663\n",
      "[epoch] 838 / 1000 | [Train] loss:619.4643, acc:0.9981 | [Valid] loss:624.5589, acc:0.0663\n",
      "[epoch] 839 / 1000 | [Train] loss:619.4196, acc:0.9984 | [Valid] loss:624.4774, acc:0.0713\n",
      "[epoch] 840 / 1000 | [Train] loss:619.4442, acc:0.9984 | [Valid] loss:624.4875, acc:0.0725\n",
      "[epoch] 841 / 1000 | [Train] loss:619.4339, acc:0.9978 | [Valid] loss:624.5454, acc:0.0688\n",
      "[epoch] 842 / 1000 | [Train] loss:619.5732, acc:0.9981 | [Valid] loss:624.5842, acc:0.0650\n",
      "[epoch] 843 / 1000 | [Train] loss:619.7153, acc:0.9984 | [Valid] loss:624.6955, acc:0.0700\n",
      "[epoch] 844 / 1000 | [Train] loss:619.5800, acc:0.9978 | [Valid] loss:624.6224, acc:0.0663\n",
      "[epoch] 845 / 1000 | [Train] loss:619.4887, acc:0.9981 | [Valid] loss:624.5098, acc:0.0650\n",
      "[epoch] 846 / 1000 | [Train] loss:619.4944, acc:0.9975 | [Valid] loss:624.5349, acc:0.0688\n",
      "[epoch] 847 / 1000 | [Train] loss:619.5222, acc:0.9981 | [Valid] loss:624.5331, acc:0.0687\n",
      "[epoch] 848 / 1000 | [Train] loss:619.5234, acc:0.9981 | [Valid] loss:624.5015, acc:0.0663\n",
      "[epoch] 849 / 1000 | [Train] loss:619.4850, acc:0.9978 | [Valid] loss:624.4687, acc:0.0700\n",
      "[epoch] 850 / 1000 | [Train] loss:619.5530, acc:0.9981 | [Valid] loss:624.4647, acc:0.0700\n",
      "[epoch] 851 / 1000 | [Train] loss:619.5152, acc:0.9984 | [Valid] loss:624.4935, acc:0.0663\n",
      "[epoch] 852 / 1000 | [Train] loss:619.4907, acc:0.9984 | [Valid] loss:624.5390, acc:0.0675\n",
      "[epoch] 853 / 1000 | [Train] loss:619.4690, acc:0.9978 | [Valid] loss:624.5045, acc:0.0675\n",
      "[epoch] 854 / 1000 | [Train] loss:619.5251, acc:0.9981 | [Valid] loss:624.6092, acc:0.0700\n",
      "[epoch] 855 / 1000 | [Train] loss:619.5860, acc:0.9978 | [Valid] loss:624.6563, acc:0.0688\n",
      "[epoch] 856 / 1000 | [Train] loss:619.6901, acc:0.9981 | [Valid] loss:624.6373, acc:0.0625\n",
      "[epoch] 857 / 1000 | [Train] loss:619.6433, acc:0.9981 | [Valid] loss:624.6108, acc:0.0737\n",
      "[epoch] 858 / 1000 | [Train] loss:619.6249, acc:0.9981 | [Valid] loss:624.6584, acc:0.0700\n",
      "[epoch] 859 / 1000 | [Train] loss:619.6522, acc:0.9981 | [Valid] loss:624.5896, acc:0.0663\n",
      "[epoch] 860 / 1000 | [Train] loss:619.5042, acc:0.9984 | [Valid] loss:624.6340, acc:0.0688\n",
      "[epoch] 861 / 1000 | [Train] loss:619.5164, acc:0.9978 | [Valid] loss:624.5801, acc:0.0688\n",
      "[epoch] 862 / 1000 | [Train] loss:619.5902, acc:0.9978 | [Valid] loss:624.6309, acc:0.0675\n",
      "[epoch] 863 / 1000 | [Train] loss:619.6029, acc:0.9978 | [Valid] loss:624.5063, acc:0.0713\n",
      "[epoch] 864 / 1000 | [Train] loss:619.5124, acc:0.9981 | [Valid] loss:624.5366, acc:0.0638\n",
      "[epoch] 865 / 1000 | [Train] loss:619.5217, acc:0.9981 | [Valid] loss:624.4976, acc:0.0725\n",
      "[epoch] 866 / 1000 | [Train] loss:619.4186, acc:0.9981 | [Valid] loss:624.4339, acc:0.0687\n",
      "[epoch] 867 / 1000 | [Train] loss:619.4686, acc:0.9981 | [Valid] loss:624.3828, acc:0.0675\n",
      "[epoch] 868 / 1000 | [Train] loss:619.3717, acc:0.9981 | [Valid] loss:624.3582, acc:0.0688\n",
      "[epoch] 869 / 1000 | [Train] loss:619.3324, acc:0.9978 | [Valid] loss:624.3431, acc:0.0700\n",
      "[epoch] 870 / 1000 | [Train] loss:619.3119, acc:0.9978 | [Valid] loss:624.2950, acc:0.0700\n",
      "[epoch] 871 / 1000 | [Train] loss:619.1859, acc:0.9984 | [Valid] loss:624.2315, acc:0.0725\n",
      "[epoch] 872 / 1000 | [Train] loss:619.2128, acc:0.9984 | [Valid] loss:624.1797, acc:0.0700\n",
      "[epoch] 873 / 1000 | [Train] loss:619.1303, acc:0.9981 | [Valid] loss:624.1585, acc:0.0675\n",
      "[epoch] 874 / 1000 | [Train] loss:619.0920, acc:0.9984 | [Valid] loss:624.1398, acc:0.0700\n",
      "[epoch] 875 / 1000 | [Train] loss:619.1532, acc:0.9978 | [Valid] loss:624.2800, acc:0.0688\n",
      "[epoch] 876 / 1000 | [Train] loss:619.2370, acc:0.9984 | [Valid] loss:624.2433, acc:0.0675\n",
      "[epoch] 877 / 1000 | [Train] loss:619.1998, acc:0.9984 | [Valid] loss:624.1764, acc:0.0663\n",
      "[epoch] 878 / 1000 | [Train] loss:619.2022, acc:0.9981 | [Valid] loss:624.2357, acc:0.0663\n",
      "[epoch] 879 / 1000 | [Train] loss:619.3098, acc:0.9984 | [Valid] loss:624.2534, acc:0.0638\n",
      "[epoch] 880 / 1000 | [Train] loss:619.1840, acc:0.9981 | [Valid] loss:624.2516, acc:0.0713\n",
      "[epoch] 881 / 1000 | [Train] loss:619.2378, acc:0.9981 | [Valid] loss:624.2638, acc:0.0663\n",
      "[epoch] 882 / 1000 | [Train] loss:619.2028, acc:0.9987 | [Valid] loss:624.3005, acc:0.0700\n",
      "[epoch] 883 / 1000 | [Train] loss:619.1945, acc:0.9984 | [Valid] loss:624.1882, acc:0.0725\n",
      "[epoch] 884 / 1000 | [Train] loss:619.0818, acc:0.9978 | [Valid] loss:624.2055, acc:0.0713\n",
      "[epoch] 885 / 1000 | [Train] loss:619.1818, acc:0.9981 | [Valid] loss:624.1800, acc:0.0725\n",
      "[epoch] 886 / 1000 | [Train] loss:619.1386, acc:0.9981 | [Valid] loss:624.1249, acc:0.0675\n",
      "[epoch] 887 / 1000 | [Train] loss:619.1913, acc:0.9981 | [Valid] loss:624.2288, acc:0.0688\n",
      "[epoch] 888 / 1000 | [Train] loss:619.1982, acc:0.9981 | [Valid] loss:624.1909, acc:0.0675\n",
      "[epoch] 889 / 1000 | [Train] loss:619.1019, acc:0.9981 | [Valid] loss:624.1220, acc:0.0675\n",
      "[epoch] 890 / 1000 | [Train] loss:619.1010, acc:0.9981 | [Valid] loss:624.2185, acc:0.0675\n",
      "[epoch] 891 / 1000 | [Train] loss:619.1807, acc:0.9978 | [Valid] loss:624.1767, acc:0.0700\n",
      "[epoch] 892 / 1000 | [Train] loss:619.1635, acc:0.9981 | [Valid] loss:624.1685, acc:0.0700\n",
      "[epoch] 893 / 1000 | [Train] loss:619.2331, acc:0.9981 | [Valid] loss:624.1226, acc:0.0663\n",
      "[epoch] 894 / 1000 | [Train] loss:619.1387, acc:0.9981 | [Valid] loss:624.1710, acc:0.0700\n",
      "[epoch] 895 / 1000 | [Train] loss:619.2499, acc:0.9981 | [Valid] loss:624.2403, acc:0.0688\n",
      "[epoch] 896 / 1000 | [Train] loss:619.1726, acc:0.9978 | [Valid] loss:624.2603, acc:0.0688\n",
      "[epoch] 897 / 1000 | [Train] loss:619.1573, acc:0.9981 | [Valid] loss:624.2184, acc:0.0700\n",
      "[epoch] 898 / 1000 | [Train] loss:619.3637, acc:0.9978 | [Valid] loss:624.3626, acc:0.0712\n",
      "[epoch] 899 / 1000 | [Train] loss:619.2732, acc:0.9981 | [Valid] loss:624.3306, acc:0.0688\n",
      "[epoch] 900 / 1000 | [Train] loss:619.3919, acc:0.9981 | [Valid] loss:624.3409, acc:0.0700\n",
      "[epoch] 901 / 1000 | [Train] loss:619.2848, acc:0.9978 | [Valid] loss:624.3137, acc:0.0725\n",
      "[epoch] 902 / 1000 | [Train] loss:619.3265, acc:0.9978 | [Valid] loss:624.3198, acc:0.0700\n",
      "[epoch] 903 / 1000 | [Train] loss:619.4335, acc:0.9981 | [Valid] loss:624.3261, acc:0.0700\n",
      "[epoch] 904 / 1000 | [Train] loss:619.3104, acc:0.9975 | [Valid] loss:624.3443, acc:0.0675\n",
      "[epoch] 905 / 1000 | [Train] loss:619.3395, acc:0.9978 | [Valid] loss:624.3426, acc:0.0663\n",
      "[epoch] 906 / 1000 | [Train] loss:619.3641, acc:0.9984 | [Valid] loss:624.3457, acc:0.0662\n",
      "[epoch] 907 / 1000 | [Train] loss:619.2456, acc:0.9978 | [Valid] loss:624.3370, acc:0.0688\n",
      "[epoch] 908 / 1000 | [Train] loss:619.3011, acc:0.9984 | [Valid] loss:624.3283, acc:0.0700\n",
      "[epoch] 909 / 1000 | [Train] loss:619.3482, acc:0.9978 | [Valid] loss:624.3154, acc:0.0713\n",
      "[epoch] 910 / 1000 | [Train] loss:619.3027, acc:0.9981 | [Valid] loss:624.3091, acc:0.0688\n",
      "[epoch] 911 / 1000 | [Train] loss:619.3053, acc:0.9978 | [Valid] loss:624.3123, acc:0.0675\n",
      "[epoch] 912 / 1000 | [Train] loss:619.2689, acc:0.9981 | [Valid] loss:624.2090, acc:0.0650\n",
      "[epoch] 913 / 1000 | [Train] loss:619.2291, acc:0.9981 | [Valid] loss:624.1786, acc:0.0712\n",
      "[epoch] 914 / 1000 | [Train] loss:619.0997, acc:0.9984 | [Valid] loss:624.1708, acc:0.0713\n",
      "[epoch] 915 / 1000 | [Train] loss:619.0579, acc:0.9984 | [Valid] loss:624.0806, acc:0.0675\n",
      "[epoch] 916 / 1000 | [Train] loss:619.0812, acc:0.9984 | [Valid] loss:624.1089, acc:0.0675\n",
      "[epoch] 917 / 1000 | [Train] loss:619.1818, acc:0.9984 | [Valid] loss:624.1524, acc:0.0700\n",
      "[epoch] 918 / 1000 | [Train] loss:619.0778, acc:0.9981 | [Valid] loss:624.1287, acc:0.0687\n",
      "[epoch] 919 / 1000 | [Train] loss:619.0762, acc:0.9984 | [Valid] loss:624.1348, acc:0.0650\n",
      "[epoch] 920 / 1000 | [Train] loss:619.1520, acc:0.9984 | [Valid] loss:624.0905, acc:0.0688\n",
      "[epoch] 921 / 1000 | [Train] loss:619.0358, acc:0.9984 | [Valid] loss:624.1547, acc:0.0700\n",
      "[epoch] 922 / 1000 | [Train] loss:619.1835, acc:0.9978 | [Valid] loss:624.1104, acc:0.0688\n",
      "[epoch] 923 / 1000 | [Train] loss:619.1334, acc:0.9984 | [Valid] loss:624.1805, acc:0.0688\n",
      "[epoch] 924 / 1000 | [Train] loss:619.1483, acc:0.9981 | [Valid] loss:624.1375, acc:0.0725\n",
      "[epoch] 925 / 1000 | [Train] loss:619.0916, acc:0.9981 | [Valid] loss:624.0522, acc:0.0700\n",
      "[epoch] 926 / 1000 | [Train] loss:618.9885, acc:0.9984 | [Valid] loss:624.0380, acc:0.0662\n",
      "[epoch] 927 / 1000 | [Train] loss:618.9944, acc:0.9978 | [Valid] loss:624.0476, acc:0.0675\n",
      "[epoch] 928 / 1000 | [Train] loss:618.8873, acc:0.9981 | [Valid] loss:624.0377, acc:0.0675\n",
      "[epoch] 929 / 1000 | [Train] loss:619.0030, acc:0.9984 | [Valid] loss:623.9735, acc:0.0700\n",
      "[epoch] 930 / 1000 | [Train] loss:618.9203, acc:0.9981 | [Valid] loss:624.0906, acc:0.0725\n",
      "[epoch] 931 / 1000 | [Train] loss:619.1000, acc:0.9984 | [Valid] loss:624.0121, acc:0.0650\n",
      "[epoch] 932 / 1000 | [Train] loss:618.9905, acc:0.9984 | [Valid] loss:623.9433, acc:0.0650\n",
      "[epoch] 933 / 1000 | [Train] loss:618.8756, acc:0.9981 | [Valid] loss:624.0467, acc:0.0713\n",
      "[epoch] 934 / 1000 | [Train] loss:618.9936, acc:0.9975 | [Valid] loss:623.9842, acc:0.0700\n",
      "[epoch] 935 / 1000 | [Train] loss:618.9294, acc:0.9981 | [Valid] loss:624.0531, acc:0.0700\n",
      "[epoch] 936 / 1000 | [Train] loss:618.9626, acc:0.9981 | [Valid] loss:624.0027, acc:0.0713\n",
      "[epoch] 937 / 1000 | [Train] loss:618.8892, acc:0.9984 | [Valid] loss:623.9245, acc:0.0712\n",
      "[epoch] 938 / 1000 | [Train] loss:618.9332, acc:0.9981 | [Valid] loss:623.9410, acc:0.0675\n",
      "[epoch] 939 / 1000 | [Train] loss:618.8517, acc:0.9981 | [Valid] loss:623.9387, acc:0.0675\n",
      "[epoch] 940 / 1000 | [Train] loss:618.9809, acc:0.9981 | [Valid] loss:623.9789, acc:0.0700\n",
      "[epoch] 941 / 1000 | [Train] loss:618.9200, acc:0.9984 | [Valid] loss:623.9342, acc:0.0663\n",
      "[epoch] 942 / 1000 | [Train] loss:618.8151, acc:0.9981 | [Valid] loss:623.9267, acc:0.0700\n",
      "[epoch] 943 / 1000 | [Train] loss:618.9045, acc:0.9978 | [Valid] loss:623.9319, acc:0.0713\n",
      "[epoch] 944 / 1000 | [Train] loss:618.9561, acc:0.9984 | [Valid] loss:623.9999, acc:0.0675\n",
      "[epoch] 945 / 1000 | [Train] loss:619.0322, acc:0.9975 | [Valid] loss:624.0900, acc:0.0650\n",
      "[epoch] 946 / 1000 | [Train] loss:619.1059, acc:0.9981 | [Valid] loss:624.1849, acc:0.0650\n",
      "[epoch] 947 / 1000 | [Train] loss:619.1327, acc:0.9978 | [Valid] loss:624.1878, acc:0.0713\n",
      "[epoch] 948 / 1000 | [Train] loss:619.1178, acc:0.9984 | [Valid] loss:624.1696, acc:0.0675\n",
      "[epoch] 949 / 1000 | [Train] loss:619.1588, acc:0.9984 | [Valid] loss:624.2196, acc:0.0675\n",
      "[epoch] 950 / 1000 | [Train] loss:619.2395, acc:0.9978 | [Valid] loss:624.1949, acc:0.0663\n",
      "[epoch] 951 / 1000 | [Train] loss:619.1450, acc:0.9984 | [Valid] loss:624.1455, acc:0.0725\n",
      "[epoch] 952 / 1000 | [Train] loss:619.0373, acc:0.9981 | [Valid] loss:624.0640, acc:0.0688\n",
      "[epoch] 953 / 1000 | [Train] loss:619.0202, acc:0.9981 | [Valid] loss:623.9842, acc:0.0700\n",
      "[epoch] 954 / 1000 | [Train] loss:618.9644, acc:0.9975 | [Valid] loss:623.9689, acc:0.0663\n",
      "[epoch] 955 / 1000 | [Train] loss:618.8853, acc:0.9984 | [Valid] loss:623.8577, acc:0.0713\n",
      "[epoch] 956 / 1000 | [Train] loss:618.8196, acc:0.9981 | [Valid] loss:623.8431, acc:0.0688\n",
      "[epoch] 957 / 1000 | [Train] loss:618.8463, acc:0.9984 | [Valid] loss:623.9371, acc:0.0713\n",
      "[epoch] 958 / 1000 | [Train] loss:618.9087, acc:0.9981 | [Valid] loss:623.9642, acc:0.0725\n",
      "[epoch] 959 / 1000 | [Train] loss:619.0249, acc:0.9978 | [Valid] loss:624.0238, acc:0.0663\n",
      "[epoch] 960 / 1000 | [Train] loss:618.9814, acc:0.9984 | [Valid] loss:624.0381, acc:0.0700\n",
      "[epoch] 961 / 1000 | [Train] loss:618.9738, acc:0.9981 | [Valid] loss:624.0198, acc:0.0700\n",
      "[epoch] 962 / 1000 | [Train] loss:618.9343, acc:0.9981 | [Valid] loss:624.0657, acc:0.0675\n",
      "[epoch] 963 / 1000 | [Train] loss:619.0881, acc:0.9981 | [Valid] loss:624.1534, acc:0.0663\n",
      "[epoch] 964 / 1000 | [Train] loss:619.1717, acc:0.9981 | [Valid] loss:624.2364, acc:0.0675\n",
      "[epoch] 965 / 1000 | [Train] loss:619.2328, acc:0.9981 | [Valid] loss:624.2155, acc:0.0650\n",
      "[epoch] 966 / 1000 | [Train] loss:619.1529, acc:0.9984 | [Valid] loss:624.1698, acc:0.0688\n",
      "[epoch] 967 / 1000 | [Train] loss:619.1677, acc:0.9984 | [Valid] loss:624.1480, acc:0.0675\n",
      "[epoch] 968 / 1000 | [Train] loss:619.1074, acc:0.9981 | [Valid] loss:624.1792, acc:0.0700\n",
      "[epoch] 969 / 1000 | [Train] loss:619.1585, acc:0.9981 | [Valid] loss:624.2445, acc:0.0700\n",
      "[epoch] 970 / 1000 | [Train] loss:619.1679, acc:0.9984 | [Valid] loss:624.2287, acc:0.0713\n",
      "[epoch] 971 / 1000 | [Train] loss:619.1795, acc:0.9978 | [Valid] loss:624.2037, acc:0.0688\n",
      "[epoch] 972 / 1000 | [Train] loss:619.1170, acc:0.9981 | [Valid] loss:624.1862, acc:0.0725\n",
      "[epoch] 973 / 1000 | [Train] loss:619.1426, acc:0.9981 | [Valid] loss:624.2112, acc:0.0675\n",
      "[epoch] 974 / 1000 | [Train] loss:619.2912, acc:0.9984 | [Valid] loss:624.3280, acc:0.0675\n",
      "[epoch] 975 / 1000 | [Train] loss:619.2341, acc:0.9978 | [Valid] loss:624.2737, acc:0.0625\n",
      "[epoch] 976 / 1000 | [Train] loss:619.2130, acc:0.9984 | [Valid] loss:624.2247, acc:0.0700\n",
      "[epoch] 977 / 1000 | [Train] loss:619.3192, acc:0.9981 | [Valid] loss:624.2089, acc:0.0663\n",
      "[epoch] 978 / 1000 | [Train] loss:619.2454, acc:0.9984 | [Valid] loss:624.0840, acc:0.0675\n",
      "[epoch] 979 / 1000 | [Train] loss:619.0332, acc:0.9978 | [Valid] loss:624.1145, acc:0.0700\n",
      "[epoch] 980 / 1000 | [Train] loss:619.0609, acc:0.9981 | [Valid] loss:624.0793, acc:0.0675\n",
      "[epoch] 981 / 1000 | [Train] loss:619.1236, acc:0.9978 | [Valid] loss:624.0682, acc:0.0650\n",
      "[epoch] 982 / 1000 | [Train] loss:619.1441, acc:0.9978 | [Valid] loss:624.1797, acc:0.0675\n",
      "[epoch] 983 / 1000 | [Train] loss:619.1441, acc:0.9984 | [Valid] loss:624.1752, acc:0.0663\n",
      "[epoch] 984 / 1000 | [Train] loss:619.2150, acc:0.9978 | [Valid] loss:624.2969, acc:0.0675\n",
      "[epoch] 985 / 1000 | [Train] loss:619.3533, acc:0.9981 | [Valid] loss:624.3816, acc:0.0700\n",
      "[epoch] 986 / 1000 | [Train] loss:619.3041, acc:0.9978 | [Valid] loss:624.3624, acc:0.0688\n",
      "[epoch] 987 / 1000 | [Train] loss:619.3573, acc:0.9978 | [Valid] loss:624.3273, acc:0.0688\n",
      "[epoch] 988 / 1000 | [Train] loss:619.3248, acc:0.9984 | [Valid] loss:624.3687, acc:0.0688\n",
      "[epoch] 989 / 1000 | [Train] loss:619.3733, acc:0.9978 | [Valid] loss:624.3769, acc:0.0675\n",
      "[epoch] 990 / 1000 | [Train] loss:619.4580, acc:0.9978 | [Valid] loss:624.4316, acc:0.0688\n",
      "[epoch] 991 / 1000 | [Train] loss:619.3328, acc:0.9984 | [Valid] loss:624.3304, acc:0.0725\n",
      "[epoch] 992 / 1000 | [Train] loss:619.3083, acc:0.9981 | [Valid] loss:624.3448, acc:0.0687\n",
      "[epoch] 993 / 1000 | [Train] loss:619.3851, acc:0.9981 | [Valid] loss:624.2990, acc:0.0663\n",
      "[epoch] 994 / 1000 | [Train] loss:619.2326, acc:0.9984 | [Valid] loss:624.2221, acc:0.0725\n",
      "[epoch] 995 / 1000 | [Train] loss:619.1758, acc:0.9984 | [Valid] loss:624.2752, acc:0.0675\n",
      "[epoch] 996 / 1000 | [Train] loss:619.2690, acc:0.9978 | [Valid] loss:624.2232, acc:0.0700\n",
      "[epoch] 997 / 1000 | [Train] loss:619.1054, acc:0.9984 | [Valid] loss:624.2470, acc:0.0713\n",
      "[epoch] 998 / 1000 | [Train] loss:619.2041, acc:0.9984 | [Valid] loss:624.3059, acc:0.0650\n",
      "[epoch] 999 / 1000 | [Train] loss:619.2174, acc:0.9978 | [Valid] loss:624.2961, acc:0.0700\n"
     ]
    }
   ],
   "source": [
    "# --- 학습 루프 시작 ---\n",
    "for iepoch in range(max_epochs):\n",
    "    # 학습/검증 인덱스 셔플\n",
    "    #교수님은 여기서 train, valid에 대한 셔플을 진행하는데. 일단 나는 뺌..\n",
    "    #매 epoch마다 셔플을 하는게 좋대. \n",
    "    np.random.shuffle(idx_train)\n",
    "    np.random.shuffle(idx_valid)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "\n",
    "    # --- 학습 단계 ---\n",
    "    for ibatch in range(0, num_train, batch_size):\n",
    "        batch_mask = idx_train[ibatch:ibatch + batch_size]\n",
    "        x_batch = x_train[batch_mask]\n",
    "        y_batch = y_train[batch_mask]\n",
    "        #y_batch_coarse = y_train_coarse[batch_mask]\n",
    "\n",
    "        # 손실 및 정확도 계산\n",
    "        _train_loss = network.loss(x_batch, y_batch)\n",
    "        _train_acc = network.accuracy(x_batch, y_batch)\n",
    "        train_loss += _train_loss\n",
    "        train_acc += _train_acc\n",
    "\n",
    "        # 기울기 계산 및 파라미터 업데이트\n",
    "        grads = network.gradient(x_batch, y_batch)\n",
    "        optimizer.update(network.params, grads)\n",
    "\n",
    "        # 배치 단위 기록\n",
    "        train_loss_list.append(_train_loss)\n",
    "        train_acc_list.append(_train_acc)\n",
    "\n",
    "    # 에폭 단위 기록\n",
    "    train_loss_per_epoch_list.append(train_loss / train_per_epoch)\n",
    "    train_acc_per_epoch_list.append(train_acc / train_per_epoch)\n",
    "\n",
    "    # --- 검증 단계 ---\n",
    "    for ibatch in range(0, num_valid, batch_size):\n",
    "        batch_mask = idx_valid[ibatch:ibatch + batch_size]\n",
    "        x_batch = x_valid[batch_mask]\n",
    "        y_batch = y_valid[batch_mask]\n",
    "\n",
    "        # 손실 및 정확도 계산 (업데이트 없음)\n",
    "        _valid_loss = network.loss(x_batch, y_batch)\n",
    "        _valid_acc = network.accuracy(x_batch, y_batch)\n",
    "        valid_loss += _valid_loss\n",
    "        valid_acc += _valid_acc\n",
    "\n",
    "        valid_loss_list.append(_valid_loss)\n",
    "        valid_acc_list.append(_valid_acc)\n",
    "\n",
    "    # 에폭 단위 기록\n",
    "    valid_loss_per_epoch_list.append(valid_loss / valid_per_epoch)\n",
    "    valid_acc_per_epoch_list.append(valid_acc / valid_per_epoch)\n",
    "    #print(\"train_loss_per_epoch_list:\", train_loss_per_epoch_list)\n",
    "\n",
    "    # --- 로그 출력 ---\n",
    "    print(f\"[epoch] {iepoch} / {max_epochs} | [Train] loss:{train_loss_per_epoch_list[-1]:.4f}, acc:{train_acc_per_epoch_list[-1]:.4f} | [Valid] loss:{valid_loss_per_epoch_list[-1]:.4f}, acc:{valid_acc_per_epoch_list[-1]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d45d18d9-fdfe-4794-9e5c-849e940b01a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] loss:624.2639, acc:0.0812\n"
     ]
    }
   ],
   "source": [
    "#교수님이 주신.. 100개 클래스에 대한 테스트 결과\n",
    "# --- 테스트 손실 및 정확도 초기화 ---\n",
    "test_loss = 0  # 전체 테스트 손실 누적 변수\n",
    "test_acc = 0   # 전체 테스트 정확도 누적 변수\n",
    "\n",
    "# --- 테스트 평가 단계 시작 ---\n",
    "for ibatch in range(0, num_test, batch_size):\n",
    "    _ibatch = ibatch + batch_size  # 미니배치 끝 인덱스\n",
    "    if _ibatch >= num_test: \n",
    "        _ibatch = num_test  # 마지막 배치 처리 (데이터 초과 방지)\n",
    "\n",
    "    batch_mask = idx_test[ibatch:_ibatch]  # 미니배치 인덱스 추출\n",
    "    x_batch = x_test[batch_mask]           # 입력 데이터 추출\n",
    "    t_batch = y_test[batch_mask]           # 정답 레이블 추출 ###이거 나중에 확인하기\n",
    "\n",
    "    # --- 손실 및 정확도 계산 (모델 파라미터 업데이트 없음) ---\n",
    "    _test_loss = network.loss(x_batch, t_batch)       # 손실 계산\n",
    "    _test_acc = network.accuracy(x_batch, t_batch)     # 정확도 계산\n",
    "\n",
    "    test_loss += _test_loss   # 손실 누적\n",
    "    test_acc += _test_acc     # 정확도 누적\n",
    "\n",
    "    test_loss_list.append(_test_loss)  # 배치 단위 손실 저장\n",
    "    test_acc_list.append(_test_acc)    # 배치 단위 정확도 저장\n",
    "\n",
    "# --- 전체 테스트 세트에 대한 평균 손실 및 정확도 계산 ---\n",
    "test_loss_per_epoch_list.append(test_loss / test_per_epoch)\n",
    "test_acc_per_epoch_list.append(test_acc / test_per_epoch)\n",
    "\n",
    "# --- 테스트 결과 출력 ---\n",
    "print(f\"[Test] loss:{test_loss_per_epoch_list[-1]:.4f}, acc:{test_acc_per_epoch_list[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f52e2418-32c6-47a6-8475-b3be0cec629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_logs_npy(log_path, {\n",
    "    'train_loss': train_loss_per_epoch_list,\n",
    "    'train_acc': train_acc_per_epoch_list,\n",
    "    'valid_loss': valid_loss_per_epoch_list,\n",
    "    'valid_acc': valid_acc_per_epoch_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6da0ed9a-6f72-4148-8ceb-b69e1136905c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 파라미터 저장 완료\n"
     ]
    }
   ],
   "source": [
    "##학습된 모델 파라미터 저장. \n",
    "## \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "save_dir = \"logs/exp1_MLP_mapping\"   # 원하는 폴더\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# network.params는 {'W1': ..., 'b1': ..., 'W2': ..., ...} 형태\n",
    "np.savez(os.path.join(save_dir, \"model_params2.npz\"), **network.params)\n",
    "\n",
    "print(\"✅ 모델 파라미터 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "82661ffe-acb5-4f3f-b009-354eafd0ef38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장된 weight 값: [[ 0.07415928  1.64570128 -0.41703197 ... -1.35573448 -0.33937163\n",
      "  -0.9568702 ]\n",
      " [ 0.06309862  1.68176402 -0.43582437 ... -1.34858703 -0.3531856\n",
      "  -0.9735085 ]\n",
      " [ 0.06881468  1.71895104 -0.4150234  ... -1.38008237 -0.332482\n",
      "  -0.96638949]\n",
      " ...\n",
      " [-0.01493041  1.39815839  0.09578059 ... -1.24903055 -0.6537008\n",
      "  -1.02060704]\n",
      " [-0.00768432  1.42674188  0.07950961 ... -1.25036035 -0.62366894\n",
      "  -1.04892803]\n",
      " [-0.00378592  1.44704933  0.08120071 ... -1.24920559 -0.62421141\n",
      "  -1.03495232]]\n",
      "저장된 acc값 : [0.0159375 0.0225    0.0359375 ... 0.998125  0.9984375 0.998125 ]\n",
      "(3000,)\n",
      "y_pred의 형태 : (800, 100)\n",
      "x_val shaep: (800, 3, 32, 32)\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "## 저장된 parameter 값 복원하는 법\n",
    "loaded = np.load(\"logs/exp1_MLP_mapping/model_params2.npz\")\n",
    "print(\"저장된 weight 값:\",loaded['W1'])\n",
    "\n",
    "#모델 복원하는 경우 이 코드 사용\n",
    "for key in loaded.files:\n",
    "    network.params[key] = loaded[key]\n",
    "    \n",
    "#network.params['W1'] = loaded['W1']\n",
    "#network.params['b1'] = loaded['b1']\n",
    "\n",
    "## 저장된 loss 값 불러오는거 되는지\n",
    "data = np.load('logs/exp1_MLP_mapping/train_acc.npy')\n",
    "print(\"저장된 acc값 :\",data)\n",
    "print(data.shape)\n",
    "\n",
    "y_pred=network.predict(x_val) #valid set을 이용해서 진행.\n",
    "print(\"y_pred의 형태 :\", y_pred.shape)\n",
    "#print(y_pred) #출력값 확인하면 음수도 있음. 아직 logit 상태. softmax 안 거침\n",
    "#x_val의 이미지들을 모델에 넣어서 예측값 뽑아내기.\n",
    "#실제 예측을 하는거\n",
    "\n",
    "\n",
    "#예측값 형태 확인(logit)과 softmax 형태로 변환하기\n",
    "print(\"x_val shaep:\",x_val.shape)\n",
    "\n",
    "softmax_output = softmax(y_pred)# softmax 적용 후 확률값\n",
    "#print(softmax_output[0]) #임의로 출력\n",
    "print(np.argmax(softmax_output[0])) #몇번째 클래스가 가장 높은지"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91dadd43-d36c-40da-b391-04708bc584f0",
   "metadata": {},
   "source": [
    "#그냥 gpt 가 쳐쥰겨.,,..\n",
    "#모델 파라미터 복수 함수\n",
    "import numpy as np\n",
    "\n",
    "def load_model_params(network, filepath):\n",
    "    \"\"\"\n",
    "    저장된 .npz 파라미터 파일을 불러와서 network에 복원\n",
    "    - network: MultiLayerNetExtend 인스턴스\n",
    "    - filepath: 저장된 npz 파일 경로\n",
    "    \"\"\"\n",
    "    loaded = np.load(filepath)\n",
    "    for key in loaded.files:\n",
    "        network.params[key] = loaded[key]\n",
    "    print(f\"✅ 모델 파라미터 복원 완료: {filepath}\")\n",
    "\n",
    "#사용 예시\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "\n",
    "# 1. 먼저 동일한 구조의 network 다시 정의\n",
    "network = MultiLayerNetExtend(\n",
    "    input_size=3072,\n",
    "    hidden_size_list=[100, 50],\n",
    "    output_size=100,\n",
    "    activation='relu',\n",
    "    weight_init_std='he',\n",
    "    weight_decay_lambda=0.1,\n",
    "    use_batchnorm=True\n",
    ")\n",
    "\n",
    "# 2. 저장된 파라미터 복원\n",
    "load_model_params(network, \"logs/exp1_MLP_mapping/model_params.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a63ee33-614f-415b-a7b6-916c3dbbcfdc",
   "metadata": {},
   "source": [
    "## 매핑 실험!!!!!  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb4919-5f74-4379-ac0a-6a158f8ea2b8",
   "metadata": {},
   "source": [
    "### 단순 mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "495b1054-9077-4714-a937-c745edfd1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#지금 y_pred는 로짓형태. 추가적으로  softmax를 추가하거나 해야함\n",
    "#매핑 테이블 만들기. 딕셔너리 형태\n",
    "fine_to_coarse = {fine: coarse for fine, coarse in zip(y_train, y_train_coarse)}\n",
    "\n",
    "def argmax_fine_to_coarse(softmax_output, fine_to_coarse):\n",
    "    \"\"\"\n",
    "    softmax_output: shape (N, 100)\n",
    "    returns: coarse_pred, shape (N,)\n",
    "    \"\"\"\n",
    "    fine_preds = np.argmax(softmax_output, axis=1)\n",
    "    coarse_preds = np.array([fine_to_coarse[f] for f in fine_preds])\n",
    "    return coarse_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "51fef71d-baeb-46db-8b9d-8180062dd4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict fine label: 34\n",
      "true fine label: 51\n",
      "predict coarse label: 3\n",
      "true coarse label: 12 \n",
      "\n",
      "predict fine label: 85\n",
      "true fine label: 52\n",
      "predict coarse label: 8\n",
      "true coarse label: 7 \n",
      "\n",
      "predict fine label: 25\n",
      "true fine label: 31\n",
      "predict coarse label: 2\n",
      "true coarse label: 9 \n",
      "\n",
      "predict fine label: 60\n",
      "true fine label: 71\n",
      "predict coarse label: 18\n",
      "true coarse label: 2 \n",
      "\n",
      "predict fine label: 68\n",
      "true fine label: 73\n",
      "predict coarse label: 15\n",
      "true coarse label: 9 \n",
      "\n",
      "predict fine label: 45\n",
      "true fine label: 4\n",
      "predict coarse label: 14\n",
      "true coarse label: 9 \n",
      "\n",
      "predict fine label: 56\n",
      "true fine label: 59\n",
      "predict coarse label: 1\n",
      "true coarse label: 2 \n",
      "\n",
      "predict fine label: 20\n",
      "true fine label: 35\n",
      "predict coarse label: 15\n",
      "true coarse label: 6 \n",
      "\n",
      "predict fine label: 82\n",
      "true fine label: 82\n",
      "predict coarse label: 10\n",
      "true coarse label: 7 \n",
      "\n",
      "predict fine label: 19\n",
      "true fine label: 62\n",
      "predict coarse label: 0\n",
      "true coarse label: 15 \n",
      "\n",
      "predict fine label: 65\n",
      "true fine label: 82\n",
      "predict coarse label: 4\n",
      "true coarse label: 13 \n",
      "\n",
      "predict fine label: 70\n",
      "true fine label: 20\n",
      "predict coarse label: 9\n",
      "true coarse label: 12 \n",
      "\n",
      "predict fine label: 22\n",
      "true fine label: 94\n",
      "predict coarse label: 15\n",
      "true coarse label: 5 \n",
      "\n",
      "predict fine label: 19\n",
      "true fine label: 19\n",
      "predict coarse label: 0\n",
      "true coarse label: 9 \n",
      "\n",
      "predict fine label: 42\n",
      "true fine label: 42\n",
      "predict coarse label: 15\n",
      "true coarse label: 9 \n",
      "\n",
      "predict fine label: 0\n",
      "true fine label: 53\n",
      "predict coarse label: 17\n",
      "true coarse label: 5 \n",
      "\n",
      "predict fine label: 79\n",
      "true fine label: 18\n",
      "predict coarse label: 16\n",
      "true coarse label: 3 \n",
      "\n",
      "predict fine label: 27\n",
      "true fine label: 32\n",
      "predict coarse label: 12\n",
      "true coarse label: 17 \n",
      "\n",
      "predict fine label: 95\n",
      "true fine label: 67\n",
      "predict coarse label: 19\n",
      "true coarse label: 10 \n",
      "\n",
      "predict fine label: 39\n",
      "true fine label: 97\n",
      "predict coarse label: 16\n",
      "true coarse label: 4 \n",
      "\n",
      "predict fine label: 64\n",
      "true fine label: 14\n",
      "predict coarse label: 17\n",
      "true coarse label: 14 \n",
      "\n",
      "predict fine label: 74\n",
      "true fine label: 74\n",
      "predict coarse label: 1\n",
      "true coarse label: 14 \n",
      "\n",
      "predict fine label: 5\n",
      "true fine label: 1\n",
      "predict coarse label: 4\n",
      "true coarse label: 17 \n",
      "\n",
      "predict fine label: 24\n",
      "true fine label: 21\n",
      "predict coarse label: 9\n",
      "true coarse label: 9 \n",
      "\n",
      "predict fine label: 0\n",
      "true fine label: 0\n",
      "predict coarse label: 17\n",
      "true coarse label: 15 \n",
      "\n",
      "predict fine label: 46\n",
      "true fine label: 57\n",
      "predict coarse label: 12\n",
      "true coarse label: 1 \n",
      "\n",
      "predict fine label: 36\n",
      "true fine label: 9\n",
      "predict coarse label: 11\n",
      "true coarse label: 14 \n",
      "\n",
      "predict fine label: 44\n",
      "true fine label: 67\n",
      "predict coarse label: 12\n",
      "true coarse label: 0 \n",
      "\n",
      "predict fine label: 71\n",
      "true fine label: 12\n",
      "predict coarse label: 8\n",
      "true coarse label: 12 \n",
      "\n",
      "predict fine label: 16\n",
      "true fine label: 16\n",
      "predict coarse label: 11\n",
      "true coarse label: 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "coarse_pred = argmax_fine_to_coarse(softmax_output, fine_to_coarse)\n",
    "\n",
    "# 예시 출력\n",
    "for a in range(0,30,1):\n",
    "    print(\"predict fine label:\", np.argmax(softmax_output[a]))\n",
    "    print(\"true fine label:\", y_val[a])  \n",
    "    print(\"predict coarse label:\", coarse_pred[a])\n",
    "    print(\"true coarse label:\", y_val_coarse[a],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605f948-6dc5-4467-b2a1-9c96ec677d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9783c74-87d4-456c-b648-a7a093e1398f",
   "metadata": {},
   "source": [
    "## 매핑의 성능 평가 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "423843e4-a6e9-4aab-b045-9e423a8d30be",
   "metadata": {},
   "source": [
    "#교수님이 주신 코드 사용. 20개의 class 성능 평가 코드\n",
    "# --- 테스트 손실 및 정확도 초기화 ---\n",
    "test_loss_coarse = 0  # 전체 테스트 손실 누적 변수\n",
    "test_acc_coarse = 0   # 전체 테스트 정확도 누적 변수\n",
    "\n",
    "coarse_pred_list=[] #이게 뭐지\n",
    "\n",
    "# --- 테스트 평가 단계 시작 ---\n",
    "for ibatch in range(0, num_test, batch_size):\n",
    "    _ibatch = ibatch + batch_size  # 미니배치 끝 인덱스\n",
    "    if _ibatch >= num_test: \n",
    "        _ibatch = num_test  # 마지막 배치 처리 (데이터 초과 방지)\n",
    "\n",
    "    batch_mask = idx_test[ibatch:_ibatch]  # 미니배치 인덱스 추출\n",
    "    x_batch = x_test[batch_mask]           # 입력 데이터 추출 (입력)\n",
    "    t_batch_coarse = y_test_coarse[batch_mask]    # 정답 레이블 추출 (이미지 데이터의 coarse label)\n",
    "\n",
    "    # --- 손실 및 정확도 계산 (모델 파라미터 업데이트 없음) ---\n",
    "    _test_loss = network.loss(x_batch, t_batch_coarse)       # 손실 계산\n",
    "    _test_acc = network.accuracy(x_batch, t_batch_coarse)     # 정확도 계산\n",
    "\n",
    "    test_loss += _test_loss   # 손실 누적\n",
    "    test_acc += _test_acc     # 정확도 누적\n",
    "\n",
    "    test_loss_list.append(_test_loss_coarse)  # 배치 단위 손실 저장\n",
    "    test_acc_list.append(_test_acc_coarse)    # 배치 단위 정확도 저장\n",
    "\n",
    "# --- 전체 테스트 세트에 대한 평균 손실 및 정확도 계산 ---\n",
    "test_loss_per_epoch_list.append(test_loss / test_per_epoch)\n",
    "test_acc_per_epoch_list.append(test_acc / test_per_epoch)\n",
    "\n",
    "# --- 테스트 결과 출력 ---\n",
    "print(f\"[Test] loss:{test_loss_per_epoch_list[-1]:.4f}, acc:{test_acc_per_epoch_list[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f3834057-5dca-4342-b090-1b67cdecde5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'map_softmax_to_coarse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m softmax_output \u001b[38;5;241m=\u001b[39m softmax(logits)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 2. softmax → coarse 확률 맵핑\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m coarse_output \u001b[38;5;241m=\u001b[39m \u001b[43mmap_softmax_to_coarse\u001b[49m(softmax_output, fine_to_coarse)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 3. coarse 예측\u001b[39;00m\n\u001b[0;32m     23\u001b[0m pred_coarse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(coarse_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'map_softmax_to_coarse' is not defined"
     ]
    }
   ],
   "source": [
    "# --- coarse 평가용 손실 및 정확도 초기화 ---\n",
    "test_loss_coarse = 0\n",
    "test_acc_coarse = 0\n",
    "\n",
    "# --- 테스트 평가 루프 시작 ---\n",
    "for ibatch in range(0, num_test, batch_size):\n",
    "    _ibatch = ibatch + batch_size\n",
    "    if _ibatch >= num_test:\n",
    "        _ibatch = num_test  # 마지막 배치 초과 방지\n",
    "\n",
    "    batch_mask = idx_test[ibatch:_ibatch]\n",
    "    x_batch = x_test[batch_mask]\n",
    "    t_batch_coarse = y_test_coarse[batch_mask]\n",
    "\n",
    "    # 1. 예측 로짓 → softmax\n",
    "    logits = network.predict(x_batch)\n",
    "    softmax_output = softmax(logits)\n",
    "\n",
    "    # 2. softmax → coarse 확률 맵핑\n",
    "    coarse_output = map_softmax_to_coarse(softmax_output, fine_to_coarse)\n",
    "\n",
    "    # 3. coarse 예측\n",
    "    pred_coarse = np.argmax(coarse_output, axis=1)\n",
    "\n",
    "    # 4. accuracy 계산\n",
    "    batch_acc = np.sum(pred_coarse == t_batch_coarse) / len(t_batch_coarse)\n",
    "    test_acc_coarse += batch_acc\n",
    "\n",
    "    # 5. cross-entropy loss 계산 (직접 구현)\n",
    "    probs = np.clip(coarse_output[np.arange(len(t_batch_coarse)), t_batch_coarse], 1e-7, 1.0)\n",
    "    batch_loss = -np.mean(np.log(probs))\n",
    "    test_loss_coarse += batch_loss\n",
    "\n",
    "# --- 평균 계산 ---\n",
    "test_loss_coarse /= test_per_epoch\n",
    "test_acc_coarse /= test_per_epoch\n",
    "\n",
    "# --- coarse 평가 결과 출력 ---\n",
    "print(f\"[Test coarse] loss: {test_loss_coarse:.4f} | acc: {test_acc_coarse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2526f-275b-432d-b5fb-6305635295b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e29c09-4f24-452a-a868-b26e48e17e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8298c-73cd-4c1d-b06a-5f0904c525bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ff640-ec99-45f2-9a76-350df7899a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c28c8b-e714-40da-a7e5-d5ed741632ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2oil",
   "language": "python",
   "name": "2oil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iD2wj12sn-W3",
    "outputId": "91cd9b80-4bcd-462f-d44d-f8d93e1af531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 169001437/169001437 [01:00<00:00, 2795616.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "X_train: (50000, 32, 32, 3)\n",
      "y_train (one-hot): (50000, 100)\n",
      "X_test: (10000, 32, 32, 3)\n",
      "y_test (one-hot): (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# 1. CIFAR-100 train/test 데이터셋 불러오기\n",
    "train_dataset = CIFAR100(root='./data', train=True, download=True)\n",
    "test_dataset = CIFAR100(root='./data', train=False, download=True)\n",
    "\n",
    "\n",
    "# 2. numpy로 변환 (float32로 스케일링)\n",
    "X_train = train_dataset.data.astype(np.float32) / 255.0  # shape: (50000, 32, 32, 3)\n",
    "y_train = np.array(train_dataset.targets)                # shape: (50000,)\n",
    "\n",
    "X_test = test_dataset.data.astype(np.float32) / 255.0\n",
    "y_test = np.array(test_dataset.targets)\n",
    "\n",
    "# 3. 정규화 (채널 별 평균/표준편차)\n",
    "mean = np.array([0.5071, 0.4867, 0.4408])\n",
    "std = np.array([0.2675, 0.2565, 0.2761])\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "# 4. One-hot encoding (100-class)\n",
    "y_train_oh = np.eye(100)[y_train]  # shape: (50000, 100)\n",
    "y_test_oh = np.eye(100)[y_test]    # shape: (10000, 100)\n",
    "\n",
    "# 확인용 출력\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train (one-hot):\", y_train_oh.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test (one-hot):\", y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QADnnGNfgs40"
   },
   "outputs": [],
   "source": [
    "def save_cifar100(X_train, y_train_oh, X_test, y_test_oh, path='./'):\n",
    "    np.save(path + 'X_train.npy', X_train)\n",
    "    np.save(path + 'y_train_oh.npy', y_train_oh)\n",
    "    np.save(path + 'X_test.npy', X_test)\n",
    "    np.save(path + 'y_test_oh.npy', y_test_oh)\n",
    "\n",
    "def load_cifar100(path='./'):\n",
    "    X_train = np.load(path + 'X_train.npy')\n",
    "    y_train_oh = np.load(path + 'y_train_oh.npy')\n",
    "    X_test = np.load(path + 'X_test.npy')\n",
    "    y_test_oh = np.load(path + 'y_test_oh.npy')\n",
    "    return X_train, y_train_oh, X_test, y_test_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "save_cifar100(X_train, y_train_oh, X_test, y_test_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "X_train, y_train_oh, X_test, y_test_oh = load_cifar100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Dn0fUhhCpTg5"
   },
   "outputs": [],
   "source": [
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        kh, kw = self.kernel_size\n",
    "        fan_in = in_channels * kh * kw\n",
    "        fan_out = out_channels * kh * kw\n",
    "        limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "        self.weights = np.random.uniform(-limit, limit, size=(out_channels, in_channels, kh, kw))\n",
    "        self.bias = np.zeros(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x  # 입력 저장 (for backward)\n",
    "        N, H, W, C = x.shape\n",
    "        kh, kw = self.kernel_size\n",
    "        sh, sw = self.stride, self.stride\n",
    "\n",
    "        x_padded = np.pad(x, ((0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)), mode='constant')\n",
    "        self.x_padded = x_padded  # backward에서 사용\n",
    "\n",
    "        H_out = (H + 2 * self.padding - kh) // sh + 1\n",
    "        W_out = (W + 2 * self.padding - kw) // sw + 1\n",
    "        out = np.zeros((N, H_out, W_out, self.out_channels))\n",
    "\n",
    "        for n in range(N):\n",
    "            for h in range(H_out):\n",
    "                for w in range(W_out):\n",
    "                    for c_out in range(self.out_channels):\n",
    "                        h_start = h * sh\n",
    "                        h_end = h_start + kh\n",
    "                        w_start = w * sw\n",
    "                        w_end = w_start + kw\n",
    "\n",
    "                        region = x_padded[n, h_start:h_end, w_start:w_end, :]  # shape: (kh, kw, in_channels)\n",
    "                        kernel = self.weights[c_out].transpose(1, 2, 0)  # (in_channels, kh, kw) → (kh, kw, in_channels)\n",
    "                        out[n, h, w, c_out] = np.sum(region * kernel) + self.bias[c_out]\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        x_padded = self.x_padded\n",
    "        N, H_out, W_out, C_out = grad_output.shape\n",
    "        kh, kw = self.kernel_size\n",
    "        sh, sw = self.stride, self.stride\n",
    "\n",
    "        self.grad_weights = np.zeros_like(self.weights)\n",
    "        self.grad_bias = np.zeros_like(self.bias)\n",
    "        dx_padded = np.zeros_like(x_padded)\n",
    "\n",
    "        for n in range(N):\n",
    "            for h in range(H_out):\n",
    "                for w in range(W_out):\n",
    "                    for c_out in range(self.out_channels):\n",
    "                        h_start = h * sh\n",
    "                        h_end = h_start + kh\n",
    "                        w_start = w * sw\n",
    "                        w_end = w_start + kw\n",
    "\n",
    "                        region = x_padded[n, h_start:h_end, w_start:w_end, :]  # (kh, kw, in_channels)\n",
    "\n",
    "                        self.grad_weights[c_out] += region.transpose(2, 0, 1) * grad_output[n, h, w, c_out]\n",
    "                        self.grad_bias[c_out] += grad_output[n, h, w, c_out]\n",
    "                        dx_padded[n, h_start:h_end, w_start:w_end, :] += self.weights[c_out].transpose(1, 2, 0) * grad_output[n, h, w, c_out]\n",
    "\n",
    "        # 패딩 제거\n",
    "        if self.padding > 0:\n",
    "            dx = dx_padded[:, self.padding:-self.padding, self.padding:-self.padding, :]\n",
    "        else:\n",
    "            dx = dx_padded\n",
    "        return dx\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.weights -= lr * self.grad_weights\n",
    "        self.bias -= lr * self.grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8yPScXcCpW8j"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.mask = (x > 0)\n",
    "        return x * self.mask\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dwScE_wapuPa"
   },
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, kernel_size=2, stride=2):\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        N, H, W, C = x.shape\n",
    "        kh, kw = self.kernel_size, self.kernel_size\n",
    "        sh, sw = self.stride, self.stride\n",
    "\n",
    "        H_out = (H - kh) // sh + 1\n",
    "        W_out = (W - kw) // sw + 1\n",
    "        out = np.zeros((N, H_out, W_out, C))\n",
    "        self.max_mask = np.zeros_like(x)\n",
    "\n",
    "        for n in range(N):\n",
    "            for h in range(H_out):\n",
    "                for w in range(W_out):\n",
    "                    for c in range(C):\n",
    "                        h_start = h * sh\n",
    "                        w_start = w * sw\n",
    "                        h_end = h_start + kh\n",
    "                        w_end = w_start + kw\n",
    "\n",
    "                        window = x[n, h_start:h_end, w_start:w_end, c]\n",
    "                        max_val = np.max(window)\n",
    "                        out[n, h, w, c] = max_val\n",
    "\n",
    "                        # mask에 해당 위치만 1로 설정\n",
    "                        for i in range(kh):\n",
    "                            for j in range(kw):\n",
    "                                if window[i, j] == max_val:\n",
    "                                    self.max_mask[n, h_start + i, w_start + j, c] = 1\n",
    "                                    break  # 첫 max만 선택\n",
    "                            else:\n",
    "                                continue\n",
    "                            break\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        N, H_out, W_out, C = grad_output.shape\n",
    "        kh, kw = self.kernel_size, self.kernel_size\n",
    "        sh, sw = self.stride, self.stride\n",
    "\n",
    "        grad_input = np.zeros_like(self.x)\n",
    "\n",
    "        for n in range(N):\n",
    "            for h in range(H_out):\n",
    "                for w in range(W_out):\n",
    "                    for c in range(C):\n",
    "                        h_start = h * sh\n",
    "                        w_start = w * sw\n",
    "                        h_end = h_start + kh\n",
    "                        w_end = w_start + kw\n",
    "\n",
    "                        for i in range(kh):\n",
    "                            for j in range(kw):\n",
    "                                if self.max_mask[n, h_start + i, w_start + j, c] == 1:\n",
    "                                    grad_input[n, h_start + i, w_start + j, c] = grad_output[n, h, w, c]\n",
    "                                    break\n",
    "                            else:\n",
    "                                continue\n",
    "                            break\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PGhXNyirp2UG"
   },
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    def forward(self, x):\n",
    "        self.input_shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1KdAxExXp7oN"
   },
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.weights = np.random.randn(in_features, out_features) * np.sqrt(2. / in_features)\n",
    "        self.bias = np.zeros(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.grad_weights = self.input.T @ grad_output\n",
    "        self.grad_bias = np.sum(grad_output, axis=0)\n",
    "        return grad_output @ self.weights.T\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.weights -= lr * self.grad_weights\n",
    "        self.bias -= lr * self.grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5JRY2n_qp8wm"
   },
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.out = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output  # CrossEntropy랑 같이 쓰면 별도 처리 불필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bKyV9ZFvp-ZU"
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred, y_true):  # y_true는 one-hot\n",
    "        self.y_pred = y_pred\n",
    "        self.y_true = y_true\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / y_pred.shape[0]\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        return (self.y_pred - self.y_true) / self.y_pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nwDN67RRqAC1"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, parameters, lr=0.01):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.parameters:\n",
    "            p.update(self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VXvuJsmcqBO7"
   },
   "outputs": [],
   "source": [
    "class BaselineCNN:\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv2D(3, 16, 3, stride=1, padding=1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.pool1 = MaxPool2D()\n",
    "\n",
    "        self.conv2 = Conv2D(16, 32, 3, stride=1, padding=1)\n",
    "        self.relu2 = ReLU()\n",
    "        self.pool2 = MaxPool2D()\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = FullyConnected(8*8*32, 512)\n",
    "        self.relu3 = ReLU()\n",
    "        self.fc2 = FullyConnected(512, 100)\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        x = self.pool1.forward(x)\n",
    "\n",
    "        x = self.conv2.forward(x)\n",
    "        x = self.relu2.forward(x)\n",
    "        x = self.pool2.forward(x)\n",
    "\n",
    "        x = self.flatten.forward(x)\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu3.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.softmax.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad = self.fc2.backward(grad_output)\n",
    "        grad = self.relu3.backward(grad)\n",
    "        grad = self.fc1.backward(grad)\n",
    "        grad = self.flatten.backward(grad)\n",
    "        # Pool, Conv backward 생략 가능 (baseline에서는)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.fc1, self.fc2]  # Conv2D도 파라미터 있으면 여기에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "a6DOD-riqbfM"
   },
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, epochs=5, batch_size=64, lr=0.01):\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    optimizer = SGD(parameters=model.parameters(), lr=lr)\n",
    "\n",
    "    N = X_train.shape[0]\n",
    "    for epoch in range(epochs):\n",
    "        perm = np.random.permutation(N)\n",
    "        X_train = X_train[perm]\n",
    "        y_train = y_train[perm]\n",
    "\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for i in range(0, N, batch_size):\n",
    "            x_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "            y_pred = model.forward(x_batch)\n",
    "            loss = loss_fn.forward(y_pred, y_batch)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Accuracy\n",
    "            pred_labels = np.argmax(y_pred, axis=1)\n",
    "            true_labels = np.argmax(y_batch, axis=1)\n",
    "            correct += np.sum(pred_labels == true_labels)\n",
    "\n",
    "            # Backward\n",
    "            grad_loss = loss_fn.backward()\n",
    "            model.backward(grad_loss)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        acc = correct / N * 100\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}, Accuracy = {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uzk_ZPq1qklk"
   },
   "outputs": [],
   "source": [
    "# 모델 선언\n",
    "model = BaselineCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kj97EeDRUPRX"
   },
   "outputs": [],
   "source": [
    "# BaselineCNN 저장 함수\n",
    "def save_model_baseline(model, filename='baseline_model.npz'):\n",
    "    np.savez(filename,\n",
    "             fc1_w=model.fc1.weights,\n",
    "             fc1_b=model.fc1.bias,\n",
    "             fc2_w=model.fc2.weights,\n",
    "             fc2_b=model.fc2.bias)\n",
    "\n",
    "# BaselineCNN 불러오기 함수\n",
    "def load_model_baseline(model, filename='baseline_model.npz'):\n",
    "    data = np.load(filename)\n",
    "    model.fc1.weights = data['fc1_w']\n",
    "    model.fc1.bias = data['fc1_b']\n",
    "    model.fc2.weights = data['fc2_w']\n",
    "    model.fc2.bias = data['fc2_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6tXVPFfpquCr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 3467.5375, Accuracy = 4.52%\n",
      "Epoch 2: Loss = 3231.1973, Accuracy = 9.90%\n",
      "Epoch 3: Loss = 3076.9595, Accuracy = 13.01%\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "train(model, X_train, y_train_oh, epochs=3, batch_size=64, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 후 저장\n",
    "save_model_baseline(model, 'baseline_epoch3.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "model = BaselineCNN()\n",
    "load_model_baseline(model, 'baseline_epoch3.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "uNDm9dOdS8V3"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test):\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    N = X_test.shape[0]\n",
    "    batch_size = 64\n",
    "\n",
    "    for i in range(0, N, batch_size):\n",
    "        x_batch = X_test[i:i+batch_size]\n",
    "        y_batch = y_test[i:i+batch_size]\n",
    "\n",
    "        y_pred = model.forward(x_batch)\n",
    "        loss = loss_fn.forward(y_pred, y_batch)\n",
    "        total_loss += loss\n",
    "\n",
    "        pred_labels = np.argmax(y_pred, axis=1)\n",
    "        true_labels = np.argmax(y_batch, axis=1)\n",
    "        correct += np.sum(pred_labels == true_labels)\n",
    "\n",
    "    acc = correct / N * 100\n",
    "    avg_loss = total_loss / (N // batch_size)\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4MmzYBa_TNbq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline Test] Loss = 4.7937, Accuracy = 1.36%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, X_test, y_test_oh)\n",
    "print(f\"[Baseline Test] Loss = {test_loss:.4f}, Accuracy = {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ySWeJMFzqwLT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv2D_vec:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size if isinstance(kernel_size, tuple) else (kernel_size, kernel_size)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        kh, kw = self.kernel_size\n",
    "        fan_in = in_channels * kh * kw\n",
    "        fan_out = out_channels * kh * kw\n",
    "        limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "        self.weights = np.random.uniform(-limit, limit, (out_channels, in_channels, kh, kw))\n",
    "        self.bias = np.zeros(out_channels)\n",
    "\n",
    "    def im2col(self, x, kh, kw, sh, sw):\n",
    "        N, H, W, C = x.shape\n",
    "        H_out = (H + 2 * self.padding - kh) // sh + 1\n",
    "        W_out = (W + 2 * self.padding - kw) // sw + 1\n",
    "\n",
    "        x_padded = np.pad(x, ((0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)), mode='constant')\n",
    "        col = np.zeros((N, H_out, W_out, kh, kw, C))\n",
    "\n",
    "        for y in range(H_out):\n",
    "            for x in range(W_out):\n",
    "                y_start = y * sh\n",
    "                x_start = x * sw\n",
    "                col[:, y, x, :, :, :] = x_padded[:, y_start:y_start+kh, x_start:x_start+kw, :]\n",
    "\n",
    "        col = col.reshape(N * H_out * W_out, kh * kw * C)\n",
    "        return col, H_out, W_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        N, H, W, C = x.shape\n",
    "        kh, kw = self.kernel_size\n",
    "        sh, sw = self.stride, self.stride\n",
    "\n",
    "        col, H_out, W_out = self.im2col(x, kh, kw, sh, sw)\n",
    "        self.col = col\n",
    "\n",
    "        W_col = self.weights.reshape(self.out_channels, -1)\n",
    "        out = col @ W_col.T + self.bias\n",
    "        out = out.reshape(N, H_out, W_out, self.out_channels)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        N, H, W, C = self.x.shape\n",
    "        kh, kw = self.kernel_size\n",
    "        sh, sw = self.stride, self.stride\n",
    "        col = self.col\n",
    "        H_out, W_out = grad_output.shape[1], grad_output.shape[2]\n",
    "\n",
    "        grad_output_reshaped = grad_output.reshape(-1, self.out_channels)\n",
    "\n",
    "        self.dW = grad_output_reshaped.T @ col\n",
    "        self.dW = self.dW.reshape(self.out_channels, kh, kw, C).transpose(0, 3, 1, 2)\n",
    "        self.db = np.sum(grad_output_reshaped, axis=0)\n",
    "\n",
    "        W_col = self.weights.reshape(self.out_channels, -1)\n",
    "        dcol = grad_output_reshaped @ W_col\n",
    "        dcol = dcol.reshape(N, H_out, W_out, kh, kw, C)\n",
    "\n",
    "        dx_padded = np.zeros((N, H + 2 * self.padding, W + 2 * self.padding, C))\n",
    "        for y in range(H_out):\n",
    "            for x in range(W_out):\n",
    "                y_start = y * sh\n",
    "                x_start = x * sw\n",
    "                dx_padded[:, y_start:y_start+kh, x_start:x_start+kw, :] += dcol[:, y, x, :, :, :]\n",
    "\n",
    "        if self.padding == 0:\n",
    "            return dx_padded\n",
    "        return dx_padded[:, self.padding:-self.padding, self.padding:-self.padding, :]\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.weights -= lr * self.dW\n",
    "        self.bias -= lr * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionBlock:\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        self.conv_1x1 = Conv2D_vec(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv_3x3 = Conv2D_vec(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = ReLU()\n",
    "        self.alpha = np.array([0.5])\n",
    "        self.grad_alpha = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.out1 = self.conv_1x1.forward(x)\n",
    "        self.out2 = self.conv_3x3.forward(x)\n",
    "        self.output = self.alpha * self.out1 + (1 - self.alpha) * self.out2\n",
    "        return self.relu.forward(self.output)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_output_relu = self.relu.backward(grad_output)\n",
    "        self.grad_alpha = np.sum((self.out1 - self.out2) * grad_output_relu)\n",
    "        grad1 = grad_output_relu * self.alpha\n",
    "        grad2 = grad_output_relu * (1 - self.alpha)\n",
    "        dx1 = self.conv_1x1.backward(grad1)\n",
    "        dx2 = self.conv_3x3.backward(grad2)\n",
    "        return dx1 + dx2\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.alpha -= lr * self.grad_alpha\n",
    "        self.alpha = np.clip(self.alpha, 0, 1)\n",
    "        self.conv_1x1.update(lr)\n",
    "        self.conv_3x3.update(lr)\n",
    "\n",
    "class ResidualBlock:\n",
    "    def __init__(self, channels):\n",
    "        self.conv1 = Conv2D_vec(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.conv2 = Conv2D_vec(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.out = self.conv1.forward(x)\n",
    "        self.out = self.relu1.forward(self.out)\n",
    "        self.out = self.conv2.forward(self.out)\n",
    "        return self.relu2.forward(self.out + x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad = self.relu2.backward(grad_output)\n",
    "        grad_skip = grad\n",
    "        grad = self.conv2.backward(grad)\n",
    "        grad = self.relu1.backward(grad)\n",
    "        grad = self.conv1.backward(grad)\n",
    "        return grad + grad_skip\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.conv1.update(lr)\n",
    "        self.conv2.update(lr)\n",
    "\n",
    "class DFFRCNN:\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv2D_vec(3, 16, 3, stride=1, padding=1)\n",
    "        self.relu1 = ReLU()\n",
    "        self.res_block = ResidualBlock(16)\n",
    "        self.fusion = FusionBlock(16, 32)\n",
    "        self.pool = MaxPool2D()\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = FullyConnected(16*16*32, 512)\n",
    "        self.relu2 = ReLU()\n",
    "        self.fc2 = FullyConnected(512, 100)\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        x = self.res_block.forward(x)\n",
    "        x = self.fusion.forward(x)\n",
    "        x = self.pool.forward(x)\n",
    "        x = self.flatten.forward(x)\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu2.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.softmax.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad = self.fc2.backward(grad_output)\n",
    "        grad = self.relu2.backward(grad)\n",
    "        grad = self.fc1.backward(grad)\n",
    "        grad = self.flatten.backward(grad)\n",
    "        grad = self.pool.backward(grad)\n",
    "        grad = self.fusion.backward(grad)\n",
    "        grad = self.res_block.backward(grad)\n",
    "        grad = self.relu1.backward(grad)\n",
    "        grad = self.conv1.backward(grad)\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.fc1, self.fc2, self.fusion, self.res_block, self.conv1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "v7fz-88fSqju"
   },
   "outputs": [],
   "source": [
    "model = DFFRCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "PvcVl2a1Ua13"
   },
   "outputs": [],
   "source": [
    "# DFFRCNN 저장 및 불러오기 함수\n",
    "def save_model_dff(model, filename='dffrcnn_model.npz'):\n",
    "    np.savez(filename,\n",
    "             conv1_w=model.conv1.weights,\n",
    "             conv1_b=model.conv1.bias,\n",
    "\n",
    "             res_conv1_w=model.res_block.conv1.weights,\n",
    "             res_conv1_b=model.res_block.conv1.bias,\n",
    "             res_conv2_w=model.res_block.conv2.weights,\n",
    "             res_conv2_b=model.res_block.conv2.bias,\n",
    "\n",
    "             fusion_conv1x1_w=model.fusion.conv_1x1.weights,\n",
    "             fusion_conv1x1_b=model.fusion.conv_1x1.bias,\n",
    "             fusion_conv3x3_w=model.fusion.conv_3x3.weights,\n",
    "             fusion_conv3x3_b=model.fusion.conv_3x3.bias,\n",
    "             fusion_alpha=model.fusion.alpha,\n",
    "\n",
    "             fc1_w=model.fc1.weights,\n",
    "             fc1_b=model.fc1.bias,\n",
    "             fc2_w=model.fc2.weights,\n",
    "             fc2_b=model.fc2.bias)\n",
    "\n",
    "def load_model_dff(model, filename='dffrcnn_model.npz'):\n",
    "    data = np.load(filename)\n",
    "    model.conv1.weights = data['conv1_w']\n",
    "    model.conv1.bias = data['conv1_b']\n",
    "\n",
    "    model.res_block.conv1.weights = data['res_conv1_w']\n",
    "    model.res_block.conv1.bias = data['res_conv1_b']\n",
    "    model.res_block.conv2.weights = data['res_conv2_w']\n",
    "    model.res_block.conv2.bias = data['res_conv2_b']\n",
    "\n",
    "    model.fusion.conv_1x1.weights = data['fusion_conv1x1_w']\n",
    "    model.fusion.conv_1x1.bias = data['fusion_conv1x1_b']\n",
    "    model.fusion.conv_3x3.weights = data['fusion_conv3x3_w']\n",
    "    model.fusion.conv_3x3.bias = data['fusion_conv3x3_b']\n",
    "    model.fusion.alpha = data['fusion_alpha']\n",
    "\n",
    "    model.fc1.weights = data['fc1_w']\n",
    "    model.fc1.bias = data['fc1_b']\n",
    "    model.fc2.weights = data['fc2_w']\n",
    "    model.fc2.bias = data['fc2_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "3vVE09E4Ue6u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 3350.7378, Accuracy = 6.71%\n",
      "Epoch 2: Loss = 2915.8623, Accuracy = 15.18%\n",
      "Epoch 3: Loss = 2694.9769, Accuracy = 19.88%\n"
     ]
    }
   ],
   "source": [
    "train(model, X_train, y_train_oh, epochs=3, batch_size=64, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ggVBXXw1gS1Q"
   },
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "save_model_dff(model, 'dffrcnn_model_epoch3.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "GjFB6IO6Slu8",
    "outputId": "6e8e89f7-ce1f-49ca-81e4-48dc64a02537"
   },
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "model = DFFRCNN()\n",
    "load_model_dff(model, 'dffrcnn_model_epoch3.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "wAl_xIyATIfn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test] Loss: 3.6379, Accuracy: 17.33%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, X_test, y_test_oh)\n",
    "print(f\"[Test] Loss: {test_loss:.4f}, Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

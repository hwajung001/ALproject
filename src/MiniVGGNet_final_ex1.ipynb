{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniVGGNet final Train code\n",
    "\n",
    "### ex 1 : train dataset 150,000 = original + random crop + horizontal flip\n",
    "##### (random seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - CIFAR-100 데이터 다운로드 및 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 already downloaded.\n",
      "Generating augmented dataset with crop + flip...\n",
      "train_cropflip: [(135000, 3, 32, 32), (135000,)]\n",
      "val_cropflip: [(15000, 3, 32, 32), (15000,)]\n",
      "test: [(10000, 3, 32, 32), (10000,)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def download_cifar100(save_path='cifar-100-python'):\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "        return\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
    "    filename = 'cifar-100-python.tar.gz'\n",
    "    print(\"Downloading CIFAR-100...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    with tarfile.open(filename, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "    os.remove(filename)\n",
    "    print(\"Download and extraction completed.\")\n",
    "\n",
    "def load_batch(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "    data = data_dict[b'data']\n",
    "    fine_labels = np.array(data_dict[b'fine_labels'])\n",
    "    data = data.reshape(-1, 3, 32, 32)\n",
    "    return data, fine_labels\n",
    "\n",
    "def normalize_images(images):\n",
    "    mean = np.array([0.5071, 0.4865, 0.4409]).reshape(1, 3, 1, 1)\n",
    "    std  = np.array([0.2673, 0.2564, 0.2762]).reshape(1, 3, 1, 1)\n",
    "    return (images - mean) / std\n",
    "\n",
    "def random_crop(x, crop_size=32, padding=4):\n",
    "    n, c, h, w = x.shape\n",
    "    padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='reflect')\n",
    "    cropped = np.empty((n, c, crop_size, crop_size), dtype=x.dtype)\n",
    "    for i in range(n):\n",
    "        top = np.random.randint(0, padding * 2 + 1)\n",
    "        left = np.random.randint(0, padding * 2 + 1)\n",
    "        cropped[i] = padded[i, :, top:top+crop_size, left:left+crop_size]\n",
    "    return cropped\n",
    "\n",
    "def horizontal_flip(x):\n",
    "    if np.random.rand() < 0.5:\n",
    "        return x[:, :, :, ::-1]\n",
    "    return x\n",
    "    \n",
    "def split_validation(images, labels, val_ratio=0.1):\n",
    "    num_samples = images.shape[0]\n",
    "    val_size = int(num_samples * val_ratio)\n",
    "    idx = np.random.permutation(num_samples)\n",
    "    images = images[idx]\n",
    "    labels = labels[idx]\n",
    "    val_images = images[:val_size]\n",
    "    val_labels = labels[:val_size]\n",
    "    train_images = images[val_size:]\n",
    "    train_labels = labels[val_size:]\n",
    "    return (train_images, train_labels), (val_images, val_labels)\n",
    "\n",
    "def load_cifar100_dataset():\n",
    "    download_cifar100()\n",
    "    train_data, train_fine = load_batch('cifar-100-python/train')\n",
    "    test_data, test_fine = load_batch('cifar-100-python/test')\n",
    "    train_data = normalize_images(train_data)\n",
    "    test_data = normalize_images(test_data)\n",
    "    return (train_data, train_fine), (test_data, test_fine)\n",
    "    \n",
    "def generate_augmented_dataset(images, labels, target_size):\n",
    "    N = images.shape[0]\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    repeat = target_size // (N * 2) + 1  \n",
    "\n",
    "    for _ in range(repeat):\n",
    "        imgs_crop = random_crop(images.copy())\n",
    "        imgs_flip = horizontal_flip(imgs_crop.copy())\n",
    "\n",
    "        augmented_images.append(imgs_crop)\n",
    "        augmented_labels.append(labels.copy())\n",
    "\n",
    "        augmented_images.append(imgs_flip)\n",
    "        augmented_labels.append(labels.copy())\n",
    "\n",
    "        if sum(x.shape[0] for x in augmented_images) >= target_size:\n",
    "            break\n",
    "\n",
    "    X = np.concatenate(augmented_images, axis=0)[:target_size]\n",
    "    y = np.concatenate(augmented_labels, axis=0)[:target_size]\n",
    "    return X, y\n",
    "\n",
    "def prepare_dataset():\n",
    "    (full_train_images, full_train_labels), (test_images, test_labels) = load_cifar100_dataset()\n",
    "    print(\"Generating augmented dataset with crop + flip...\")\n",
    "\n",
    "    X_aug, y_aug = generate_augmented_dataset(full_train_images, full_train_labels, target_size=150000)\n",
    "    train_aug, val_aug = split_validation(X_aug, y_aug)\n",
    "\n",
    "    return {\n",
    "        'train_cropflip': train_aug,\n",
    "        'val_cropflip': val_aug,\n",
    "        'test': (test_images, test_labels)\n",
    "    }\n",
    "    \n",
    "data = prepare_dataset()\n",
    "for k, v in data.items():\n",
    "    if isinstance(v, tuple):\n",
    "        print(f\"{k}: {[x.shape for x in v]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Convolution, BatchNormalization, Relu, Pooling, Affine\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "def fake_quantize(x, num_bits=8):\n",
    "    qmin, qmax = 0., 2.**num_bits - 1.\n",
    "    x_min, x_max = np.min(x), np.max(x)\n",
    "    if x_max == x_min:\n",
    "        return x\n",
    "    scale = (x_max - x_min) / (qmax - qmin)\n",
    "    zero_point = np.clip(np.round(qmin - x_min / scale), qmin, qmax)\n",
    "    q_x = np.clip(np.round(zero_point + x / scale), qmin, qmax)\n",
    "    return scale * (q_x - zero_point)\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.orig_shape = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.orig_shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.orig_shape)\n",
    "\n",
    "\n",
    "class MiniVGGNet:\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_classes=100):\n",
    "        in_channels, _, _ = input_dim\n",
    "        weight_std = np.sqrt(2. / in_channels)\n",
    "\n",
    "        self.conv1 = Convolution(np.random.randn(64, in_channels, 3, 3) * weight_std, np.zeros(64), stride=1, pad=1)\n",
    "        self.bn1   = BatchNormalization(np.ones(64), np.zeros(64))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.conv2 = Convolution(np.random.randn(64, 64, 3, 3) * weight_std, np.zeros(64), stride=1, pad=1)\n",
    "        self.bn2   = BatchNormalization(np.ones(64), np.zeros(64))\n",
    "        self.relu2 = Relu()\n",
    "        self.pool1 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        self.conv3 = Convolution(np.random.randn(128, 64, 3, 3) * weight_std, np.zeros(128), stride=1, pad=1)\n",
    "        self.bn3   = BatchNormalization(np.ones(128), np.zeros(128))\n",
    "        self.relu3 = Relu()\n",
    "\n",
    "        self.conv4 = Convolution(np.random.randn(128, 128, 3, 3) * weight_std, np.zeros(128), stride=1, pad=1)\n",
    "        self.bn4   = BatchNormalization(np.ones(128), np.zeros(128))\n",
    "        self.relu4 = Relu()\n",
    "        self.pool2 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        self.conv5 = Convolution(np.random.randn(256, 128, 3, 3) * weight_std, np.zeros(256), stride=1, pad=1)\n",
    "        self.bn5   = BatchNormalization(np.ones(256), np.zeros(256)) #conv5 also \n",
    "        self.relu5 = Relu()\n",
    "        self.pool3 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Affine(np.random.randn(4096, 512) * weight_std, np.zeros(512))\n",
    "        self.relu6 = Relu()\n",
    "        self.fc2 = Affine(np.random.randn(512, num_classes) * 0.01, np.zeros(num_classes))\n",
    "\n",
    "        self.layers = [\n",
    "            self.conv1, self.bn1, self.relu1,\n",
    "            self.conv2, self.bn2, self.relu2, self.pool1,\n",
    "            self.conv3, self.bn3, self.relu3,\n",
    "            self.conv4, self.bn4, self.relu4, self.pool2,\n",
    "            self.conv5, self.bn5, self.relu5, self.pool3, #conv5\n",
    "            self.flatten, self.fc1, self.relu6, self.fc2\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNormalization):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.fc2.backward(dout)\n",
    "        dout = self.relu6.backward(dout)\n",
    "        dout = self.fc1.backward(dout)\n",
    "        dout = self.flatten.backward(dout)\n",
    "\n",
    "        dout = self.pool3.backward(dout)\n",
    "        dout = self.relu5.backward(dout)\n",
    "        dout = self.bn5.backward(dout)\n",
    "        dout = self.conv5.backward(dout)\n",
    "\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.relu4.backward(dout)\n",
    "        dout = self.bn4.backward(dout)\n",
    "        dout = self.conv4.backward(dout)\n",
    "\n",
    "        dout = self.relu3.backward(dout)\n",
    "        dout = self.bn3.backward(dout)\n",
    "        dout = self.conv3.backward(dout)\n",
    "\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "\n",
    "        return dout\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.fc2.backward(dout)\n",
    "        dout = self.relu6.backward(dout)\n",
    "        dout = self.fc1.backward(dout)\n",
    "        dout = self.flatten.backward(dout)\n",
    "\n",
    "        dout = self.pool3.backward(dout)\n",
    "        dout = self.relu5.backward(dout)\n",
    "        dout = self.bn5.backward(dout)\n",
    "        dout = self.conv5.backward(dout)\n",
    "\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.relu4.backward(dout)\n",
    "        dout = self.bn4.backward(dout)\n",
    "        dout = self.conv4.backward(dout)\n",
    "\n",
    "        dout = self.relu3.backward(dout)\n",
    "        dout = self.bn3.backward(dout)\n",
    "        dout = self.conv3.backward(dout)\n",
    "\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "\n",
    "        grads = {\n",
    "            'W1': self.conv1.W, 'b1': self.conv1.b,\n",
    "            'gamma1': self.bn1.gamma, 'beta1': self.bn1.beta,\n",
    "            'W2': self.conv2.W, 'b2': self.conv2.b,\n",
    "            'gamma2': self.bn2.gamma, 'beta2': self.bn2.beta,\n",
    "            'W3': self.conv3.W, 'b3': self.conv3.b,\n",
    "            'gamma3': self.bn3.gamma, 'beta3': self.bn3.beta,\n",
    "            'W4': self.conv4.W, 'b4': self.conv4.b,\n",
    "            'gamma4': self.bn4.gamma, 'beta4': self.bn4.beta,\n",
    "            'W5': self.conv5.W, 'b5': self.conv5.b,\n",
    "            'gamma5': self.bn5.gamma, 'beta5': self.bn5.beta,  \n",
    "            'W6': self.fc1.W, 'b6': self.fc1.b,\n",
    "            'W7': self.fc2.W, 'b7': self.fc2.b,\n",
    "        }\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def predict(self, x, batch_size=100):\n",
    "        return np.concatenate([self.forward(x[i:i+batch_size], False) for i in range(0, x.shape[0], batch_size)], axis=0)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.forward(x, True)\n",
    "        y_softmax = softmax(y)  \n",
    "        return cross_entropy_error(y_softmax, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        pred = np.argmax(self.predict(x, batch_size), axis=1)\n",
    "        true = t if t.ndim == 1 else np.argmax(t, axis=1)\n",
    "        return np.mean(pred == true)\n",
    "\n",
    "    def clip_weights(self, clip_value=1.0): \n",
    "        for layer in [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.fc1, self.fc2]:\n",
    "            layer.W = np.clip(layer.W, -clip_value, clip_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 구조 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type)                       Output Shape                Param #\n",
      "===========================================================================\n",
      " 1. Conv1                           (1, 64, 32, 32)               1,792\n",
      " 2. BN1                             (1, 64, 32, 32)                 128\n",
      " 3. Conv2                           (1, 64, 32, 32)              36,928\n",
      " 4. BN2                             (1, 64, 32, 32)                 128\n",
      " 5. Conv3                           (1, 128, 16, 16)             73,856\n",
      " 6. BN3                             (1, 128, 16, 16)                256\n",
      " 7. Conv4                           (1, 128, 16, 16)            147,584\n",
      " 8. BN4                             (1, 128, 16, 16)                256\n",
      " 9. Conv5                           (1, 256, 8, 8)              295,168\n",
      "10. Flatten                         (1, 4096)                         0\n",
      "11. FC1                             (1, 512)                  2,097,664\n",
      "12. FC2                             (1, 100)                     51,300\n",
      "===========================================================================\n",
      "Total weight layers:                                        13\n",
      "Total params:                                               2,705,060\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "from common.layers import Convolution, BatchNormalization, Relu, Pooling, Affine\n",
    "import numpy as np\n",
    "\n",
    "def count_params(layer):\n",
    "    count = 0\n",
    "    if hasattr(layer, 'W'):\n",
    "        count += np.prod(layer.W.shape)\n",
    "    if hasattr(layer, 'b'):\n",
    "        count += np.prod(layer.b.shape)\n",
    "    if hasattr(layer, 'gamma'):\n",
    "        count += np.prod(layer.gamma.shape)\n",
    "    if hasattr(layer, 'beta'):\n",
    "        count += np.prod(layer.beta.shape)\n",
    "    return count\n",
    "\n",
    "def print_vggnet_summary(model, input_shape=(1, 3, 32, 32)):\n",
    "    print(\"=\" * 75)\n",
    "    print(f\"{'Layer (type)':<35}{'Output Shape':<25}{'Param #':>10}\")\n",
    "    print(\"=\" * 75)\n",
    "\n",
    "    x = np.zeros(input_shape)\n",
    "    total_params = 0\n",
    "    layer_idx = 1\n",
    "\n",
    "    def log(name, x, p):\n",
    "        nonlocal total_params, layer_idx\n",
    "        print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\")\n",
    "        total_params += p\n",
    "        layer_idx += 1\n",
    "\n",
    "    x = model.conv1.forward(x)\n",
    "    log(\"Conv1\", x, count_params(model.conv1))\n",
    "    x = model.bn1.forward(x, train_flg=False)\n",
    "    log(\"BN1\", x, count_params(model.bn1))\n",
    "    x = model.relu1.forward(x)\n",
    "\n",
    "    x = model.conv2.forward(x)\n",
    "    log(\"Conv2\", x, count_params(model.conv2))\n",
    "    x = model.bn2.forward(x, train_flg=False)\n",
    "    log(\"BN2\", x, count_params(model.bn2))\n",
    "    x = model.relu2.forward(x)\n",
    "    x = model.pool1.forward(x)\n",
    "\n",
    "    x = model.conv3.forward(x)\n",
    "    log(\"Conv3\", x, count_params(model.conv3))\n",
    "    x = model.bn3.forward(x, train_flg=False)\n",
    "    log(\"BN3\", x, count_params(model.bn3))\n",
    "    x = model.relu3.forward(x)\n",
    "\n",
    "    x = model.conv4.forward(x)\n",
    "    log(\"Conv4\", x, count_params(model.conv4))\n",
    "    x = model.bn4.forward(x, train_flg=False)\n",
    "    log(\"BN4\", x, count_params(model.bn4))\n",
    "    x = model.relu4.forward(x)\n",
    "    x = model.pool2.forward(x)\n",
    "\n",
    "    x = model.conv5.forward(x)\n",
    "    log(\"Conv5\", x, count_params(model.conv5))\n",
    "    x = model.relu5.forward(x)\n",
    "    x = model.pool3.forward(x)\n",
    "\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    log(\"Flatten\", x, 0)\n",
    "\n",
    "    x = model.fc1.forward(x)\n",
    "    log(\"FC1\", x, count_params(model.fc1))\n",
    "    x = model.relu6.forward(x)\n",
    "    x = model.fc2.forward(x)\n",
    "    log(\"FC2\", x, count_params(model.fc2))\n",
    "\n",
    "    print(\"=\" * 75)\n",
    "    print(f\"{'Total weight layers:':<60}{layer_idx}\")\n",
    "    print(f\"{'Total params:':<60}{total_params:,}\")\n",
    "    print(\"=\" * 75)\n",
    "\n",
    "model = MiniVGGNet()\n",
    "print_vggnet_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from common.optimizer import Adam\n",
    "from common.functions import softmax\n",
    "\n",
    "def smooth_labels(y, smoothing=0.1, num_classes=100):\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = (y.shape[0], num_classes)\n",
    "    smooth = np.full(label_shape, smoothing / (num_classes - 1))\n",
    "    smooth[np.arange(y.shape[0]), y] = confidence\n",
    "    return smooth\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name,\n",
    "                 train_data, val_data, test_data,\n",
    "                 epochs=20, batch_size=64, lr=0.01,\n",
    "                 smoothing=0.15):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.val_x, self.val_t = val_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "        self.optimizer = Adam(lr=lr)\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def smooth_labels(self, y, num_classes=100):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        label_shape = (y.shape[0], num_classes)\n",
    "        smooth = np.full(label_shape, self.smoothing / (num_classes - 1), dtype=np.float32)\n",
    "        smooth[np.arange(y.shape[0]), y] = confidence\n",
    "        return smooth\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            dx = (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            dx /= batch_size\n",
    "        return dx, y\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "\n",
    "        for name in ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc1', 'fc2']:\n",
    "            layer = getattr(self.model, name)\n",
    "            if hasattr(layer, 'W'):\n",
    "                param_dict[f'{name}_W'] = layer.W\n",
    "                param_dict[f'{name}_b'] = layer.b\n",
    "                grad_dict[f'{name}_W'] = layer.dW\n",
    "                grad_dict[f'{name}_b'] = layer.db\n",
    "\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def get_bn_param_dict(self):\n",
    "        bn_dict = {}\n",
    "        idx = 0\n",
    "        for name in ['bn1', 'bn2', 'bn3', 'bn4', 'bn5']:\n",
    "            bn = getattr(self.model, name)\n",
    "            bn_dict[f'bn{idx}_gamma'] = bn.gamma.copy()\n",
    "            bn_dict[f'bn{idx}_beta'] = bn.beta.copy()\n",
    "            bn_dict[f'bn{idx}_running_mean'] = bn.running_mean.copy()\n",
    "            bn_dict[f'bn{idx}_running_var'] = bn.running_var.copy()\n",
    "            idx += 1\n",
    "        return bn_dict\n",
    "    \n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        if t_batch.ndim == 1:\n",
    "            t_batch = self.smooth_labels(t_batch)\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        dx, _ = self.loss_grad(x_batch, t_batch)\n",
    "\n",
    "        self.model.backward(dx)\n",
    "\n",
    "        if hasattr(self.model, 'clip_weights'):\n",
    "            self.model.clip_weights(clip_value=1.0)\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        patience = 10\n",
    "        best_val_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\n[Epoch {epoch + 1}/{self.epochs}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0 or i == self.iter_per_epoch - 1:\n",
    "                    print(f\"  Iter {i+1:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            val_acc = self.model.accuracy(self.val_x, self.val_t)\n",
    "            val_loss = self.batched_loss(self.val_x, self.val_t, batch_size=128)\n",
    "\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.val_acc_list.append(val_acc)\n",
    "            self.val_loss_list.append(val_loss)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Fine Train Loss: {avg_loss:.4f}, Fine Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}\", flush=True)\n",
    "            print(f\"Time: {elapsed:.2f}s\", flush=True)\n",
    "\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_model(f\"{self.model_name}_epoch{epoch+1}.pkl\")\n",
    "                print(f\">>> Model saved to {self.model_name}_epoch{epoch+1}.pkl\", flush=True)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improve_count = 0\n",
    "                self.save_model(f\"{self.model_name}_best.pkl\")\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "    def batched_loss(self, x, t, batch_size=128):\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        num_classes = 100  \n",
    "    \n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "    \n",
    "            if t_batch.ndim == 1:\n",
    "                t_onehot = np.zeros((t_batch.size, num_classes), dtype=np.float32)\n",
    "                t_onehot[np.arange(t_batch.size), t_batch] = 1.0\n",
    "            else:\n",
    "                t_onehot = t_batch  \n",
    "    \n",
    "            loss = self.model.loss(x_batch, t_onehot)\n",
    "            total_loss += loss * len(x_batch)\n",
    "            total_count += len(x_batch)\n",
    "    \n",
    "        return total_loss / total_count\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "        bn_state = self.get_bn_param_dict()\n",
    "        \n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': self.optimizer.beta1,\n",
    "            'beta2': self.optimizer.beta2,\n",
    "            'm': self.optimizer.m,\n",
    "            'v': self.optimizer.v,\n",
    "            't': self.optimizer.iter\n",
    "        }\n",
    "\n",
    "        save_data = {\n",
    "            'model': model_state,\n",
    "            'bn': bn_state,\n",
    "            'optimizer': optimizer_state,\n",
    "            'train_loss_list': self.train_loss_list,\n",
    "            'train_acc_list': self.train_acc_list,\n",
    "            'val_acc_list': self.val_acc_list,\n",
    "            'val_loss_list': self.val_loss_list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename,\n",
    "                 loss=np.array(self.train_loss_list),\n",
    "                 train_acc=np.array(self.train_acc_list),\n",
    "                 val_acc=np.array(self.val_acc_list),\n",
    "                 val_loss=np.array(self.val_loss_list))\n",
    "        print(f\"Log saved to {filename}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Running ex1 : train dataset 150,000 = original + random crop + horizontal flip ====\n",
      "\n",
      "[Epoch 1/100]\n",
      "  Iter   1/2109: Loss 14.5565\n",
      "  Iter  11/2109: Loss 11.3691\n",
      "  Iter  21/2109: Loss 8.4269\n",
      "  Iter  31/2109: Loss 7.5838\n",
      "  Iter  41/2109: Loss 5.1357\n",
      "  Iter  51/2109: Loss 4.7611\n",
      "  Iter  61/2109: Loss 4.2946\n",
      "  Iter  71/2109: Loss 4.8646\n",
      "  Iter  81/2109: Loss 4.4630\n",
      "  Iter  91/2109: Loss 4.2320\n",
      "  Iter 101/2109: Loss 4.2025\n",
      "  Iter 111/2109: Loss 4.1275\n",
      "  Iter 121/2109: Loss 3.9764\n",
      "  Iter 131/2109: Loss 4.3469\n",
      "  Iter 141/2109: Loss 4.1311\n",
      "  Iter 151/2109: Loss 4.1309\n",
      "  Iter 161/2109: Loss 4.0722\n",
      "  Iter 171/2109: Loss 3.9450\n",
      "  Iter 181/2109: Loss 4.1979\n",
      "  Iter 191/2109: Loss 3.7491\n",
      "  Iter 201/2109: Loss 4.0138\n",
      "  Iter 211/2109: Loss 3.7739\n",
      "  Iter 221/2109: Loss 3.8354\n",
      "  Iter 231/2109: Loss 3.6513\n",
      "  Iter 241/2109: Loss 3.7886\n",
      "  Iter 251/2109: Loss 3.5014\n",
      "  Iter 261/2109: Loss 4.1756\n",
      "  Iter 271/2109: Loss 3.8946\n",
      "  Iter 281/2109: Loss 4.1404\n",
      "  Iter 291/2109: Loss 3.9581\n",
      "  Iter 301/2109: Loss 4.0970\n",
      "  Iter 311/2109: Loss 3.8373\n",
      "  Iter 321/2109: Loss 4.1568\n",
      "  Iter 331/2109: Loss 3.8929\n",
      "  Iter 341/2109: Loss 3.7358\n",
      "  Iter 351/2109: Loss 3.9676\n",
      "  Iter 361/2109: Loss 3.5792\n",
      "  Iter 371/2109: Loss 3.8687\n",
      "  Iter 381/2109: Loss 3.7599\n",
      "  Iter 391/2109: Loss 3.7921\n",
      "  Iter 401/2109: Loss 3.8532\n",
      "  Iter 411/2109: Loss 3.8446\n",
      "  Iter 421/2109: Loss 3.7126\n",
      "  Iter 431/2109: Loss 3.4477\n",
      "  Iter 441/2109: Loss 3.7222\n",
      "  Iter 451/2109: Loss 3.5071\n",
      "  Iter 461/2109: Loss 3.5323\n",
      "  Iter 471/2109: Loss 3.6167\n",
      "  Iter 481/2109: Loss 3.8282\n",
      "  Iter 491/2109: Loss 3.9034\n",
      "  Iter 501/2109: Loss 3.8051\n",
      "  Iter 511/2109: Loss 3.6133\n",
      "  Iter 521/2109: Loss 3.9421\n",
      "  Iter 531/2109: Loss 3.6128\n",
      "  Iter 541/2109: Loss 3.5166\n",
      "  Iter 551/2109: Loss 3.6422\n",
      "  Iter 561/2109: Loss 3.3930\n",
      "  Iter 571/2109: Loss 3.9265\n",
      "  Iter 581/2109: Loss 3.7285\n",
      "  Iter 591/2109: Loss 3.7488\n",
      "  Iter 601/2109: Loss 3.6967\n",
      "  Iter 611/2109: Loss 3.6913\n",
      "  Iter 621/2109: Loss 3.2596\n",
      "  Iter 631/2109: Loss 3.8271\n",
      "  Iter 641/2109: Loss 3.4691\n",
      "  Iter 651/2109: Loss 3.3715\n",
      "  Iter 661/2109: Loss 3.7318\n",
      "  Iter 671/2109: Loss 3.4513\n",
      "  Iter 681/2109: Loss 3.4761\n",
      "  Iter 691/2109: Loss 3.5431\n",
      "  Iter 701/2109: Loss 3.6151\n",
      "  Iter 711/2109: Loss 3.6243\n",
      "  Iter 721/2109: Loss 3.3747\n",
      "  Iter 731/2109: Loss 3.7194\n",
      "  Iter 741/2109: Loss 3.7696\n",
      "  Iter 751/2109: Loss 3.1335\n",
      "  Iter 761/2109: Loss 3.4243\n",
      "  Iter 771/2109: Loss 3.6560\n",
      "  Iter 781/2109: Loss 3.5571\n",
      "  Iter 791/2109: Loss 3.2107\n",
      "  Iter 801/2109: Loss 3.5878\n",
      "  Iter 811/2109: Loss 3.3071\n",
      "  Iter 821/2109: Loss 3.0774\n",
      "  Iter 831/2109: Loss 3.3467\n",
      "  Iter 841/2109: Loss 3.5013\n",
      "  Iter 851/2109: Loss 3.3746\n",
      "  Iter 861/2109: Loss 3.8528\n",
      "  Iter 871/2109: Loss 3.4400\n",
      "  Iter 881/2109: Loss 3.3456\n",
      "  Iter 891/2109: Loss 3.2557\n",
      "  Iter 901/2109: Loss 3.4844\n",
      "  Iter 911/2109: Loss 3.7241\n",
      "  Iter 921/2109: Loss 3.4223\n",
      "  Iter 931/2109: Loss 3.5842\n",
      "  Iter 941/2109: Loss 3.3589\n",
      "  Iter 951/2109: Loss 3.4260\n",
      "  Iter 961/2109: Loss 3.2955\n",
      "  Iter 971/2109: Loss 3.7371\n",
      "  Iter 981/2109: Loss 3.6576\n",
      "  Iter 991/2109: Loss 3.2523\n",
      "  Iter 1001/2109: Loss 3.2400\n",
      "  Iter 1011/2109: Loss 3.3656\n",
      "  Iter 1021/2109: Loss 3.3051\n",
      "  Iter 1031/2109: Loss 3.7159\n",
      "  Iter 1041/2109: Loss 3.4520\n",
      "  Iter 1051/2109: Loss 3.5333\n",
      "  Iter 1061/2109: Loss 3.6109\n",
      "  Iter 1071/2109: Loss 3.3189\n",
      "  Iter 1081/2109: Loss 3.1150\n",
      "  Iter 1091/2109: Loss 3.4482\n",
      "  Iter 1101/2109: Loss 3.7084\n",
      "  Iter 1111/2109: Loss 3.2794\n",
      "  Iter 1121/2109: Loss 3.6590\n",
      "  Iter 1131/2109: Loss 3.2215\n",
      "  Iter 1141/2109: Loss 3.5942\n",
      "  Iter 1151/2109: Loss 3.5029\n",
      "  Iter 1161/2109: Loss 3.4474\n",
      "  Iter 1171/2109: Loss 3.3559\n",
      "  Iter 1181/2109: Loss 3.8426\n",
      "  Iter 1191/2109: Loss 3.2169\n",
      "  Iter 1201/2109: Loss 3.6337\n",
      "  Iter 1211/2109: Loss 3.4529\n",
      "  Iter 1221/2109: Loss 3.4908\n",
      "  Iter 1231/2109: Loss 3.1319\n",
      "  Iter 1241/2109: Loss 3.4343\n",
      "  Iter 1251/2109: Loss 3.1981\n",
      "  Iter 1261/2109: Loss 3.2627\n",
      "  Iter 1271/2109: Loss 3.1160\n",
      "  Iter 1281/2109: Loss 3.7104\n",
      "  Iter 1291/2109: Loss 3.2848\n",
      "  Iter 1301/2109: Loss 3.2242\n",
      "  Iter 1311/2109: Loss 3.3418\n",
      "  Iter 1321/2109: Loss 3.2003\n",
      "  Iter 1331/2109: Loss 2.9917\n",
      "  Iter 1341/2109: Loss 3.1050\n",
      "  Iter 1351/2109: Loss 3.2282\n",
      "  Iter 1361/2109: Loss 3.4405\n",
      "  Iter 1371/2109: Loss 3.3633\n",
      "  Iter 1381/2109: Loss 3.3478\n",
      "  Iter 1391/2109: Loss 3.1271\n",
      "  Iter 1401/2109: Loss 3.3381\n",
      "  Iter 1411/2109: Loss 3.5404\n",
      "  Iter 1421/2109: Loss 3.4286\n",
      "  Iter 1431/2109: Loss 3.3683\n",
      "  Iter 1441/2109: Loss 3.2251\n",
      "  Iter 1451/2109: Loss 2.8111\n",
      "  Iter 1461/2109: Loss 3.5621\n",
      "  Iter 1471/2109: Loss 3.3450\n",
      "  Iter 1481/2109: Loss 3.5318\n",
      "  Iter 1491/2109: Loss 3.3819\n",
      "  Iter 1501/2109: Loss 3.1350\n",
      "  Iter 1511/2109: Loss 3.2242\n",
      "  Iter 1521/2109: Loss 3.6008\n",
      "  Iter 1531/2109: Loss 3.0317\n",
      "  Iter 1541/2109: Loss 3.0800\n",
      "  Iter 1551/2109: Loss 3.4733\n",
      "  Iter 1561/2109: Loss 3.0803\n",
      "  Iter 1571/2109: Loss 3.4390\n",
      "  Iter 1581/2109: Loss 3.2571\n",
      "  Iter 1591/2109: Loss 3.0540\n",
      "  Iter 1601/2109: Loss 3.1504\n",
      "  Iter 1611/2109: Loss 3.5263\n",
      "  Iter 1621/2109: Loss 2.8507\n",
      "  Iter 1631/2109: Loss 3.4481\n",
      "  Iter 1641/2109: Loss 3.2732\n",
      "  Iter 1651/2109: Loss 3.4661\n",
      "  Iter 1661/2109: Loss 3.4018\n",
      "  Iter 1671/2109: Loss 3.1863\n",
      "  Iter 1681/2109: Loss 3.5805\n",
      "  Iter 1691/2109: Loss 3.2924\n",
      "  Iter 1701/2109: Loss 3.0400\n",
      "  Iter 1711/2109: Loss 3.8802\n",
      "  Iter 1721/2109: Loss 3.2854\n",
      "  Iter 1731/2109: Loss 3.6109\n",
      "  Iter 1741/2109: Loss 3.1654\n",
      "  Iter 1751/2109: Loss 3.1672\n",
      "  Iter 1761/2109: Loss 3.0062\n",
      "  Iter 1771/2109: Loss 3.4039\n",
      "  Iter 1781/2109: Loss 3.5850\n",
      "  Iter 1791/2109: Loss 3.6084\n",
      "  Iter 1801/2109: Loss 3.1420\n",
      "  Iter 1811/2109: Loss 3.7740\n",
      "  Iter 1821/2109: Loss 3.4242\n",
      "  Iter 1831/2109: Loss 3.3347\n",
      "  Iter 1841/2109: Loss 3.3257\n",
      "  Iter 1851/2109: Loss 2.8913\n",
      "  Iter 1861/2109: Loss 3.0355\n",
      "  Iter 1871/2109: Loss 3.3925\n",
      "  Iter 1881/2109: Loss 3.5450\n",
      "  Iter 1891/2109: Loss 2.9252\n",
      "  Iter 1901/2109: Loss 3.5567\n",
      "  Iter 1911/2109: Loss 3.4926\n",
      "  Iter 1921/2109: Loss 3.4646\n",
      "  Iter 1931/2109: Loss 3.3396\n",
      "  Iter 1941/2109: Loss 3.0776\n",
      "  Iter 1951/2109: Loss 3.3436\n",
      "  Iter 1961/2109: Loss 3.1644\n",
      "  Iter 1971/2109: Loss 3.1440\n",
      "  Iter 1981/2109: Loss 3.3228\n",
      "  Iter 1991/2109: Loss 3.2734\n",
      "  Iter 2001/2109: Loss 3.3766\n",
      "  Iter 2011/2109: Loss 3.2535\n",
      "  Iter 2021/2109: Loss 3.1361\n",
      "  Iter 2031/2109: Loss 3.1618\n",
      "  Iter 2041/2109: Loss 2.9621\n",
      "  Iter 2051/2109: Loss 3.4472\n",
      "  Iter 2061/2109: Loss 3.5767\n",
      "  Iter 2071/2109: Loss 3.4408\n",
      "  Iter 2081/2109: Loss 2.8850\n",
      "  Iter 2091/2109: Loss 3.4353\n",
      "  Iter 2101/2109: Loss 3.0400\n",
      "  Iter 2109/2109: Loss 3.1728\n",
      "Fine Train Loss: 3.6257, Fine Train Acc: 0.2210, Val Acc: 0.1990, Val Loss: 3.3577\n",
      "Time: 7802.38s\n",
      "\n",
      "[Epoch 2/100]\n",
      "  Iter   1/2109: Loss 3.2455\n",
      "  Iter  11/2109: Loss 3.1742\n",
      "  Iter  21/2109: Loss 3.1422\n",
      "  Iter  31/2109: Loss 3.1477\n",
      "  Iter  41/2109: Loss 3.1013\n",
      "  Iter  51/2109: Loss 3.1406\n",
      "  Iter  61/2109: Loss 3.3137\n",
      "  Iter  71/2109: Loss 3.0288\n",
      "  Iter  81/2109: Loss 3.1394\n",
      "  Iter  91/2109: Loss 3.1429\n",
      "  Iter 101/2109: Loss 3.1487\n",
      "  Iter 111/2109: Loss 3.1535\n",
      "  Iter 121/2109: Loss 3.4048\n",
      "  Iter 131/2109: Loss 3.4541\n",
      "  Iter 141/2109: Loss 3.0878\n",
      "  Iter 151/2109: Loss 3.0101\n",
      "  Iter 161/2109: Loss 3.1602\n",
      "  Iter 171/2109: Loss 3.3632\n",
      "  Iter 181/2109: Loss 3.5836\n",
      "  Iter 191/2109: Loss 3.1402\n",
      "  Iter 201/2109: Loss 3.2643\n",
      "  Iter 211/2109: Loss 3.2226\n",
      "  Iter 221/2109: Loss 3.1960\n",
      "  Iter 231/2109: Loss 2.7727\n",
      "  Iter 241/2109: Loss 3.2678\n",
      "  Iter 251/2109: Loss 2.6444\n",
      "  Iter 261/2109: Loss 3.0167\n",
      "  Iter 271/2109: Loss 3.3723\n",
      "  Iter 281/2109: Loss 3.5007\n",
      "  Iter 291/2109: Loss 3.1501\n",
      "  Iter 301/2109: Loss 3.0228\n",
      "  Iter 311/2109: Loss 3.1128\n",
      "  Iter 321/2109: Loss 3.6077\n",
      "  Iter 331/2109: Loss 3.2590\n",
      "  Iter 341/2109: Loss 2.8915\n",
      "  Iter 351/2109: Loss 2.9124\n",
      "  Iter 361/2109: Loss 3.0119\n",
      "  Iter 371/2109: Loss 3.0974\n",
      "  Iter 381/2109: Loss 2.9766\n",
      "  Iter 391/2109: Loss 3.0672\n",
      "  Iter 401/2109: Loss 3.3272\n",
      "  Iter 411/2109: Loss 3.2498\n",
      "  Iter 421/2109: Loss 3.1390\n",
      "  Iter 431/2109: Loss 3.0358\n",
      "  Iter 441/2109: Loss 3.1248\n",
      "  Iter 451/2109: Loss 2.8874\n",
      "  Iter 461/2109: Loss 3.3988\n",
      "  Iter 471/2109: Loss 3.1601\n",
      "  Iter 481/2109: Loss 3.0710\n",
      "  Iter 491/2109: Loss 3.0672\n",
      "  Iter 501/2109: Loss 3.1936\n",
      "  Iter 511/2109: Loss 2.9501\n",
      "  Iter 521/2109: Loss 3.4352\n",
      "  Iter 531/2109: Loss 3.2745\n",
      "  Iter 541/2109: Loss 2.6720\n",
      "  Iter 551/2109: Loss 2.7186\n",
      "  Iter 561/2109: Loss 2.9769\n",
      "  Iter 571/2109: Loss 3.2703\n",
      "  Iter 581/2109: Loss 2.9558\n",
      "  Iter 591/2109: Loss 3.0345\n",
      "  Iter 601/2109: Loss 2.8506\n",
      "  Iter 611/2109: Loss 3.1539\n",
      "  Iter 621/2109: Loss 2.9906\n",
      "  Iter 631/2109: Loss 3.2917\n",
      "  Iter 641/2109: Loss 3.1207\n",
      "  Iter 651/2109: Loss 2.8715\n",
      "  Iter 661/2109: Loss 3.0272\n",
      "  Iter 671/2109: Loss 2.8692\n",
      "  Iter 681/2109: Loss 3.4689\n",
      "  Iter 691/2109: Loss 2.9578\n",
      "  Iter 701/2109: Loss 3.5553\n",
      "  Iter 711/2109: Loss 2.9441\n",
      "  Iter 721/2109: Loss 3.3028\n",
      "  Iter 731/2109: Loss 2.6416\n",
      "  Iter 741/2109: Loss 3.3285\n",
      "  Iter 751/2109: Loss 3.2947\n",
      "  Iter 761/2109: Loss 3.6595\n",
      "  Iter 771/2109: Loss 2.6757\n",
      "  Iter 781/2109: Loss 3.2904\n",
      "  Iter 791/2109: Loss 3.2974\n",
      "  Iter 801/2109: Loss 3.1698\n",
      "  Iter 811/2109: Loss 2.8694\n",
      "  Iter 821/2109: Loss 2.8780\n",
      "  Iter 831/2109: Loss 2.8686\n",
      "  Iter 841/2109: Loss 3.4394\n",
      "  Iter 851/2109: Loss 3.0008\n",
      "  Iter 861/2109: Loss 3.2432\n",
      "  Iter 871/2109: Loss 2.7850\n",
      "  Iter 881/2109: Loss 2.9825\n",
      "  Iter 891/2109: Loss 2.9951\n",
      "  Iter 901/2109: Loss 2.9803\n",
      "  Iter 911/2109: Loss 2.8507\n",
      "  Iter 921/2109: Loss 3.0351\n",
      "  Iter 931/2109: Loss 3.0746\n",
      "  Iter 941/2109: Loss 3.3122\n",
      "  Iter 951/2109: Loss 2.8155\n",
      "  Iter 961/2109: Loss 2.8955\n",
      "  Iter 971/2109: Loss 3.1936\n",
      "  Iter 981/2109: Loss 2.8244\n",
      "  Iter 991/2109: Loss 3.3427\n",
      "  Iter 1001/2109: Loss 3.0944\n",
      "  Iter 1011/2109: Loss 3.3694\n",
      "  Iter 1021/2109: Loss 3.3559\n",
      "  Iter 1031/2109: Loss 3.5502\n",
      "  Iter 1041/2109: Loss 2.9868\n",
      "  Iter 1051/2109: Loss 3.1699\n",
      "  Iter 1061/2109: Loss 2.9324\n",
      "  Iter 1071/2109: Loss 3.3879\n",
      "  Iter 1081/2109: Loss 3.1434\n",
      "  Iter 1091/2109: Loss 3.0740\n",
      "  Iter 1101/2109: Loss 2.9578\n",
      "  Iter 1111/2109: Loss 3.4300\n",
      "  Iter 1121/2109: Loss 3.1074\n",
      "  Iter 1131/2109: Loss 3.0192\n",
      "  Iter 1141/2109: Loss 2.9714\n",
      "  Iter 1151/2109: Loss 3.0319\n",
      "  Iter 1161/2109: Loss 3.1300\n",
      "  Iter 1171/2109: Loss 2.9498\n",
      "  Iter 1181/2109: Loss 2.9508\n",
      "  Iter 1191/2109: Loss 3.0393\n",
      "  Iter 1201/2109: Loss 3.2645\n",
      "  Iter 1211/2109: Loss 2.8815\n",
      "  Iter 1221/2109: Loss 3.1650\n",
      "  Iter 1231/2109: Loss 3.2957\n",
      "  Iter 1241/2109: Loss 3.3343\n",
      "  Iter 1251/2109: Loss 3.0803\n",
      "  Iter 1261/2109: Loss 3.2844\n",
      "  Iter 1271/2109: Loss 3.2885\n",
      "  Iter 1281/2109: Loss 3.0451\n",
      "  Iter 1291/2109: Loss 2.9777\n",
      "  Iter 1301/2109: Loss 3.1316\n",
      "  Iter 1311/2109: Loss 2.9817\n",
      "  Iter 1321/2109: Loss 2.3928\n",
      "  Iter 1331/2109: Loss 3.3016\n",
      "  Iter 1341/2109: Loss 3.0751\n",
      "  Iter 1351/2109: Loss 2.8534\n",
      "  Iter 1361/2109: Loss 3.0562\n",
      "  Iter 1371/2109: Loss 2.8474\n",
      "  Iter 1381/2109: Loss 2.8789\n",
      "  Iter 1391/2109: Loss 3.0602\n",
      "  Iter 1401/2109: Loss 2.9364\n",
      "  Iter 1411/2109: Loss 2.9561\n",
      "  Iter 1421/2109: Loss 2.8608\n",
      "  Iter 1431/2109: Loss 2.8895\n",
      "  Iter 1441/2109: Loss 2.7492\n",
      "  Iter 1451/2109: Loss 3.1655\n",
      "  Iter 1461/2109: Loss 3.2069\n",
      "  Iter 1471/2109: Loss 3.0257\n",
      "  Iter 1481/2109: Loss 2.9490\n",
      "  Iter 1491/2109: Loss 2.8845\n",
      "  Iter 1501/2109: Loss 2.8409\n",
      "  Iter 1511/2109: Loss 3.0423\n",
      "  Iter 1521/2109: Loss 2.6846\n",
      "  Iter 1531/2109: Loss 3.1409\n",
      "  Iter 1541/2109: Loss 3.2685\n",
      "  Iter 1551/2109: Loss 3.1464\n",
      "  Iter 1561/2109: Loss 3.0039\n",
      "  Iter 1571/2109: Loss 2.8447\n",
      "  Iter 1581/2109: Loss 2.7823\n",
      "  Iter 1591/2109: Loss 2.9770\n",
      "  Iter 1601/2109: Loss 3.1640\n",
      "  Iter 1611/2109: Loss 2.9465\n",
      "  Iter 1621/2109: Loss 3.0527\n",
      "  Iter 1631/2109: Loss 3.1780\n",
      "  Iter 1641/2109: Loss 3.3146\n",
      "  Iter 1651/2109: Loss 2.9835\n",
      "  Iter 1661/2109: Loss 2.9001\n",
      "  Iter 1671/2109: Loss 3.2480\n",
      "  Iter 1681/2109: Loss 3.0178\n",
      "  Iter 1691/2109: Loss 2.9396\n",
      "  Iter 1701/2109: Loss 3.0817\n",
      "  Iter 1711/2109: Loss 3.0699\n",
      "  Iter 1721/2109: Loss 3.1380\n",
      "  Iter 1731/2109: Loss 3.2130\n",
      "  Iter 1741/2109: Loss 2.8932\n",
      "  Iter 1751/2109: Loss 2.7647\n",
      "  Iter 1761/2109: Loss 2.8403\n",
      "  Iter 1771/2109: Loss 2.8517\n",
      "  Iter 1781/2109: Loss 3.1798\n",
      "  Iter 1791/2109: Loss 3.0200\n",
      "  Iter 1801/2109: Loss 2.8448\n",
      "  Iter 1811/2109: Loss 2.9512\n",
      "  Iter 1821/2109: Loss 2.9864\n",
      "  Iter 1831/2109: Loss 2.9901\n",
      "  Iter 1841/2109: Loss 3.0468\n",
      "  Iter 1851/2109: Loss 2.9527\n",
      "  Iter 1861/2109: Loss 3.3256\n",
      "  Iter 1871/2109: Loss 2.6661\n",
      "  Iter 1881/2109: Loss 2.9129\n",
      "  Iter 1891/2109: Loss 2.7130\n",
      "  Iter 1901/2109: Loss 3.0080\n",
      "  Iter 1911/2109: Loss 3.1967\n",
      "  Iter 1921/2109: Loss 2.9388\n",
      "  Iter 1931/2109: Loss 2.6284\n",
      "  Iter 1941/2109: Loss 3.0839\n",
      "  Iter 1951/2109: Loss 2.8717\n",
      "  Iter 1961/2109: Loss 2.6108\n",
      "  Iter 1971/2109: Loss 2.6742\n",
      "  Iter 1981/2109: Loss 2.5981\n",
      "  Iter 1991/2109: Loss 2.8356\n",
      "  Iter 2001/2109: Loss 2.8283\n",
      "  Iter 2011/2109: Loss 2.9066\n",
      "  Iter 2021/2109: Loss 2.7149\n",
      "  Iter 2031/2109: Loss 3.0287\n",
      "  Iter 2041/2109: Loss 3.0530\n",
      "  Iter 2051/2109: Loss 2.6940\n",
      "  Iter 2061/2109: Loss 2.8056\n",
      "  Iter 2071/2109: Loss 3.2386\n",
      "  Iter 2081/2109: Loss 2.7334\n",
      "  Iter 2091/2109: Loss 2.8378\n",
      "  Iter 2101/2109: Loss 2.6410\n",
      "  Iter 2109/2109: Loss 3.3727\n",
      "Fine Train Loss: 3.0623, Fine Train Acc: 0.3000, Val Acc: 0.2481, Val Loss: 3.1042\n",
      "Time: 7795.25s\n",
      "\n",
      "[Epoch 3/100]\n",
      "  Iter   1/2109: Loss 3.1432\n",
      "  Iter  11/2109: Loss 2.9915\n",
      "  Iter  21/2109: Loss 2.5520\n",
      "  Iter  31/2109: Loss 2.6775\n",
      "  Iter  41/2109: Loss 3.3594\n",
      "  Iter  51/2109: Loss 2.7836\n",
      "  Iter  61/2109: Loss 2.6319\n",
      "  Iter  71/2109: Loss 3.2223\n",
      "  Iter  81/2109: Loss 3.2871\n",
      "  Iter  91/2109: Loss 2.6137\n",
      "  Iter 101/2109: Loss 2.8023\n",
      "  Iter 111/2109: Loss 2.7400\n",
      "  Iter 121/2109: Loss 2.7267\n",
      "  Iter 131/2109: Loss 2.8657\n",
      "  Iter 141/2109: Loss 2.9895\n",
      "  Iter 151/2109: Loss 2.7642\n",
      "  Iter 161/2109: Loss 2.8032\n",
      "  Iter 171/2109: Loss 2.9999\n",
      "  Iter 181/2109: Loss 2.8750\n",
      "  Iter 191/2109: Loss 2.7750\n",
      "  Iter 201/2109: Loss 2.9657\n",
      "  Iter 211/2109: Loss 2.6578\n",
      "  Iter 221/2109: Loss 2.9851\n",
      "  Iter 231/2109: Loss 2.9740\n",
      "  Iter 241/2109: Loss 2.7445\n",
      "  Iter 251/2109: Loss 2.8990\n",
      "  Iter 261/2109: Loss 3.0575\n",
      "  Iter 271/2109: Loss 2.7094\n",
      "  Iter 281/2109: Loss 2.5376\n",
      "  Iter 291/2109: Loss 2.7918\n",
      "  Iter 301/2109: Loss 2.6542\n",
      "  Iter 311/2109: Loss 2.9478\n",
      "  Iter 321/2109: Loss 3.3212\n",
      "  Iter 331/2109: Loss 2.8898\n",
      "  Iter 341/2109: Loss 2.6760\n",
      "  Iter 351/2109: Loss 2.8399\n",
      "  Iter 361/2109: Loss 3.1993\n",
      "  Iter 371/2109: Loss 3.1038\n",
      "  Iter 381/2109: Loss 2.8196\n",
      "  Iter 391/2109: Loss 3.3259\n",
      "  Iter 401/2109: Loss 2.7715\n",
      "  Iter 411/2109: Loss 3.0329\n",
      "  Iter 421/2109: Loss 3.0078\n",
      "  Iter 431/2109: Loss 3.1743\n",
      "  Iter 441/2109: Loss 2.5462\n",
      "  Iter 451/2109: Loss 3.1782\n",
      "  Iter 461/2109: Loss 2.5266\n",
      "  Iter 471/2109: Loss 2.6547\n",
      "  Iter 481/2109: Loss 3.0532\n",
      "  Iter 491/2109: Loss 2.9972\n",
      "  Iter 501/2109: Loss 2.7076\n",
      "  Iter 511/2109: Loss 2.6051\n",
      "  Iter 521/2109: Loss 3.0274\n",
      "  Iter 531/2109: Loss 2.7559\n",
      "  Iter 541/2109: Loss 2.6863\n",
      "  Iter 551/2109: Loss 2.7714\n",
      "  Iter 561/2109: Loss 3.2285\n",
      "  Iter 571/2109: Loss 2.7483\n",
      "  Iter 581/2109: Loss 2.5890\n",
      "  Iter 591/2109: Loss 3.1387\n",
      "  Iter 601/2109: Loss 2.4902\n",
      "  Iter 611/2109: Loss 2.7565\n",
      "  Iter 621/2109: Loss 3.0119\n",
      "  Iter 631/2109: Loss 2.6323\n",
      "  Iter 641/2109: Loss 2.8994\n",
      "  Iter 651/2109: Loss 3.3019\n",
      "  Iter 661/2109: Loss 2.6742\n",
      "  Iter 671/2109: Loss 2.9489\n",
      "  Iter 681/2109: Loss 3.1063\n",
      "  Iter 691/2109: Loss 2.4343\n",
      "  Iter 701/2109: Loss 3.0968\n",
      "  Iter 711/2109: Loss 2.9402\n",
      "  Iter 721/2109: Loss 2.3980\n",
      "  Iter 731/2109: Loss 2.5833\n",
      "  Iter 741/2109: Loss 3.2349\n",
      "  Iter 751/2109: Loss 2.7288\n",
      "  Iter 761/2109: Loss 2.7742\n",
      "  Iter 771/2109: Loss 2.9756\n",
      "  Iter 781/2109: Loss 2.6684\n",
      "  Iter 791/2109: Loss 3.0153\n",
      "  Iter 801/2109: Loss 2.7817\n",
      "  Iter 811/2109: Loss 2.6133\n",
      "  Iter 821/2109: Loss 2.8313\n",
      "  Iter 831/2109: Loss 2.5549\n",
      "  Iter 841/2109: Loss 2.8404\n",
      "  Iter 851/2109: Loss 2.5961\n",
      "  Iter 861/2109: Loss 3.0067\n",
      "  Iter 871/2109: Loss 2.8338\n",
      "  Iter 881/2109: Loss 3.0026\n",
      "  Iter 891/2109: Loss 2.7257\n",
      "  Iter 901/2109: Loss 2.6617\n",
      "  Iter 911/2109: Loss 3.1812\n",
      "  Iter 921/2109: Loss 2.9730\n",
      "  Iter 931/2109: Loss 3.0724\n",
      "  Iter 941/2109: Loss 2.7830\n",
      "  Iter 951/2109: Loss 2.8579\n",
      "  Iter 961/2109: Loss 2.9942\n",
      "  Iter 971/2109: Loss 2.4990\n",
      "  Iter 981/2109: Loss 2.5919\n",
      "  Iter 991/2109: Loss 2.8536\n",
      "  Iter 1001/2109: Loss 3.0373\n",
      "  Iter 1011/2109: Loss 3.0357\n",
      "  Iter 1021/2109: Loss 3.0271\n",
      "  Iter 1031/2109: Loss 2.5247\n",
      "  Iter 1041/2109: Loss 2.5618\n",
      "  Iter 1051/2109: Loss 2.5924\n",
      "  Iter 1061/2109: Loss 2.7023\n",
      "  Iter 1071/2109: Loss 2.7101\n",
      "  Iter 1081/2109: Loss 2.5822\n",
      "  Iter 1091/2109: Loss 2.7046\n",
      "  Iter 1101/2109: Loss 2.7238\n",
      "  Iter 1111/2109: Loss 2.6135\n",
      "  Iter 1121/2109: Loss 2.6358\n",
      "  Iter 1131/2109: Loss 2.3958\n",
      "  Iter 1141/2109: Loss 2.5118\n",
      "  Iter 1151/2109: Loss 3.0532\n",
      "  Iter 1161/2109: Loss 2.4871\n",
      "  Iter 1171/2109: Loss 2.6038\n",
      "  Iter 1181/2109: Loss 2.9710\n",
      "  Iter 1191/2109: Loss 2.9935\n",
      "  Iter 1201/2109: Loss 2.7020\n",
      "  Iter 1211/2109: Loss 2.4818\n",
      "  Iter 1221/2109: Loss 2.9202\n",
      "  Iter 1231/2109: Loss 2.9635\n",
      "  Iter 1241/2109: Loss 2.4311\n",
      "  Iter 1251/2109: Loss 2.9646\n",
      "  Iter 1261/2109: Loss 2.7698\n",
      "  Iter 1271/2109: Loss 2.4114\n",
      "  Iter 1281/2109: Loss 2.7077\n",
      "  Iter 1291/2109: Loss 2.9154\n",
      "  Iter 1301/2109: Loss 2.3774\n",
      "  Iter 1311/2109: Loss 2.6390\n",
      "  Iter 1321/2109: Loss 2.4346\n",
      "  Iter 1331/2109: Loss 2.4608\n",
      "  Iter 1341/2109: Loss 2.6728\n",
      "  Iter 1351/2109: Loss 2.6257\n",
      "  Iter 1361/2109: Loss 2.9555\n",
      "  Iter 1371/2109: Loss 2.6989\n",
      "  Iter 1381/2109: Loss 3.1252\n",
      "  Iter 1391/2109: Loss 2.3702\n",
      "  Iter 1401/2109: Loss 2.9222\n",
      "  Iter 1411/2109: Loss 2.8219\n",
      "  Iter 1421/2109: Loss 3.1842\n",
      "  Iter 1431/2109: Loss 2.8814\n",
      "  Iter 1441/2109: Loss 2.6715\n",
      "  Iter 1451/2109: Loss 2.4789\n",
      "  Iter 1461/2109: Loss 2.6401\n",
      "  Iter 1471/2109: Loss 2.5961\n",
      "  Iter 1481/2109: Loss 2.6984\n",
      "  Iter 1491/2109: Loss 2.6617\n",
      "  Iter 1501/2109: Loss 2.6233\n",
      "  Iter 1511/2109: Loss 2.5699\n",
      "  Iter 1521/2109: Loss 2.7370\n",
      "  Iter 1531/2109: Loss 3.2304\n",
      "  Iter 1541/2109: Loss 2.6658\n",
      "  Iter 1551/2109: Loss 2.5632\n",
      "  Iter 1561/2109: Loss 2.8037\n",
      "  Iter 1571/2109: Loss 2.8093\n",
      "  Iter 1581/2109: Loss 2.5205\n",
      "  Iter 1591/2109: Loss 2.7562\n",
      "  Iter 1601/2109: Loss 3.0501\n",
      "  Iter 1611/2109: Loss 2.9071\n",
      "  Iter 1621/2109: Loss 2.7508\n",
      "  Iter 1631/2109: Loss 3.2428\n",
      "  Iter 1641/2109: Loss 2.5649\n",
      "  Iter 1651/2109: Loss 2.4908\n",
      "  Iter 1661/2109: Loss 2.8038\n",
      "  Iter 1671/2109: Loss 2.4960\n",
      "  Iter 1681/2109: Loss 2.5641\n",
      "  Iter 1691/2109: Loss 3.0259\n",
      "  Iter 1701/2109: Loss 2.5558\n",
      "  Iter 1711/2109: Loss 2.7412\n",
      "  Iter 1721/2109: Loss 2.7147\n",
      "  Iter 1731/2109: Loss 2.5374\n",
      "  Iter 1741/2109: Loss 3.0796\n",
      "  Iter 1751/2109: Loss 3.0854\n",
      "  Iter 1761/2109: Loss 2.4827\n",
      "  Iter 1771/2109: Loss 2.8256\n",
      "  Iter 1781/2109: Loss 2.8032\n",
      "  Iter 1791/2109: Loss 2.4918\n",
      "  Iter 1801/2109: Loss 2.3827\n",
      "  Iter 1811/2109: Loss 2.7927\n",
      "  Iter 1821/2109: Loss 2.6455\n",
      "  Iter 1831/2109: Loss 2.6974\n",
      "  Iter 1841/2109: Loss 3.0518\n",
      "  Iter 1851/2109: Loss 2.4785\n",
      "  Iter 1861/2109: Loss 2.7272\n",
      "  Iter 1871/2109: Loss 2.9737\n",
      "  Iter 1881/2109: Loss 2.9770\n",
      "  Iter 1891/2109: Loss 2.5697\n",
      "  Iter 1901/2109: Loss 2.1618\n",
      "  Iter 1911/2109: Loss 2.8013\n",
      "  Iter 1921/2109: Loss 2.6632\n",
      "  Iter 1931/2109: Loss 2.8797\n",
      "  Iter 1941/2109: Loss 2.6779\n",
      "  Iter 1951/2109: Loss 2.7544\n",
      "  Iter 1961/2109: Loss 2.6689\n",
      "  Iter 1971/2109: Loss 2.6113\n",
      "  Iter 1981/2109: Loss 2.8504\n",
      "  Iter 1991/2109: Loss 2.7651\n",
      "  Iter 2001/2109: Loss 2.5952\n",
      "  Iter 2011/2109: Loss 2.6590\n",
      "  Iter 2021/2109: Loss 2.5575\n",
      "  Iter 2031/2109: Loss 2.5207\n",
      "  Iter 2041/2109: Loss 3.0252\n",
      "  Iter 2051/2109: Loss 2.7413\n",
      "  Iter 2061/2109: Loss 3.0207\n",
      "  Iter 2071/2109: Loss 2.9089\n",
      "  Iter 2081/2109: Loss 2.6446\n",
      "  Iter 2091/2109: Loss 2.4374\n",
      "  Iter 2101/2109: Loss 2.9115\n",
      "  Iter 2109/2109: Loss 2.5217\n",
      "Fine Train Loss: 2.7794, Fine Train Acc: 0.3440, Val Acc: 0.2897, Val Loss: 2.9038\n",
      "Time: 40179.04s\n",
      "\n",
      "[Epoch 4/100]\n",
      "  Iter   1/2109: Loss 2.4909\n",
      "  Iter  11/2109: Loss 2.7007\n",
      "  Iter  21/2109: Loss 2.7523\n",
      "  Iter  31/2109: Loss 2.7633\n",
      "  Iter  41/2109: Loss 2.8625\n",
      "  Iter  51/2109: Loss 2.6291\n",
      "  Iter  61/2109: Loss 2.6329\n",
      "  Iter  71/2109: Loss 2.4187\n",
      "  Iter  81/2109: Loss 2.7860\n",
      "  Iter  91/2109: Loss 2.7654\n",
      "  Iter 101/2109: Loss 2.7226\n",
      "  Iter 111/2109: Loss 2.5840\n",
      "  Iter 121/2109: Loss 2.6853\n",
      "  Iter 131/2109: Loss 2.5092\n",
      "  Iter 141/2109: Loss 2.5683\n",
      "  Iter 151/2109: Loss 2.4898\n",
      "  Iter 161/2109: Loss 2.7141\n",
      "  Iter 171/2109: Loss 3.0531\n",
      "  Iter 181/2109: Loss 2.6987\n",
      "  Iter 191/2109: Loss 2.5502\n",
      "  Iter 201/2109: Loss 2.4329\n",
      "  Iter 211/2109: Loss 2.5392\n",
      "  Iter 221/2109: Loss 2.4924\n",
      "  Iter 231/2109: Loss 2.8493\n",
      "  Iter 241/2109: Loss 2.9193\n",
      "  Iter 251/2109: Loss 2.8132\n",
      "  Iter 261/2109: Loss 2.6658\n",
      "  Iter 271/2109: Loss 2.6823\n",
      "  Iter 281/2109: Loss 2.5921\n",
      "  Iter 291/2109: Loss 2.5449\n",
      "  Iter 301/2109: Loss 2.7914\n",
      "  Iter 311/2109: Loss 2.6396\n",
      "  Iter 321/2109: Loss 2.6328\n",
      "  Iter 331/2109: Loss 2.5421\n",
      "  Iter 341/2109: Loss 2.1458\n",
      "  Iter 351/2109: Loss 2.5839\n",
      "  Iter 361/2109: Loss 2.5498\n",
      "  Iter 371/2109: Loss 2.5286\n",
      "  Iter 381/2109: Loss 2.9115\n",
      "  Iter 391/2109: Loss 2.4002\n",
      "  Iter 401/2109: Loss 2.6251\n",
      "  Iter 411/2109: Loss 3.0698\n",
      "  Iter 421/2109: Loss 2.3067\n",
      "  Iter 431/2109: Loss 2.5993\n",
      "  Iter 441/2109: Loss 2.6877\n",
      "  Iter 451/2109: Loss 2.7111\n",
      "  Iter 461/2109: Loss 2.3755\n",
      "  Iter 471/2109: Loss 2.6237\n",
      "  Iter 481/2109: Loss 2.6575\n",
      "  Iter 491/2109: Loss 2.5792\n",
      "  Iter 501/2109: Loss 2.9300\n",
      "  Iter 511/2109: Loss 2.5050\n",
      "  Iter 521/2109: Loss 2.8423\n",
      "  Iter 531/2109: Loss 2.7269\n",
      "  Iter 541/2109: Loss 2.3983\n",
      "  Iter 551/2109: Loss 2.9302\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==== Running ex1 : train dataset 150,000 = original + random crop + horizontal flip ====\")\n",
    "model = MiniVGGNet()\n",
    "\n",
    "x_train, y_train = data['train_cropflip']\n",
    "x_val, y_val = data['val_cropflip']\n",
    "x_test, y_test = data['test']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    model_name='MiniVGGNet_final_ex1',\n",
    "    train_data=(x_train, y_train),\n",
    "    val_data=(x_val, y_val),\n",
    "    test_data=(x_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    lr=0.001,\n",
    "    smoothing=0.1\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_log(\"MiniVGGNet_final_ex1_log.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"MiniVGGNet_final_ex1_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"MiniVGGNet_final_ex1_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"learning_curve_loss.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"learning_curve_accuracy.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

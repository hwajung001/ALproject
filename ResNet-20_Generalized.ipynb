{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from common.util import shuffle_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CIFAR-100 다운로드 및 압축 해제\n",
    "def download_cifar100(dest=\"./cifar-100-python\"):\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "\n",
    "    def is_within_directory(directory, target):\n",
    "        abs_directory = os.path.abspath(directory)\n",
    "        abs_target = os.path.abspath(target)\n",
    "        return os.path.commonprefix([abs_directory, abs_target]) == abs_directory\n",
    "\n",
    "    def safe_extract(tar, path=\".\", members=None):\n",
    "        for member in tar.getmembers():\n",
    "            member_path = os.path.join(path, member.name)\n",
    "            if not is_within_directory(path, member_path):\n",
    "                raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "        tar.extractall(path, members)\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest, exist_ok=True)\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        with tarfile.open(filename, \"r:gz\") as tar:\n",
    "            safe_extract(tar, path=\"./\")\n",
    "        print(\"CIFAR-100 downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "\n",
    "# 데이터 배치 로딩\n",
    "def load_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "        data = data_dict[b'data']\n",
    "        fine_labels = np.array(data_dict[b'fine_labels'])\n",
    "        coarse_labels = np.array(data_dict[b'coarse_labels'])\n",
    "        return data, fine_labels, coarse_labels\n",
    "\n",
    "# 메타데이터 로딩\n",
    "def load_meta(data_dir=\"./cifar-100-python\"):\n",
    "    with open(os.path.join(data_dir, \"meta\"), 'rb') as f:\n",
    "        meta_dict = pickle.load(f, encoding='bytes')\n",
    "        fine_label_names = [name.decode('utf-8') for name in meta_dict[b'fine_label_names']]\n",
    "        coarse_label_names = [name.decode('utf-8') for name in meta_dict[b'coarse_label_names']]\n",
    "        return {\"fine_label_names\": fine_label_names, \"coarse_label_names\": coarse_label_names}\n",
    "\n",
    "# 정규화 함수\n",
    "def normalize(x):\n",
    "    mean = np.array([0.507, 0.487, 0.441]).reshape(1, 3, 1, 1)\n",
    "    std = np.array([0.267, 0.256, 0.276]).reshape(1, 3, 1, 1)\n",
    "    return (x - mean) / std\n",
    "\n",
    "# 전체 데이터 로딩\n",
    "def load_cifar100(data_dir=\"./cifar-100-python\"):\n",
    "    x_train, y_train_fine, y_train_coarse = load_batch(os.path.join(data_dir, \"train\"))\n",
    "    x_test, y_test_fine, y_test_coarse = load_batch(os.path.join(data_dir, \"test\"))\n",
    "\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "\n",
    "    x_train = normalize(x_train)\n",
    "    x_test = normalize(x_test)\n",
    "\n",
    "    val_size = int(0.1 * len(x_train))\n",
    "    x_val, y_val_fine, y_val_coarse = (\n",
    "        x_train[:val_size], y_train_fine[:val_size], y_train_coarse[:val_size]\n",
    "    )\n",
    "    x_train, y_train_fine, y_train_coarse = (\n",
    "        x_train[val_size:], y_train_fine[val_size:], y_train_coarse[val_size:]\n",
    "    )\n",
    "\n",
    "    x_train, y_train_fine = shuffle_dataset(x_train, y_train_fine)\n",
    "    x_train, y_train_coarse = shuffle_dataset(x_train, y_train_coarse)\n",
    "\n",
    "    return (x_train, y_train_fine, y_train_coarse), (x_val, y_val_fine, y_val_coarse), (x_test, y_test_fine, y_test_coarse)\n",
    "\n",
    "download_cifar100()\n",
    "(x_train, y_train_fine, y_train_coarse), (x_val, y_val_fine, y_val_coarse), (x_test, y_test_fine, y_test_coarse) = load_cifar100()\n",
    "meta = load_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강 적용 함수 (on-the-fly)\n",
    "def apply_augmentations(x, crop_size=32, padding=4, cutout_size=16):\n",
    "    # random crop\n",
    "    n, c, h, w = x.shape\n",
    "    padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='reflect')\n",
    "    cropped = np.empty((n, c, crop_size, crop_size), dtype=x.dtype)\n",
    "    for i in range(n):\n",
    "        top = np.random.randint(0, padding * 2 + 1)\n",
    "        left = np.random.randint(0, padding * 2 + 1)\n",
    "        cropped[i] = padded[i, :, top:top+crop_size, left:left+crop_size]\n",
    "\n",
    "    # cutout\n",
    "    for i in range(n):\n",
    "        cy, cx = np.random.randint(crop_size), np.random.randint(crop_size)\n",
    "        y1 = np.clip(cy - cutout_size // 2, 0, crop_size)\n",
    "        y2 = np.clip(cy + cutout_size // 2, 0, crop_size)\n",
    "        x1 = np.clip(cx - cutout_size // 2, 0, crop_size)\n",
    "        x2 = np.clip(cx + cutout_size // 2, 0, crop_size)\n",
    "        cropped[i, :, y1:y2, x1:x2] = 0\n",
    "\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_labels(y, smoothing=0.1, num_classes=100):\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = (y.shape[0], num_classes)\n",
    "    smooth = np.full(label_shape, smoothing / (num_classes - 1))\n",
    "    smooth[np.arange(y.shape[0]), y] = confidence\n",
    "    return smooth\n",
    "\n",
    "meta = load_meta()\n",
    "label_names = meta['fine_label_names']\n",
    "\n",
    "# 원본 x_train 로드\n",
    "x_train, y_train_fine, _ = load_batch(\"./cifar-100-python/train\")\n",
    "x_train = x_train.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "x_train = normalize(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake Quantization 함수\n",
    "def fake_quantize(x, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    \n",
    "    if x_max == x_min:\n",
    "        return x  # avoid divide by zero\n",
    "    \n",
    "    scale = (x_max - x_min) / (qmax - qmin)\n",
    "    zero_point = qmin - x_min / scale\n",
    "    zero_point = np.clip(np.round(zero_point), qmin, qmax)\n",
    "\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x = np.clip(np.round(q_x), qmin, qmax)\n",
    "    fq_x = scale * (q_x - zero_point)\n",
    "    return fq_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type)                       Output Shape                Param #\n",
      "===========================================================================\n",
      " 1. Conv1                           (1, 16, 32, 32)                 448\n",
      " 2. Block[1-1]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 3. Block[1-1]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 4. Block[1-2]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 5. Block[1-2]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 6. Block[1-3]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 7. Block[1-3]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 8. Block[2-1]_Conv1                (1, 32, 16, 16)               4,640\n",
      " 9. Block[2-1]_Conv2                (1, 32, 16, 16)               9,248\n",
      "    └─ Shortcut[2-1]                (1, 32, 16, 16)                 544\n",
      "10. Block[2-2]_Conv1                (1, 32, 16, 16)               9,248\n",
      "11. Block[2-2]_Conv2                (1, 32, 16, 16)               9,248\n",
      "12. Block[2-3]_Conv1                (1, 32, 16, 16)               9,248\n",
      "13. Block[2-3]_Conv2                (1, 32, 16, 16)               9,248\n",
      "14. Block[3-1]_Conv1                (1, 64, 8, 8)                18,496\n",
      "15. Block[3-1]_Conv2                (1, 64, 8, 8)                36,928\n",
      "    └─ Shortcut[3-1]                (1, 64, 8, 8)                 2,112\n",
      "16. Block[3-2]_Conv1                (1, 64, 8, 8)                36,928\n",
      "17. Block[3-2]_Conv2                (1, 64, 8, 8)                36,928\n",
      "18. Block[3-3]_Conv1                (1, 64, 8, 8)                36,928\n",
      "19. Block[3-3]_Conv2                (1, 64, 8, 8)                36,928\n",
      "    GlobalAvgPool                   (1, 64)                           0\n",
      "20. FC                              (1, 100)                      6,500\n",
      "===========================================================================\n",
      "Total weight layers:                                        20\n",
      "Total params:                                               277,540\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "# 모델 레이어 및 ResNet-20 정의\n",
    "from common.layers import Convolution, Affine, Relu, BatchNormalization\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.util import im2col, col2im\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        # Fake Quantization\n",
    "        W_q = fake_quantize(self.W)\n",
    "        b_q = fake_quantize(self.b)\n",
    "        x_q = fake_quantize(self.x)\n",
    "\n",
    "        out = np.dot(x_q, W_q) + b_q\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)\n",
    "        return dx\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, _, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        # Fake Quantization\n",
    "        W_q = fake_quantize(self.W)\n",
    "        b_q = fake_quantize(self.b)\n",
    "        x_q = fake_quantize(x)\n",
    "\n",
    "        col = im2col(x_q, FH, FW, self.stride, self.pad)\n",
    "        col_W = W_q.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + b_q\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout).transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class ResidualBlock:\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        self.stride = stride\n",
    "        self.equal_in_out = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        self.conv1 = Convolution(\n",
    "            W=np.random.randn(out_channels, in_channels, 3, 3) * np.sqrt(2. / in_channels),\n",
    "            b=np.zeros(out_channels),\n",
    "            stride=stride,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn1 = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.conv2 = Convolution(\n",
    "            W=np.random.randn(out_channels, out_channels, 3, 3) * np.sqrt(2. / out_channels),\n",
    "            b=np.zeros(out_channels),\n",
    "            stride=1,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn2 = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "        self.relu2 = Relu()\n",
    "\n",
    "        if not self.equal_in_out:\n",
    "            self.shortcut = Convolution(\n",
    "                W=np.random.randn(out_channels, in_channels, 1, 1) * np.sqrt(2. / in_channels),\n",
    "                b=np.zeros(out_channels),\n",
    "                stride=stride,\n",
    "                pad=0\n",
    "            )\n",
    "            self.bn_shortcut = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "\n",
    "    def forward(self, x, train_flg=True, skip_prob=0.0):\n",
    "        self.x = x\n",
    "\n",
    "        if train_flg and np.random.rand() < skip_prob:\n",
    "            return x  # skip this residual block\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out, train_flg)\n",
    "        out = self.relu1.forward(out)\n",
    "\n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out, train_flg)\n",
    "        self.out_main = out\n",
    "\n",
    "        if self.equal_in_out:\n",
    "            shortcut = x\n",
    "        else:\n",
    "            shortcut = self.shortcut.forward(x)\n",
    "            shortcut = self.bn_shortcut.forward(shortcut, train_flg)\n",
    "        self.out_shortcut = shortcut\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu2.forward(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.relu2.backward(dout)\n",
    "\n",
    "        dshortcut = dout.copy()\n",
    "        dmain = dout.copy()\n",
    "\n",
    "        dmain = self.bn2.backward(dmain)\n",
    "        dmain = self.conv2.backward(dmain)\n",
    "\n",
    "        dmain = self.relu1.backward(dmain)\n",
    "        dmain = self.bn1.backward(dmain)\n",
    "        dmain = self.conv1.backward(dmain)\n",
    "\n",
    "        if not self.equal_in_out:\n",
    "            dshortcut = self.bn_shortcut.backward(dshortcut)\n",
    "            dshortcut = self.shortcut.backward(dshortcut)\n",
    "\n",
    "        dx = dmain + dshortcut\n",
    "        return dx\n",
    "\n",
    "class ResNet20:\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_classes=100):\n",
    "        self.params = []\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        self.conv1 = Convolution(\n",
    "            W=np.random.randn(16, 3, 3, 3) * np.sqrt(2. / 3),\n",
    "            b=np.zeros(16),\n",
    "            stride=1,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn1 = BatchNormalization(gamma=np.ones(16), beta=np.zeros(16))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.layer1 = [ResidualBlock(16, 16, stride=1) for _ in range(3)]\n",
    "        self.layer2 = [ResidualBlock(16 if i == 0 else 32, 32, stride=2 if i == 0 else 1) for i in range(3)]\n",
    "        self.layer3 = [ResidualBlock(32 if i == 0 else 64, 64, stride=2 if i == 0 else 1) for i in range(3)]\n",
    "\n",
    "        self.fc = Affine(W=np.random.randn(64, num_classes) * np.sqrt(2. / 64), b=np.zeros(num_classes))\n",
    "\n",
    "    def clip_weights(self, clip_value=1.0):\n",
    "    # 개별 레이어의 weight들을 [-clip_value, clip_value]로 제한\n",
    "        self.conv1.W = np.clip(self.conv1.W, -clip_value, clip_value)\n",
    "        self.fc.W = np.clip(self.fc.W, -clip_value, clip_value)\n",
    "\n",
    "        for block in self.layer1 + self.layer2 + self.layer3:\n",
    "            block.conv1.W = np.clip(block.conv1.W, -clip_value, clip_value)\n",
    "            block.conv2.W = np.clip(block.conv2.W, -clip_value, clip_value)\n",
    "            if not block.equal_in_out:\n",
    "                block.shortcut.W = np.clip(block.shortcut.W, -clip_value, clip_value)\n",
    "\n",
    "    def forward(self, x, train_flg=True, skip_prob=0.0):\n",
    "        self.input = x\n",
    "\n",
    "        if train_flg and np.random.rand() < skip_prob:\n",
    "            return x  # skip this residual block\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out, train_flg)\n",
    "        out = self.relu1.forward(out)\n",
    "\n",
    "        for block in self.layer1:\n",
    "            out = block.forward(out, train_flg)\n",
    "        for block in self.layer2:\n",
    "            out = block.forward(out, train_flg)\n",
    "        for block in self.layer3:\n",
    "            out = block.forward(out, train_flg)\n",
    "\n",
    "        self.feature_map = out\n",
    "\n",
    "        N, C, H, W = out.shape\n",
    "        out = out.mean(axis=(2, 3))\n",
    "\n",
    "        self.pooled = out\n",
    "        out = self.fc.forward(out)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x, batch_size=100):\n",
    "        y_list = []\n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            y_batch = self.forward(x_batch, train_flg=False)\n",
    "            y_list.append(y_batch)\n",
    "        return np.concatenate(y_list, axis=0)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.forward(x, train_flg=True)\n",
    "        return cross_entropy_error(softmax(y), t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        acc = 0.0\n",
    "        total = x.shape[0]\n",
    "        for i in range(0, total, batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "\n",
    "            y = self.predict(x_batch)\n",
    "            y = np.argmax(y, axis=1)\n",
    "\n",
    "            if t.ndim != 1:\n",
    "                t_batch = np.argmax(t_batch, axis=1)\n",
    "\n",
    "            acc += np.sum(y == t_batch)\n",
    "\n",
    "        return acc / total\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.fc.backward(dout)\n",
    "        dout = dout.reshape(self.feature_map.shape[0], self.feature_map.shape[1], 1, 1)\n",
    "        dout = dout.repeat(self.feature_map.shape[2], axis=2).repeat(self.feature_map.shape[3], axis=3)\n",
    "\n",
    "        for block in reversed(self.layer3):\n",
    "            dout = block.backward(dout)\n",
    "        for block in reversed(self.layer2):\n",
    "            dout = block.backward(dout)\n",
    "        for block in reversed(self.layer1):\n",
    "            dout = block.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        return dout\n",
    "\n",
    "# 모델 구조 출력\n",
    "\n",
    "def count_params(layer):\n",
    "    count = 0\n",
    "    if hasattr(layer, 'W'):\n",
    "        count += np.prod(layer.W.shape)\n",
    "    if hasattr(layer, 'b'):\n",
    "        count += np.prod(layer.b.shape)\n",
    "    return count\n",
    "\n",
    "def print_resnet20_summary(model, input_shape=(1, 3, 32, 32)):\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Layer (type)':<35}{'Output Shape':<25}{'Param #':>10}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "    x = np.zeros(input_shape)\n",
    "    total_params = 0\n",
    "    layer_idx = 1\n",
    "\n",
    "    # Conv1\n",
    "    x = model.conv1.forward(x)\n",
    "    p = count_params(model.conv1)\n",
    "    print(f\"{layer_idx:>2}. {'Conv1':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "    layer_idx += 1\n",
    "\n",
    "    x = model.bn1.forward(x, train_flg=False)\n",
    "    x = model.relu1.forward(x)\n",
    "\n",
    "    # Residual Blocks\n",
    "    for i, layer_block in enumerate([model.layer1, model.layer2, model.layer3]):\n",
    "        for j, block in enumerate(layer_block):\n",
    "            residual = x.copy()\n",
    "\n",
    "            # Conv1\n",
    "            x = block.conv1.forward(x)\n",
    "            p = count_params(block.conv1)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv1\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn1.forward(x, train_flg=False)\n",
    "            x = block.relu1.forward(x)\n",
    "\n",
    "            # Conv2\n",
    "            x = block.conv2.forward(x)\n",
    "            p = count_params(block.conv2)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv2\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn2.forward(x, train_flg=False)\n",
    "\n",
    "            # Shortcut (optional)\n",
    "            if not block.equal_in_out:\n",
    "                x_sc = block.shortcut.forward(residual)\n",
    "                p = count_params(block.shortcut)\n",
    "                name = f\"└─ Shortcut[{i+1}-{j+1}]\"\n",
    "                print(f\"{'':>3} {name:<32}{str(x_sc.shape):<25}{p:>10,}\", flush=True)\n",
    "                total_params += p\n",
    "                x = x + x_sc\n",
    "                x = block.bn_shortcut.forward(x, train_flg=False)\n",
    "            else:\n",
    "                x = x + residual\n",
    "\n",
    "            x = block.relu2.forward(x)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = x.mean(axis=(2, 3))\n",
    "    print(f\"{'':>3} {'GlobalAvgPool':<32}{str(x.shape):<25}{'0':>10}\", flush=True)\n",
    "\n",
    "    # FC\n",
    "    x = model.fc.forward(x)\n",
    "    p = count_params(model.fc)\n",
    "    print(f\"{layer_idx:>2}. {'FC':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Total weight layers:':<60}{'20'}\", flush=True)\n",
    "    print(f\"{'Total params:':<60}{total_params:,}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "model = ResNet20()\n",
    "print_resnet20_summary(model, input_shape=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "from common.optimizer import Adam\n",
    "from common.functions import softmax\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name, train_data, val_data, test_data, epochs=20, batch_size=64, optimizer_name='adam', lr=0.01, smoothing=0.1):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.val_x, self.val_t = val_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "        self.max_iter = self.epochs * self.iter_per_epoch\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        if optimizer_name == 'adam':\n",
    "            self.optimizer = Adam(lr=lr)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "        if hasattr(self.model.fc, 'W'):\n",
    "            param_dict['fc_W'] = self.model.fc.W\n",
    "            param_dict['fc_b'] = self.model.fc.b\n",
    "            grad_dict['fc_W'] = self.model.fc.dW\n",
    "            grad_dict['fc_b'] = self.model.fc.db\n",
    "\n",
    "        idx = 0\n",
    "        for layer in self.model.layer1 + self.model.layer2 + self.model.layer3:\n",
    "            for attr in ['conv1', 'conv2', 'shortcut']:\n",
    "                if hasattr(layer, attr):\n",
    "                    conv = getattr(layer, attr)\n",
    "                    param_dict[f'{idx}_W'] = conv.W\n",
    "                    param_dict[f'{idx}_b'] = conv.b\n",
    "                    grad_dict[f'{idx}_W'] = conv.dW\n",
    "                    grad_dict[f'{idx}_b'] = conv.db\n",
    "                    idx += 1\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            return (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            return dx / batch_size\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        x_batch = apply_augmentations(x_batch)\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "        if t_batch.ndim == 1:\n",
    "            t_batch = smooth_labels(t_batch, smoothing=self.smoothing, num_classes=100)\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        self.model.backward(self.loss_grad(x_batch, t_batch))\n",
    "\n",
    "        if hasattr(self.model, 'clip_weights'):\n",
    "            self.model.clip_weights(clip_value=1.0)\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.optimizer.lr = cosine_annealing_with_warmup(epoch, self.epochs, base_lr=self.optimizer.lr)\n",
    "            print(f\"[Epoch {epoch + 1}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"  Iter {i:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            val_acc = self.model.accuracy(self.val_x, self.val_t)\n",
    "            val_loss = self.batched_loss(self.val_x, self.val_t, batch_size=128)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(val_acc)\n",
    "            self.val_loss_list.append(val_loss)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Train acc: {train_acc:.4f}, Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f} (Time: {elapsed:.2f}s)\\n\", flush=True)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                model_filename = f\"{self.model_name}_epoch_{epoch+1}.pkl\"\n",
    "                self.save_model(model_filename)\n",
    "                print(f\">>> Saved model to {model_filename}\", flush=True)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "\n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': getattr(self.optimizer, 'beta1', None),\n",
    "            'beta2': getattr(self.optimizer, 'beta2', None),\n",
    "            'eps': getattr(self.optimizer, 'eps', None),\n",
    "            'm': getattr(self.optimizer, 'm', {}),\n",
    "            'v': getattr(self.optimizer, 'v', {}),\n",
    "            't': getattr(self.optimizer, 't', 0),\n",
    "        }\n",
    "\n",
    "        save_data = {\n",
    "            'model': model_state,\n",
    "            'optimizer': optimizer_state,\n",
    "            'train_loss_list': self.train_loss_list,\n",
    "            'train_acc_list': self.train_acc_list,\n",
    "            'test_acc_list': self.test_acc_list,\n",
    "            'val_loss_list': self.val_loss_list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename,\n",
    "                 loss=np.array(self.train_loss_list),\n",
    "                 train_acc=np.array(self.train_acc_list),\n",
    "                 test_acc=np.array(self.test_acc_list),\n",
    "                 val_loss=np.array(self.val_loss_list))\n",
    "        print(f\"Log saved to {filename}\", flush=True)\n",
    "\n",
    "    def batched_loss(self, x, t, batch_size=128):\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "            loss = self.model.loss(x_batch, t_batch)\n",
    "            total_loss += loss * len(x_batch)\n",
    "            total_count += len(x_batch)\n",
    "        return total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [모델 설정 1] LR=0.01, BS=64 ===\n",
      "[Epoch 1]\n",
      "  Iter   0/781: Loss 5.8948\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m             trainer.save_model(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodel_cfg\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_smooth\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msmooth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mrun_experiments\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     26\u001b[39m model = ResNet20()\n\u001b[32m     27\u001b[39m trainer = Trainer(\n\u001b[32m     28\u001b[39m     model=model,\n\u001b[32m     29\u001b[39m     model_name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResNet20_cfg\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_smooth\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msmooth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     smoothing=smooth\n\u001b[32m     38\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m trainer.save_log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlog_cfg\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_smooth\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msmooth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npz\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m trainer.save_model(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodel_cfg\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_smooth\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msmooth\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     85\u001b[39m start_time = time.time()\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.iter_per_epoch):\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     epoch_loss += loss\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mTrainer.train_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m t_batch.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m     67\u001b[39m     t_batch = smooth_labels(t_batch, smoothing=\u001b[38;5;28mself\u001b[39m.smoothing, num_classes=\u001b[32m100\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mself\u001b[39m.model.backward(\u001b[38;5;28mself\u001b[39m.loss_grad(x_batch, t_batch))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m'\u001b[39m\u001b[33mclip_weights\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 226\u001b[39m, in \u001b[36mResNet20.loss\u001b[39m\u001b[34m(self, x, t)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_flg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_error(softmax(y), t)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 202\u001b[39m, in \u001b[36mResNet20.forward\u001b[39m\u001b[34m(self, x, train_flg, skip_prob)\u001b[39m\n\u001b[32m    199\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu1.forward(out)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layer1:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     out = \u001b[43mblock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_flg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layer2:\n\u001b[32m    204\u001b[39m     out = block.forward(out, train_flg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mResidualBlock.forward\u001b[39m\u001b[34m(self, x, train_flg, skip_prob)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_flg \u001b[38;5;129;01mand\u001b[39;00m np.random.rand() < skip_prob:\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x  \u001b[38;5;66;03m# skip this residual block\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m out = \u001b[38;5;28mself\u001b[39m.bn1.forward(out, train_flg)\n\u001b[32m    123\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu1.forward(out)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mConvolution.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     58\u001b[39m b_q = fake_quantize(\u001b[38;5;28mself\u001b[39m.b)\n\u001b[32m     59\u001b[39m x_q = fake_quantize(x)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m col = \u001b[43mim2col\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m col_W = W_q.reshape(FN, -\u001b[32m1\u001b[39m).T\n\u001b[32m     63\u001b[39m out = np.dot(col, col_W) + b_q\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/4-1/인공신경망/project/ANNproject/common/util.py:65\u001b[39m, in \u001b[36mim2col\u001b[39m\u001b[34m(input_data, filter_h, filter_w, stride, pad)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(filter_w):\n\u001b[32m     64\u001b[39m         x_max = x + stride*out_w\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m         col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\u001b[32m     67\u001b[39m col = col.transpose(\u001b[32m0\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m).reshape(N*out_h*out_w, -\u001b[32m1\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m col\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "(x_train, y_train_fine, y_train_coarse),\n",
    "(x_val, y_val_fine, y_val_coarse),\n",
    "(x_test, y_test_fine, y_test_coarse) = load_cifar100()\n",
    "\n",
    "# fine label 기준으로 사용\n",
    "train_data = (x_train, y_train_fine)\n",
    "val_data = (x_val, y_val_fine)\n",
    "test_data = (x_test, y_test_fine)\n",
    "\n",
    "# 고정된 두 실험 조건\n",
    "model_configs = [\n",
    "    {\"lr\": 0.01, \"batch_size\": 64},\n",
    "    {\"lr\": 0.001, \"batch_size\": 32},\n",
    "]\n",
    "\n",
    "# 튜닝할 smoothing 값\n",
    "smoothing_values = [0.05, 0.1, 0.15]\n",
    "\n",
    "def run_experiments():\n",
    "    for model_id, cfg in enumerate(model_configs):\n",
    "        print(f\"=== [모델 설정 {model_id+1}] LR={cfg['lr']}, BS={cfg['batch_size']} ===\")\n",
    "\n",
    "        for smooth in smoothing_values:\n",
    "            model = ResNet20()\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                model_name=f\"ResNet20_cfg{model_id+1}_smooth{smooth}\",\n",
    "                train_data=(x_train, y_train_fine),\n",
    "                val_data=(x_val, y_val_fine),\n",
    "                test_data=(x_test, y_test_fine),\n",
    "                epochs=10,\n",
    "                batch_size=cfg[\"batch_size\"],\n",
    "                optimizer_name=\"adam\",  \n",
    "                lr=cfg[\"lr\"],\n",
    "                smoothing=smooth\n",
    "            )\n",
    "            trainer.train()\n",
    "            trainer.save_log(f\"log_cfg{model_id+1}_smooth{smooth}.npz\")\n",
    "            trainer.save_model(f\"model_cfg{model_id+1}_smooth{smooth}.pkl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

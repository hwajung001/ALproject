{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-bRNsxD1dJJ",
        "outputId": "9168d7d7-ddca-419f-fd6b-641a942495d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: (50000, 32, 32, 3)\n",
            "Train fine labels: (50000,)\n",
            "Test images: (10000, 32, 32, 3)\n",
            "Test fine labels: (10000,)\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-100 데이터 불러오기\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# 1. 데이터 다운로드 및 압축 풀기\n",
        "!wget -q https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar -xzf cifar-100-python.tar.gz\n",
        "\n",
        "# 2. 데이터 로딩 함수\n",
        "def load_cifar100(data_dir='/content/cifar-100-python'):\n",
        "    def load_file(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            data = pickle.load(f, encoding='latin1')\n",
        "        images = data['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "        images = images.astype('float32') / 255.0  # 정규화\n",
        "        fine_labels = np.array(data['fine_labels'])\n",
        "        coarse_labels = np.array(data['coarse_labels'])\n",
        "        return images, fine_labels, coarse_labels\n",
        "\n",
        "    train_images, train_fine_labels, train_coarse_labels = load_file(os.path.join(data_dir, 'train'))\n",
        "    test_images, test_fine_labels, test_coarse_labels = load_file(os.path.join(data_dir, 'test'))\n",
        "    return (train_images, train_fine_labels, train_coarse_labels), (test_images, test_fine_labels, test_coarse_labels)\n",
        "\n",
        "# 3. 데이터 불러오기\n",
        "(train_images, train_fine_labels, train_coarse_labels), (test_images, test_fine_labels, test_coarse_labels) = load_cifar100()\n",
        "\n",
        "# 4. 확인\n",
        "print(\"Train images:\", train_images.shape)\n",
        "print(\"Train fine labels:\", train_fine_labels.shape)\n",
        "print(\"Test images:\", test_images.shape)\n",
        "print(\"Test fine labels:\", test_fine_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleMLP:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate=0.5):\n",
        "        self.params = {}\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.init_weights(input_size, hidden_sizes, output_size)\n",
        "\n",
        "    def init_weights(self, input_size, hidden_sizes, output_size):\n",
        "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            fan_in, fan_out = layer_sizes[i], layer_sizes[i+1]\n",
        "            limit = np.sqrt(2 / fan_in)  # He 초기화\n",
        "            self.params[f'W{i+1}'] = np.random.uniform(-limit, limit, (fan_in, fan_out)).astype(np.float32)\n",
        "            self.params[f'b{i+1}'] = np.zeros((1, fan_out), dtype=np.float32)\n",
        "\n",
        "    def leaky_relu(self, x, alpha=0.01):\n",
        "        return np.where(x > 0, x, alpha * x)\n",
        "\n",
        "    def leaky_relu_derivative(self, x, alpha=0.01):\n",
        "        dx = np.ones_like(x, dtype=np.float32)\n",
        "        dx[x < 0] = alpha\n",
        "        return dx\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)).astype(np.float32)\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X, training=True):\n",
        "        cache = {}\n",
        "        A = X\n",
        "        num_layers = len(self.params) // 2\n",
        "\n",
        "        for i in range(1, num_layers):\n",
        "            Z = np.dot(A, self.params[f'W{i}']) + self.params[f'b{i}']\n",
        "            A = self.leaky_relu(Z)\n",
        "            if training:\n",
        "                dropout_mask = (np.random.rand(*A.shape) > self.dropout_rate).astype(np.float32)\n",
        "                A *= dropout_mask\n",
        "                A /= (1.0 - self.dropout_rate)\n",
        "                cache[f'dropout_mask{i}'] = dropout_mask\n",
        "            cache[f'Z{i}'] = Z\n",
        "            cache[f'A{i}'] = A\n",
        "\n",
        "        Z_final = np.dot(A, self.params[f'W{num_layers}']) + self.params[f'b{num_layers}']\n",
        "        A_final = self.softmax(Z_final)\n",
        "        cache[f'Z{num_layers}'] = Z_final\n",
        "        cache[f'A{num_layers}'] = A_final\n",
        "\n",
        "        return A_final, cache\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        n_samples = y_true.shape[0]\n",
        "        log_probs = -np.log(y_pred[np.arange(n_samples), y_true] + 1e-8)\n",
        "        return np.sum(log_probs) / n_samples\n",
        "\n",
        "    def backward(self, X, y_true, cache):\n",
        "        grads = {}\n",
        "        num_layers = len(self.params) // 2\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        A_final = cache[f'A{num_layers}']\n",
        "        delta = A_final\n",
        "        delta[np.arange(n_samples), y_true] -= 1\n",
        "        delta /= n_samples\n",
        "\n",
        "        grads[f'W{num_layers}'] = np.dot(cache[f'A{num_layers-1}'].T, delta)\n",
        "        grads[f'b{num_layers}'] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        for i in reversed(range(1, num_layers)):\n",
        "            delta = np.dot(delta, self.params[f'W{i+1}'].T) * self.leaky_relu_derivative(cache[f'Z{i}'])\n",
        "            if f'dropout_mask{i}' in cache:\n",
        "                delta *= cache[f'dropout_mask{i}']\n",
        "                delta /= (1.0 - self.dropout_rate)\n",
        "            A_prev = X if i == 1 else cache[f'A{i-1}']\n",
        "            grads[f'W{i}'] = np.dot(A_prev.T, delta)\n",
        "            grads[f'b{i}'] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update_params(self, grads, learning_rate):\n",
        "        num_layers = len(self.params) // 2\n",
        "        for i in range(1, num_layers + 1):\n",
        "            self.params[f'W{i}'] -= learning_rate * grads[f'W{i}']\n",
        "            self.params[f'b{i}'] -= learning_rate * grads[f'b{i}']\n",
        "\n",
        "    def save(self, filename):\n",
        "        np.savez(filename, **self.params)\n",
        "\n",
        "    def load(self, filename):\n",
        "        data = np.load(filename)\n",
        "        for key in self.params.keys():\n",
        "            self.params[key] = data[key]\n",
        "\n",
        "    def train(self, X_train_full, y_train_full, epochs, learning_rate, batch_size=128, save_path=None, validation_split=0.1):\n",
        "        n_samples = X_train_full.shape[0]\n",
        "        n_train = int(n_samples * (1 - validation_split))\n",
        "\n",
        "     # 1. train/validation 데이터 분리\n",
        "        X_train = X_train_full[:n_train]\n",
        "        y_train = y_train_full[:n_train]\n",
        "        X_val = X_train_full[n_train:]\n",
        "        y_val = y_train_full[n_train:]\n",
        "\n",
        "        num_batches = n_train // batch_size\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            # train 데이터 셔플\n",
        "            indices = np.arange(n_train)\n",
        "            np.random.shuffle(indices)\n",
        "            X_train = X_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "\n",
        "            # 2. 미니배치 학습\n",
        "            for batch_idx in range(num_batches):\n",
        "                start = batch_idx * batch_size\n",
        "                end = start + batch_size\n",
        "                X_batch = X_train[start:end]\n",
        "                y_batch = y_train[start:end]\n",
        "\n",
        "                y_pred, cache = self.forward(X_batch, training=True)\n",
        "                loss = self.compute_loss(y_batch, y_pred)\n",
        "                grads = self.backward(X_batch, y_batch, cache)\n",
        "                self.update_params(grads, learning_rate)\n",
        "\n",
        "                epoch_loss += loss\n",
        "\n",
        "            # 3. 에포크 끝나고 validation 평가\n",
        "            avg_train_loss = epoch_loss / num_batches\n",
        "            train_acc = self.evaluate(X_train, y_train)\n",
        "            val_acc = self.evaluate(X_val, y_val)\n",
        "\n",
        "            y_val_pred, _ = self.forward(X_val, training=False)\n",
        "            val_loss = self.compute_loss(y_val, y_val_pred)\n",
        "\n",
        "            if save_path is not None:\n",
        "                self.save(save_path)\n",
        "\n",
        "            print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Val Loss={val_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred, _ = self.forward(X, training=False)\n",
        "        predictions = np.argmax(y_pred, axis=1)\n",
        "        accuracy = np.mean(predictions == y_true)\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "jJYmSukB3YfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 데이터 준비\n",
        "X_train = train_images.reshape(-1, 32*32*3).astype(np.float32)  # (50000, 3072)\n",
        "y_train = train_fine_labels\n",
        "\n",
        "# 모델 생성\n",
        "model = SimpleMLP(input_size=32*32*3, hidden_sizes=[1024, 512, 256], output_size=100, dropout_rate=0.5)\n",
        "\n",
        "# 저장 경로\n",
        "model_save_path = '/content/optimized_simple_mlp_checkpoint.npz'\n",
        "\n",
        "# 저장된 모델 불러오기\n",
        "if os.path.exists(model_save_path):\n",
        "    print(\"저장된 모델을 불러옵니다...\")\n",
        "    model.load(model_save_path)\n",
        "else:\n",
        "    print(\"새 모델로 학습을 시작합니다...\")\n",
        "\n",
        "# 학습 설정\n",
        "epochs = 50\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "\n",
        "# 학습 시작\n",
        "model.train(X_train, y_train, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, save_path=model_save_path)\n",
        "\n",
        "# 최종 정확도 출력\n",
        "final_accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"\\n최종 학습 정확도: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRuIiVpB3mW8",
        "outputId": "66bf9c2a-a103-4aa5-d128-a3d5d5e0d119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "저장된 모델을 불러옵니다...\n",
            "Epoch 1: Train Loss=3.6719, Val Loss=3.5166, Train Acc=0.1752, Val Acc=0.1718\n",
            "Epoch 2: Train Loss=3.6659, Val Loss=3.5154, Train Acc=0.1783, Val Acc=0.1736\n",
            "Epoch 3: Train Loss=3.6653, Val Loss=3.5156, Train Acc=0.1786, Val Acc=0.1752\n",
            "Epoch 4: Train Loss=3.6605, Val Loss=3.5067, Train Acc=0.1821, Val Acc=0.1740\n",
            "Epoch 5: Train Loss=3.6440, Val Loss=3.5045, Train Acc=0.1810, Val Acc=0.1784\n",
            "Epoch 6: Train Loss=3.6432, Val Loss=3.4951, Train Acc=0.1834, Val Acc=0.1798\n",
            "Epoch 7: Train Loss=3.6322, Val Loss=3.4968, Train Acc=0.1848, Val Acc=0.1790\n",
            "Epoch 8: Train Loss=3.6293, Val Loss=3.4883, Train Acc=0.1844, Val Acc=0.1790\n",
            "Epoch 9: Train Loss=3.6246, Val Loss=3.4820, Train Acc=0.1855, Val Acc=0.1788\n",
            "Epoch 10: Train Loss=3.6151, Val Loss=3.4769, Train Acc=0.1912, Val Acc=0.1876\n",
            "Epoch 11: Train Loss=3.6118, Val Loss=3.4701, Train Acc=0.1905, Val Acc=0.1852\n",
            "Epoch 12: Train Loss=3.6020, Val Loss=3.4717, Train Acc=0.1911, Val Acc=0.1836\n",
            "Epoch 13: Train Loss=3.5924, Val Loss=3.4622, Train Acc=0.1917, Val Acc=0.1828\n",
            "Epoch 14: Train Loss=3.5971, Val Loss=3.4605, Train Acc=0.1949, Val Acc=0.1886\n",
            "Epoch 15: Train Loss=3.5835, Val Loss=3.4503, Train Acc=0.1959, Val Acc=0.1894\n",
            "Epoch 16: Train Loss=3.5781, Val Loss=3.4461, Train Acc=0.1936, Val Acc=0.1864\n",
            "Epoch 17: Train Loss=3.5714, Val Loss=3.4500, Train Acc=0.1979, Val Acc=0.1896\n",
            "Epoch 18: Train Loss=3.5656, Val Loss=3.4332, Train Acc=0.2003, Val Acc=0.1924\n",
            "Epoch 19: Train Loss=3.5606, Val Loss=3.4426, Train Acc=0.2015, Val Acc=0.1910\n",
            "Epoch 20: Train Loss=3.5614, Val Loss=3.4406, Train Acc=0.2021, Val Acc=0.1914\n",
            "Epoch 21: Train Loss=3.5498, Val Loss=3.4234, Train Acc=0.2034, Val Acc=0.1926\n",
            "Epoch 22: Train Loss=3.5462, Val Loss=3.4282, Train Acc=0.2023, Val Acc=0.1886\n",
            "Epoch 23: Train Loss=3.5356, Val Loss=3.4184, Train Acc=0.2045, Val Acc=0.1904\n",
            "Epoch 24: Train Loss=3.5306, Val Loss=3.4061, Train Acc=0.2055, Val Acc=0.1958\n",
            "Epoch 25: Train Loss=3.5284, Val Loss=3.4098, Train Acc=0.2089, Val Acc=0.1960\n",
            "Epoch 26: Train Loss=3.5250, Val Loss=3.4060, Train Acc=0.2091, Val Acc=0.1986\n",
            "Epoch 27: Train Loss=3.5143, Val Loss=3.4002, Train Acc=0.2109, Val Acc=0.2012\n",
            "Epoch 28: Train Loss=3.5098, Val Loss=3.3930, Train Acc=0.2112, Val Acc=0.1972\n",
            "Epoch 29: Train Loss=3.5033, Val Loss=3.3978, Train Acc=0.2123, Val Acc=0.1990\n",
            "Epoch 30: Train Loss=3.5002, Val Loss=3.3904, Train Acc=0.2117, Val Acc=0.1988\n",
            "Epoch 31: Train Loss=3.4980, Val Loss=3.3832, Train Acc=0.2107, Val Acc=0.1964\n",
            "Epoch 32: Train Loss=3.4894, Val Loss=3.3751, Train Acc=0.2152, Val Acc=0.2010\n",
            "Epoch 33: Train Loss=3.4869, Val Loss=3.3807, Train Acc=0.2185, Val Acc=0.2014\n",
            "Epoch 34: Train Loss=3.4850, Val Loss=3.3736, Train Acc=0.2183, Val Acc=0.2010\n",
            "Epoch 35: Train Loss=3.4806, Val Loss=3.3691, Train Acc=0.2184, Val Acc=0.2060\n",
            "Epoch 36: Train Loss=3.4702, Val Loss=3.3697, Train Acc=0.2160, Val Acc=0.1994\n",
            "Epoch 37: Train Loss=3.4666, Val Loss=3.3593, Train Acc=0.2158, Val Acc=0.1994\n",
            "Epoch 38: Train Loss=3.4597, Val Loss=3.3499, Train Acc=0.2195, Val Acc=0.2022\n",
            "Epoch 39: Train Loss=3.4611, Val Loss=3.3548, Train Acc=0.2203, Val Acc=0.2052\n",
            "Epoch 40: Train Loss=3.4542, Val Loss=3.3433, Train Acc=0.2242, Val Acc=0.2048\n",
            "Epoch 41: Train Loss=3.4457, Val Loss=3.3413, Train Acc=0.2228, Val Acc=0.2034\n",
            "Epoch 42: Train Loss=3.4464, Val Loss=3.3488, Train Acc=0.2260, Val Acc=0.2046\n",
            "Epoch 43: Train Loss=3.4384, Val Loss=3.3465, Train Acc=0.2218, Val Acc=0.2040\n",
            "Epoch 44: Train Loss=3.4340, Val Loss=3.3407, Train Acc=0.2271, Val Acc=0.2084\n",
            "Epoch 45: Train Loss=3.4304, Val Loss=3.3312, Train Acc=0.2299, Val Acc=0.2098\n",
            "Epoch 46: Train Loss=3.4245, Val Loss=3.3290, Train Acc=0.2281, Val Acc=0.2072\n",
            "Epoch 47: Train Loss=3.4266, Val Loss=3.3209, Train Acc=0.2303, Val Acc=0.2092\n",
            "Epoch 48: Train Loss=3.4227, Val Loss=3.3223, Train Acc=0.2291, Val Acc=0.2096\n",
            "Epoch 49: Train Loss=3.4097, Val Loss=3.3150, Train Acc=0.2331, Val Acc=0.2122\n",
            "Epoch 50: Train Loss=3.4067, Val Loss=3.3086, Train Acc=0.2330, Val Acc=0.2098\n",
            "\n",
            "최종 학습 정확도: 0.2307\n"
          ]
        }
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet-22 final Train code\n",
    "\n",
    "### ex 3 : train dataset 100,000 = original + horizontal flip\n",
    "##### (random seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - CIFAR-100 데이터 다운로드 및 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 already downloaded.\n",
      "Generating augmented dataset with horizontal flip only...\n",
      "train_flip: [(90000, 3, 32, 32), (90000,)]\n",
      "val_flip: [(10000, 3, 32, 32), (10000,)]\n",
      "test: [(10000, 3, 32, 32), (10000,)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "def download_cifar100(save_path='cifar-100-python'):\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "        return\n",
    "\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
    "    filename = 'cifar-100-python.tar.gz'\n",
    "    print(\"Downloading CIFAR-100...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "    with tarfile.open(filename, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "    os.remove(filename)\n",
    "    print(\"Download and extraction completed.\")\n",
    "\n",
    "def load_batch(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "    data = data_dict[b'data']\n",
    "    fine_labels = np.array(data_dict[b'fine_labels'])\n",
    "    data = data.reshape(-1, 3, 32, 32)\n",
    "    return data, fine_labels\n",
    "\n",
    "def normalize_images(images):\n",
    "    return images.astype(np.float32) / 255.0\n",
    "\n",
    "def split_validation(images, labels, val_ratio=0.1):\n",
    "    num_samples = images.shape[0]\n",
    "    val_size = int(num_samples * val_ratio)\n",
    "\n",
    "    idx = np.random.permutation(num_samples)\n",
    "    images = images[idx]\n",
    "    labels = labels[idx]\n",
    "\n",
    "    val_images = images[:val_size]\n",
    "    val_labels = labels[:val_size]\n",
    "    train_images = images[val_size:]\n",
    "    train_labels = labels[val_size:]\n",
    "\n",
    "    return (train_images, train_labels), (val_images, val_labels)\n",
    "\n",
    "def random_crop(x, crop_size=32, padding=4):\n",
    "    n, c, h, w = x.shape\n",
    "    padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='reflect')\n",
    "    cropped = np.empty((n, c, crop_size, crop_size), dtype=x.dtype)\n",
    "    for i in range(n):\n",
    "        top = np.random.randint(0, padding * 2 + 1)\n",
    "        left = np.random.randint(0, padding * 2 + 1)\n",
    "        cropped[i] = padded[i, :, top:top+crop_size, left:left+crop_size]\n",
    "    return cropped\n",
    "\n",
    "def horizontal_flip(x):\n",
    "    return x[:, :, :, ::-1]\n",
    "\n",
    "def load_cifar100_dataset():\n",
    "    download_cifar100()\n",
    "    train_data, train_fine = load_batch('cifar-100-python/train')\n",
    "    test_data, test_fine = load_batch('cifar-100-python/test')\n",
    "    train_data = normalize_images(train_data)\n",
    "    test_data = normalize_images(test_data)\n",
    "    return (train_data, train_fine), (test_data, test_fine)\n",
    "\n",
    "def generate_augmented_dataset(images, labels, target_size):\n",
    "    N = images.shape[0]\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    repeat = target_size // (N * 2) + 1  # original + flip\n",
    "\n",
    "    for _ in range(repeat):\n",
    "        imgs_original = images.copy()\n",
    "        imgs_flip = horizontal_flip(images.copy())\n",
    "\n",
    "        augmented_images.append(imgs_original)\n",
    "        augmented_labels.append(labels.copy())\n",
    "\n",
    "        augmented_images.append(imgs_flip)\n",
    "        augmented_labels.append(labels.copy())\n",
    "\n",
    "        if sum(x.shape[0] for x in augmented_images) >= target_size:\n",
    "            break\n",
    "\n",
    "    X = np.concatenate(augmented_images, axis=0)[:target_size]\n",
    "    y = np.concatenate(augmented_labels, axis=0)[:target_size]\n",
    "    return X, y\n",
    "\n",
    "def prepare_dataset():\n",
    "    (full_train_images, full_train_labels), (test_images, test_labels) = load_cifar100_dataset()\n",
    "    print(\"Generating augmented dataset with horizontal flip only...\")\n",
    "\n",
    "    X_aug, y_aug = generate_augmented_dataset(full_train_images, full_train_labels, target_size=100000)\n",
    "    train_aug, val_aug = split_validation(X_aug, y_aug)\n",
    "\n",
    "    return {\n",
    "        'train_flip': train_aug,\n",
    "        'val_flip': val_aug,\n",
    "        'test': (test_images, test_labels)\n",
    "    }\n",
    "\n",
    "data = prepare_dataset()\n",
    "for k, v in data.items():\n",
    "    if isinstance(v, tuple):\n",
    "        print(f\"{k}: {[x.shape for x in v]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - DenseNet-22 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Convolution, BatchNormalization, Relu, Affine\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.util import im2col, col2im\n",
    "\n",
    "def fake_quantize(x, num_bits=8):\n",
    "    qmin, qmax = 0., 2.**num_bits - 1.\n",
    "    x_min, x_max = np.min(x), np.max(x)\n",
    "    if x_max == x_min:\n",
    "        return x\n",
    "    scale = (x_max - x_min) / (qmax - qmin)\n",
    "    zero_point = np.clip(np.round(qmin - x_min / scale), qmin, qmax)\n",
    "    q_x = np.clip(np.round(zero_point + x / scale), qmin, qmax)\n",
    "    return scale * (q_x - zero_point)\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        self.bn = BatchNormalization(np.ones(in_channels), np.zeros(in_channels))\n",
    "        self.relu = Relu()\n",
    "        self.conv = Convolution(\n",
    "            np.random.randn(growth_rate, in_channels, 3, 3) * np.sqrt(2. / in_channels),\n",
    "            np.zeros(growth_rate), stride=1, pad=1)\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        out = self.bn.forward(x, train_flg)\n",
    "        out = self.relu.forward(out)\n",
    "        out = self.conv.forward(out)\n",
    "        self.out = out\n",
    "        return np.concatenate([x, out], axis=1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx_main = dout[:, -self.out.shape[1]:, :, :]\n",
    "        dx_input = dout[:, :-self.out.shape[1], :, :]\n",
    "        dx_main = self.conv.backward(dx_main)\n",
    "        dx_main = self.relu.backward(dx_main)\n",
    "        dx_main = self.bn.backward(dx_main)\n",
    "        return dx_input + dx_main\n",
    "\n",
    "class TransitionLayer:\n",
    "    def __init__(self, in_channels):\n",
    "        out_channels = in_channels // 2\n",
    "        self.bn = BatchNormalization(np.ones(in_channels), np.zeros(in_channels))\n",
    "        self.relu = Relu()\n",
    "        self.conv = Convolution(\n",
    "            np.random.randn(out_channels, in_channels, 1, 1) * np.sqrt(2. / in_channels),\n",
    "            np.zeros(out_channels), stride=1, pad=0)\n",
    "        self.pool = lambda x: x[:, :, ::2, ::2]  # 2x2 average pool (stride=2)\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        out = self.bn.forward(x, train_flg)\n",
    "        out = self.relu.forward(out)\n",
    "        out = self.conv.forward(out)\n",
    "        return self.pool(out)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, C, H, W = dout.shape\n",
    "        d_upsampled = np.zeros((N, C, H*2, W*2))\n",
    "        d_upsampled[:, :, ::2, ::2] = dout  # unpool\n",
    "        d_conv = self.conv.backward(d_upsampled)\n",
    "        d_relu = self.relu.backward(d_conv)\n",
    "        return self.bn.backward(d_relu)\n",
    "\n",
    "class DenseBlock:\n",
    "    def __init__(self, num_layers, in_channels, growth_rate):\n",
    "        self.layers = []\n",
    "        channels = in_channels\n",
    "        for _ in range(num_layers):\n",
    "            layer = DenseLayer(channels, growth_rate)\n",
    "            self.layers.append(layer)\n",
    "            channels += growth_rate\n",
    "        self.out_channels = channels\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x, train_flg)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "class DenseNet22:\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_classes=100, growth_rate=12):\n",
    "        self.growth_rate = growth_rate\n",
    "        self.conv1 = Convolution(\n",
    "            np.random.randn(16, 3, 3, 3) * np.sqrt(2. / 3), np.zeros(16), stride=1, pad=1)\n",
    "        self.bn1 = BatchNormalization(np.ones(16), np.zeros(16))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.block1 = DenseBlock(6, 16, growth_rate)\n",
    "        self.trans1 = TransitionLayer(self.block1.out_channels)\n",
    "\n",
    "        self.block2 = DenseBlock(6, self.block1.out_channels // 2, growth_rate)\n",
    "        self.trans2 = TransitionLayer(self.block2.out_channels)\n",
    "\n",
    "        self.block3 = DenseBlock(6, self.block2.out_channels // 2, growth_rate)\n",
    "\n",
    "        final_channels = self.block3.out_channels\n",
    "        self.fc = Affine(np.random.randn(final_channels, num_classes) * np.sqrt(2. / final_channels), np.zeros(num_classes))\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        out = self.relu1.forward(self.bn1.forward(self.conv1.forward(x), train_flg))\n",
    "        out = self.block1.forward(out, train_flg)\n",
    "        out = self.trans1.forward(out, train_flg)\n",
    "        out = self.block2.forward(out, train_flg)\n",
    "        out = self.trans2.forward(out, train_flg)\n",
    "        out = self.block3.forward(out, train_flg)\n",
    "        self.feature_map = out\n",
    "        out = out.mean(axis=(2, 3))  # global avg pool\n",
    "        self.pooled = out\n",
    "        return self.fc.forward(out)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.fc.backward(dout)\n",
    "        dout = dout[:, :, None, None]\n",
    "        dout = dout.repeat(self.feature_map.shape[2], axis=2)\n",
    "        dout = dout.repeat(self.feature_map.shape[3], axis=3)\n",
    "        dout = self.block3.backward(dout)\n",
    "        dout = self.trans2.backward(dout)\n",
    "        dout = self.block2.backward(dout)\n",
    "        dout = self.trans1.backward(dout)\n",
    "        dout = self.block1.backward(dout)\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        return self.conv1.backward(dout)\n",
    "\n",
    "    def predict(self, x, batch_size=100):\n",
    "        return np.concatenate([self.forward(x[i:i+batch_size], False) for i in range(0, x.shape[0], batch_size)], axis=0)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        return cross_entropy_error(softmax(self.forward(x, True)), t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        pred = np.argmax(self.predict(x, batch_size), axis=1)\n",
    "        true = t if t.ndim == 1 else np.argmax(t, axis=1)\n",
    "        return np.mean(pred == true)\n",
    "\n",
    "    def clip_weights(self, clip_value=1.0):\n",
    "        self.conv1.W = np.clip(self.conv1.W, -clip_value, clip_value)\n",
    "        self.fc.W = np.clip(self.fc.W, -clip_value, clip_value)\n",
    "        for block in [self.block1, self.block2, self.block3]:\n",
    "            for layer in block.layers:\n",
    "                layer.conv.W = np.clip(layer.conv.W, -clip_value, clip_value)\n",
    "        for trans in [self.trans1, self.trans2]:\n",
    "            trans.conv.W = np.clip(trans.conv.W, -clip_value, clip_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - DenseNet-22 모델 구조 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type)                       Output Shape                Param #\n",
      "===========================================================================\n",
      " 1. Conv1                           (1, 16, 32, 32)                 448\n",
      " 2. DenseBlock1_Layer1              (1, 28, 32, 32)               1,740\n",
      " 3. DenseBlock1_Layer2              (1, 40, 32, 32)               3,036\n",
      " 4. DenseBlock1_Layer3              (1, 52, 32, 32)               4,332\n",
      " 5. DenseBlock1_Layer4              (1, 64, 32, 32)               5,628\n",
      " 6. DenseBlock1_Layer5              (1, 76, 32, 32)               6,924\n",
      " 7. DenseBlock1_Layer6              (1, 88, 32, 32)               8,220\n",
      " 8. Transition1                     (1, 44, 16, 16)               3,916\n",
      " 9. DenseBlock2_Layer1              (1, 56, 16, 16)               4,764\n",
      "10. DenseBlock2_Layer2              (1, 68, 16, 16)               6,060\n",
      "11. DenseBlock2_Layer3              (1, 80, 16, 16)               7,356\n",
      "12. DenseBlock2_Layer4              (1, 92, 16, 16)               8,652\n",
      "13. DenseBlock2_Layer5              (1, 104, 16, 16)              9,948\n",
      "14. DenseBlock2_Layer6              (1, 116, 16, 16)             11,244\n",
      "15. Transition2                     (1, 58, 8, 8)                 6,786\n",
      "16. DenseBlock3_Layer1              (1, 70, 8, 8)                 6,276\n",
      "17. DenseBlock3_Layer2              (1, 82, 8, 8)                 7,572\n",
      "18. DenseBlock3_Layer3              (1, 94, 8, 8)                 8,868\n",
      "19. DenseBlock3_Layer4              (1, 106, 8, 8)               10,164\n",
      "20. DenseBlock3_Layer5              (1, 118, 8, 8)               11,460\n",
      "21. DenseBlock3_Layer6              (1, 130, 8, 8)               12,756\n",
      "    GlobalAvgPool                   (1, 130)                          0\n",
      "22. FC                              (1, 100)                     13,100\n",
      "===========================================================================\n",
      "Total weight layers:                                        22\n",
      "Total params:                                               159,250\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_params(layer):\n",
    "    count = 0\n",
    "    if hasattr(layer, 'W'):\n",
    "        count += np.prod(layer.W.shape)\n",
    "    if hasattr(layer, 'b'):\n",
    "        count += np.prod(layer.b.shape)\n",
    "    return count\n",
    "\n",
    "def print_densenet22_summary(model, input_shape=(1, 3, 32, 32)):\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Layer (type)':<35}{'Output Shape':<25}{'Param #':>10}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "    x = np.zeros(input_shape)\n",
    "    total_params = 0\n",
    "    layer_idx = 1\n",
    "\n",
    "    x = model.conv1.forward(x)\n",
    "    p = count_params(model.conv1)\n",
    "    print(f\"{layer_idx:>2}. {'Conv1':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "    layer_idx += 1\n",
    "\n",
    "    x = model.bn1.forward(x, train_flg=False)\n",
    "    x = model.relu1.forward(x)\n",
    "\n",
    "    for block_idx, (block, trans) in enumerate([(model.block1, model.trans1), (model.block2, model.trans2)]):\n",
    "        for j, layer in enumerate(block.layers):\n",
    "            x = layer.forward(x, train_flg=False)\n",
    "            p = count_params(layer.conv)\n",
    "            name = f\"DenseBlock{block_idx+1}_Layer{j+1}\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "        x = trans.forward(x, train_flg=False)\n",
    "        p = count_params(trans.conv)\n",
    "        name = f\"Transition{block_idx+1}\"\n",
    "        print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "        total_params += p\n",
    "        layer_idx += 1\n",
    "\n",
    "    for j, layer in enumerate(model.block3.layers):\n",
    "        x = layer.forward(x, train_flg=False)\n",
    "        p = count_params(layer.conv)\n",
    "        name = f\"DenseBlock3_Layer{j+1}\"\n",
    "        print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "        total_params += p\n",
    "        layer_idx += 1\n",
    "\n",
    "    x = x.mean(axis=(2, 3))\n",
    "    print(f\"{'':>3} {'GlobalAvgPool':<32}{str(x.shape):<25}{'0':>10}\", flush=True)\n",
    "\n",
    "    x = model.fc.forward(x)\n",
    "    p = count_params(model.fc)\n",
    "    print(f\"{layer_idx:>2}. {'FC':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Total weight layers:':<60}{layer_idx}\", flush=True)\n",
    "    print(f\"{'Total params:':<60}{total_params:,}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "model = DenseNet22()\n",
    "print_densenet22_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - DenseNet-22 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from common.optimizer import Adam\n",
    "from common.functions import softmax\n",
    "\n",
    "def smooth_labels(y, smoothing=0.1, num_classes=100):\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = (y.shape[0], num_classes)\n",
    "    smooth = np.full(label_shape, smoothing / (num_classes - 1))\n",
    "    smooth[np.arange(y.shape[0]), y] = confidence\n",
    "    return smooth\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name,\n",
    "                 train_data, val_data, test_data,\n",
    "                 epochs=20, batch_size=64, lr=0.01,\n",
    "                 smoothing=0.15):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.val_x, self.val_t = val_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "        self.optimizer = Adam(lr=lr)\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def smooth_labels(self, y, num_classes=100):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        label_shape = (y.shape[0], num_classes)\n",
    "        smooth = np.full(label_shape, self.smoothing / (num_classes - 1), dtype=np.float32)\n",
    "        smooth[np.arange(y.shape[0]), y] = confidence\n",
    "        return smooth\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            dx = (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            dx /= batch_size\n",
    "        return dx, y\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "        if hasattr(self.model.fc, 'W'):\n",
    "            param_dict['fc_W'] = self.model.fc.W\n",
    "            param_dict['fc_b'] = self.model.fc.b\n",
    "            grad_dict['fc_W'] = self.model.fc.dW\n",
    "            grad_dict['fc_b'] = self.model.fc.db\n",
    "\n",
    "        idx = 0\n",
    "        for block in [self.model.block1, self.model.block2, self.model.block3]:\n",
    "            for layer in block.layers:\n",
    "                if hasattr(layer, 'conv'):\n",
    "                    param_dict[f'{idx}_W'] = layer.conv.W\n",
    "                    param_dict[f'{idx}_b'] = layer.conv.b\n",
    "                    grad_dict[f'{idx}_W'] = layer.conv.dW\n",
    "                    grad_dict[f'{idx}_b'] = layer.conv.db\n",
    "                    idx += 1\n",
    "            if hasattr(block, 'trans'):\n",
    "                param_dict[f'{idx}_W'] = block.trans.conv.W\n",
    "                param_dict[f'{idx}_b'] = block.trans.conv.b\n",
    "                grad_dict[f'{idx}_W'] = block.trans.conv.dW\n",
    "                grad_dict[f'{idx}_b'] = block.trans.conv.db\n",
    "                idx += 1\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        if t_batch.ndim == 1:\n",
    "            t_batch = self.smooth_labels(t_batch)\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        dx, _ = self.loss_grad(x_batch, t_batch)\n",
    "        self.model.backward(dx)\n",
    "\n",
    "        if hasattr(self.model, 'clip_weights'):\n",
    "            self.model.clip_weights(clip_value=1.0)\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        patience = 10\n",
    "        best_val_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\n[Epoch {epoch + 1}/{self.epochs}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0 or i == self.iter_per_epoch - 1:\n",
    "                    print(f\"  Iter {i+1:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            val_acc = self.model.accuracy(self.val_x, self.val_t)\n",
    "            val_loss = self.batched_loss(self.val_x, self.val_t, batch_size=128)\n",
    "\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.val_acc_list.append(val_acc)\n",
    "            self.val_loss_list.append(val_loss)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Fine Train Loss: {avg_loss:.4f}, Fine Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}\", flush=True)\n",
    "            print(f\"Time: {elapsed:.2f}s\", flush=True)\n",
    "\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_model(f\"{self.model_name}_epoch{epoch+1}.pkl\")\n",
    "                print(f\">>> Model saved to {self.model_name}_epoch{epoch+1}.pkl\", flush=True)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improve_count = 0\n",
    "                self.save_model(f\"{self.model_name}_best.pkl\")\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "    def batched_loss(self, x, t, batch_size=128):\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "            loss = self.model.loss(x_batch, t_batch)\n",
    "            total_loss += loss * len(x_batch)\n",
    "            total_count += len(x_batch)\n",
    "        return total_loss / total_count\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "        model_state['conv1_W'] = self.model.conv1.W.copy()\n",
    "        model_state['conv1_b'] = self.model.conv1.b.copy()\n",
    "\n",
    "        bn_state = {}\n",
    "        idx = 0\n",
    "        bn_state[f'bn{idx}_gamma'] = self.model.bn1.gamma.copy()\n",
    "        bn_state[f'bn{idx}_beta'] = self.model.bn1.beta.copy()\n",
    "        bn_state[f'bn{idx}_running_mean'] = self.model.bn1.running_mean.copy()\n",
    "        bn_state[f'bn{idx}_running_var'] = self.model.bn1.running_var.copy()\n",
    "        idx += 1\n",
    "\n",
    "        for block in [self.model.block1, self.model.block2, self.model.block3]:\n",
    "            for layer in block.layers:\n",
    "                bn_state[f'bn{idx}_gamma'] = layer.bn.gamma.copy()\n",
    "                bn_state[f'bn{idx}_beta'] = layer.bn.beta.copy()\n",
    "                bn_state[f'bn{idx}_running_mean'] = layer.bn.running_mean.copy()\n",
    "                bn_state[f'bn{idx}_running_var'] = layer.bn.running_var.copy()\n",
    "                idx += 1\n",
    "        for trans in [self.model.trans1, self.model.trans2]:\n",
    "            bn_state[f'bn{idx}_gamma'] = trans.bn.gamma.copy()\n",
    "            bn_state[f'bn{idx}_beta'] = trans.bn.beta.copy()\n",
    "            bn_state[f'bn{idx}_running_mean'] = trans.bn.running_mean.copy()\n",
    "            bn_state[f'bn{idx}_running_var'] = trans.bn.running_var.copy()\n",
    "            idx += 1\n",
    "\n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': self.optimizer.beta1,\n",
    "            'beta2': self.optimizer.beta2,\n",
    "            'm': self.optimizer.m,\n",
    "            'v': self.optimizer.v,\n",
    "            't': self.optimizer.iter\n",
    "        }\n",
    "\n",
    "        save_data = {\n",
    "            'model': model_state,\n",
    "            'bn': bn_state,\n",
    "            'optimizer': optimizer_state,\n",
    "            'train_loss_list': self.train_loss_list,\n",
    "            'train_acc_list': self.train_acc_list,\n",
    "            'val_acc_list': self.val_acc_list,\n",
    "            'val_loss_list': self.val_loss_list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename,\n",
    "                 loss=np.array(self.train_loss_list),\n",
    "                 train_acc=np.array(self.train_acc_list),\n",
    "                 val_acc=np.array(self.val_acc_list),\n",
    "                 val_loss=np.array(self.val_loss_list))\n",
    "        print(f\"Log saved to {filename}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - DenseNet-22_ex1 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Running ex3 : train dataset 100,000 = original + horizontal flip ====\n",
      "\n",
      "[Epoch 1/100]\n",
      "  Iter   1/1406: Loss 6.0309\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      6\u001b[39m x_test, y_test = data[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m trainer = Trainer(\n\u001b[32m      9\u001b[39m     model=model,\n\u001b[32m     10\u001b[39m     model_name=\u001b[33m'\u001b[39m\u001b[33mDenseNet-22_ex3\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     smoothing=\u001b[32m0.15\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m trainer.save_log(\u001b[33m\"\u001b[39m\u001b[33mDenseNet-22_ex3_log.npz\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    109\u001b[39m start_time = time.time()\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.iter_per_epoch):\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     epoch_loss += loss\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i == \u001b[38;5;28mself\u001b[39m.iter_per_epoch - \u001b[32m1\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mTrainer.train_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     89\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.model.loss(x_batch, t_batch)\n\u001b[32m     90\u001b[39m dx, _ = \u001b[38;5;28mself\u001b[39m.loss_grad(x_batch, t_batch)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m'\u001b[39m\u001b[33mclip_weights\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28mself\u001b[39m.model.clip_weights(clip_value=\u001b[32m1.0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mDenseNet22.backward\u001b[39m\u001b[34m(self, dout)\u001b[39m\n\u001b[32m    119\u001b[39m dout = \u001b[38;5;28mself\u001b[39m.block3.backward(dout)\n\u001b[32m    120\u001b[39m dout = \u001b[38;5;28mself\u001b[39m.trans2.backward(dout)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m dout = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m dout = \u001b[38;5;28mself\u001b[39m.trans1.backward(dout)\n\u001b[32m    123\u001b[39m dout = \u001b[38;5;28mself\u001b[39m.block1.backward(dout)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mDenseBlock.backward\u001b[39m\u001b[34m(self, dout)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dout):\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         dout = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dout\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mDenseLayer.backward\u001b[39m\u001b[34m(self, dout)\u001b[39m\n\u001b[32m     34\u001b[39m dx_main = \u001b[38;5;28mself\u001b[39m.conv.backward(dx_main)\n\u001b[32m     35\u001b[39m dx_main = \u001b[38;5;28mself\u001b[39m.relu.backward(dx_main)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m dx_main = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdx_main\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dx_input + dx_main\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/4-1/인공신경망/project/common/layers.py:178\u001b[39m, in \u001b[36mBatchNormalization.backward\u001b[39m\u001b[34m(self, dout)\u001b[39m\n\u001b[32m    175\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.gamma * xn + \u001b[38;5;28mself\u001b[39m.beta\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dout):\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.conv_mode:\n\u001b[32m    180\u001b[39m         N, C, H, W = dout.shape\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n==== Running ex3 : train dataset 100,000 = original + horizontal flip ====\")\n",
    "model = DenseNet22()  \n",
    "\n",
    "x_train, y_train = data['train_flip']\n",
    "x_val, y_val = data['val_flip']\n",
    "x_test, y_test = data['test']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    model_name='DenseNet-22_ex3',\n",
    "    train_data=(x_train, y_train),\n",
    "    val_data=(x_val, y_val),\n",
    "    test_data=(x_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    lr=0.001,\n",
    "    smoothing=0.15\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_log(\"DenseNet-22_ex3_log.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"DenseNet-22_ex3_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"DenseNet-22_ex3_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"learning_curve_loss.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"learning_curve_accuracy.png\", dpi=300)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

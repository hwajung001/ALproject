{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet-22 final Train code\n",
    "\n",
    "### ex 1 : train dataset 150,000 = original + random crop + horizontal flip\n",
    "##### (random seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - CIFAR-100 데이터 다운로드 및 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 already downloaded.\n",
      "Generating augmented dataset with crop + flip...\n",
      "train_cropflip: [(135000, 3, 32, 32), (135000,)]\n",
      "val_cropflip: [(15000, 3, 32, 32), (15000,)]\n",
      "test: [(10000, 3, 32, 32), (10000,)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # ex1 random seed\n",
    "\n",
    "def download_cifar100(save_path='cifar-100-python'):\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "        return\n",
    "\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
    "    filename = 'cifar-100-python.tar.gz'\n",
    "    print(\"Downloading CIFAR-100...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "    with tarfile.open(filename, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "    os.remove(filename)\n",
    "    print(\"Download and extraction completed.\")\n",
    "\n",
    "def load_batch(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "    data = data_dict[b'data']\n",
    "    fine_labels = np.array(data_dict[b'fine_labels'])\n",
    "    data = data.reshape(-1, 3, 32, 32)\n",
    "    return data, fine_labels\n",
    "\n",
    "def normalize_images(images):\n",
    "    return images.astype(np.float32) / 255.0\n",
    "\n",
    "def split_validation(images, labels, val_ratio=0.1):\n",
    "    num_samples = images.shape[0]\n",
    "    val_size = int(num_samples * val_ratio)\n",
    "\n",
    "    idx = np.random.permutation(num_samples)\n",
    "    images = images[idx]\n",
    "    labels = labels[idx]\n",
    "\n",
    "    val_images = images[:val_size]\n",
    "    val_labels = labels[:val_size]\n",
    "    train_images = images[val_size:]\n",
    "    train_labels = labels[val_size:]\n",
    "\n",
    "    return (train_images, train_labels), (val_images, val_labels)\n",
    "\n",
    "def random_crop(x, crop_size=32, padding=4):\n",
    "    n, c, h, w = x.shape\n",
    "    padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='reflect')\n",
    "    cropped = np.empty((n, c, crop_size, crop_size), dtype=x.dtype)\n",
    "    for i in range(n):\n",
    "        top = np.random.randint(0, padding * 2 + 1)\n",
    "        left = np.random.randint(0, padding * 2 + 1)\n",
    "        cropped[i] = padded[i, :, top:top+crop_size, left:left+crop_size]\n",
    "    return cropped\n",
    "\n",
    "def horizontal_flip(x):\n",
    "    return x[:, :, :, ::-1]\n",
    "\n",
    "def load_cifar100_dataset():\n",
    "    download_cifar100()\n",
    "    train_data, train_fine = load_batch('cifar-100-python/train')\n",
    "    test_data, test_fine = load_batch('cifar-100-python/test')\n",
    "    train_data = normalize_images(train_data)\n",
    "    test_data = normalize_images(test_data)\n",
    "    return (train_data, train_fine), (test_data, test_fine)\n",
    "\n",
    "def generate_augmented_dataset(images, labels, target_size):\n",
    "    N = images.shape[0]\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    repeat = target_size // (N * 2) + 1  \n",
    "\n",
    "    for _ in range(repeat):\n",
    "        imgs_crop = random_crop(images.copy())\n",
    "        imgs_flip = horizontal_flip(imgs_crop.copy())\n",
    "\n",
    "        augmented_images.append(imgs_crop)\n",
    "        augmented_labels.append(labels.copy())\n",
    "\n",
    "        augmented_images.append(imgs_flip)\n",
    "        augmented_labels.append(labels.copy())\n",
    "\n",
    "        if sum(x.shape[0] for x in augmented_images) >= target_size:\n",
    "            break\n",
    "\n",
    "    X = np.concatenate(augmented_images, axis=0)[:target_size]\n",
    "    y = np.concatenate(augmented_labels, axis=0)[:target_size]\n",
    "    return X, y\n",
    "\n",
    "def prepare_dataset():\n",
    "    (full_train_images, full_train_labels), (test_images, test_labels) = load_cifar100_dataset()\n",
    "    print(\"Generating augmented dataset with crop + flip...\")\n",
    "\n",
    "    X_aug, y_aug = generate_augmented_dataset(full_train_images, full_train_labels, target_size=150000)\n",
    "    train_aug, val_aug = split_validation(X_aug, y_aug)\n",
    "\n",
    "    return {\n",
    "        'train_cropflip': train_aug,\n",
    "        'val_cropflip': val_aug,\n",
    "        'test': (test_images, test_labels)\n",
    "    }\n",
    "\n",
    "data = prepare_dataset()\n",
    "for k, v in data.items():\n",
    "    if isinstance(v, tuple):\n",
    "        print(f\"{k}: {[x.shape for x in v]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Convolution, BatchNormalization, Relu, Pooling, Affine\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "def fake_quantize(x, num_bits=8):\n",
    "    qmin, qmax = 0., 2.**num_bits - 1.\n",
    "    x_min, x_max = np.min(x), np.max(x)\n",
    "    if x_max == x_min:\n",
    "        return x\n",
    "    scale = (x_max - x_min) / (qmax - qmin)\n",
    "    zero_point = np.clip(np.round(qmin - x_min / scale), qmin, qmax)\n",
    "    q_x = np.clip(np.round(zero_point + x / scale), qmin, qmax)\n",
    "    return scale * (q_x - zero_point)\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.orig_shape = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.orig_shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.orig_shape)\n",
    "\n",
    "\n",
    "class MiniVGGNet_Modified:\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_classes=100):\n",
    "        in_channels, _, _ = input_dim\n",
    "        weight_std = np.sqrt(2. / in_channels)\n",
    "\n",
    "        # Block 1\n",
    "        self.conv1 = Convolution(np.random.randn(64, in_channels, 3, 3) * weight_std, np.zeros(64), stride=1, pad=1)\n",
    "        self.bn1   = BatchNormalization(np.ones(64), np.zeros(64))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.conv2 = Convolution(np.random.randn(64, 64, 3, 3) * weight_std, np.zeros(64), stride=1, pad=1)\n",
    "        self.bn2   = BatchNormalization(np.ones(64), np.zeros(64))\n",
    "        self.relu2 = Relu()\n",
    "        self.pool1 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        # Block 2\n",
    "        self.conv3 = Convolution(np.random.randn(128, 64, 3, 3) * weight_std, np.zeros(128), stride=1, pad=1)\n",
    "        self.bn3   = BatchNormalization(np.ones(128), np.zeros(128))\n",
    "        self.relu3 = Relu()\n",
    "\n",
    "        self.conv4 = Convolution(np.random.randn(128, 128, 3, 3) * weight_std, np.zeros(128), stride=1, pad=1)\n",
    "        self.bn4   = BatchNormalization(np.ones(128), np.zeros(128))\n",
    "        self.relu4 = Relu()\n",
    "        self.pool2 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        # Block 3\n",
    "        self.conv5 = Convolution(np.random.randn(256, 128, 3, 3) * weight_std, np.zeros(256), stride=1, pad=1)\n",
    "        self.bn5   = BatchNormalization(np.ones(256), np.zeros(256)) #conv5 also \n",
    "        self.relu5 = Relu()\n",
    "        self.pool3 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        # Classifier\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Affine(np.random.randn(4096, 512) * weight_std, np.zeros(512))\n",
    "        self.relu6 = Relu()\n",
    "        self.fc2 = Affine(np.random.randn(512, num_classes) * 0.01, np.zeros(num_classes))\n",
    "\n",
    "        self.layers = [\n",
    "            self.conv1, self.bn1, self.relu1,\n",
    "            self.conv2, self.bn2, self.relu2, self.pool1,\n",
    "            self.conv3, self.bn3, self.relu3,\n",
    "            self.conv4, self.bn4, self.relu4, self.pool2,\n",
    "            self.conv5, self.bn5, self.relu5, self.pool3, #conv5\n",
    "            self.flatten, self.fc1, self.relu6, self.fc2\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNormalization):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # Trainer에서 fc2.backward(dx) 먼저 해줄 것!\n",
    "        dout = self.relu6.backward(dout)\n",
    "        dout = self.fc1.backward(dout)\n",
    "        dout = self.flatten.backward(dout)\n",
    "\n",
    "        dout = self.pool3.backward(dout)\n",
    "        dout = self.relu5.backward(dout)\n",
    "        dout = self.bn5.backward(dout)\n",
    "        dout = self.conv5.backward(dout)\n",
    "\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.relu4.backward(dout)\n",
    "        dout = self.bn4.backward(dout)\n",
    "        dout = self.conv4.backward(dout)\n",
    "\n",
    "        dout = self.relu3.backward(dout)\n",
    "        dout = self.bn3.backward(dout)\n",
    "        dout = self.conv3.backward(dout)\n",
    "\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "\n",
    "        return dout\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # Forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # Backward\n",
    "        dout = 1\n",
    "        dout = self.fc2.backward(dout)\n",
    "        dout = self.relu6.backward(dout)\n",
    "        dout = self.fc1.backward(dout)\n",
    "        dout = self.flatten.backward(dout)\n",
    "\n",
    "        dout = self.pool3.backward(dout)\n",
    "        dout = self.relu5.backward(dout)\n",
    "        dout = self.bn5.backward(dout)\n",
    "        dout = self.conv5.backward(dout)\n",
    "\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.relu4.backward(dout)\n",
    "        dout = self.bn4.backward(dout)\n",
    "        dout = self.conv4.backward(dout)\n",
    "\n",
    "        dout = self.relu3.backward(dout)\n",
    "        dout = self.bn3.backward(dout)\n",
    "        dout = self.conv3.backward(dout)\n",
    "\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "\n",
    "        # Gather gradients\n",
    "        grads = {\n",
    "            'W1': self.conv1.W, 'b1': self.conv1.b,\n",
    "            'gamma1': self.bn1.gamma, 'beta1': self.bn1.beta,\n",
    "            'W2': self.conv2.W, 'b2': self.conv2.b,\n",
    "            'gamma2': self.bn2.gamma, 'beta2': self.bn2.beta,\n",
    "            'W3': self.conv3.W, 'b3': self.conv3.b,\n",
    "            'gamma3': self.bn3.gamma, 'beta3': self.bn3.beta,\n",
    "            'W4': self.conv4.W, 'b4': self.conv4.b,\n",
    "            'gamma4': self.bn4.gamma, 'beta4': self.bn4.beta,\n",
    "            'W5': self.conv5.W, 'b5': self.conv5.b,\n",
    "            'gamma5': self.bn5.gamma, 'beta5': self.bn5.beta,  \n",
    "            'W6': self.fc1.W, 'b6': self.fc1.b,\n",
    "            'W7': self.fc2.W, 'b7': self.fc2.b,\n",
    "        }\n",
    "\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def predict(self, x, batch_size=100):\n",
    "        return np.concatenate([self.forward(x[i:i+batch_size], False) for i in range(0, x.shape[0], batch_size)], axis=0)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.forward(x, True)\n",
    "        y_softmax = softmax(y)\n",
    "    \n",
    "        #print(\"logit range before softmax:\", np.min(y), np.max(y))  # 로그 찍어봐\n",
    "        #print(\"softmax range:\", np.min(y_softmax), np.max(y_softmax))\n",
    "    \n",
    "        return cross_entropy_error(y_softmax, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        pred = np.argmax(self.predict(x, batch_size), axis=1)\n",
    "        true = t if t.ndim == 1 else np.argmax(t, axis=1)\n",
    "        return np.mean(pred == true)\n",
    "\n",
    "    def clip_weights(self, clip_value=1.0): \n",
    "        for layer in [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.fc1, self.fc2]:\n",
    "            layer.W = np.clip(layer.W, -clip_value, clip_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 구조 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type)                       Output Shape                Param #\n",
      "===========================================================================\n",
      " 1. Conv1                           (1, 64, 32, 32)               1,792\n",
      " 2. BN1                             (1, 64, 32, 32)                 128\n",
      " 3. Conv2                           (1, 64, 32, 32)              36,928\n",
      " 4. BN2                             (1, 64, 32, 32)                 128\n",
      " 5. Conv3                           (1, 128, 16, 16)             73,856\n",
      " 6. BN3                             (1, 128, 16, 16)                256\n",
      " 7. Conv4                           (1, 128, 16, 16)            147,584\n",
      " 8. BN4                             (1, 128, 16, 16)                256\n",
      " 9. Conv5                           (1, 256, 8, 8)              295,168\n",
      "10. Flatten                         (1, 4096)                         0\n",
      "11. FC1                             (1, 512)                  2,097,664\n",
      "12. FC2                             (1, 100)                     51,300\n",
      "===========================================================================\n",
      "Total weight layers:                                        13\n",
      "Total params:                                               2,705,060\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "from common.layers import Convolution, BatchNormalization, Relu, Pooling, Affine\n",
    "import numpy as np\n",
    "\n",
    "def count_params(layer):\n",
    "    count = 0\n",
    "    if hasattr(layer, 'W'):\n",
    "        count += np.prod(layer.W.shape)\n",
    "    if hasattr(layer, 'b'):\n",
    "        count += np.prod(layer.b.shape)\n",
    "    if hasattr(layer, 'gamma'):\n",
    "        count += np.prod(layer.gamma.shape)\n",
    "    if hasattr(layer, 'beta'):\n",
    "        count += np.prod(layer.beta.shape)\n",
    "    return count\n",
    "\n",
    "def print_vggnet_summary(model, input_shape=(1, 3, 32, 32)):\n",
    "    print(\"=\" * 75)\n",
    "    print(f\"{'Layer (type)':<35}{'Output Shape':<25}{'Param #':>10}\")\n",
    "    print(\"=\" * 75)\n",
    "\n",
    "    x = np.zeros(input_shape)\n",
    "    total_params = 0\n",
    "    layer_idx = 1\n",
    "\n",
    "    def log(name, x, p):\n",
    "        nonlocal total_params, layer_idx\n",
    "        print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\")\n",
    "        total_params += p\n",
    "        layer_idx += 1\n",
    "\n",
    "    # Block 1\n",
    "    x = model.conv1.forward(x)\n",
    "    log(\"Conv1\", x, count_params(model.conv1))\n",
    "    x = model.bn1.forward(x, train_flg=False)\n",
    "    log(\"BN1\", x, count_params(model.bn1))\n",
    "    x = model.relu1.forward(x)\n",
    "\n",
    "    x = model.conv2.forward(x)\n",
    "    log(\"Conv2\", x, count_params(model.conv2))\n",
    "    x = model.bn2.forward(x, train_flg=False)\n",
    "    log(\"BN2\", x, count_params(model.bn2))\n",
    "    x = model.relu2.forward(x)\n",
    "    x = model.pool1.forward(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = model.conv3.forward(x)\n",
    "    log(\"Conv3\", x, count_params(model.conv3))\n",
    "    x = model.bn3.forward(x, train_flg=False)\n",
    "    log(\"BN3\", x, count_params(model.bn3))\n",
    "    x = model.relu3.forward(x)\n",
    "\n",
    "    x = model.conv4.forward(x)\n",
    "    log(\"Conv4\", x, count_params(model.conv4))\n",
    "    x = model.bn4.forward(x, train_flg=False)\n",
    "    log(\"BN4\", x, count_params(model.bn4))\n",
    "    x = model.relu4.forward(x)\n",
    "    x = model.pool2.forward(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = model.conv5.forward(x)\n",
    "    log(\"Conv5\", x, count_params(model.conv5))\n",
    "    x = model.relu5.forward(x)\n",
    "    x = model.pool3.forward(x)\n",
    "\n",
    "    # Flatten\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    log(\"Flatten\", x, 0)\n",
    "\n",
    "    # FC layers\n",
    "    x = model.fc1.forward(x)\n",
    "    log(\"FC1\", x, count_params(model.fc1))\n",
    "    x = model.relu6.forward(x)\n",
    "    x = model.fc2.forward(x)\n",
    "    log(\"FC2\", x, count_params(model.fc2))\n",
    "\n",
    "    print(\"=\" * 75)\n",
    "    print(f\"{'Total weight layers:':<60}{layer_idx}\")\n",
    "    print(f\"{'Total params:':<60}{total_params:,}\")\n",
    "    print(\"=\" * 75)\n",
    "\n",
    "\n",
    "\n",
    "model = MiniVGGNet_Modified()\n",
    "print_vggnet_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from common.optimizer import Adam\n",
    "from common.functions import softmax\n",
    "\n",
    "def smooth_labels(y, smoothing=0.1, num_classes=100):\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = (y.shape[0], num_classes)\n",
    "    smooth = np.full(label_shape, smoothing / (num_classes - 1))\n",
    "    smooth[np.arange(y.shape[0]), y] = confidence\n",
    "    return smooth\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name,\n",
    "                 train_data, val_data, test_data,\n",
    "                 epochs=20, batch_size=64, lr=0.01,\n",
    "                 smoothing=0.15):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.val_x, self.val_t = val_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "        self.optimizer = Adam(lr=lr)\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def smooth_labels(self, y, num_classes=100):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        label_shape = (y.shape[0], num_classes)\n",
    "        smooth = np.full(label_shape, self.smoothing / (num_classes - 1), dtype=np.float32)\n",
    "        smooth[np.arange(y.shape[0]), y] = confidence\n",
    "        return smooth\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            dx = (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            dx /= batch_size\n",
    "        return dx, y\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "\n",
    "        for name in ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc1', 'fc2']:\n",
    "            layer = getattr(self.model, name)\n",
    "            if hasattr(layer, 'W'):\n",
    "                param_dict[f'{name}_W'] = layer.W\n",
    "                param_dict[f'{name}_b'] = layer.b\n",
    "                grad_dict[f'{name}_W'] = layer.dW\n",
    "                grad_dict[f'{name}_b'] = layer.db\n",
    "\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        if t_batch.ndim == 1:\n",
    "            t_batch = self.smooth_labels(t_batch)\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        dx, _ = self.loss_grad(x_batch, t_batch)\n",
    "        dout = self.model.fc2.backward(dx)\n",
    "\n",
    "        # 나머지는 모델 내부 backward 흐름에 위임 (fc2는 제외된 상태여야 함)\n",
    "        self.model.backward(dout)\n",
    "\n",
    "        if hasattr(self.model, 'clip_weights'):\n",
    "            self.model.clip_weights(clip_value=1.0)\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        patience = 10\n",
    "        best_val_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\n[Epoch {epoch + 1}/{self.epochs}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0 or i == self.iter_per_epoch - 1:\n",
    "                    print(f\"  Iter {i+1:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            val_acc = self.model.accuracy(self.val_x, self.val_t)\n",
    "            val_loss = self.batched_loss(self.val_x, self.val_t, batch_size=128)\n",
    "\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.val_acc_list.append(val_acc)\n",
    "            self.val_loss_list.append(val_loss)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Fine Train Loss: {avg_loss:.4f}, Fine Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}\", flush=True)\n",
    "            print(f\"Time: {elapsed:.2f}s\", flush=True)\n",
    "\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_model(f\"{self.model_name}_epoch{epoch+1}.pkl\")\n",
    "                print(f\">>> Model saved to {self.model_name}_epoch{epoch+1}.pkl\", flush=True)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improve_count = 0\n",
    "                self.save_model(f\"{self.model_name}_best.pkl\")\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "    def batched_loss(self, x, t, batch_size=128):\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        num_classes = 100  # CIFAR-100 기준\n",
    "    \n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "    \n",
    "            # 정수형이면 one-hot으로 변환\n",
    "            if t_batch.ndim == 1:\n",
    "                t_onehot = np.zeros((t_batch.size, num_classes), dtype=np.float32)\n",
    "                t_onehot[np.arange(t_batch.size), t_batch] = 1.0\n",
    "            else:\n",
    "                t_onehot = t_batch  # 이미 one-hot이면 그대로\n",
    "    \n",
    "            loss = self.model.loss(x_batch, t_onehot)\n",
    "            total_loss += loss * len(x_batch)\n",
    "            total_count += len(x_batch)\n",
    "    \n",
    "        return total_loss / total_count\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "\n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': self.optimizer.beta1,\n",
    "            'beta2': self.optimizer.beta2,\n",
    "            'm': self.optimizer.m,\n",
    "            'v': self.optimizer.v,\n",
    "            't': self.optimizer.iter\n",
    "        }\n",
    "\n",
    "        save_data = {\n",
    "            'model': model_state,\n",
    "            'optimizer': optimizer_state,\n",
    "            'train_loss_list': self.train_loss_list,\n",
    "            'train_acc_list': self.train_acc_list,\n",
    "            'val_acc_list': self.val_acc_list,\n",
    "            'val_loss_list': self.val_loss_list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename,\n",
    "                 loss=np.array(self.train_loss_list),\n",
    "                 train_acc=np.array(self.train_acc_list),\n",
    "                 val_acc=np.array(self.val_acc_list),\n",
    "                 val_loss=np.array(self.val_loss_list))\n",
    "        print(f\"Log saved to {filename}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vgg_style(x):\n",
    "    mean = np.array([0.5071, 0.4865, 0.4409]).reshape(1, 3, 1, 1)\n",
    "    std  = np.array([0.2673, 0.2564, 0.2762]).reshape(1, 3, 1, 1)\n",
    "    return (x - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Running ex4 : train dataset 150,000 = original +BN (O) ====\n",
      "\n",
      "[Epoch 1/100]\n",
      "  Iter   1/312: Loss 14.2077\n",
      "  Iter  11/312: Loss 12.2191\n",
      "  Iter  21/312: Loss 8.7956\n",
      "  Iter  31/312: Loss 6.7306\n",
      "  Iter  41/312: Loss 5.3680\n",
      "  Iter  51/312: Loss 4.9275\n",
      "  Iter  61/312: Loss 4.4535\n",
      "  Iter  71/312: Loss 4.1258\n",
      "  Iter  81/312: Loss 4.5980\n",
      "  Iter  91/312: Loss 3.7955\n",
      "  Iter 101/312: Loss 4.1181\n",
      "  Iter 111/312: Loss 4.1928\n",
      "  Iter 121/312: Loss 4.0127\n",
      "  Iter 131/312: Loss 4.2075\n",
      "  Iter 141/312: Loss 3.9046\n",
      "  Iter 151/312: Loss 3.7905\n",
      "  Iter 161/312: Loss 4.0537\n",
      "  Iter 171/312: Loss 3.7469\n",
      "  Iter 181/312: Loss 3.9985\n",
      "  Iter 191/312: Loss 3.7527\n",
      "  Iter 201/312: Loss 3.7788\n",
      "  Iter 211/312: Loss 3.6829\n",
      "  Iter 221/312: Loss 3.6891\n",
      "  Iter 231/312: Loss 3.4636\n",
      "  Iter 241/312: Loss 3.4301\n",
      "  Iter 251/312: Loss 3.6785\n",
      "  Iter 261/312: Loss 3.5381\n",
      "  Iter 271/312: Loss 3.0091\n",
      "  Iter 281/312: Loss 3.6153\n",
      "  Iter 291/312: Loss 3.6438\n",
      "  Iter 301/312: Loss 3.5661\n",
      "  Iter 311/312: Loss 3.3126\n",
      "  Iter 312/312: Loss 3.2150\n",
      "Fine Train Loss: 4.5453, Fine Train Acc: 0.2270, Val Acc: 0.1198, Val Loss: 3.9850\n",
      "Time: 1287.76s\n",
      "\n",
      "[Epoch 2/100]\n",
      "  Iter   1/312: Loss 3.7266\n",
      "  Iter  11/312: Loss 3.5077\n",
      "  Iter  21/312: Loss 3.2574\n",
      "  Iter  31/312: Loss 3.4736\n",
      "  Iter  41/312: Loss 3.0196\n",
      "  Iter  51/312: Loss 3.5967\n",
      "  Iter  61/312: Loss 3.6613\n",
      "  Iter  71/312: Loss 3.3964\n",
      "  Iter  81/312: Loss 3.4332\n",
      "  Iter  91/312: Loss 3.5569\n",
      "  Iter 101/312: Loss 3.2040\n",
      "  Iter 111/312: Loss 3.0209\n",
      "  Iter 121/312: Loss 3.0354\n",
      "  Iter 131/312: Loss 3.3255\n",
      "  Iter 141/312: Loss 2.9564\n",
      "  Iter 151/312: Loss 3.1437\n",
      "  Iter 161/312: Loss 3.0372\n",
      "  Iter 171/312: Loss 3.3200\n",
      "  Iter 181/312: Loss 3.1343\n",
      "  Iter 191/312: Loss 3.2367\n",
      "  Iter 201/312: Loss 3.1796\n",
      "  Iter 211/312: Loss 3.2330\n",
      "  Iter 221/312: Loss 3.3101\n",
      "  Iter 231/312: Loss 3.5020\n",
      "  Iter 241/312: Loss 3.0506\n",
      "  Iter 251/312: Loss 3.1693\n",
      "  Iter 261/312: Loss 3.1214\n",
      "  Iter 271/312: Loss 2.8817\n",
      "  Iter 281/312: Loss 2.9359\n",
      "  Iter 291/312: Loss 3.0172\n",
      "  Iter 301/312: Loss 3.1045\n",
      "  Iter 311/312: Loss 2.8153\n",
      "  Iter 312/312: Loss 3.2277\n",
      "Fine Train Loss: 3.1787, Fine Train Acc: 0.3270, Val Acc: 0.1394, Val Loss: 3.9339\n",
      "Time: 1295.35s\n",
      "\n",
      "[Epoch 3/100]\n",
      "  Iter   1/312: Loss 2.8162\n",
      "  Iter  11/312: Loss 2.8134\n",
      "  Iter  21/312: Loss 3.1818\n",
      "  Iter  31/312: Loss 2.4083\n",
      "  Iter  41/312: Loss 2.9312\n",
      "  Iter  51/312: Loss 2.9349\n",
      "  Iter  61/312: Loss 3.1490\n",
      "  Iter  71/312: Loss 2.6592\n",
      "  Iter  81/312: Loss 3.1755\n",
      "  Iter  91/312: Loss 2.9356\n",
      "  Iter 101/312: Loss 2.9645\n",
      "  Iter 111/312: Loss 2.9770\n",
      "  Iter 121/312: Loss 3.0648\n",
      "  Iter 131/312: Loss 2.9510\n",
      "  Iter 141/312: Loss 3.0561\n",
      "  Iter 151/312: Loss 2.8696\n",
      "  Iter 161/312: Loss 2.6939\n",
      "  Iter 171/312: Loss 2.8884\n",
      "  Iter 181/312: Loss 2.9888\n",
      "  Iter 191/312: Loss 2.6143\n",
      "  Iter 201/312: Loss 2.9355\n",
      "  Iter 211/312: Loss 2.7779\n",
      "  Iter 221/312: Loss 2.5877\n",
      "  Iter 231/312: Loss 2.5041\n",
      "  Iter 241/312: Loss 2.3148\n",
      "  Iter 251/312: Loss 2.9503\n",
      "  Iter 261/312: Loss 2.4173\n",
      "  Iter 271/312: Loss 2.5312\n",
      "  Iter 281/312: Loss 2.5518\n",
      "  Iter 291/312: Loss 2.6021\n",
      "  Iter 301/312: Loss 2.6153\n",
      "  Iter 311/312: Loss 2.4980\n",
      "  Iter 312/312: Loss 2.8234\n",
      "Fine Train Loss: 2.8122, Fine Train Acc: 0.3820, Val Acc: 0.1464, Val Loss: 3.9876\n",
      "Time: 1298.03s\n",
      "\n",
      "[Epoch 4/100]\n",
      "  Iter   1/312: Loss 2.6302\n",
      "  Iter  11/312: Loss 2.7357\n",
      "  Iter  21/312: Loss 2.3869\n",
      "  Iter  31/312: Loss 2.4751\n",
      "  Iter  41/312: Loss 2.5306\n",
      "  Iter  51/312: Loss 2.6579\n",
      "  Iter  61/312: Loss 2.2367\n",
      "  Iter  71/312: Loss 2.6586\n",
      "  Iter  81/312: Loss 2.5628\n",
      "  Iter  91/312: Loss 2.6539\n",
      "  Iter 101/312: Loss 2.4700\n",
      "  Iter 111/312: Loss 2.5998\n",
      "  Iter 121/312: Loss 2.5869\n",
      "  Iter 131/312: Loss 2.6099\n",
      "  Iter 141/312: Loss 2.2988\n",
      "  Iter 151/312: Loss 2.9117\n",
      "  Iter 161/312: Loss 2.2555\n",
      "  Iter 171/312: Loss 2.1574\n",
      "  Iter 181/312: Loss 2.3743\n",
      "  Iter 191/312: Loss 2.3988\n",
      "  Iter 201/312: Loss 2.0127\n",
      "  Iter 211/312: Loss 2.5497\n",
      "  Iter 221/312: Loss 2.7293\n",
      "  Iter 231/312: Loss 2.5532\n",
      "  Iter 241/312: Loss 2.1823\n",
      "  Iter 251/312: Loss 2.1421\n",
      "  Iter 261/312: Loss 2.5030\n",
      "  Iter 271/312: Loss 2.5187\n",
      "  Iter 281/312: Loss 2.1465\n",
      "  Iter 291/312: Loss 2.3937\n",
      "  Iter 301/312: Loss 2.4864\n",
      "  Iter 311/312: Loss 2.6893\n",
      "  Iter 312/312: Loss 2.1459\n",
      "Fine Train Loss: 2.5105, Fine Train Acc: 0.4560, Val Acc: 0.1430, Val Loss: 4.0513\n",
      "Time: 1299.37s\n",
      "\n",
      "[Epoch 5/100]\n",
      "  Iter   1/312: Loss 2.5012\n",
      "  Iter  11/312: Loss 2.5603\n",
      "  Iter  21/312: Loss 2.3060\n",
      "  Iter  31/312: Loss 2.3524\n",
      "  Iter  41/312: Loss 2.1278\n",
      "  Iter  51/312: Loss 2.3620\n",
      "  Iter  61/312: Loss 2.0699\n",
      "  Iter  71/312: Loss 2.3412\n",
      "  Iter  81/312: Loss 2.2880\n",
      "  Iter  91/312: Loss 2.0022\n",
      "  Iter 101/312: Loss 2.7193\n",
      "  Iter 111/312: Loss 2.5066\n",
      "  Iter 121/312: Loss 2.2241\n",
      "  Iter 131/312: Loss 2.1695\n",
      "  Iter 141/312: Loss 2.1545\n",
      "  Iter 151/312: Loss 2.1955\n",
      "  Iter 161/312: Loss 2.2652\n",
      "  Iter 171/312: Loss 2.0661\n",
      "  Iter 181/312: Loss 2.1794\n",
      "  Iter 191/312: Loss 1.8881\n",
      "  Iter 201/312: Loss 2.0980\n",
      "  Iter 211/312: Loss 2.3579\n",
      "  Iter 221/312: Loss 2.3084\n",
      "  Iter 231/312: Loss 2.0884\n",
      "  Iter 241/312: Loss 2.2040\n",
      "  Iter 251/312: Loss 2.1871\n",
      "  Iter 261/312: Loss 2.5622\n",
      "  Iter 271/312: Loss 2.7500\n",
      "  Iter 281/312: Loss 2.1330\n",
      "  Iter 291/312: Loss 2.2541\n",
      "  Iter 301/312: Loss 2.1034\n",
      "  Iter 311/312: Loss 2.0987\n",
      "  Iter 312/312: Loss 2.4449\n",
      "Fine Train Loss: 2.2394, Fine Train Acc: 0.5120, Val Acc: 0.1494, Val Loss: 4.1563\n",
      "Time: 1300.50s\n",
      ">>> Model saved to vgg_ex4_epoch5.pkl\n",
      "\n",
      "[Epoch 6/100]\n",
      "  Iter   1/312: Loss 2.3677\n",
      "  Iter  11/312: Loss 2.3299\n",
      "  Iter  21/312: Loss 1.7118\n",
      "  Iter  31/312: Loss 1.6498\n",
      "  Iter  41/312: Loss 1.7932\n",
      "  Iter  51/312: Loss 1.8368\n",
      "  Iter  61/312: Loss 1.7789\n",
      "  Iter  71/312: Loss 2.2696\n",
      "  Iter  81/312: Loss 1.8041\n",
      "  Iter  91/312: Loss 2.3371\n",
      "  Iter 101/312: Loss 2.3694\n",
      "  Iter 111/312: Loss 1.9205\n",
      "  Iter 121/312: Loss 2.0343\n",
      "  Iter 131/312: Loss 2.3463\n",
      "  Iter 141/312: Loss 2.1256\n",
      "  Iter 151/312: Loss 1.8096\n",
      "  Iter 161/312: Loss 1.6887\n",
      "  Iter 171/312: Loss 1.5241\n",
      "  Iter 181/312: Loss 1.9812\n",
      "  Iter 191/312: Loss 1.8090\n",
      "  Iter 201/312: Loss 2.0034\n",
      "  Iter 211/312: Loss 2.2178\n",
      "  Iter 221/312: Loss 2.3027\n",
      "  Iter 231/312: Loss 2.2643\n",
      "  Iter 241/312: Loss 2.0407\n",
      "  Iter 251/312: Loss 1.8392\n",
      "  Iter 261/312: Loss 1.8892\n",
      "  Iter 271/312: Loss 1.9135\n",
      "  Iter 281/312: Loss 1.8238\n",
      "  Iter 291/312: Loss 1.7366\n",
      "  Iter 301/312: Loss 2.2022\n",
      "  Iter 311/312: Loss 1.9565\n",
      "  Iter 312/312: Loss 2.1244\n",
      "Fine Train Loss: 2.0099, Fine Train Acc: 0.5590, Val Acc: 0.1578, Val Loss: 4.2682\n",
      "Time: 1307.22s\n",
      "\n",
      "[Epoch 7/100]\n",
      "  Iter   1/312: Loss 1.9965\n",
      "  Iter  11/312: Loss 1.7219\n",
      "  Iter  21/312: Loss 1.8331\n",
      "  Iter  31/312: Loss 1.6965\n",
      "  Iter  41/312: Loss 1.9169\n",
      "  Iter  51/312: Loss 1.8371\n",
      "  Iter  61/312: Loss 1.4786\n",
      "  Iter  71/312: Loss 1.8161\n",
      "  Iter  81/312: Loss 1.9199\n",
      "  Iter  91/312: Loss 1.7385\n",
      "  Iter 101/312: Loss 1.8306\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==== Running ex4 : train dataset 150,000 = original +BN (O) ====\")\n",
    "model = MiniVGGNet_Modified()\n",
    "\n",
    "x_train, y_train = data['train_cropflip']\n",
    "x_val, y_val = data['val_cropflip']\n",
    "x_test, y_test = data['test']\n",
    "\n",
    "x_train, y_train = x_train[:20000], y_train[:20000]\n",
    "x_val, y_val     = x_val[:5000], y_val[:5000]\n",
    "x_test, y_test   = x_test[:10000], y_test[:10000]\n",
    "\n",
    "\n",
    "x_train = normalize_vgg_style(x_train)\n",
    "x_val = normalize_vgg_style(x_val)\n",
    "x_test = normalize_vgg_style(x_test)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    model_name='vgg_ex4',\n",
    "    train_data=(x_train, y_train),\n",
    "    val_data=(x_val, y_val),\n",
    "    test_data=(x_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    lr=0.001,\n",
    "    smoothing=0.1\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_log(\"miniVGG_baseline_ex4_log.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'miniVGG_baseline_ex2_log.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m smooth_curve\n\u001b[1;32m----> 5\u001b[0m log \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminiVGG_baseline_ex2_log.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'miniVGG_baseline_ex2_log.npz'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"miniVGG_baseline_ex4_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"miniVGG_baseline_ex3_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"learning_curve_loss.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"learning_curve_accuracy.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

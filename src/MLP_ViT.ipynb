{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e625f02-581d-42ef-8184-a4c6535cfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드 및 전처리\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "def download_cifar100(dest=\"./cifar-100-python\"):\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    \n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest, exist_ok=True)\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        with tarfile.open(filename, \"r:gz\") as tar:\n",
    "            tar.extractall()\n",
    "        print(\"CIFAR-100 downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "\n",
    "def load_cifar100(data_dir=\"./cifar-100-python\"):\n",
    "    def load_batch(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            dict = pickle.load(f, encoding='bytes')\n",
    "            data = dict[b'data']\n",
    "            labels = dict[b'fine_labels']\n",
    "            coarse_labels = dict[b'coarse_labels']\n",
    "            return data, labels, coarse_labels\n",
    "\n",
    "    x_train, y_train, y_train_coarse = load_batch(os.path.join(data_dir, \"train\"))\n",
    "    x_test, y_test, y_test_coarse = load_batch(os.path.join(data_dir, \"test\"))\n",
    "\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1).astype(np.float32) / 255.0\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1).astype(np.float32) / 255.0\n",
    "\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    y_train_coarse = np.array(y_train_coarse)\n",
    "    y_test_coarse = np.array(y_test_coarse)\n",
    "\n",
    "    val_size = int(0.1 * len(x_train))\n",
    "    x_val = x_train[:val_size]\n",
    "    y_val = y_train[:val_size]\n",
    "    x_train = x_train[val_size:]\n",
    "    y_train = y_train[val_size:]\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test), (y_train_coarse, y_test_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7204343-985a-4038-8cb8-95b5afd0b082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "688fcd26-8281-4ad7-88b9-55c817526dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline MLP - multi_layer_net 기반 구현\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.dot(x, self.W) + self.b\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        return dout\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = self.softmax(x)\n",
    "        self.loss = self.cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx\n",
    "\n",
    "    def softmax(self, x):\n",
    "        x = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x)\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def cross_entropy_error(self, y, t):\n",
    "        if t.ndim == 1:\n",
    "            t = np.eye(y.shape[1])[t]\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / t.shape[0]\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = 0.01 * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = 0.01 * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = [\n",
    "            Affine(self.params['W1'], self.params['b1']),\n",
    "            ReLU(),\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "        ]\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = self.last_layer.backward()\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {\n",
    "            'W1': self.layers[0].dW, 'b1': self.layers[0].db,\n",
    "            'W2': self.layers[2].dW, 'b2': self.layers[2].db\n",
    "        }\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b23566e6-61a8-45b7-9c07-c30497b172f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 수정된 ViT-Lite 전체 구조 (NumPy, optimizer 연동 완전 지원) =====\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.gamma = np.ones((dim,))\n",
    "        self.beta = np.zeros((dim,))\n",
    "        self.eps = eps\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.mean = x.mean(axis=-1, keepdims=True)\n",
    "        self.var = x.var(axis=-1, keepdims=True)\n",
    "        self.norm = (x - self.mean) / np.sqrt(self.var + self.eps)\n",
    "        return self.gamma * self.norm + self.beta\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N = dout.shape[-1]\n",
    "        dx_hat = dout * self.gamma\n",
    "        std_inv = 1.0 / np.sqrt(self.var + self.eps)\n",
    "\n",
    "        dvar = np.sum(dx_hat * (self.x - self.mean) * -0.5 * std_inv**3, axis=-1, keepdims=True)\n",
    "        dmean = np.sum(dx_hat * -std_inv, axis=-1, keepdims=True) + dvar * np.mean(-2. * (self.x - self.mean), axis=-1, keepdims=True)\n",
    "\n",
    "        dx = dx_hat * std_inv + dvar * 2 * (self.x - self.mean) / N + dmean / N\n",
    "        return dx\n",
    "\n",
    "class PatchEmbedding:\n",
    "    def __init__(self, img_size, patch_size, in_channels, emb_dim):\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj_weight = np.random.randn(patch_size * patch_size * in_channels, emb_dim) * 0.02\n",
    "        self.pos_embedding = np.random.randn(1, self.num_patches + 1, emb_dim) * 0.02\n",
    "        self.cls_token = np.zeros((1, 1, emb_dim))\n",
    "\n",
    "    def forward(self, x):  # x: (B, H, W, C)\n",
    "        B, H, W, C = x.shape\n",
    "        p = self.patch_size\n",
    "        x_patches = []\n",
    "        for i in range(0, H, p):\n",
    "            for j in range(0, W, p):\n",
    "                patch = x[:, i:i+p, j:j+p, :].reshape(B, -1)\n",
    "                x_patches.append(patch)\n",
    "        x_patches = np.stack(x_patches, axis=1)  # (B, N, patch_dim)\n",
    "        self.last_input = x_patches\n",
    "        x_embed = np.dot(x_patches, self.proj_weight)  # (B, N, D)\n",
    "        cls = np.tile(self.cls_token, (B, 1, 1))\n",
    "        x_embed = np.concatenate([cls, x_embed], axis=1)\n",
    "        return x_embed + self.pos_embedding\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout[:, 1:, :]  # remove CLS grad\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        self.W1 = np.random.randn(in_dim, hidden_dim) * 0.02\n",
    "        self.b1 = np.zeros((hidden_dim,))\n",
    "        self.W2 = np.random.randn(hidden_dim, out_dim) * 0.02\n",
    "        self.b2 = np.zeros((out_dim,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.h = np.maximum(0, np.dot(x, self.W1) + self.b1)\n",
    "        self.out = np.dot(self.h, self.W2) + self.b2\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dh = np.dot(dout, self.W2.T)\n",
    "        dh[self.h <= 0] = 0\n",
    "        dW2 = np.dot(self.h.reshape(-1, self.h.shape[-1]).T, dout.reshape(-1, dout.shape[-1]))\n",
    "        db2 = np.sum(dout, axis=(0, 1))\n",
    "        dW1 = np.dot(self.x.reshape(-1, self.x.shape[-1]).T, dh.reshape(-1, dh.shape[-1]))\n",
    "        db1 = np.sum(dh, axis=(0, 1))\n",
    "\n",
    "        dx = np.dot(dh, self.W1.T).reshape(self.x.shape)\n",
    "        self.grads = {\"W1\": dW1, \"b1\": db1, \"W2\": dW2, \"b2\": db2}\n",
    "        return dx\n",
    "\n",
    "class ViTLite:\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, emb_dim=64, mlp_dim=128, num_classes=100):\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, emb_dim)\n",
    "        self.norm = LayerNorm(emb_dim)\n",
    "        self.mlp = MLP(emb_dim, mlp_dim, emb_dim)\n",
    "\n",
    "        self.params = {\n",
    "            'W1': self.mlp.W1, 'b1': self.mlp.b1,\n",
    "            'W2_mlp': self.mlp.W2, 'b2_mlp': self.mlp.b2,\n",
    "            'W2': np.random.randn(emb_dim, num_classes) * 0.02,\n",
    "            'b2': np.zeros((num_classes,))\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.patch_embed.forward(x)  # (B, N+1, D)\n",
    "        x = self.norm(x)\n",
    "        x = self.mlp.forward(x)\n",
    "        self.cls_output = x[:, 0]  # (B, D)\n",
    "        logits = np.dot(self.cls_output, self.params['W2']) + self.params['b2']\n",
    "        return logits\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        self.t = t\n",
    "        logits = self.forward(x)\n",
    "        logits -= np.max(logits, axis=1, keepdims=True)\n",
    "        exp = np.exp(logits)\n",
    "        self.probs = exp / np.sum(exp, axis=1, keepdims=True)\n",
    "        loss = -np.log(self.probs[np.arange(len(t)), t] + 1e-7)\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def backward(self):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dout = self.probs.copy()\n",
    "        dout[np.arange(batch_size), self.t] -= 1\n",
    "        dout /= batch_size\n",
    "\n",
    "        dW2 = np.dot(self.cls_output.T, dout)\n",
    "        db2 = np.sum(dout, axis=0)\n",
    "\n",
    "        dcls = np.dot(dout, self.params['W2'].T)\n",
    "        dx = np.zeros((batch_size, self.num_patches + 1, self.emb_dim))\n",
    "        dx[:, 0] = dcls\n",
    "\n",
    "        dx = self.mlp.backward(dx)\n",
    "        dx = self.norm.backward(dx)\n",
    "        dx = self.patch_embed.backward(dx)\n",
    "\n",
    "        grads = {\n",
    "            'W1': self.mlp.grads['W1'], 'b1': self.mlp.grads['b1'],\n",
    "            'W2_mlp': self.mlp.grads['W2'], 'b2_mlp': self.mlp.grads['b2'],\n",
    "            'W2': dW2, 'b2': db2\n",
    "        }\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f86108a1-42cc-420b-b92f-cb818e09992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Optimizer 구성: SGD / Momentum / Adam - NumPy만 사용\n",
    "import numpy as np\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params:\n",
    "            params[key] -= self.lr * grads[key]\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = {}\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key in params:\n",
    "            if key not in self.v:\n",
    "                self.v[key] = np.zeros_like(params[key])\n",
    "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
    "            params[key] += self.v[key]\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.t += 1\n",
    "        for key in grads:\n",
    "            if key not in self.m:\n",
    "                self.m[key] = np.zeros_like(grads[key])\n",
    "                self.v[key] = np.zeros_like(grads[key])\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
    "\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "879f2dc6-156b-44f7-81f2-047064273847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. 학습 루프 및 평가 함수 구현 (NumPy)\n",
    "import numpy as np\n",
    "\n",
    "def to_one_hot(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    if y_true.ndim != 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    return np.mean(y_pred == y_true)\n",
    "\n",
    "def train_model(model, optimizer, x_train, y_train, x_val, y_val, epochs=10, batch_size=64):\n",
    "    train_size = x_train.shape[0]\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # shuffle\n",
    "        idx = np.random.permutation(train_size)\n",
    "        x_train = x_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        for i in range(0, train_size, batch_size):\n",
    "            x_batch = x_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            if y_batch.ndim == 1:\n",
    "                y_batch = to_one_hot(y_batch, model.params['W2'].shape[1] if hasattr(model, 'params') else 100)\n",
    "\n",
    "            grads = model.gradient(x_batch, y_batch)\n",
    "            optimizer.update(model.params if hasattr(model, 'params') else vars(model), grads)\n",
    "\n",
    "        # 평가\n",
    "        y_train_pred = model.predict(x_train[:1000])\n",
    "        y_val_pred = model.predict(x_val)\n",
    "        acc_train = compute_accuracy(y_train_pred, y_train[:1000])\n",
    "        acc_val = compute_accuracy(y_val_pred, y_val)\n",
    "        loss = model.loss(x_train[:1000], to_one_hot(y_train[:1000], y_train_pred.shape[1]))\n",
    "        loss_list.append(loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch}] Loss: {loss:.4f} | Train Acc: {acc_train:.4f} | Val Acc: {acc_val:.4f}\")\n",
    "\n",
    "def test_model(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    acc = compute_accuracy(y_pred, y_test)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f553c4-2510-4752-b1dc-9d7d57869c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-100 fine label → coarse label 매핑용 테이블\n",
    "def load_label_names():\n",
    "    import pickle\n",
    "    with open('./cifar-100-python/meta', 'rb') as f:\n",
    "        meta = pickle.load(f, encoding='latin1')\n",
    "    fine_label_names = meta['fine_label_names']\n",
    "    coarse_label_names = meta['coarse_label_names']\n",
    "    fine_to_coarse = meta['coarse_label_names']\n",
    "    return fine_label_names, coarse_label_names, meta['coarse_label_names']\n",
    "\n",
    "def fine_to_coarse_labels(y_fine, mapping_array):\n",
    "    return np.array(mapping_array)[y_fine]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d394783-9545-445a-b32e-7329519c96d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터셋 로드\n",
    "download_cifar100()\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test), (y_train_coarse, y_test_coarse) = load_cifar100()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3efe4521-6644-47f8-a455-b404cf00eb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 4.5565 | Train Acc: 0.0240 | Val Acc: 0.0252\n",
      "[Epoch 2] Loss: 4.3405 | Train Acc: 0.0420 | Val Acc: 0.0478\n",
      "[Epoch 3] Loss: 4.1593 | Train Acc: 0.0610 | Val Acc: 0.0626\n",
      "[Epoch 4] Loss: 4.1001 | Train Acc: 0.0750 | Val Acc: 0.0682\n",
      "[Epoch 5] Loss: 4.0216 | Train Acc: 0.0940 | Val Acc: 0.0776\n",
      "[Epoch 6] Loss: 3.9722 | Train Acc: 0.1080 | Val Acc: 0.0946\n",
      "[Epoch 7] Loss: 3.9010 | Train Acc: 0.0990 | Val Acc: 0.1018\n",
      "[Epoch 8] Loss: 3.8075 | Train Acc: 0.1420 | Val Acc: 0.1092\n",
      "[Epoch 9] Loss: 3.8464 | Train Acc: 0.1270 | Val Acc: 0.1170\n",
      "[Epoch 10] Loss: 3.8283 | Train Acc: 0.1240 | Val Acc: 0.1212\n",
      "Test Accuracy: 0.1273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1273"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. MLP 모델 + SGD 학습\n",
    "mlp = MLP(input_size=32*32*3, hidden_size=128, output_size=100)\n",
    "sgd = SGD(lr=0.01)\n",
    "train_model(mlp, sgd, x_train.reshape(len(x_train), -1), y_train, x_val.reshape(len(x_val), -1), y_val, epochs=10)\n",
    "test_model(mlp, x_test.reshape(len(x_test), -1), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34f95596-af52-4c6c-a194-a5ae5e4d6c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. 학습 루프 및 평가 함수 구현 (NumPy - ViTLite 전용)\n",
    "import numpy as np\n",
    "\n",
    "def to_one_hot(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    if y_true.ndim != 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    return np.mean(y_pred == y_true)\n",
    "\n",
    "def train_model(model, optimizer, x_train, y_train, x_val, y_val, epochs=10, batch_size=64):\n",
    "    train_size = x_train.shape[0]\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        idx = np.random.permutation(train_size)\n",
    "        x_train = x_train[idx]\n",
    "        y_train = y_train[idx]\n",
    "\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for i in range(0, train_size, batch_size):\n",
    "            x_batch = x_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "            loss = model.loss(x_batch, y_batch)\n",
    "            grads = model.backward()\n",
    "\n",
    "            for k in grads:\n",
    "                model.params[k] -= optimizer.lr * grads[k]\n",
    "\n",
    "            pred = np.argmax(model.probs, axis=1)\n",
    "            correct += np.sum(pred == y_batch)\n",
    "            total_loss += loss * len(x_batch)\n",
    "\n",
    "        acc = correct / train_size\n",
    "        avg_loss = total_loss / train_size\n",
    "\n",
    "        # Validation\n",
    "        val_logits = model.forward(x_val)\n",
    "        val_pred = np.argmax(val_logits, axis=1) if val_logits.ndim > 1 else val_logits\n",
    "        val_acc = compute_accuracy(val_pred, y_val)\n",
    "\n",
    "        print(f\"[Epoch {epoch}] Loss: {avg_loss:.4f} | Train Acc: {acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "def test_model(model, x_test, y_test):\n",
    "    logits = model.forward(x_test)\n",
    "    y_pred = np.argmax(logits, axis=1) if logits.ndim > 1 else logits\n",
    "    acc = compute_accuracy(y_pred, y_test)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "75b8636a-14fd-4a3e-b5ef-45465340f312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 4.6052 | Train Acc: 0.0100 | Val Acc: 0.0098\n",
      "[Epoch 2] Loss: 4.6052 | Train Acc: 0.0100 | Val Acc: 0.0098\n",
      "[Epoch 3] Loss: 4.6052 | Train Acc: 0.0095 | Val Acc: 0.0098\n",
      "[Epoch 4] Loss: 4.6052 | Train Acc: 0.0093 | Val Acc: 0.0098\n",
      "[Epoch 5] Loss: 4.6052 | Train Acc: 0.0100 | Val Acc: 0.0098\n",
      "[Epoch 6] Loss: 4.6052 | Train Acc: 0.0098 | Val Acc: 0.0098\n",
      "[Epoch 7] Loss: 4.6052 | Train Acc: 0.0100 | Val Acc: 0.0098\n",
      "[Epoch 8] Loss: 4.6052 | Train Acc: 0.0092 | Val Acc: 0.0068\n",
      "[Epoch 9] Loss: 4.6052 | Train Acc: 0.0102 | Val Acc: 0.0068\n",
      "[Epoch 10] Loss: 4.6052 | Train Acc: 0.0104 | Val Acc: 0.0068\n",
      "Test Accuracy: 0.0100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. ViT-Lite + Adam 학습\n",
    "vit = ViTLite(img_size=32, patch_size=4, in_channels=3, emb_dim=64, mlp_dim=128, num_classes=100)\n",
    "optimizer = Adam(lr=0.003)\n",
    "train_model(vit, optimizer, x_train, y_train, x_val, y_val, epochs=10)\n",
    "test_model(vit, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7b7d48-5433-440b-b534-101d162dc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. (선택) coarse label 변환 후 평가\n",
    "# y_train_coarse, y_test_coarse 사용하거나 mapping 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "836e43a7-44c7-4c9b-8e07-a2a1aa5d4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ViT-Lite with Self-Attention, LayerNorm, PatchEmbedding, Full Backward + Adam Support =====\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ---------- LayerNorm ----------\n",
    "class LayerNorm:\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.gamma = np.ones((dim,))\n",
    "        self.beta = np.zeros((dim,))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        self.var = np.var(x, axis=-1, keepdims=True)\n",
    "        self.norm = (x - self.mean) / np.sqrt(self.var + self.eps)\n",
    "        return self.gamma * self.norm + self.beta\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N = dout.shape[-1]\n",
    "        dx_hat = dout * self.gamma\n",
    "        std_inv = 1. / np.sqrt(self.var + self.eps)\n",
    "\n",
    "        dvar = np.sum(dx_hat * (self.x - self.mean) * -0.5 * std_inv**3, axis=-1, keepdims=True)\n",
    "        dmean = np.sum(dx_hat * -std_inv, axis=-1, keepdims=True) + dvar * np.mean(-2. * (self.x - self.mean), axis=-1, keepdims=True)\n",
    "\n",
    "        dx = dx_hat * std_inv + dvar * 2 * (self.x - self.mean) / N + dmean / N\n",
    "        self.grads = {\n",
    "            \"gamma\": np.sum(dout * self.norm, axis=(0, 1)),\n",
    "            \"beta\": np.sum(dout, axis=(0, 1))\n",
    "        }\n",
    "        return dx\n",
    "\n",
    "# ---------- Patch Embedding ----------\n",
    "class PatchEmbedding:\n",
    "    def __init__(self, img_size, patch_size, in_channels, emb_dim):\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.proj_weight = np.random.randn(patch_size * patch_size * in_channels, emb_dim) * 0.02\n",
    "        self.pos_embedding = np.random.randn(1, self.num_patches + 1, emb_dim) * 0.02\n",
    "        self.cls_token = np.zeros((1, 1, emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        p = self.patch_size\n",
    "        x_patches = [x[:, i:i+p, j:j+p, :].reshape(B, -1)\n",
    "                     for i in range(0, H, p) for j in range(0, W, p)]\n",
    "        self.x_patches = np.stack(x_patches, axis=1)\n",
    "        x_embed = np.dot(self.x_patches, self.proj_weight)\n",
    "        cls = np.tile(self.cls_token, (B, 1, 1))\n",
    "        return np.concatenate([cls, x_embed], axis=1) + self.pos_embedding\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout_patch = dout[:, 1:, :]\n",
    "        dW = np.dot(self.x_patches.reshape(-1, self.x_patches.shape[-1]).T,\n",
    "                    dout_patch.reshape(-1, dout_patch.shape[-1]))\n",
    "        self.grads = {\"proj_weight\": dW}\n",
    "        return dout_patch\n",
    "\n",
    "# ---------- Multi-Head Self Attention ----------\n",
    "class MultiHeadSelfAttention:\n",
    "    def __init__(self, emb_dim, num_heads=4):\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = emb_dim // num_heads\n",
    "        self.W_q = np.random.randn(emb_dim, emb_dim)\n",
    "        self.W_k = np.random.randn(emb_dim, emb_dim)\n",
    "        self.W_v = np.random.randn(emb_dim, emb_dim)\n",
    "        self.W_o = np.random.randn(emb_dim, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        Q = np.dot(x, self.W_q).reshape(B, N, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        K = np.dot(x, self.W_k).reshape(B, N, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "        V = np.dot(x, self.W_v).reshape(B, N, self.num_heads, self.head_dim).transpose(0, 2, 1, 3)\n",
    "\n",
    "        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / np.sqrt(self.head_dim)\n",
    "        self.attn = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "        self.attn /= np.sum(self.attn, axis=-1, keepdims=True)\n",
    "\n",
    "        out = np.matmul(self.attn, V).transpose(0, 2, 1, 3).reshape(B, N, D)\n",
    "        return np.dot(out, self.W_o)\n",
    "\n",
    "# ---------- MLP ----------\n",
    "class MLP:\n",
    "    def __init__(self, in_dim, hidden_dim):\n",
    "        self.W1 = np.random.randn(in_dim, hidden_dim) * 0.02\n",
    "        self.b1 = np.zeros((hidden_dim,))\n",
    "        self.W2 = np.random.randn(hidden_dim, in_dim) * 0.02\n",
    "        self.b2 = np.zeros((in_dim,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.h = np.maximum(0, np.dot(x, self.W1) + self.b1)\n",
    "        return np.dot(self.h, self.W2) + self.b2\n",
    "\n",
    "# ---------- ViT-Lite Class ----------\n",
    "class ViTLite:\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, emb_dim=64, mlp_dim=128, num_classes=100):\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, emb_dim)\n",
    "        self.norm1 = LayerNorm(emb_dim)\n",
    "        self.attn = MultiHeadSelfAttention(emb_dim)\n",
    "        self.norm2 = LayerNorm(emb_dim)\n",
    "        self.mlp = MLP(emb_dim, mlp_dim)\n",
    "        self.W_cls = np.random.randn(emb_dim, num_classes) * 0.02\n",
    "        self.b_cls = np.zeros((num_classes,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_patch = self.patch_embed.forward(x)\n",
    "        x = self.x_patch + self.attn.forward(self.norm1.forward(self.x_patch))\n",
    "        x = x + self.mlp.forward(self.norm2.forward(x))\n",
    "        self.cls_output = x[:, 0]\n",
    "        return np.dot(self.cls_output, self.W_cls) + self.b_cls\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        self.t = t\n",
    "        logits = self.forward(x)\n",
    "        logits -= np.max(logits, axis=1, keepdims=True)\n",
    "        self.probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "        loss = -np.log(self.probs[np.arange(len(t)), t] + 1e-7)\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def backward(self):\n",
    "        B = self.t.shape[0]\n",
    "        dout = self.probs.copy()\n",
    "        dout[np.arange(B), self.t] -= 1\n",
    "        dout /= B\n",
    "        dW_cls = np.dot(self.cls_output.T, dout)\n",
    "        db_cls = np.sum(dout, axis=0)\n",
    "        return {\"W_cls\": dW_cls, \"b_cls\": db_cls}\n",
    "\n",
    "# ---------- Optimizer, Criterion, Training ----------\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m, self.v, self.t = {}, {}, {}\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for k in grads:\n",
    "            if k not in self.m:\n",
    "                self.m[k] = np.zeros_like(grads[k])\n",
    "                self.v[k] = np.zeros_like(grads[k])\n",
    "                self.t[k] = 0\n",
    "            self.t[k] += 1\n",
    "            self.m[k] = self.beta1 * self.m[k] + (1 - self.beta1) * grads[k]\n",
    "            self.v[k] = self.beta2 * self.v[k] + (1 - self.beta2) * (grads[k] ** 2)\n",
    "            m_hat = self.m[k] / (1 - self.beta1 ** self.t[k])\n",
    "            v_hat = self.v[k] / (1 - self.beta2 ** self.t[k])\n",
    "            params[k] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "class SoftmaxCrossEntropy:\n",
    "    def __call__(self, logits, labels):\n",
    "        logits -= np.max(logits, axis=1, keepdims=True)\n",
    "        probs = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
    "        loss = -np.log(probs[np.arange(len(labels)), labels] + 1e-7)\n",
    "        return np.mean(loss), probs\n",
    "\n",
    "    def backward(self, probs, labels):\n",
    "        B = labels.shape[0]\n",
    "        grad = probs.copy()\n",
    "        grad[np.arange(B), labels] -= 1\n",
    "        return grad / B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c21abd6f-6e4a-4af5-81d1-7ff42b0ce252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    return np.mean(np.argmax(y_pred, axis=1) == y_true)\n",
    "\n",
    "def train_model(model, x_train, y_train, x_val, y_val, epochs=10, batch_size=64, save_path=None):\n",
    "    optimizer = Adam(lr=0.001)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        idx = np.random.permutation(len(x_train))\n",
    "        x_train, y_train = x_train[idx], y_train[idx]\n",
    "\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "            loss = model.loss(x_batch, y_batch)       # self.probs가 계산됨\n",
    "            total_loss += loss * len(x_batch)\n",
    "\n",
    "            pred = np.argmax(model.probs, axis=1)\n",
    "            correct += np.sum(pred == y_batch)\n",
    "            total += len(y_batch)\n",
    "\n",
    "            grads = model.backward()\n",
    "            optimizer.update(model.__dict__, grads)\n",
    "\n",
    "        avg_loss = total_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # 검증\n",
    "        val_logits = model.forward(x_val)\n",
    "        val_acc = compute_accuracy(val_logits, y_val)\n",
    "\n",
    "        print(f\"[Epoch {epoch}] Loss: {avg_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        np.savez(save_path, **model.__dict__)\n",
    "        print(f\"Model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c029a968-9481-42f8-b1a9-340ab1f8e12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 5.7672 | Train Acc: 0.0359 | Val Acc: 0.0390\n",
      "[Epoch 2] Loss: 5.1058 | Train Acc: 0.0442 | Val Acc: 0.0334\n",
      "[Epoch 3] Loss: 5.0804 | Train Acc: 0.0468 | Val Acc: 0.0436\n",
      "[Epoch 4] Loss: 5.0678 | Train Acc: 0.0480 | Val Acc: 0.0480\n",
      "[Epoch 5] Loss: 5.0434 | Train Acc: 0.0486 | Val Acc: 0.0420\n",
      "[Epoch 6] Loss: 5.0565 | Train Acc: 0.0483 | Val Acc: 0.0400\n",
      "[Epoch 7] Loss: 5.0721 | Train Acc: 0.0500 | Val Acc: 0.0396\n",
      "[Epoch 8] Loss: 5.0433 | Train Acc: 0.0499 | Val Acc: 0.0426\n",
      "[Epoch 9] Loss: 4.9934 | Train Acc: 0.0498 | Val Acc: 0.0476\n",
      "[Epoch 10] Loss: 5.0430 | Train Acc: 0.0509 | Val Acc: 0.0466\n",
      "[Epoch 11] Loss: 5.0358 | Train Acc: 0.0506 | Val Acc: 0.0496\n",
      "[Epoch 12] Loss: 5.0229 | Train Acc: 0.0507 | Val Acc: 0.0468\n",
      "[Epoch 13] Loss: 5.0355 | Train Acc: 0.0510 | Val Acc: 0.0486\n",
      "[Epoch 14] Loss: 5.0328 | Train Acc: 0.0505 | Val Acc: 0.0508\n",
      "[Epoch 15] Loss: 5.0226 | Train Acc: 0.0506 | Val Acc: 0.0458\n",
      "[Epoch 16] Loss: 5.0102 | Train Acc: 0.0501 | Val Acc: 0.0498\n",
      "[Epoch 17] Loss: 5.0232 | Train Acc: 0.0499 | Val Acc: 0.0486\n",
      "[Epoch 18] Loss: 5.0355 | Train Acc: 0.0507 | Val Acc: 0.0388\n",
      "[Epoch 19] Loss: 5.0334 | Train Acc: 0.0510 | Val Acc: 0.0426\n",
      "[Epoch 20] Loss: 5.0251 | Train Acc: 0.0516 | Val Acc: 0.0480\n",
      "Model saved to ./saved/vit_lite.npz\n"
     ]
    }
   ],
   "source": [
    "vit = ViTLite(img_size=32, patch_size=4, in_channels=3, emb_dim=64, mlp_dim=128, num_classes=100)\n",
    "\n",
    "train_model(\n",
    "    model=vit,\n",
    "    x_train=x_train,  # (N, 32, 32, 3) 형태의 NumPy 배열\n",
    "    y_train=y_train,  # (N,) 정수 클래스 라벨\n",
    "    x_val=x_val,      # 검증 이미지\n",
    "    y_val=y_val,      # 검증 라벨\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    save_path=\"./saved/vit_lite.npz\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

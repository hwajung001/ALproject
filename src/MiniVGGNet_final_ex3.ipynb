{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniVGGNet final Train code\n",
    "\n",
    "### ex 1 : train dataset 150,000 = original + random crop + horizontal flip\n",
    "##### (random seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - CIFAR-100 데이터 다운로드 및 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 already downloaded.\n",
      "Generating augmented dataset with horizontal flip...\n",
      "train_flip: [(90000, 3, 32, 32), (90000,)]\n",
      "val_flip: [(10000, 3, 32, 32), (10000,)]\n",
      "test: [(10000, 3, 32, 32), (10000,)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def download_cifar100(save_path='cifar-100-python'):\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "        return\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
    "    filename = 'cifar-100-python.tar.gz'\n",
    "    print(\"Downloading CIFAR-100...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    with tarfile.open(filename, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "    os.remove(filename)\n",
    "    print(\"Download and extraction completed.\")\n",
    "\n",
    "def load_batch(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "    data = data_dict[b'data']\n",
    "    fine_labels = np.array(data_dict[b'fine_labels'])\n",
    "    data = data.reshape(-1, 3, 32, 32)\n",
    "    return data, fine_labels\n",
    "\n",
    "def normalize_images(images):\n",
    "    mean = np.array([0.5071, 0.4865, 0.4409]).reshape(1, 3, 1, 1)\n",
    "    std  = np.array([0.2673, 0.2564, 0.2762]).reshape(1, 3, 1, 1)\n",
    "    return (images - mean) / std\n",
    "\n",
    "def random_crop(x, crop_size=32, padding=4):\n",
    "    n, c, h, w = x.shape\n",
    "    padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='reflect')\n",
    "    cropped = np.empty((n, c, crop_size, crop_size), dtype=x.dtype)\n",
    "    for i in range(n):\n",
    "        top = np.random.randint(0, padding * 2 + 1)\n",
    "        left = np.random.randint(0, padding * 2 + 1)\n",
    "        cropped[i] = padded[i, :, top:top+crop_size, left:left+crop_size]\n",
    "    return cropped\n",
    "\n",
    "def horizontal_flip(x):\n",
    "    if np.random.rand() < 0.5:\n",
    "        return x[:, :, :, ::-1]\n",
    "    return x\n",
    "    \n",
    "def split_validation(images, labels, val_ratio=0.1):\n",
    "    num_samples = images.shape[0]\n",
    "    val_size = int(num_samples * val_ratio)\n",
    "    idx = np.random.permutation(num_samples)\n",
    "    images = images[idx]\n",
    "    labels = labels[idx]\n",
    "    val_images = images[:val_size]\n",
    "    val_labels = labels[:val_size]\n",
    "    train_images = images[val_size:]\n",
    "    train_labels = labels[val_size:]\n",
    "    return (train_images, train_labels), (val_images, val_labels)\n",
    "\n",
    "def load_cifar100_dataset():\n",
    "    download_cifar100()\n",
    "    train_data, train_fine = load_batch('cifar-100-python/train')\n",
    "    test_data, test_fine = load_batch('cifar-100-python/test')\n",
    "    train_data = normalize_images(train_data)\n",
    "    test_data = normalize_images(test_data)\n",
    "    return (train_data, train_fine), (test_data, test_fine)\n",
    "    \n",
    "def generate_augmented_dataset(images, labels, target_size):\n",
    "    N = images.shape[0]\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    repeat = target_size // (N) + 1 \n",
    "\n",
    "    for _ in range(repeat):\n",
    "        imgs_flip = horizontal_flip(images.copy())\n",
    "        \n",
    "        augmented_images.append(imgs_flip)\n",
    "        augmented_labels.append(labels.copy())\n",
    "\n",
    "        if sum(x.shape[0] for x in augmented_images) >= target_size:\n",
    "            break\n",
    "\n",
    "    X = np.concatenate(augmented_images, axis=0)[:target_size]\n",
    "    y = np.concatenate(augmented_labels, axis=0)[:target_size]\n",
    "    return X, y\n",
    "\n",
    "def prepare_dataset():\n",
    "    (full_train_images, full_train_labels), (test_images, test_labels) = load_cifar100_dataset()\n",
    "    print(\"Generating augmented dataset with horizontal flip...\")\n",
    "\n",
    "    X_aug, y_aug = generate_augmented_dataset(full_train_images, full_train_labels, target_size=100000)\n",
    "    train_aug, val_aug = split_validation(X_aug, y_aug)\n",
    "\n",
    "    return {\n",
    "        'train_flip': train_aug,\n",
    "        'val_flip': val_aug,\n",
    "        'test': (test_images, test_labels)\n",
    "    }\n",
    "    \n",
    "data = prepare_dataset()\n",
    "for k, v in data.items():\n",
    "    if isinstance(v, tuple):\n",
    "        print(f\"{k}: {[x.shape for x in v]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Convolution, BatchNormalization, Relu, Pooling, Affine\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "def fake_quantize(x, num_bits=8):\n",
    "    qmin, qmax = 0., 2.**num_bits - 1.\n",
    "    x_min, x_max = np.min(x), np.max(x)\n",
    "    if x_max == x_min:\n",
    "        return x\n",
    "    scale = (x_max - x_min) / (qmax - qmin)\n",
    "    zero_point = np.clip(np.round(qmin - x_min / scale), qmin, qmax)\n",
    "    q_x = np.clip(np.round(zero_point + x / scale), qmin, qmax)\n",
    "    return scale * (q_x - zero_point)\n",
    "\n",
    "class Flatten:\n",
    "    def __init__(self):\n",
    "        self.orig_shape = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.orig_shape = x.shape\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout.reshape(self.orig_shape)\n",
    "\n",
    "\n",
    "class MiniVGGNet:\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_classes=100):\n",
    "        in_channels, _, _ = input_dim\n",
    "        weight_std = np.sqrt(2. / in_channels)\n",
    "\n",
    "        self.conv1 = Convolution(np.random.randn(64, in_channels, 3, 3) * weight_std, np.zeros(64), stride=1, pad=1)\n",
    "        self.bn1   = BatchNormalization(np.ones(64), np.zeros(64))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.conv2 = Convolution(np.random.randn(64, 64, 3, 3) * weight_std, np.zeros(64), stride=1, pad=1)\n",
    "        self.bn2   = BatchNormalization(np.ones(64), np.zeros(64))\n",
    "        self.relu2 = Relu()\n",
    "        self.pool1 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        self.conv3 = Convolution(np.random.randn(128, 64, 3, 3) * weight_std, np.zeros(128), stride=1, pad=1)\n",
    "        self.bn3   = BatchNormalization(np.ones(128), np.zeros(128))\n",
    "        self.relu3 = Relu()\n",
    "\n",
    "        self.conv4 = Convolution(np.random.randn(128, 128, 3, 3) * weight_std, np.zeros(128), stride=1, pad=1)\n",
    "        self.bn4   = BatchNormalization(np.ones(128), np.zeros(128))\n",
    "        self.relu4 = Relu()\n",
    "        self.pool2 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        self.conv5 = Convolution(np.random.randn(256, 128, 3, 3) * weight_std, np.zeros(256), stride=1, pad=1)\n",
    "        self.bn5   = BatchNormalization(np.ones(256), np.zeros(256)) #conv5 also \n",
    "        self.relu5 = Relu()\n",
    "        self.pool3 = Pooling(2, 2, stride=2)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Affine(np.random.randn(4096, 512) * weight_std, np.zeros(512))\n",
    "        self.relu6 = Relu()\n",
    "        self.fc2 = Affine(np.random.randn(512, num_classes) * 0.01, np.zeros(num_classes))\n",
    "\n",
    "        self.layers = [\n",
    "            self.conv1, self.bn1, self.relu1,\n",
    "            self.conv2, self.bn2, self.relu2, self.pool1,\n",
    "            self.conv3, self.bn3, self.relu3,\n",
    "            self.conv4, self.bn4, self.relu4, self.pool2,\n",
    "            self.conv5, self.bn5, self.relu5, self.pool3, #conv5\n",
    "            self.flatten, self.fc1, self.relu6, self.fc2\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, BatchNormalization):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.fc2.backward(dout)\n",
    "        dout = self.relu6.backward(dout)\n",
    "        dout = self.fc1.backward(dout)\n",
    "        dout = self.flatten.backward(dout)\n",
    "\n",
    "        dout = self.pool3.backward(dout)\n",
    "        dout = self.relu5.backward(dout)\n",
    "        dout = self.bn5.backward(dout)\n",
    "        dout = self.conv5.backward(dout)\n",
    "\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.relu4.backward(dout)\n",
    "        dout = self.bn4.backward(dout)\n",
    "        dout = self.conv4.backward(dout)\n",
    "\n",
    "        dout = self.relu3.backward(dout)\n",
    "        dout = self.bn3.backward(dout)\n",
    "        dout = self.conv3.backward(dout)\n",
    "\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "\n",
    "        return dout\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.fc2.backward(dout)\n",
    "        dout = self.relu6.backward(dout)\n",
    "        dout = self.fc1.backward(dout)\n",
    "        dout = self.flatten.backward(dout)\n",
    "\n",
    "        dout = self.pool3.backward(dout)\n",
    "        dout = self.relu5.backward(dout)\n",
    "        dout = self.bn5.backward(dout)\n",
    "        dout = self.conv5.backward(dout)\n",
    "\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.relu4.backward(dout)\n",
    "        dout = self.bn4.backward(dout)\n",
    "        dout = self.conv4.backward(dout)\n",
    "\n",
    "        dout = self.relu3.backward(dout)\n",
    "        dout = self.bn3.backward(dout)\n",
    "        dout = self.conv3.backward(dout)\n",
    "\n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "\n",
    "        grads = {\n",
    "            'W1': self.conv1.W, 'b1': self.conv1.b,\n",
    "            'gamma1': self.bn1.gamma, 'beta1': self.bn1.beta,\n",
    "            'W2': self.conv2.W, 'b2': self.conv2.b,\n",
    "            'gamma2': self.bn2.gamma, 'beta2': self.bn2.beta,\n",
    "            'W3': self.conv3.W, 'b3': self.conv3.b,\n",
    "            'gamma3': self.bn3.gamma, 'beta3': self.bn3.beta,\n",
    "            'W4': self.conv4.W, 'b4': self.conv4.b,\n",
    "            'gamma4': self.bn4.gamma, 'beta4': self.bn4.beta,\n",
    "            'W5': self.conv5.W, 'b5': self.conv5.b,\n",
    "            'gamma5': self.bn5.gamma, 'beta5': self.bn5.beta,  \n",
    "            'W6': self.fc1.W, 'b6': self.fc1.b,\n",
    "            'W7': self.fc2.W, 'b7': self.fc2.b,\n",
    "        }\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def predict(self, x, batch_size=100):\n",
    "        return np.concatenate([self.forward(x[i:i+batch_size], False) for i in range(0, x.shape[0], batch_size)], axis=0)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.forward(x, True)\n",
    "        y_softmax = softmax(y)  \n",
    "        return cross_entropy_error(y_softmax, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        pred = np.argmax(self.predict(x, batch_size), axis=1)\n",
    "        true = t if t.ndim == 1 else np.argmax(t, axis=1)\n",
    "        return np.mean(pred == true)\n",
    "\n",
    "    def clip_weights(self, clip_value=1.0): \n",
    "        for layer in [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.fc1, self.fc2]:\n",
    "            layer.W = np.clip(layer.W, -clip_value, clip_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 구조 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type)                       Output Shape                Param #\n",
      "===========================================================================\n",
      " 1. Conv1                           (1, 64, 32, 32)               1,792\n",
      " 2. BN1                             (1, 64, 32, 32)                 128\n",
      " 3. Conv2                           (1, 64, 32, 32)              36,928\n",
      " 4. BN2                             (1, 64, 32, 32)                 128\n",
      " 5. Conv3                           (1, 128, 16, 16)             73,856\n",
      " 6. BN3                             (1, 128, 16, 16)                256\n",
      " 7. Conv4                           (1, 128, 16, 16)            147,584\n",
      " 8. BN4                             (1, 128, 16, 16)                256\n",
      " 9. Conv5                           (1, 256, 8, 8)              295,168\n",
      "10. Flatten                         (1, 4096)                         0\n",
      "11. FC1                             (1, 512)                  2,097,664\n",
      "12. FC2                             (1, 100)                     51,300\n",
      "===========================================================================\n",
      "Total weight layers:                                        13\n",
      "Total params:                                               2,705,060\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "from common.layers import Convolution, BatchNormalization, Relu, Pooling, Affine\n",
    "import numpy as np\n",
    "\n",
    "def count_params(layer):\n",
    "    count = 0\n",
    "    if hasattr(layer, 'W'):\n",
    "        count += np.prod(layer.W.shape)\n",
    "    if hasattr(layer, 'b'):\n",
    "        count += np.prod(layer.b.shape)\n",
    "    if hasattr(layer, 'gamma'):\n",
    "        count += np.prod(layer.gamma.shape)\n",
    "    if hasattr(layer, 'beta'):\n",
    "        count += np.prod(layer.beta.shape)\n",
    "    return count\n",
    "\n",
    "def print_vggnet_summary(model, input_shape=(1, 3, 32, 32)):\n",
    "    print(\"=\" * 75)\n",
    "    print(f\"{'Layer (type)':<35}{'Output Shape':<25}{'Param #':>10}\")\n",
    "    print(\"=\" * 75)\n",
    "\n",
    "    x = np.zeros(input_shape)\n",
    "    total_params = 0\n",
    "    layer_idx = 1\n",
    "\n",
    "    def log(name, x, p):\n",
    "        nonlocal total_params, layer_idx\n",
    "        print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\")\n",
    "        total_params += p\n",
    "        layer_idx += 1\n",
    "\n",
    "    x = model.conv1.forward(x)\n",
    "    log(\"Conv1\", x, count_params(model.conv1))\n",
    "    x = model.bn1.forward(x, train_flg=False)\n",
    "    log(\"BN1\", x, count_params(model.bn1))\n",
    "    x = model.relu1.forward(x)\n",
    "\n",
    "    x = model.conv2.forward(x)\n",
    "    log(\"Conv2\", x, count_params(model.conv2))\n",
    "    x = model.bn2.forward(x, train_flg=False)\n",
    "    log(\"BN2\", x, count_params(model.bn2))\n",
    "    x = model.relu2.forward(x)\n",
    "    x = model.pool1.forward(x)\n",
    "\n",
    "    x = model.conv3.forward(x)\n",
    "    log(\"Conv3\", x, count_params(model.conv3))\n",
    "    x = model.bn3.forward(x, train_flg=False)\n",
    "    log(\"BN3\", x, count_params(model.bn3))\n",
    "    x = model.relu3.forward(x)\n",
    "\n",
    "    x = model.conv4.forward(x)\n",
    "    log(\"Conv4\", x, count_params(model.conv4))\n",
    "    x = model.bn4.forward(x, train_flg=False)\n",
    "    log(\"BN4\", x, count_params(model.bn4))\n",
    "    x = model.relu4.forward(x)\n",
    "    x = model.pool2.forward(x)\n",
    "\n",
    "    x = model.conv5.forward(x)\n",
    "    log(\"Conv5\", x, count_params(model.conv5))\n",
    "    x = model.relu5.forward(x)\n",
    "    x = model.pool3.forward(x)\n",
    "\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    log(\"Flatten\", x, 0)\n",
    "\n",
    "    x = model.fc1.forward(x)\n",
    "    log(\"FC1\", x, count_params(model.fc1))\n",
    "    x = model.relu6.forward(x)\n",
    "    x = model.fc2.forward(x)\n",
    "    log(\"FC2\", x, count_params(model.fc2))\n",
    "\n",
    "    print(\"=\" * 75)\n",
    "    print(f\"{'Total weight layers:':<60}{layer_idx}\")\n",
    "    print(f\"{'Total params:':<60}{total_params:,}\")\n",
    "    print(\"=\" * 75)\n",
    "\n",
    "model = MiniVGGNet()\n",
    "print_vggnet_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from common.optimizer import Adam\n",
    "from common.functions import softmax\n",
    "\n",
    "def smooth_labels(y, smoothing=0.1, num_classes=100):\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = (y.shape[0], num_classes)\n",
    "    smooth = np.full(label_shape, smoothing / (num_classes - 1))\n",
    "    smooth[np.arange(y.shape[0]), y] = confidence\n",
    "    return smooth\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name,\n",
    "                 train_data, val_data, test_data,\n",
    "                 epochs=20, batch_size=64, lr=0.01,\n",
    "                 smoothing=0.15):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.val_x, self.val_t = val_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "        self.optimizer = Adam(lr=lr)\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def smooth_labels(self, y, num_classes=100):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        label_shape = (y.shape[0], num_classes)\n",
    "        smooth = np.full(label_shape, self.smoothing / (num_classes - 1), dtype=np.float32)\n",
    "        smooth[np.arange(y.shape[0]), y] = confidence\n",
    "        return smooth\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            dx = (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            dx /= batch_size\n",
    "        return dx, y\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "\n",
    "        for name in ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc1', 'fc2']:\n",
    "            layer = getattr(self.model, name)\n",
    "            if hasattr(layer, 'W'):\n",
    "                param_dict[f'{name}_W'] = layer.W\n",
    "                param_dict[f'{name}_b'] = layer.b\n",
    "                grad_dict[f'{name}_W'] = layer.dW\n",
    "                grad_dict[f'{name}_b'] = layer.db\n",
    "\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def get_bn_param_dict(self):\n",
    "        bn_dict = {}\n",
    "        idx = 0\n",
    "        for name in ['bn1', 'bn2', 'bn3', 'bn4', 'bn5']:\n",
    "            bn = getattr(self.model, name)\n",
    "            bn_dict[f'bn{idx}_gamma'] = bn.gamma.copy()\n",
    "            bn_dict[f'bn{idx}_beta'] = bn.beta.copy()\n",
    "            bn_dict[f'bn{idx}_running_mean'] = bn.running_mean.copy()\n",
    "            bn_dict[f'bn{idx}_running_var'] = bn.running_var.copy()\n",
    "            idx += 1\n",
    "        return bn_dict\n",
    "    \n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        if t_batch.ndim == 1:\n",
    "            t_batch = self.smooth_labels(t_batch)\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        dx, _ = self.loss_grad(x_batch, t_batch)\n",
    "\n",
    "        self.model.backward(dx)\n",
    "\n",
    "        if hasattr(self.model, 'clip_weights'):\n",
    "            self.model.clip_weights(clip_value=1.0)\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        patience = 10\n",
    "        best_val_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\n[Epoch {epoch + 1}/{self.epochs}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0 or i == self.iter_per_epoch - 1:\n",
    "                    print(f\"  Iter {i+1:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            val_acc = self.model.accuracy(self.val_x, self.val_t)\n",
    "            val_loss = self.batched_loss(self.val_x, self.val_t, batch_size=128)\n",
    "\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.val_acc_list.append(val_acc)\n",
    "            self.val_loss_list.append(val_loss)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Fine Train Loss: {avg_loss:.4f}, Fine Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}\", flush=True)\n",
    "            print(f\"Time: {elapsed:.2f}s\", flush=True)\n",
    "\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_model(f\"{self.model_name}_epoch{epoch+1}.pkl\")\n",
    "                print(f\">>> Model saved to {self.model_name}_epoch{epoch+1}.pkl\", flush=True)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improve_count = 0\n",
    "                self.save_model(f\"{self.model_name}_best.pkl\")\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "    def batched_loss(self, x, t, batch_size=128):\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        num_classes = 100  \n",
    "    \n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "    \n",
    "            if t_batch.ndim == 1:\n",
    "                t_onehot = np.zeros((t_batch.size, num_classes), dtype=np.float32)\n",
    "                t_onehot[np.arange(t_batch.size), t_batch] = 1.0\n",
    "            else:\n",
    "                t_onehot = t_batch  \n",
    "    \n",
    "            loss = self.model.loss(x_batch, t_onehot)\n",
    "            total_loss += loss * len(x_batch)\n",
    "            total_count += len(x_batch)\n",
    "    \n",
    "        return total_loss / total_count\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "        bn_state = self.get_bn_param_dict()\n",
    "        \n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': self.optimizer.beta1,\n",
    "            'beta2': self.optimizer.beta2,\n",
    "            'm': self.optimizer.m,\n",
    "            'v': self.optimizer.v,\n",
    "            't': self.optimizer.iter\n",
    "        }\n",
    "\n",
    "        save_data = {\n",
    "            'model': model_state,\n",
    "            'bn': bn_state,\n",
    "            'optimizer': optimizer_state,\n",
    "            'train_loss_list': self.train_loss_list,\n",
    "            'train_acc_list': self.train_acc_list,\n",
    "            'val_acc_list': self.val_acc_list,\n",
    "            'val_loss_list': self.val_loss_list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename,\n",
    "                 loss=np.array(self.train_loss_list),\n",
    "                 train_acc=np.array(self.train_acc_list),\n",
    "                 val_acc=np.array(self.val_acc_list),\n",
    "                 val_loss=np.array(self.val_loss_list))\n",
    "        print(f\"Log saved to {filename}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - MiniVGGNet 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Running ex3 : train dataset 100,000 = original + horizontal flip ====\n",
      "\n",
      "[Epoch 1/100]\n",
      "  Iter   1/1406: Loss 14.9222\n",
      "  Iter  11/1406: Loss 11.4599\n",
      "  Iter  21/1406: Loss 9.3204\n",
      "  Iter  31/1406: Loss 7.9555\n",
      "  Iter  41/1406: Loss 6.6707\n",
      "  Iter  51/1406: Loss 5.8942\n",
      "  Iter  61/1406: Loss 5.5013\n",
      "  Iter  71/1406: Loss 5.0798\n",
      "  Iter  81/1406: Loss 4.8626\n",
      "  Iter  91/1406: Loss 4.7282\n",
      "  Iter 101/1406: Loss 4.4827\n",
      "  Iter 111/1406: Loss 4.1566\n",
      "  Iter 121/1406: Loss 4.2913\n",
      "  Iter 131/1406: Loss 4.0643\n",
      "  Iter 141/1406: Loss 3.9658\n",
      "  Iter 151/1406: Loss 3.9095\n",
      "  Iter 161/1406: Loss 3.6972\n",
      "  Iter 171/1406: Loss 3.8001\n",
      "  Iter 181/1406: Loss 3.7412\n",
      "  Iter 191/1406: Loss 3.6265\n",
      "  Iter 201/1406: Loss 3.7643\n",
      "  Iter 211/1406: Loss 3.9458\n",
      "  Iter 221/1406: Loss 3.9208\n",
      "  Iter 231/1406: Loss 3.9119\n",
      "  Iter 241/1406: Loss 3.7530\n",
      "  Iter 251/1406: Loss 3.7738\n",
      "  Iter 261/1406: Loss 3.8225\n",
      "  Iter 271/1406: Loss 4.2337\n",
      "  Iter 281/1406: Loss 3.8077\n",
      "  Iter 291/1406: Loss 3.6728\n",
      "  Iter 301/1406: Loss 3.9731\n",
      "  Iter 311/1406: Loss 3.7200\n",
      "  Iter 321/1406: Loss 3.6988\n",
      "  Iter 331/1406: Loss 3.4866\n",
      "  Iter 341/1406: Loss 3.8734\n",
      "  Iter 351/1406: Loss 3.6866\n",
      "  Iter 361/1406: Loss 3.6796\n",
      "  Iter 371/1406: Loss 3.6807\n",
      "  Iter 381/1406: Loss 3.8599\n",
      "  Iter 391/1406: Loss 3.7183\n",
      "  Iter 401/1406: Loss 3.5902\n",
      "  Iter 411/1406: Loss 3.4533\n",
      "  Iter 421/1406: Loss 3.2295\n",
      "  Iter 431/1406: Loss 3.8010\n",
      "  Iter 441/1406: Loss 3.1781\n",
      "  Iter 451/1406: Loss 3.6879\n",
      "  Iter 461/1406: Loss 3.3134\n",
      "  Iter 471/1406: Loss 3.3750\n",
      "  Iter 481/1406: Loss 3.5834\n",
      "  Iter 491/1406: Loss 3.8472\n",
      "  Iter 501/1406: Loss 3.7397\n",
      "  Iter 511/1406: Loss 3.3124\n",
      "  Iter 521/1406: Loss 3.7618\n",
      "  Iter 531/1406: Loss 3.6063\n",
      "  Iter 541/1406: Loss 3.5105\n",
      "  Iter 551/1406: Loss 3.6393\n",
      "  Iter 561/1406: Loss 3.2710\n",
      "  Iter 571/1406: Loss 3.2772\n",
      "  Iter 581/1406: Loss 3.4678\n",
      "  Iter 591/1406: Loss 3.5352\n",
      "  Iter 601/1406: Loss 3.3940\n",
      "  Iter 611/1406: Loss 3.4204\n",
      "  Iter 621/1406: Loss 3.3647\n",
      "  Iter 631/1406: Loss 3.5061\n",
      "  Iter 641/1406: Loss 3.1495\n",
      "  Iter 651/1406: Loss 3.3336\n",
      "  Iter 661/1406: Loss 3.7133\n",
      "  Iter 671/1406: Loss 3.6567\n",
      "  Iter 681/1406: Loss 3.4900\n",
      "  Iter 691/1406: Loss 3.5154\n",
      "  Iter 701/1406: Loss 3.4495\n",
      "  Iter 711/1406: Loss 3.2052\n",
      "  Iter 721/1406: Loss 3.4723\n",
      "  Iter 731/1406: Loss 3.4484\n",
      "  Iter 741/1406: Loss 3.5757\n",
      "  Iter 751/1406: Loss 3.3773\n",
      "  Iter 761/1406: Loss 3.4015\n",
      "  Iter 771/1406: Loss 3.2973\n",
      "  Iter 781/1406: Loss 3.1882\n",
      "  Iter 791/1406: Loss 3.5125\n",
      "  Iter 801/1406: Loss 3.4347\n",
      "  Iter 811/1406: Loss 3.4175\n",
      "  Iter 821/1406: Loss 2.9146\n",
      "  Iter 831/1406: Loss 3.2040\n",
      "  Iter 841/1406: Loss 3.4749\n",
      "  Iter 851/1406: Loss 3.1473\n",
      "  Iter 861/1406: Loss 3.2859\n",
      "  Iter 871/1406: Loss 3.1192\n",
      "  Iter 881/1406: Loss 3.4169\n",
      "  Iter 891/1406: Loss 3.0150\n",
      "  Iter 901/1406: Loss 3.3281\n",
      "  Iter 911/1406: Loss 3.5513\n",
      "  Iter 921/1406: Loss 3.3524\n",
      "  Iter 931/1406: Loss 3.2427\n",
      "  Iter 941/1406: Loss 3.4172\n",
      "  Iter 951/1406: Loss 3.2043\n",
      "  Iter 961/1406: Loss 3.5513\n",
      "  Iter 971/1406: Loss 3.3698\n",
      "  Iter 981/1406: Loss 3.4828\n",
      "  Iter 991/1406: Loss 3.7909\n",
      "  Iter 1001/1406: Loss 3.3660\n",
      "  Iter 1011/1406: Loss 3.6086\n",
      "  Iter 1021/1406: Loss 2.8979\n",
      "  Iter 1031/1406: Loss 3.4945\n",
      "  Iter 1041/1406: Loss 3.3367\n",
      "  Iter 1051/1406: Loss 3.2044\n",
      "  Iter 1061/1406: Loss 3.1350\n",
      "  Iter 1071/1406: Loss 3.4048\n",
      "  Iter 1081/1406: Loss 3.1219\n",
      "  Iter 1091/1406: Loss 3.0187\n",
      "  Iter 1101/1406: Loss 3.0750\n",
      "  Iter 1111/1406: Loss 3.0562\n",
      "  Iter 1121/1406: Loss 3.2820\n",
      "  Iter 1131/1406: Loss 3.2367\n",
      "  Iter 1141/1406: Loss 3.5180\n",
      "  Iter 1151/1406: Loss 3.0922\n",
      "  Iter 1161/1406: Loss 2.9611\n",
      "  Iter 1171/1406: Loss 3.3187\n",
      "  Iter 1181/1406: Loss 3.5058\n",
      "  Iter 1191/1406: Loss 3.4346\n",
      "  Iter 1201/1406: Loss 3.1419\n",
      "  Iter 1211/1406: Loss 3.0114\n",
      "  Iter 1221/1406: Loss 3.4430\n",
      "  Iter 1231/1406: Loss 3.4045\n",
      "  Iter 1241/1406: Loss 3.3168\n",
      "  Iter 1251/1406: Loss 3.0929\n",
      "  Iter 1261/1406: Loss 3.2876\n",
      "  Iter 1271/1406: Loss 3.0581\n",
      "  Iter 1281/1406: Loss 3.4896\n",
      "  Iter 1291/1406: Loss 3.4219\n",
      "  Iter 1301/1406: Loss 3.3608\n",
      "  Iter 1311/1406: Loss 3.1211\n",
      "  Iter 1321/1406: Loss 3.0460\n",
      "  Iter 1331/1406: Loss 3.1921\n",
      "  Iter 1341/1406: Loss 3.2917\n",
      "  Iter 1351/1406: Loss 3.1831\n",
      "  Iter 1361/1406: Loss 2.8364\n",
      "  Iter 1371/1406: Loss 3.2287\n",
      "  Iter 1381/1406: Loss 3.4402\n",
      "  Iter 1391/1406: Loss 3.0913\n",
      "  Iter 1401/1406: Loss 3.3197\n",
      "  Iter 1406/1406: Loss 3.1787\n",
      "Fine Train Loss: 3.6966, Fine Train Acc: 0.2590, Val Acc: 0.2049, Val Loss: 3.3947\n",
      "Time: 4800.94s\n",
      "\n",
      "[Epoch 2/100]\n",
      "  Iter   1/1406: Loss 2.8660\n",
      "  Iter  11/1406: Loss 2.9229\n",
      "  Iter  21/1406: Loss 3.3735\n",
      "  Iter  31/1406: Loss 2.8924\n",
      "  Iter  41/1406: Loss 3.1660\n",
      "  Iter  51/1406: Loss 3.3261\n",
      "  Iter  61/1406: Loss 2.9267\n",
      "  Iter  71/1406: Loss 2.9207\n",
      "  Iter  81/1406: Loss 3.1438\n",
      "  Iter  91/1406: Loss 2.7547\n",
      "  Iter 101/1406: Loss 3.1408\n",
      "  Iter 111/1406: Loss 3.2359\n",
      "  Iter 121/1406: Loss 3.2332\n",
      "  Iter 131/1406: Loss 3.1894\n",
      "  Iter 141/1406: Loss 3.1651\n",
      "  Iter 151/1406: Loss 2.8483\n",
      "  Iter 161/1406: Loss 3.5284\n",
      "  Iter 171/1406: Loss 3.2048\n",
      "  Iter 181/1406: Loss 3.1904\n",
      "  Iter 191/1406: Loss 3.6707\n",
      "  Iter 201/1406: Loss 3.3093\n",
      "  Iter 211/1406: Loss 2.7947\n",
      "  Iter 221/1406: Loss 2.8458\n",
      "  Iter 231/1406: Loss 3.0708\n",
      "  Iter 241/1406: Loss 3.0589\n",
      "  Iter 251/1406: Loss 2.9472\n",
      "  Iter 261/1406: Loss 3.0259\n",
      "  Iter 271/1406: Loss 3.0313\n",
      "  Iter 281/1406: Loss 3.2388\n",
      "  Iter 291/1406: Loss 3.0164\n",
      "  Iter 301/1406: Loss 3.3212\n",
      "  Iter 311/1406: Loss 2.9649\n",
      "  Iter 321/1406: Loss 3.1500\n",
      "  Iter 331/1406: Loss 3.1640\n",
      "  Iter 341/1406: Loss 2.9928\n",
      "  Iter 351/1406: Loss 2.9519\n",
      "  Iter 361/1406: Loss 3.2317\n",
      "  Iter 371/1406: Loss 3.1824\n",
      "  Iter 381/1406: Loss 3.1480\n",
      "  Iter 391/1406: Loss 2.7727\n",
      "  Iter 401/1406: Loss 3.1936\n",
      "  Iter 411/1406: Loss 3.1202\n",
      "  Iter 421/1406: Loss 2.9905\n",
      "  Iter 431/1406: Loss 3.3592\n",
      "  Iter 441/1406: Loss 3.0604\n",
      "  Iter 451/1406: Loss 3.2529\n",
      "  Iter 461/1406: Loss 3.3274\n",
      "  Iter 471/1406: Loss 2.9625\n",
      "  Iter 481/1406: Loss 3.1869\n",
      "  Iter 491/1406: Loss 3.3236\n",
      "  Iter 501/1406: Loss 2.7753\n",
      "  Iter 511/1406: Loss 3.0334\n",
      "  Iter 521/1406: Loss 3.1413\n",
      "  Iter 531/1406: Loss 3.4165\n",
      "  Iter 541/1406: Loss 3.0210\n",
      "  Iter 551/1406: Loss 2.9774\n",
      "  Iter 561/1406: Loss 3.1035\n",
      "  Iter 571/1406: Loss 3.1055\n",
      "  Iter 581/1406: Loss 2.9420\n",
      "  Iter 591/1406: Loss 2.9542\n",
      "  Iter 601/1406: Loss 3.1597\n",
      "  Iter 611/1406: Loss 2.9536\n",
      "  Iter 621/1406: Loss 3.2263\n",
      "  Iter 631/1406: Loss 3.2071\n",
      "  Iter 641/1406: Loss 2.9606\n",
      "  Iter 651/1406: Loss 2.8834\n",
      "  Iter 661/1406: Loss 2.9323\n",
      "  Iter 671/1406: Loss 3.5489\n",
      "  Iter 681/1406: Loss 2.5762\n",
      "  Iter 691/1406: Loss 3.3327\n",
      "  Iter 701/1406: Loss 2.9039\n",
      "  Iter 711/1406: Loss 2.9095\n",
      "  Iter 721/1406: Loss 2.8631\n",
      "  Iter 731/1406: Loss 2.9104\n",
      "  Iter 741/1406: Loss 3.0465\n",
      "  Iter 751/1406: Loss 2.9443\n",
      "  Iter 761/1406: Loss 2.9260\n",
      "  Iter 771/1406: Loss 3.3999\n",
      "  Iter 781/1406: Loss 2.8017\n",
      "  Iter 791/1406: Loss 3.0101\n",
      "  Iter 801/1406: Loss 2.9867\n",
      "  Iter 811/1406: Loss 2.8741\n",
      "  Iter 821/1406: Loss 2.8786\n",
      "  Iter 831/1406: Loss 2.8009\n",
      "  Iter 841/1406: Loss 2.8304\n",
      "  Iter 851/1406: Loss 3.0934\n",
      "  Iter 861/1406: Loss 3.1740\n",
      "  Iter 871/1406: Loss 3.3129\n",
      "  Iter 881/1406: Loss 2.8344\n",
      "  Iter 891/1406: Loss 2.7121\n",
      "  Iter 901/1406: Loss 3.2608\n",
      "  Iter 911/1406: Loss 2.9488\n",
      "  Iter 921/1406: Loss 2.9579\n",
      "  Iter 931/1406: Loss 2.9270\n",
      "  Iter 941/1406: Loss 2.8400\n",
      "  Iter 951/1406: Loss 3.0804\n",
      "  Iter 961/1406: Loss 2.7746\n",
      "  Iter 971/1406: Loss 2.9748\n",
      "  Iter 981/1406: Loss 2.9320\n",
      "  Iter 991/1406: Loss 2.9918\n",
      "  Iter 1001/1406: Loss 3.0428\n",
      "  Iter 1011/1406: Loss 2.8791\n",
      "  Iter 1021/1406: Loss 2.9298\n",
      "  Iter 1031/1406: Loss 2.8737\n",
      "  Iter 1041/1406: Loss 2.8255\n",
      "  Iter 1051/1406: Loss 2.7445\n",
      "  Iter 1061/1406: Loss 2.7814\n",
      "  Iter 1071/1406: Loss 2.9603\n",
      "  Iter 1081/1406: Loss 3.1054\n",
      "  Iter 1091/1406: Loss 3.2354\n",
      "  Iter 1101/1406: Loss 3.1692\n",
      "  Iter 1111/1406: Loss 3.0425\n",
      "  Iter 1121/1406: Loss 2.9443\n",
      "  Iter 1131/1406: Loss 2.6336\n",
      "  Iter 1141/1406: Loss 2.4092\n",
      "  Iter 1151/1406: Loss 2.8717\n",
      "  Iter 1161/1406: Loss 2.8816\n",
      "  Iter 1171/1406: Loss 2.8005\n",
      "  Iter 1181/1406: Loss 2.8440\n",
      "  Iter 1191/1406: Loss 2.6045\n",
      "  Iter 1201/1406: Loss 3.2885\n",
      "  Iter 1211/1406: Loss 3.0511\n",
      "  Iter 1221/1406: Loss 3.0150\n",
      "  Iter 1231/1406: Loss 2.9018\n",
      "  Iter 1241/1406: Loss 3.0658\n",
      "  Iter 1251/1406: Loss 2.9889\n",
      "  Iter 1261/1406: Loss 3.4393\n",
      "  Iter 1271/1406: Loss 2.4727\n",
      "  Iter 1281/1406: Loss 3.1064\n",
      "  Iter 1291/1406: Loss 3.1363\n",
      "  Iter 1301/1406: Loss 2.8864\n",
      "  Iter 1311/1406: Loss 2.9965\n",
      "  Iter 1321/1406: Loss 2.8832\n",
      "  Iter 1331/1406: Loss 3.0036\n",
      "  Iter 1341/1406: Loss 2.8821\n",
      "  Iter 1351/1406: Loss 2.6932\n",
      "  Iter 1361/1406: Loss 3.1216\n",
      "  Iter 1371/1406: Loss 2.9533\n",
      "  Iter 1381/1406: Loss 2.8445\n",
      "  Iter 1391/1406: Loss 2.8614\n",
      "  Iter 1401/1406: Loss 2.9512\n",
      "  Iter 1406/1406: Loss 2.8985\n",
      "Fine Train Loss: 3.0396, Fine Train Acc: 0.2970, Val Acc: 0.2389, Val Loss: 3.1968\n",
      "Time: 4771.19s\n",
      "\n",
      "[Epoch 3/100]\n",
      "  Iter   1/1406: Loss 2.7731\n",
      "  Iter  11/1406: Loss 3.0966\n",
      "  Iter  21/1406: Loss 2.7495\n",
      "  Iter  31/1406: Loss 2.8786\n",
      "  Iter  41/1406: Loss 2.8253\n",
      "  Iter  51/1406: Loss 2.9579\n",
      "  Iter  61/1406: Loss 3.0011\n",
      "  Iter  71/1406: Loss 2.9798\n",
      "  Iter  81/1406: Loss 2.7707\n",
      "  Iter  91/1406: Loss 2.9351\n",
      "  Iter 101/1406: Loss 2.7339\n",
      "  Iter 111/1406: Loss 2.8231\n",
      "  Iter 121/1406: Loss 2.5468\n",
      "  Iter 131/1406: Loss 3.3978\n",
      "  Iter 141/1406: Loss 2.8558\n",
      "  Iter 151/1406: Loss 2.9104\n",
      "  Iter 161/1406: Loss 2.6951\n",
      "  Iter 171/1406: Loss 2.8462\n",
      "  Iter 181/1406: Loss 2.6569\n",
      "  Iter 191/1406: Loss 2.9047\n",
      "  Iter 201/1406: Loss 2.8992\n",
      "  Iter 211/1406: Loss 3.0338\n",
      "  Iter 221/1406: Loss 2.9074\n",
      "  Iter 231/1406: Loss 2.7662\n",
      "  Iter 241/1406: Loss 2.6586\n",
      "  Iter 251/1406: Loss 2.8791\n",
      "  Iter 261/1406: Loss 2.9043\n",
      "  Iter 271/1406: Loss 2.4788\n",
      "  Iter 281/1406: Loss 2.9658\n",
      "  Iter 291/1406: Loss 2.6477\n",
      "  Iter 301/1406: Loss 3.2807\n",
      "  Iter 311/1406: Loss 2.8649\n",
      "  Iter 321/1406: Loss 2.8048\n",
      "  Iter 331/1406: Loss 3.1366\n",
      "  Iter 341/1406: Loss 2.7024\n",
      "  Iter 351/1406: Loss 2.5696\n",
      "  Iter 361/1406: Loss 2.8549\n",
      "  Iter 371/1406: Loss 2.7312\n",
      "  Iter 381/1406: Loss 2.8208\n",
      "  Iter 391/1406: Loss 2.9496\n",
      "  Iter 401/1406: Loss 3.0773\n",
      "  Iter 411/1406: Loss 2.7041\n",
      "  Iter 421/1406: Loss 2.8821\n",
      "  Iter 431/1406: Loss 2.4917\n",
      "  Iter 441/1406: Loss 2.9502\n",
      "  Iter 451/1406: Loss 2.7672\n",
      "  Iter 461/1406: Loss 2.7601\n",
      "  Iter 471/1406: Loss 2.8540\n",
      "  Iter 481/1406: Loss 3.0558\n",
      "  Iter 491/1406: Loss 2.7819\n",
      "  Iter 501/1406: Loss 2.8996\n",
      "  Iter 511/1406: Loss 2.9668\n",
      "  Iter 521/1406: Loss 3.0529\n",
      "  Iter 531/1406: Loss 2.6153\n",
      "  Iter 541/1406: Loss 2.9393\n",
      "  Iter 551/1406: Loss 3.1295\n",
      "  Iter 561/1406: Loss 2.5285\n",
      "  Iter 571/1406: Loss 2.8951\n",
      "  Iter 581/1406: Loss 2.6665\n",
      "  Iter 591/1406: Loss 2.9676\n",
      "  Iter 601/1406: Loss 2.8408\n",
      "  Iter 611/1406: Loss 2.7065\n",
      "  Iter 621/1406: Loss 2.5960\n",
      "  Iter 631/1406: Loss 2.6379\n",
      "  Iter 641/1406: Loss 2.8697\n",
      "  Iter 651/1406: Loss 2.8717\n",
      "  Iter 661/1406: Loss 2.7417\n",
      "  Iter 671/1406: Loss 2.9550\n",
      "  Iter 681/1406: Loss 2.6191\n",
      "  Iter 691/1406: Loss 2.7065\n",
      "  Iter 701/1406: Loss 3.0263\n",
      "  Iter 711/1406: Loss 2.7160\n",
      "  Iter 721/1406: Loss 2.7785\n",
      "  Iter 731/1406: Loss 2.4706\n",
      "  Iter 741/1406: Loss 2.5583\n",
      "  Iter 751/1406: Loss 2.5390\n",
      "  Iter 761/1406: Loss 2.8316\n",
      "  Iter 771/1406: Loss 2.5620\n",
      "  Iter 781/1406: Loss 2.8103\n",
      "  Iter 791/1406: Loss 3.0135\n",
      "  Iter 801/1406: Loss 2.7319\n",
      "  Iter 811/1406: Loss 2.8259\n",
      "  Iter 821/1406: Loss 3.0328\n",
      "  Iter 831/1406: Loss 2.8364\n",
      "  Iter 841/1406: Loss 2.8998\n",
      "  Iter 851/1406: Loss 2.5702\n",
      "  Iter 861/1406: Loss 3.1642\n",
      "  Iter 871/1406: Loss 2.6664\n",
      "  Iter 881/1406: Loss 3.2699\n",
      "  Iter 891/1406: Loss 2.5418\n",
      "  Iter 901/1406: Loss 2.6637\n",
      "  Iter 911/1406: Loss 2.7264\n",
      "  Iter 921/1406: Loss 2.4375\n",
      "  Iter 931/1406: Loss 2.9213\n",
      "  Iter 941/1406: Loss 3.1066\n",
      "  Iter 951/1406: Loss 3.0300\n",
      "  Iter 961/1406: Loss 2.6449\n",
      "  Iter 971/1406: Loss 2.8024\n",
      "  Iter 981/1406: Loss 3.1708\n",
      "  Iter 991/1406: Loss 2.9097\n",
      "  Iter 1001/1406: Loss 2.8387\n",
      "  Iter 1011/1406: Loss 2.7418\n",
      "  Iter 1021/1406: Loss 2.5461\n",
      "  Iter 1031/1406: Loss 2.7171\n",
      "  Iter 1041/1406: Loss 2.4502\n",
      "  Iter 1051/1406: Loss 2.7734\n",
      "  Iter 1061/1406: Loss 2.6753\n",
      "  Iter 1071/1406: Loss 2.8027\n",
      "  Iter 1081/1406: Loss 2.8869\n",
      "  Iter 1091/1406: Loss 2.8852\n",
      "  Iter 1101/1406: Loss 2.3854\n",
      "  Iter 1111/1406: Loss 3.1111\n",
      "  Iter 1121/1406: Loss 2.5038\n",
      "  Iter 1131/1406: Loss 2.5723\n",
      "  Iter 1141/1406: Loss 2.3930\n",
      "  Iter 1151/1406: Loss 2.6519\n",
      "  Iter 1161/1406: Loss 2.7286\n",
      "  Iter 1171/1406: Loss 2.5466\n",
      "  Iter 1181/1406: Loss 2.5613\n",
      "  Iter 1191/1406: Loss 2.5290\n",
      "  Iter 1201/1406: Loss 2.9632\n",
      "  Iter 1211/1406: Loss 2.7836\n",
      "  Iter 1221/1406: Loss 2.5737\n",
      "  Iter 1231/1406: Loss 2.6435\n",
      "  Iter 1241/1406: Loss 2.4275\n",
      "  Iter 1251/1406: Loss 2.5517\n",
      "  Iter 1261/1406: Loss 2.8583\n",
      "  Iter 1271/1406: Loss 2.9273\n",
      "  Iter 1281/1406: Loss 2.7293\n",
      "  Iter 1291/1406: Loss 2.7340\n",
      "  Iter 1301/1406: Loss 2.9067\n",
      "  Iter 1311/1406: Loss 2.7922\n",
      "  Iter 1321/1406: Loss 2.7456\n",
      "  Iter 1331/1406: Loss 2.8250\n",
      "  Iter 1341/1406: Loss 2.6552\n",
      "  Iter 1351/1406: Loss 2.4803\n",
      "  Iter 1361/1406: Loss 2.6053\n",
      "  Iter 1371/1406: Loss 2.4747\n",
      "  Iter 1381/1406: Loss 2.7018\n",
      "  Iter 1391/1406: Loss 2.9335\n",
      "  Iter 1401/1406: Loss 2.8537\n",
      "  Iter 1406/1406: Loss 2.7749\n",
      "Fine Train Loss: 2.8080, Fine Train Acc: 0.3410, Val Acc: 0.2700, Val Loss: 3.0405\n",
      "Time: 4789.02s\n",
      "\n",
      "[Epoch 4/100]\n",
      "  Iter   1/1406: Loss 2.5444\n",
      "  Iter  11/1406: Loss 2.8405\n",
      "  Iter  21/1406: Loss 2.6808\n",
      "  Iter  31/1406: Loss 2.9332\n",
      "  Iter  41/1406: Loss 2.3507\n",
      "  Iter  51/1406: Loss 2.4634\n",
      "  Iter  61/1406: Loss 2.4981\n",
      "  Iter  71/1406: Loss 2.4517\n",
      "  Iter  81/1406: Loss 2.7508\n",
      "  Iter  91/1406: Loss 2.7154\n",
      "  Iter 101/1406: Loss 2.3848\n",
      "  Iter 111/1406: Loss 2.8735\n",
      "  Iter 121/1406: Loss 2.8639\n",
      "  Iter 131/1406: Loss 2.7606\n",
      "  Iter 141/1406: Loss 2.9212\n",
      "  Iter 151/1406: Loss 2.7618\n",
      "  Iter 161/1406: Loss 2.6773\n",
      "  Iter 171/1406: Loss 2.5208\n",
      "  Iter 181/1406: Loss 2.3171\n",
      "  Iter 191/1406: Loss 2.6958\n",
      "  Iter 201/1406: Loss 2.7121\n",
      "  Iter 211/1406: Loss 2.4034\n",
      "  Iter 221/1406: Loss 3.0663\n",
      "  Iter 231/1406: Loss 2.4713\n",
      "  Iter 241/1406: Loss 2.3862\n",
      "  Iter 251/1406: Loss 2.5636\n",
      "  Iter 261/1406: Loss 2.6505\n",
      "  Iter 271/1406: Loss 2.6582\n",
      "  Iter 281/1406: Loss 3.0498\n",
      "  Iter 291/1406: Loss 2.7331\n",
      "  Iter 301/1406: Loss 2.9216\n",
      "  Iter 311/1406: Loss 2.6903\n",
      "  Iter 321/1406: Loss 2.7422\n",
      "  Iter 331/1406: Loss 2.3878\n",
      "  Iter 341/1406: Loss 2.5687\n",
      "  Iter 351/1406: Loss 2.5449\n",
      "  Iter 361/1406: Loss 2.8165\n",
      "  Iter 371/1406: Loss 2.5070\n",
      "  Iter 381/1406: Loss 3.0002\n",
      "  Iter 391/1406: Loss 2.7614\n",
      "  Iter 401/1406: Loss 2.5651\n",
      "  Iter 411/1406: Loss 2.7396\n",
      "  Iter 421/1406: Loss 2.4728\n",
      "  Iter 431/1406: Loss 2.2806\n",
      "  Iter 441/1406: Loss 2.5252\n",
      "  Iter 451/1406: Loss 2.9201\n",
      "  Iter 461/1406: Loss 2.6948\n",
      "  Iter 471/1406: Loss 3.0832\n",
      "  Iter 481/1406: Loss 2.4482\n",
      "  Iter 491/1406: Loss 2.7145\n",
      "  Iter 501/1406: Loss 2.6355\n",
      "  Iter 511/1406: Loss 2.4716\n",
      "  Iter 521/1406: Loss 2.6112\n",
      "  Iter 531/1406: Loss 2.6943\n",
      "  Iter 541/1406: Loss 2.1589\n",
      "  Iter 551/1406: Loss 2.3943\n",
      "  Iter 561/1406: Loss 2.8450\n",
      "  Iter 571/1406: Loss 2.4314\n",
      "  Iter 581/1406: Loss 3.1176\n",
      "  Iter 591/1406: Loss 2.5731\n",
      "  Iter 601/1406: Loss 2.9007\n",
      "  Iter 611/1406: Loss 2.5775\n",
      "  Iter 621/1406: Loss 2.6505\n",
      "  Iter 631/1406: Loss 2.3484\n",
      "  Iter 641/1406: Loss 2.5435\n",
      "  Iter 651/1406: Loss 2.7871\n",
      "  Iter 661/1406: Loss 2.7481\n",
      "  Iter 671/1406: Loss 2.9584\n",
      "  Iter 681/1406: Loss 2.3518\n",
      "  Iter 691/1406: Loss 2.8966\n",
      "  Iter 701/1406: Loss 2.9470\n",
      "  Iter 711/1406: Loss 2.5308\n",
      "  Iter 721/1406: Loss 2.7759\n",
      "  Iter 731/1406: Loss 2.6582\n",
      "  Iter 741/1406: Loss 2.5469\n",
      "  Iter 751/1406: Loss 2.5907\n",
      "  Iter 761/1406: Loss 2.6922\n",
      "  Iter 771/1406: Loss 2.4665\n",
      "  Iter 781/1406: Loss 2.9948\n",
      "  Iter 791/1406: Loss 2.4689\n",
      "  Iter 801/1406: Loss 2.7491\n",
      "  Iter 811/1406: Loss 2.7161\n",
      "  Iter 821/1406: Loss 2.2640\n",
      "  Iter 831/1406: Loss 2.8488\n",
      "  Iter 841/1406: Loss 2.6780\n",
      "  Iter 851/1406: Loss 2.6894\n",
      "  Iter 861/1406: Loss 2.8338\n",
      "  Iter 871/1406: Loss 2.0960\n",
      "  Iter 881/1406: Loss 2.4170\n",
      "  Iter 891/1406: Loss 2.3387\n",
      "  Iter 901/1406: Loss 2.6285\n",
      "  Iter 911/1406: Loss 2.3874\n",
      "  Iter 921/1406: Loss 2.8816\n",
      "  Iter 931/1406: Loss 2.7306\n",
      "  Iter 941/1406: Loss 2.8840\n",
      "  Iter 951/1406: Loss 2.5519\n",
      "  Iter 961/1406: Loss 2.8584\n",
      "  Iter 971/1406: Loss 2.4216\n",
      "  Iter 981/1406: Loss 2.8056\n",
      "  Iter 991/1406: Loss 2.6970\n",
      "  Iter 1001/1406: Loss 2.7664\n",
      "  Iter 1011/1406: Loss 2.6162\n",
      "  Iter 1021/1406: Loss 2.4002\n",
      "  Iter 1031/1406: Loss 2.7230\n",
      "  Iter 1041/1406: Loss 2.2718\n",
      "  Iter 1051/1406: Loss 2.8033\n",
      "  Iter 1061/1406: Loss 2.2750\n",
      "  Iter 1071/1406: Loss 2.3115\n",
      "  Iter 1081/1406: Loss 2.7632\n",
      "  Iter 1091/1406: Loss 2.7011\n",
      "  Iter 1101/1406: Loss 2.6185\n",
      "  Iter 1111/1406: Loss 2.2558\n",
      "  Iter 1121/1406: Loss 2.5666\n",
      "  Iter 1131/1406: Loss 2.7314\n",
      "  Iter 1141/1406: Loss 2.7648\n",
      "  Iter 1151/1406: Loss 2.6175\n",
      "  Iter 1161/1406: Loss 2.3691\n",
      "  Iter 1171/1406: Loss 2.3007\n",
      "  Iter 1181/1406: Loss 2.6014\n",
      "  Iter 1191/1406: Loss 2.3322\n",
      "  Iter 1201/1406: Loss 2.9939\n",
      "  Iter 1211/1406: Loss 2.4519\n",
      "  Iter 1221/1406: Loss 2.7116\n",
      "  Iter 1231/1406: Loss 2.6658\n",
      "  Iter 1241/1406: Loss 2.2981\n",
      "  Iter 1251/1406: Loss 2.7747\n",
      "  Iter 1261/1406: Loss 2.5002\n",
      "  Iter 1271/1406: Loss 2.9885\n",
      "  Iter 1281/1406: Loss 2.6614\n",
      "  Iter 1291/1406: Loss 2.9323\n",
      "  Iter 1301/1406: Loss 2.7599\n",
      "  Iter 1311/1406: Loss 2.6421\n",
      "  Iter 1321/1406: Loss 2.5494\n",
      "  Iter 1331/1406: Loss 2.2821\n",
      "  Iter 1341/1406: Loss 3.0570\n",
      "  Iter 1351/1406: Loss 2.8634\n",
      "  Iter 1361/1406: Loss 2.6153\n",
      "  Iter 1371/1406: Loss 2.3128\n",
      "  Iter 1381/1406: Loss 2.9299\n",
      "  Iter 1391/1406: Loss 2.7154\n",
      "  Iter 1401/1406: Loss 2.6099\n",
      "  Iter 1406/1406: Loss 2.5945\n",
      "Fine Train Loss: 2.6276, Fine Train Acc: 0.3710, Val Acc: 0.2873, Val Loss: 2.9528\n",
      "Time: 4782.98s\n",
      "\n",
      "[Epoch 5/100]\n",
      "  Iter   1/1406: Loss 3.0335\n",
      "  Iter  11/1406: Loss 2.7164\n",
      "  Iter  21/1406: Loss 2.6992\n",
      "  Iter  31/1406: Loss 2.6100\n",
      "  Iter  41/1406: Loss 2.5542\n",
      "  Iter  51/1406: Loss 2.4383\n",
      "  Iter  61/1406: Loss 2.5075\n",
      "  Iter  71/1406: Loss 2.5500\n",
      "  Iter  81/1406: Loss 2.6623\n",
      "  Iter  91/1406: Loss 2.5236\n",
      "  Iter 101/1406: Loss 2.1469\n",
      "  Iter 111/1406: Loss 2.4346\n",
      "  Iter 121/1406: Loss 2.6283\n",
      "  Iter 131/1406: Loss 2.6284\n",
      "  Iter 141/1406: Loss 2.4834\n",
      "  Iter 151/1406: Loss 2.9202\n",
      "  Iter 161/1406: Loss 2.5015\n",
      "  Iter 171/1406: Loss 2.4990\n",
      "  Iter 181/1406: Loss 2.2632\n",
      "  Iter 191/1406: Loss 2.3714\n",
      "  Iter 201/1406: Loss 2.7529\n",
      "  Iter 211/1406: Loss 2.4681\n",
      "  Iter 221/1406: Loss 2.2671\n",
      "  Iter 231/1406: Loss 2.4706\n",
      "  Iter 241/1406: Loss 2.5954\n",
      "  Iter 251/1406: Loss 2.2212\n",
      "  Iter 261/1406: Loss 2.9043\n",
      "  Iter 271/1406: Loss 2.5382\n",
      "  Iter 281/1406: Loss 2.6312\n",
      "  Iter 291/1406: Loss 2.3438\n",
      "  Iter 301/1406: Loss 2.4605\n",
      "  Iter 311/1406: Loss 2.2619\n",
      "  Iter 321/1406: Loss 2.7336\n",
      "  Iter 331/1406: Loss 2.5572\n",
      "  Iter 341/1406: Loss 2.4295\n",
      "  Iter 351/1406: Loss 2.4386\n",
      "  Iter 361/1406: Loss 2.4221\n",
      "  Iter 371/1406: Loss 2.5139\n",
      "  Iter 381/1406: Loss 2.5950\n",
      "  Iter 391/1406: Loss 2.4662\n",
      "  Iter 401/1406: Loss 2.3758\n",
      "  Iter 411/1406: Loss 2.5650\n",
      "  Iter 421/1406: Loss 2.0587\n",
      "  Iter 431/1406: Loss 2.0170\n",
      "  Iter 441/1406: Loss 2.6811\n",
      "  Iter 451/1406: Loss 2.3739\n",
      "  Iter 461/1406: Loss 2.6464\n",
      "  Iter 471/1406: Loss 2.5473\n",
      "  Iter 481/1406: Loss 2.6031\n",
      "  Iter 491/1406: Loss 2.4471\n",
      "  Iter 501/1406: Loss 2.2468\n",
      "  Iter 511/1406: Loss 2.7109\n",
      "  Iter 521/1406: Loss 2.3163\n",
      "  Iter 531/1406: Loss 2.3702\n",
      "  Iter 541/1406: Loss 2.0489\n",
      "  Iter 551/1406: Loss 2.5736\n",
      "  Iter 561/1406: Loss 2.3501\n",
      "  Iter 571/1406: Loss 2.3042\n",
      "  Iter 581/1406: Loss 2.4548\n",
      "  Iter 591/1406: Loss 2.4822\n",
      "  Iter 601/1406: Loss 2.4138\n",
      "  Iter 611/1406: Loss 2.6429\n",
      "  Iter 621/1406: Loss 2.3153\n",
      "  Iter 631/1406: Loss 2.8120\n",
      "  Iter 641/1406: Loss 2.4867\n",
      "  Iter 651/1406: Loss 2.1936\n",
      "  Iter 661/1406: Loss 2.3482\n",
      "  Iter 671/1406: Loss 2.5433\n",
      "  Iter 681/1406: Loss 2.4199\n",
      "  Iter 691/1406: Loss 2.7840\n",
      "  Iter 701/1406: Loss 1.8947\n",
      "  Iter 711/1406: Loss 1.9134\n",
      "  Iter 721/1406: Loss 2.1690\n",
      "  Iter 731/1406: Loss 2.8299\n",
      "  Iter 741/1406: Loss 2.3337\n",
      "  Iter 751/1406: Loss 2.4353\n",
      "  Iter 761/1406: Loss 2.4550\n",
      "  Iter 771/1406: Loss 2.9286\n",
      "  Iter 781/1406: Loss 2.5102\n",
      "  Iter 791/1406: Loss 2.6009\n",
      "  Iter 801/1406: Loss 2.4359\n",
      "  Iter 811/1406: Loss 2.5564\n",
      "  Iter 821/1406: Loss 2.2229\n",
      "  Iter 831/1406: Loss 2.5885\n",
      "  Iter 841/1406: Loss 2.7330\n",
      "  Iter 851/1406: Loss 2.2176\n",
      "  Iter 861/1406: Loss 2.3888\n",
      "  Iter 871/1406: Loss 2.2851\n",
      "  Iter 881/1406: Loss 2.3292\n",
      "  Iter 891/1406: Loss 2.7954\n",
      "  Iter 901/1406: Loss 2.1880\n",
      "  Iter 911/1406: Loss 2.8212\n",
      "  Iter 921/1406: Loss 2.3543\n",
      "  Iter 931/1406: Loss 2.3402\n",
      "  Iter 941/1406: Loss 2.6875\n",
      "  Iter 951/1406: Loss 2.6170\n",
      "  Iter 961/1406: Loss 2.4923\n",
      "  Iter 971/1406: Loss 2.3040\n",
      "  Iter 981/1406: Loss 2.6663\n",
      "  Iter 991/1406: Loss 2.3566\n",
      "  Iter 1001/1406: Loss 2.9673\n",
      "  Iter 1011/1406: Loss 2.2287\n",
      "  Iter 1021/1406: Loss 2.4456\n",
      "  Iter 1031/1406: Loss 2.3831\n",
      "  Iter 1041/1406: Loss 2.3921\n",
      "  Iter 1051/1406: Loss 2.6490\n",
      "  Iter 1061/1406: Loss 2.4312\n",
      "  Iter 1071/1406: Loss 2.2078\n",
      "  Iter 1081/1406: Loss 2.5360\n",
      "  Iter 1091/1406: Loss 2.4190\n",
      "  Iter 1101/1406: Loss 2.4743\n",
      "  Iter 1111/1406: Loss 2.9130\n",
      "  Iter 1121/1406: Loss 2.2751\n",
      "  Iter 1131/1406: Loss 2.5551\n",
      "  Iter 1141/1406: Loss 2.9209\n",
      "  Iter 1151/1406: Loss 2.3689\n",
      "  Iter 1161/1406: Loss 2.2882\n",
      "  Iter 1171/1406: Loss 2.5581\n",
      "  Iter 1181/1406: Loss 1.8990\n",
      "  Iter 1191/1406: Loss 2.6559\n",
      "  Iter 1201/1406: Loss 2.8059\n",
      "  Iter 1211/1406: Loss 2.4324\n",
      "  Iter 1221/1406: Loss 2.0015\n",
      "  Iter 1231/1406: Loss 2.4022\n",
      "  Iter 1241/1406: Loss 2.6030\n",
      "  Iter 1251/1406: Loss 2.4286\n",
      "  Iter 1261/1406: Loss 2.3112\n",
      "  Iter 1271/1406: Loss 2.4285\n",
      "  Iter 1281/1406: Loss 2.7793\n",
      "  Iter 1291/1406: Loss 2.2565\n",
      "  Iter 1301/1406: Loss 2.0571\n",
      "  Iter 1311/1406: Loss 2.6379\n",
      "  Iter 1321/1406: Loss 2.2499\n",
      "  Iter 1331/1406: Loss 2.1447\n",
      "  Iter 1341/1406: Loss 2.1860\n",
      "  Iter 1351/1406: Loss 1.8440\n",
      "  Iter 1361/1406: Loss 2.4209\n",
      "  Iter 1371/1406: Loss 2.5877\n",
      "  Iter 1381/1406: Loss 2.4274\n",
      "  Iter 1391/1406: Loss 2.5254\n",
      "  Iter 1401/1406: Loss 2.3475\n",
      "  Iter 1406/1406: Loss 2.1012\n",
      "Fine Train Loss: 2.4486, Fine Train Acc: 0.4130, Val Acc: 0.3169, Val Loss: 2.8220\n",
      "Time: 4781.51s\n",
      ">>> Model saved to MiniVGGNet_final_ex3_epoch5.pkl\n",
      "\n",
      "[Epoch 6/100]\n",
      "  Iter   1/1406: Loss 2.4621\n",
      "  Iter  11/1406: Loss 2.1127\n",
      "  Iter  21/1406: Loss 2.2292\n",
      "  Iter  31/1406: Loss 2.1651\n",
      "  Iter  41/1406: Loss 2.4697\n",
      "  Iter  51/1406: Loss 2.4560\n",
      "  Iter  61/1406: Loss 2.2242\n",
      "  Iter  71/1406: Loss 2.4184\n",
      "  Iter  81/1406: Loss 2.5050\n",
      "  Iter  91/1406: Loss 2.9418\n",
      "  Iter 101/1406: Loss 2.6185\n",
      "  Iter 111/1406: Loss 2.2445\n",
      "  Iter 121/1406: Loss 2.5065\n",
      "  Iter 131/1406: Loss 2.6789\n",
      "  Iter 141/1406: Loss 2.4858\n",
      "  Iter 151/1406: Loss 2.6072\n",
      "  Iter 161/1406: Loss 2.3978\n",
      "  Iter 171/1406: Loss 2.1927\n",
      "  Iter 181/1406: Loss 2.5885\n",
      "  Iter 191/1406: Loss 2.4178\n",
      "  Iter 201/1406: Loss 2.4676\n",
      "  Iter 211/1406: Loss 2.2337\n",
      "  Iter 221/1406: Loss 2.1196\n",
      "  Iter 231/1406: Loss 2.3456\n",
      "  Iter 241/1406: Loss 2.5134\n",
      "  Iter 251/1406: Loss 2.1596\n",
      "  Iter 261/1406: Loss 2.4741\n",
      "  Iter 271/1406: Loss 2.1990\n",
      "  Iter 281/1406: Loss 2.3309\n",
      "  Iter 291/1406: Loss 2.2959\n",
      "  Iter 301/1406: Loss 2.2810\n",
      "  Iter 311/1406: Loss 2.4074\n",
      "  Iter 321/1406: Loss 2.3838\n",
      "  Iter 331/1406: Loss 2.2830\n",
      "  Iter 341/1406: Loss 2.4762\n",
      "  Iter 351/1406: Loss 2.3697\n",
      "  Iter 361/1406: Loss 2.3383\n",
      "  Iter 371/1406: Loss 2.0024\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==== Running ex3 : train dataset 100,000 = original + horizontal flip ====\")\n",
    "model = MiniVGGNet()\n",
    "\n",
    "x_train, y_train = data['train_flip']\n",
    "x_val, y_val = data['val_flip']\n",
    "x_test, y_test = data['test']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    model_name='MiniVGGNet_final_ex3',\n",
    "    train_data=(x_train, y_train),\n",
    "    val_data=(x_val, y_val),\n",
    "    test_data=(x_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    lr=0.001,\n",
    "    smoothing=0.1\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_log(\"MiniVGGNet_final_ex3_log.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"MiniVGGNet_final_ex3_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"MiniVGGNet_final_ex3_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"learning_curve_loss.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"learning_curve_accuracy.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

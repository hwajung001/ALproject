{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2htNbKViOhS",
        "outputId": "2f396d82-3994-4e81-e96d-1ffd92b76d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: (50000, 32, 32, 3)\n",
            "Train fine labels: (50000,)\n",
            "Test images: (10000, 32, 32, 3)\n",
            "Test fine labels: (10000,)\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-100 데이터 불러오기\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# 1. 데이터 다운로드 및 압축 풀기\n",
        "!wget -q https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar -xzf cifar-100-python.tar.gz\n",
        "\n",
        "# 2. 데이터 로딩 함수\n",
        "def load_cifar100(data_dir='/content/cifar-100-python'):\n",
        "    def load_file(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            data = pickle.load(f, encoding='latin1')\n",
        "        images = data['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "        images = images.astype('float32') / 255.0  # 정규화\n",
        "        fine_labels = np.array(data['fine_labels'])\n",
        "        coarse_labels = np.array(data['coarse_labels'])\n",
        "        return images, fine_labels, coarse_labels\n",
        "\n",
        "    train_images, train_fine_labels, train_coarse_labels = load_file(os.path.join(data_dir, 'train'))\n",
        "    test_images, test_fine_labels, test_coarse_labels = load_file(os.path.join(data_dir, 'test'))\n",
        "    return (train_images, train_fine_labels, train_coarse_labels), (test_images, test_fine_labels, test_coarse_labels)\n",
        "\n",
        "# 3. 데이터 불러오기\n",
        "(train_images, train_fine_labels, train_coarse_labels), (test_images, test_fine_labels, test_coarse_labels) = load_cifar100()\n",
        "\n",
        "# 4. 확인\n",
        "print(\"Train images:\", train_images.shape)\n",
        "print(\"Train fine labels:\", train_fine_labels.shape)\n",
        "print(\"Test images:\", test_images.shape)\n",
        "print(\"Test fine labels:\", test_fine_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class DeepFeatureMLP:\n",
        "    def __init__(self, input_size=768, hidden_sizes=[1024, 512, 256], output_size=100):\n",
        "        self.params = {}\n",
        "        self.init_weights(input_size, hidden_sizes, output_size)\n",
        "\n",
        "    def init_weights(self, input_size, hidden_sizes, output_size):\n",
        "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            fan_in, fan_out = layer_sizes[i], layer_sizes[i+1]\n",
        "            limit = np.sqrt(6 / (fan_in + fan_out))  # Xavier Initialization\n",
        "            self.params[f'W{i+1}'] = np.random.uniform(-limit, limit, (fan_in, fan_out)).astype(np.float32)\n",
        "            self.params[f'b{i+1}'] = np.zeros((1, fan_out), dtype=np.float32)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        cache = {}\n",
        "        A = X\n",
        "        num_layers = len(self.params) // 2\n",
        "\n",
        "        for i in range(1, num_layers):\n",
        "            Z = np.dot(A, self.params[f'W{i}']) + self.params[f'b{i}']\n",
        "            A = self.relu(Z)\n",
        "            cache[f'Z{i}'] = Z\n",
        "            cache[f'A{i}'] = A\n",
        "\n",
        "        Z_final = np.dot(A, self.params[f'W{num_layers}']) + self.params[f'b{num_layers}']\n",
        "        A_final = self.softmax(Z_final)\n",
        "        cache[f'Z{num_layers}'] = Z_final\n",
        "        cache[f'A{num_layers}'] = A_final\n",
        "\n",
        "        return A_final, cache\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        n_samples = y_true.shape[0]\n",
        "        log_probs = -np.log(y_pred[np.arange(n_samples), y_true] + 1e-8)\n",
        "        return np.sum(log_probs) / n_samples\n",
        "\n",
        "    def backward(self, X, y_true, cache):\n",
        "        grads = {}\n",
        "        num_layers = len(self.params) // 2\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        A_final = cache[f'A{num_layers}']\n",
        "        delta = A_final\n",
        "        delta[np.arange(n_samples), y_true] -= 1\n",
        "        delta /= n_samples\n",
        "\n",
        "        grads[f'W{num_layers}'] = np.dot(cache[f'A{num_layers-1}'].T, delta)\n",
        "        grads[f'b{num_layers}'] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        for i in reversed(range(1, num_layers)):\n",
        "            delta = np.dot(delta, self.params[f'W{i+1}'].T) * self.relu_derivative(cache[f'Z{i}'])\n",
        "            A_prev = X if i == 1 else cache[f'A{i-1}']\n",
        "            grads[f'W{i}'] = np.dot(A_prev.T, delta)\n",
        "            grads[f'b{i}'] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update_params(self, grads, learning_rate):\n",
        "        num_layers = len(self.params) // 2\n",
        "        for i in range(1, num_layers + 1):\n",
        "            self.params[f'W{i}'] -= learning_rate * grads[f'W{i}']\n",
        "            self.params[f'b{i}'] -= learning_rate * grads[f'b{i}']\n",
        "\n",
        "    def save(self, filename):\n",
        "        np.savez(filename, **self.params)\n",
        "\n",
        "    def load(self, filename):\n",
        "        data = np.load(filename)\n",
        "        for key in self.params.keys():\n",
        "            self.params[key] = data[key]\n",
        "\n",
        "    def train(self, X_train_full, y_train_full, epochs, learning_rate, batch_size=128, save_path=None, validation_split=0.1):\n",
        "        n_samples = X_train_full.shape[0]\n",
        "        n_train = int(n_samples * (1 - validation_split))\n",
        "\n",
        "        X_train = X_train_full[:n_train]\n",
        "        y_train = y_train_full[:n_train]\n",
        "        X_val = X_train_full[n_train:]\n",
        "        y_val = y_train_full[n_train:]\n",
        "\n",
        "        num_batches = n_train // batch_size\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            indices = np.arange(n_train)\n",
        "            np.random.shuffle(indices)\n",
        "            X_train = X_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for batch_idx in range(num_batches):\n",
        "                start = batch_idx * batch_size\n",
        "                end = start + batch_size\n",
        "                X_batch = X_train[start:end]\n",
        "                y_batch = y_train[start:end]\n",
        "\n",
        "                y_pred, cache = self.forward(X_batch)\n",
        "                loss = self.compute_loss(y_batch, y_pred)\n",
        "                grads = self.backward(X_batch, y_batch, cache)\n",
        "                self.update_params(grads, learning_rate)\n",
        "\n",
        "                epoch_loss += loss\n",
        "\n",
        "            avg_train_loss = epoch_loss / num_batches\n",
        "            train_acc = self.evaluate(X_train, y_train)\n",
        "            val_acc = self.evaluate(X_val, y_val)\n",
        "\n",
        "            y_val_pred, _ = self.forward(X_val)\n",
        "            val_loss = self.compute_loss(y_val, y_val_pred)\n",
        "\n",
        "            if save_path is not None:\n",
        "                self.save(save_path)\n",
        "\n",
        "            print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Val Loss={val_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred, _ = self.forward(X)\n",
        "        predictions = np.argmax(y_pred, axis=1)\n",
        "        accuracy = np.mean(predictions == y_true)\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "SNcC7FnJi4Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ✅ 데이터 준비\n",
        "# 다운샘플링 (32x32x3 → 16x16x3)\n",
        "def downsample_images(images):\n",
        "    return images[:, ::2, ::2, :]  # 2픽셀 간격으로 추출\n",
        "\n",
        "X_train = downsample_images(train_images)\n",
        "X_train = X_train.reshape(-1, 16*16*3).astype(np.float32)  # (50000, 768)\n",
        "y_train = train_fine_labels\n",
        "\n",
        "# ✅ 모델 생성\n",
        "model = DeepFeatureMLP(input_size=16*16*3, hidden_sizes=[1024, 512, 256], output_size=100)\n",
        "\n",
        "# ✅ 저장 경로\n",
        "model_save_path = '/content/optimized_deep_feature_mlp_checkpoint.npz'\n",
        "\n",
        "# ✅ 저장된 모델 불러오기\n",
        "if os.path.exists(model_save_path):\n",
        "    print(\"저장된 모델을 불러옵니다...\")\n",
        "    model.load(model_save_path)\n",
        "else:\n",
        "    print(\"새 모델로 학습을 시작합니다...\")\n",
        "\n",
        "# ✅ 학습 설정\n",
        "epochs = 50\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "\n",
        "# ✅ 학습 시작\n",
        "model.train(X_train, y_train, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, save_path=model_save_path)\n",
        "\n",
        "# ✅ 최종 정확도 출력\n",
        "final_accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"\\n최종 학습 정확도: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlh27XM8jBca",
        "outputId": "6e4913c1-219c-4c7d-b4be-8a09345886c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "새 모델로 학습을 시작합니다...\n",
            "Epoch 1: Train Loss=4.5702, Val Loss=4.5310, Train Acc=0.0336, Val Acc=0.0282\n",
            "Epoch 2: Train Loss=4.4643, Val Loss=4.4015, Train Acc=0.0678, Val Acc=0.0624\n",
            "Epoch 3: Train Loss=4.2965, Val Loss=4.2281, Train Acc=0.0786, Val Acc=0.0748\n",
            "Epoch 4: Train Loss=4.1270, Val Loss=4.0947, Train Acc=0.0938, Val Acc=0.0882\n",
            "Epoch 5: Train Loss=4.0210, Val Loss=4.0115, Train Acc=0.1076, Val Acc=0.0978\n",
            "Epoch 6: Train Loss=3.9470, Val Loss=3.9559, Train Acc=0.1134, Val Acc=0.1044\n",
            "Epoch 7: Train Loss=3.8834, Val Loss=3.9018, Train Acc=0.1211, Val Acc=0.1074\n",
            "Epoch 8: Train Loss=3.8252, Val Loss=3.8414, Train Acc=0.1361, Val Acc=0.1222\n",
            "Epoch 9: Train Loss=3.7731, Val Loss=3.7962, Train Acc=0.1418, Val Acc=0.1302\n",
            "Epoch 10: Train Loss=3.7283, Val Loss=3.7612, Train Acc=0.1468, Val Acc=0.1384\n",
            "Epoch 11: Train Loss=3.6920, Val Loss=3.7234, Train Acc=0.1544, Val Acc=0.1470\n",
            "Epoch 12: Train Loss=3.6574, Val Loss=3.7055, Train Acc=0.1597, Val Acc=0.1480\n",
            "Epoch 13: Train Loss=3.6296, Val Loss=3.6789, Train Acc=0.1631, Val Acc=0.1544\n",
            "Epoch 14: Train Loss=3.6019, Val Loss=3.6494, Train Acc=0.1736, Val Acc=0.1622\n",
            "Epoch 15: Train Loss=3.5762, Val Loss=3.6422, Train Acc=0.1750, Val Acc=0.1568\n",
            "Epoch 16: Train Loss=3.5548, Val Loss=3.6208, Train Acc=0.1794, Val Acc=0.1608\n",
            "Epoch 17: Train Loss=3.5326, Val Loss=3.6037, Train Acc=0.1838, Val Acc=0.1612\n",
            "Epoch 18: Train Loss=3.5113, Val Loss=3.5827, Train Acc=0.1888, Val Acc=0.1686\n",
            "Epoch 19: Train Loss=3.4902, Val Loss=3.5710, Train Acc=0.1877, Val Acc=0.1716\n",
            "Epoch 20: Train Loss=3.4732, Val Loss=3.5495, Train Acc=0.1945, Val Acc=0.1730\n",
            "Epoch 21: Train Loss=3.4535, Val Loss=3.5336, Train Acc=0.2001, Val Acc=0.1776\n",
            "Epoch 22: Train Loss=3.4350, Val Loss=3.5381, Train Acc=0.1978, Val Acc=0.1702\n",
            "Epoch 23: Train Loss=3.4157, Val Loss=3.5168, Train Acc=0.2015, Val Acc=0.1736\n",
            "Epoch 24: Train Loss=3.4006, Val Loss=3.4923, Train Acc=0.2082, Val Acc=0.1880\n",
            "Epoch 25: Train Loss=3.3834, Val Loss=3.4902, Train Acc=0.2102, Val Acc=0.1824\n",
            "Epoch 26: Train Loss=3.3658, Val Loss=3.4849, Train Acc=0.2106, Val Acc=0.1856\n",
            "Epoch 27: Train Loss=3.3516, Val Loss=3.4580, Train Acc=0.2172, Val Acc=0.1880\n",
            "Epoch 28: Train Loss=3.3366, Val Loss=3.4611, Train Acc=0.2178, Val Acc=0.1896\n",
            "Epoch 29: Train Loss=3.3207, Val Loss=3.4601, Train Acc=0.2191, Val Acc=0.1916\n",
            "Epoch 30: Train Loss=3.3064, Val Loss=3.4377, Train Acc=0.2219, Val Acc=0.1916\n",
            "Epoch 31: Train Loss=3.2910, Val Loss=3.4274, Train Acc=0.2261, Val Acc=0.1948\n",
            "Epoch 32: Train Loss=3.2770, Val Loss=3.4086, Train Acc=0.2289, Val Acc=0.2006\n",
            "Epoch 33: Train Loss=3.2634, Val Loss=3.3971, Train Acc=0.2325, Val Acc=0.2048\n",
            "Epoch 34: Train Loss=3.2508, Val Loss=3.3958, Train Acc=0.2324, Val Acc=0.2018\n",
            "Epoch 35: Train Loss=3.2366, Val Loss=3.3854, Train Acc=0.2373, Val Acc=0.2052\n",
            "Epoch 36: Train Loss=3.2237, Val Loss=3.3912, Train Acc=0.2350, Val Acc=0.2030\n",
            "Epoch 37: Train Loss=3.2096, Val Loss=3.3675, Train Acc=0.2414, Val Acc=0.2090\n",
            "Epoch 38: Train Loss=3.1956, Val Loss=3.3705, Train Acc=0.2440, Val Acc=0.2046\n",
            "Epoch 39: Train Loss=3.1831, Val Loss=3.3591, Train Acc=0.2454, Val Acc=0.2078\n",
            "Epoch 40: Train Loss=3.1691, Val Loss=3.3470, Train Acc=0.2471, Val Acc=0.2100\n",
            "Epoch 41: Train Loss=3.1589, Val Loss=3.3531, Train Acc=0.2507, Val Acc=0.2070\n",
            "Epoch 42: Train Loss=3.1455, Val Loss=3.3275, Train Acc=0.2562, Val Acc=0.2128\n",
            "Epoch 43: Train Loss=3.1320, Val Loss=3.3345, Train Acc=0.2552, Val Acc=0.2086\n",
            "Epoch 44: Train Loss=3.1170, Val Loss=3.3211, Train Acc=0.2558, Val Acc=0.2124\n",
            "Epoch 45: Train Loss=3.1069, Val Loss=3.3501, Train Acc=0.2498, Val Acc=0.2118\n",
            "Epoch 46: Train Loss=3.0942, Val Loss=3.3098, Train Acc=0.2607, Val Acc=0.2118\n",
            "Epoch 47: Train Loss=3.0825, Val Loss=3.2914, Train Acc=0.2644, Val Acc=0.2182\n",
            "Epoch 48: Train Loss=3.0687, Val Loss=3.2973, Train Acc=0.2664, Val Acc=0.2142\n",
            "Epoch 49: Train Loss=3.0587, Val Loss=3.3091, Train Acc=0.2644, Val Acc=0.2208\n",
            "Epoch 50: Train Loss=3.0455, Val Loss=3.2788, Train Acc=0.2707, Val Acc=0.2198\n",
            "\n",
            "최종 학습 정확도: 0.2656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 모델 생성\n",
        "model = DeepFeatureMLP(input_size=16*16*3, hidden_sizes=[1024, 512, 256], output_size=100)\n",
        "\n",
        "# 저장된 모델 불러오기\n",
        "model_save_path = '/content/optimized_deep_feature_mlp_checkpoint.npz'\n",
        "\n",
        "if os.path.exists(model_save_path):\n",
        "    print(\"저장된 모델을 불러옵니다...\")\n",
        "    model.load(model_save_path)\n",
        "else:\n",
        "    print(\"모델을 찾을 수 없습니다. 새로 학습을 시작합니다.\")\n",
        "\n",
        "# 추가 학습 설정\n",
        "epochs = 50  # 추가로 50 epoch\n",
        "learning_rate = 0.005  # 🔥 더 부드럽게 학습하기 위해 줄임\n",
        "batch_size = 128\n",
        "\n",
        "# 추가 학습 시작\n",
        "model.train(X_train, y_train, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, save_path=model_save_path)\n",
        "\n",
        "# 추가 학습 후 최종 정확도 출력\n",
        "final_accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"\\n추가 학습 후 최종 학습 정확도: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZZFE0qgl49y",
        "outputId": "dc2bc87c-0db2-45a0-8de3-7578317d2a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "저장된 모델을 불러옵니다...\n",
            "Epoch 1: Train Loss=3.0086, Val Loss=3.2584, Train Acc=0.2774, Val Acc=0.2256\n",
            "Epoch 2: Train Loss=3.0009, Val Loss=3.2571, Train Acc=0.2809, Val Acc=0.2254\n",
            "Epoch 3: Train Loss=2.9951, Val Loss=3.2600, Train Acc=0.2782, Val Acc=0.2268\n",
            "Epoch 4: Train Loss=2.9901, Val Loss=3.2480, Train Acc=0.2823, Val Acc=0.2260\n",
            "Epoch 5: Train Loss=2.9829, Val Loss=3.2509, Train Acc=0.2826, Val Acc=0.2266\n",
            "Epoch 6: Train Loss=2.9757, Val Loss=3.2573, Train Acc=0.2827, Val Acc=0.2294\n",
            "Epoch 7: Train Loss=2.9706, Val Loss=3.2403, Train Acc=0.2851, Val Acc=0.2302\n",
            "Epoch 8: Train Loss=2.9630, Val Loss=3.2390, Train Acc=0.2840, Val Acc=0.2266\n",
            "Epoch 9: Train Loss=2.9577, Val Loss=3.2334, Train Acc=0.2880, Val Acc=0.2282\n",
            "Epoch 10: Train Loss=2.9515, Val Loss=3.2343, Train Acc=0.2887, Val Acc=0.2296\n",
            "Epoch 11: Train Loss=2.9446, Val Loss=3.2355, Train Acc=0.2878, Val Acc=0.2272\n",
            "Epoch 12: Train Loss=2.9391, Val Loss=3.2291, Train Acc=0.2934, Val Acc=0.2296\n",
            "Epoch 13: Train Loss=2.9331, Val Loss=3.2272, Train Acc=0.2930, Val Acc=0.2330\n",
            "Epoch 14: Train Loss=2.9272, Val Loss=3.2193, Train Acc=0.2946, Val Acc=0.2318\n",
            "Epoch 15: Train Loss=2.9209, Val Loss=3.2176, Train Acc=0.2941, Val Acc=0.2328\n",
            "Epoch 16: Train Loss=2.9160, Val Loss=3.2285, Train Acc=0.2952, Val Acc=0.2344\n",
            "Epoch 17: Train Loss=2.9085, Val Loss=3.2304, Train Acc=0.2926, Val Acc=0.2350\n",
            "Epoch 18: Train Loss=2.9029, Val Loss=3.2157, Train Acc=0.2987, Val Acc=0.2336\n",
            "Epoch 19: Train Loss=2.8973, Val Loss=3.2140, Train Acc=0.3009, Val Acc=0.2286\n",
            "Epoch 20: Train Loss=2.8908, Val Loss=3.2206, Train Acc=0.2993, Val Acc=0.2286\n",
            "Epoch 21: Train Loss=2.8862, Val Loss=3.2030, Train Acc=0.3048, Val Acc=0.2342\n",
            "Epoch 22: Train Loss=2.8799, Val Loss=3.2128, Train Acc=0.3036, Val Acc=0.2266\n",
            "Epoch 23: Train Loss=2.8740, Val Loss=3.1956, Train Acc=0.3060, Val Acc=0.2380\n",
            "Epoch 24: Train Loss=2.8685, Val Loss=3.1947, Train Acc=0.3068, Val Acc=0.2320\n",
            "Epoch 25: Train Loss=2.8615, Val Loss=3.2018, Train Acc=0.3074, Val Acc=0.2364\n",
            "Epoch 26: Train Loss=2.8560, Val Loss=3.2059, Train Acc=0.3036, Val Acc=0.2408\n",
            "Epoch 27: Train Loss=2.8504, Val Loss=3.1916, Train Acc=0.3108, Val Acc=0.2384\n",
            "Epoch 28: Train Loss=2.8451, Val Loss=3.2022, Train Acc=0.3080, Val Acc=0.2356\n",
            "Epoch 29: Train Loss=2.8392, Val Loss=3.1879, Train Acc=0.3112, Val Acc=0.2394\n",
            "Epoch 30: Train Loss=2.8327, Val Loss=3.2079, Train Acc=0.3098, Val Acc=0.2356\n",
            "Epoch 31: Train Loss=2.8272, Val Loss=3.1829, Train Acc=0.3136, Val Acc=0.2410\n",
            "Epoch 32: Train Loss=2.8209, Val Loss=3.1873, Train Acc=0.3140, Val Acc=0.2386\n",
            "Epoch 33: Train Loss=2.8161, Val Loss=3.1933, Train Acc=0.3135, Val Acc=0.2370\n",
            "Epoch 34: Train Loss=2.8092, Val Loss=3.1974, Train Acc=0.3137, Val Acc=0.2372\n",
            "Epoch 35: Train Loss=2.8044, Val Loss=3.1835, Train Acc=0.3192, Val Acc=0.2394\n",
            "Epoch 36: Train Loss=2.7993, Val Loss=3.1847, Train Acc=0.3175, Val Acc=0.2390\n",
            "Epoch 37: Train Loss=2.7937, Val Loss=3.1729, Train Acc=0.3193, Val Acc=0.2432\n",
            "Epoch 38: Train Loss=2.7870, Val Loss=3.1725, Train Acc=0.3225, Val Acc=0.2460\n",
            "Epoch 39: Train Loss=2.7824, Val Loss=3.1768, Train Acc=0.3198, Val Acc=0.2390\n",
            "Epoch 40: Train Loss=2.7753, Val Loss=3.1680, Train Acc=0.3240, Val Acc=0.2404\n",
            "Epoch 41: Train Loss=2.7701, Val Loss=3.1627, Train Acc=0.3265, Val Acc=0.2434\n",
            "Epoch 42: Train Loss=2.7638, Val Loss=3.1665, Train Acc=0.3252, Val Acc=0.2460\n",
            "Epoch 43: Train Loss=2.7595, Val Loss=3.1785, Train Acc=0.3255, Val Acc=0.2406\n",
            "Epoch 44: Train Loss=2.7526, Val Loss=3.1603, Train Acc=0.3298, Val Acc=0.2464\n",
            "Epoch 45: Train Loss=2.7471, Val Loss=3.1583, Train Acc=0.3304, Val Acc=0.2446\n",
            "Epoch 46: Train Loss=2.7407, Val Loss=3.1777, Train Acc=0.3317, Val Acc=0.2384\n",
            "Epoch 47: Train Loss=2.7351, Val Loss=3.1622, Train Acc=0.3332, Val Acc=0.2408\n",
            "Epoch 48: Train Loss=2.7299, Val Loss=3.1720, Train Acc=0.3339, Val Acc=0.2414\n",
            "Epoch 49: Train Loss=2.7257, Val Loss=3.1754, Train Acc=0.3286, Val Acc=0.2398\n",
            "Epoch 50: Train Loss=2.7183, Val Loss=3.1524, Train Acc=0.3356, Val Acc=0.2454\n",
            "\n",
            "추가 학습 후 최종 학습 정확도: 0.3266\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-hX3hmEVkfl",
        "outputId": "fe9cd7fe-8cfa-4393-e610-7c674bcd9ad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: (50000, 32, 32, 3)\n",
            "Train fine labels: (50000,)\n",
            "Test images: (10000, 32, 32, 3)\n",
            "Test fine labels: (10000,)\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-100 데이터 불러오기\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# 1. 데이터 다운로드 및 압축 풀기\n",
        "!wget -q https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar -xzf cifar-100-python.tar.gz\n",
        "\n",
        "# 2. 데이터 로딩 함수\n",
        "def load_cifar100(data_dir='/content/cifar-100-python'):\n",
        "    def load_file(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            data = pickle.load(f, encoding='latin1')\n",
        "        images = data['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "        images = images.astype('float32') / 255.0  # 정규화\n",
        "        fine_labels = np.array(data['fine_labels'])\n",
        "        coarse_labels = np.array(data['coarse_labels'])\n",
        "        return images, fine_labels, coarse_labels\n",
        "\n",
        "    train_images, train_fine_labels, train_coarse_labels = load_file(os.path.join(data_dir, 'train'))\n",
        "    test_images, test_fine_labels, test_coarse_labels = load_file(os.path.join(data_dir, 'test'))\n",
        "    return (train_images, train_fine_labels, train_coarse_labels), (test_images, test_fine_labels, test_coarse_labels)\n",
        "\n",
        "# 3. 데이터 불러오기\n",
        "(train_images, train_fine_labels, train_coarse_labels), (test_images, test_fine_labels, test_coarse_labels) = load_cifar100()\n",
        "\n",
        "# 4. 확인\n",
        "print(\"Train images:\", train_images.shape)\n",
        "print(\"Train fine labels:\", train_fine_labels.shape)\n",
        "print(\"Test images:\", test_images.shape)\n",
        "print(\"Test fine labels:\", test_fine_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class SimpleFeatureMLP:\n",
        "    def __init__(self, input_size=768, hidden_sizes=[512, 256], output_size=100):\n",
        "        self.params = {}\n",
        "        self.init_weights(input_size, hidden_sizes, output_size)\n",
        "\n",
        "    def init_weights(self, input_size, hidden_sizes, output_size):\n",
        "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            fan_in, fan_out = layer_sizes[i], layer_sizes[i+1]\n",
        "            limit = np.sqrt(6 / (fan_in + fan_out))  # Xavier 초기화\n",
        "            self.params[f'W{i+1}'] = np.random.uniform(-limit, limit, (fan_in, fan_out)).astype(np.float32)\n",
        "            self.params[f'b{i+1}'] = np.zeros((1, fan_out), dtype=np.float32)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        cache = {}\n",
        "        A = X\n",
        "        num_layers = len(self.params) // 2\n",
        "\n",
        "        for i in range(1, num_layers):\n",
        "            Z = np.dot(A, self.params[f'W{i}']) + self.params[f'b{i}']\n",
        "            A = self.relu(Z)\n",
        "            cache[f'Z{i}'] = Z\n",
        "            cache[f'A{i}'] = A\n",
        "\n",
        "        Z_final = np.dot(A, self.params[f'W{num_layers}']) + self.params[f'b{num_layers}']\n",
        "        A_final = self.softmax(Z_final)\n",
        "        cache[f'Z{num_layers}'] = Z_final\n",
        "        cache[f'A{num_layers}'] = A_final\n",
        "\n",
        "        return A_final, cache\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        n_samples = y_true.shape[0]\n",
        "        log_probs = -np.log(y_pred[np.arange(n_samples), y_true] + 1e-8)\n",
        "        return np.sum(log_probs) / n_samples\n",
        "\n",
        "    def backward(self, X, y_true, cache):\n",
        "        grads = {}\n",
        "        num_layers = len(self.params) // 2\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        A_final = cache[f'A{num_layers}']\n",
        "        delta = A_final\n",
        "        delta[np.arange(n_samples), y_true] -= 1\n",
        "        delta /= n_samples\n",
        "\n",
        "        grads[f'W{num_layers}'] = np.dot(cache[f'A{num_layers-1}'].T, delta)\n",
        "        grads[f'b{num_layers}'] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        for i in reversed(range(1, num_layers)):\n",
        "            delta = np.dot(delta, self.params[f'W{i+1}'].T) * self.relu_derivative(cache[f'Z{i}'])\n",
        "            A_prev = X if i == 1 else cache[f'A{i-1}']\n",
        "            grads[f'W{i}'] = np.dot(A_prev.T, delta)\n",
        "            grads[f'b{i}'] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update_params(self, grads, learning_rate):\n",
        "        num_layers = len(self.params) // 2\n",
        "        for i in range(1, num_layers + 1):\n",
        "            self.params[f'W{i}'] -= learning_rate * grads[f'W{i}']\n",
        "            self.params[f'b{i}'] -= learning_rate * grads[f'b{i}']\n",
        "\n",
        "    def save(self, filename):\n",
        "        np.savez(filename, **self.params)\n",
        "\n",
        "    def load(self, filename):\n",
        "        data = np.load(filename)\n",
        "        for key in self.params.keys():\n",
        "            self.params[key] = data[key]\n",
        "\n",
        "    def train(self, X_train_full, y_train_full, epochs, learning_rate, batch_size=128, save_path=None, validation_split=0.1):\n",
        "        n_samples = X_train_full.shape[0]\n",
        "        n_train = int(n_samples * (1 - validation_split))\n",
        "\n",
        "        X_train = X_train_full[:n_train]\n",
        "        y_train = y_train_full[:n_train]\n",
        "        X_val = X_train_full[n_train:]\n",
        "        y_val = y_train_full[n_train:]\n",
        "\n",
        "        num_batches = n_train // batch_size\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            indices = np.arange(n_train)\n",
        "            np.random.shuffle(indices)\n",
        "            X_train = X_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for batch_idx in range(num_batches):\n",
        "                start = batch_idx * batch_size\n",
        "                end = start + batch_size\n",
        "                X_batch = X_train[start:end]\n",
        "                y_batch = y_train[start:end]\n",
        "\n",
        "                y_pred, cache = self.forward(X_batch)\n",
        "                loss = self.compute_loss(y_batch, y_pred)\n",
        "                grads = self.backward(X_batch, y_batch, cache)\n",
        "                self.update_params(grads, learning_rate)\n",
        "\n",
        "                epoch_loss += loss\n",
        "\n",
        "            avg_train_loss = epoch_loss / num_batches\n",
        "            train_acc = self.evaluate(X_train, y_train)\n",
        "            val_acc = self.evaluate(X_val, y_val)\n",
        "\n",
        "            y_val_pred, _ = self.forward(X_val)\n",
        "            val_loss = self.compute_loss(y_val, y_val_pred)\n",
        "\n",
        "            if save_path is not None:\n",
        "                self.save(save_path)\n",
        "\n",
        "            print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Val Loss={val_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred, _ = self.forward(X)\n",
        "        predictions = np.argmax(y_pred, axis=1)\n",
        "        accuracy = np.mean(predictions == y_true)\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "pc5rWszKV4xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ✅ 데이터 준비\n",
        "# (32x32x3 이미지를 16x16x3으로 다운샘플링 + Flatten)\n",
        "def downsample_images(images):\n",
        "    # 2픽셀마다 하나씩 추출해서 16x16으로 줄이기\n",
        "    return images[:, ::2, ::2, :]  # (50000, 16, 16, 3)\n",
        "\n",
        "X_train = downsample_images(train_images)\n",
        "X_train = X_train.reshape(-1, 16*16*3).astype(np.float32)  # (50000, 768)\n",
        "y_train = train_fine_labels\n",
        "\n",
        "# ✅ 모델 생성\n",
        "model = SimpleFeatureMLP(input_size=16*16*3, hidden_sizes=[512, 256], output_size=100)\n",
        "\n",
        "# ✅ 저장 경로\n",
        "model_save_path = '/content/optimized_simple_feature_mlp_checkpoint.npz'\n",
        "\n",
        "# ✅ 저장된 모델 불러오기\n",
        "if os.path.exists(model_save_path):\n",
        "    print(\"저장된 모델을 불러옵니다...\")\n",
        "    model.load(model_save_path)\n",
        "else:\n",
        "    print(\"새 모델로 학습을 시작합니다...\")\n",
        "\n",
        "# ✅ 학습 설정\n",
        "epochs = 50\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "\n",
        "# ✅ 학습 시작\n",
        "model.train(X_train, y_train, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, save_path=model_save_path)\n",
        "\n",
        "# ✅ 최종 정확도 출력\n",
        "final_accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"\\n최종 학습 정확도: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B1bnrC-WAM1",
        "outputId": "e17fdbf2-fd71-4be6-d7f6-48d508d50c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "새 모델로 학습을 시작합니다...\n",
            "Epoch 1: Train Loss=4.5724, Val Loss=4.5195, Train Acc=0.0406, Val Acc=0.0416\n",
            "Epoch 2: Train Loss=4.4512, Val Loss=4.3931, Train Acc=0.0588, Val Acc=0.0562\n",
            "Epoch 3: Train Loss=4.3062, Val Loss=4.2525, Train Acc=0.0746, Val Acc=0.0694\n",
            "Epoch 4: Train Loss=4.1692, Val Loss=4.1402, Train Acc=0.0838, Val Acc=0.0802\n",
            "Epoch 5: Train Loss=4.0724, Val Loss=4.0715, Train Acc=0.0980, Val Acc=0.0904\n",
            "Epoch 6: Train Loss=4.0056, Val Loss=4.0218, Train Acc=0.1083, Val Acc=0.0958\n",
            "Epoch 7: Train Loss=3.9534, Val Loss=3.9706, Train Acc=0.1166, Val Acc=0.1072\n",
            "Epoch 8: Train Loss=3.9074, Val Loss=3.9344, Train Acc=0.1208, Val Acc=0.1132\n",
            "Epoch 9: Train Loss=3.8664, Val Loss=3.8934, Train Acc=0.1309, Val Acc=0.1180\n",
            "Epoch 10: Train Loss=3.8284, Val Loss=3.8637, Train Acc=0.1359, Val Acc=0.1194\n",
            "Epoch 11: Train Loss=3.7934, Val Loss=3.8363, Train Acc=0.1392, Val Acc=0.1268\n",
            "Epoch 12: Train Loss=3.7614, Val Loss=3.8072, Train Acc=0.1428, Val Acc=0.1328\n",
            "Epoch 13: Train Loss=3.7325, Val Loss=3.7841, Train Acc=0.1501, Val Acc=0.1342\n",
            "Epoch 14: Train Loss=3.7058, Val Loss=3.7577, Train Acc=0.1536, Val Acc=0.1392\n",
            "Epoch 15: Train Loss=3.6821, Val Loss=3.7390, Train Acc=0.1559, Val Acc=0.1440\n",
            "Epoch 16: Train Loss=3.6602, Val Loss=3.7149, Train Acc=0.1630, Val Acc=0.1514\n",
            "Epoch 17: Train Loss=3.6386, Val Loss=3.6991, Train Acc=0.1664, Val Acc=0.1522\n",
            "Epoch 18: Train Loss=3.6199, Val Loss=3.6843, Train Acc=0.1716, Val Acc=0.1578\n",
            "Epoch 19: Train Loss=3.6015, Val Loss=3.6711, Train Acc=0.1740, Val Acc=0.1596\n",
            "Epoch 20: Train Loss=3.5843, Val Loss=3.6571, Train Acc=0.1788, Val Acc=0.1582\n",
            "Epoch 21: Train Loss=3.5659, Val Loss=3.6409, Train Acc=0.1801, Val Acc=0.1652\n",
            "Epoch 22: Train Loss=3.5508, Val Loss=3.6301, Train Acc=0.1850, Val Acc=0.1654\n",
            "Epoch 23: Train Loss=3.5345, Val Loss=3.6242, Train Acc=0.1833, Val Acc=0.1642\n",
            "Epoch 24: Train Loss=3.5203, Val Loss=3.6106, Train Acc=0.1864, Val Acc=0.1664\n",
            "Epoch 25: Train Loss=3.5055, Val Loss=3.5998, Train Acc=0.1895, Val Acc=0.1632\n",
            "Epoch 26: Train Loss=3.4922, Val Loss=3.5874, Train Acc=0.1919, Val Acc=0.1666\n",
            "Epoch 27: Train Loss=3.4788, Val Loss=3.5776, Train Acc=0.1929, Val Acc=0.1714\n",
            "Epoch 28: Train Loss=3.4660, Val Loss=3.5641, Train Acc=0.1999, Val Acc=0.1736\n",
            "Epoch 29: Train Loss=3.4526, Val Loss=3.5587, Train Acc=0.1996, Val Acc=0.1764\n",
            "Epoch 30: Train Loss=3.4392, Val Loss=3.5424, Train Acc=0.2015, Val Acc=0.1752\n",
            "Epoch 31: Train Loss=3.4283, Val Loss=3.5345, Train Acc=0.2041, Val Acc=0.1786\n",
            "Epoch 32: Train Loss=3.4163, Val Loss=3.5337, Train Acc=0.2031, Val Acc=0.1770\n",
            "Epoch 33: Train Loss=3.4041, Val Loss=3.5204, Train Acc=0.2104, Val Acc=0.1818\n",
            "Epoch 34: Train Loss=3.3934, Val Loss=3.5158, Train Acc=0.2085, Val Acc=0.1796\n",
            "Epoch 35: Train Loss=3.3810, Val Loss=3.5095, Train Acc=0.2106, Val Acc=0.1834\n",
            "Epoch 36: Train Loss=3.3709, Val Loss=3.5054, Train Acc=0.2118, Val Acc=0.1798\n",
            "Epoch 37: Train Loss=3.3593, Val Loss=3.4923, Train Acc=0.2157, Val Acc=0.1832\n",
            "Epoch 38: Train Loss=3.3500, Val Loss=3.4805, Train Acc=0.2168, Val Acc=0.1850\n",
            "Epoch 39: Train Loss=3.3389, Val Loss=3.4689, Train Acc=0.2184, Val Acc=0.1886\n",
            "Epoch 40: Train Loss=3.3296, Val Loss=3.4664, Train Acc=0.2227, Val Acc=0.1898\n",
            "Epoch 41: Train Loss=3.3194, Val Loss=3.4646, Train Acc=0.2218, Val Acc=0.1914\n",
            "Epoch 42: Train Loss=3.3078, Val Loss=3.4531, Train Acc=0.2240, Val Acc=0.1940\n",
            "Epoch 43: Train Loss=3.2993, Val Loss=3.4444, Train Acc=0.2248, Val Acc=0.1950\n",
            "Epoch 44: Train Loss=3.2900, Val Loss=3.4439, Train Acc=0.2306, Val Acc=0.1958\n",
            "Epoch 45: Train Loss=3.2809, Val Loss=3.4301, Train Acc=0.2315, Val Acc=0.1938\n",
            "Epoch 46: Train Loss=3.2700, Val Loss=3.4328, Train Acc=0.2280, Val Acc=0.1998\n",
            "Epoch 47: Train Loss=3.2621, Val Loss=3.4280, Train Acc=0.2330, Val Acc=0.1892\n",
            "Epoch 48: Train Loss=3.2524, Val Loss=3.4182, Train Acc=0.2363, Val Acc=0.1948\n",
            "Epoch 49: Train Loss=3.2430, Val Loss=3.4064, Train Acc=0.2361, Val Acc=0.2012\n",
            "Epoch 50: Train Loss=3.2351, Val Loss=3.4110, Train Acc=0.2374, Val Acc=0.2004\n",
            "\n",
            "최종 학습 정확도: 0.2337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "model = SimpleFeatureMLP(input_size=16*16*3, hidden_sizes=[512, 256], output_size=100)\n",
        "\n",
        "# 저장된 모델 불러오기\n",
        "model_save_path = '/content/optimized_simple_feature_mlp_checkpoint.npz'\n",
        "\n",
        "if os.path.exists(model_save_path):\n",
        "    print(\"저장된 모델을 불러옵니다...\")\n",
        "    model.load(model_save_path)\n",
        "else:\n",
        "    print(\"모델을 찾을 수 없습니다. 새로 학습을 시작합니다.\")\n",
        "\n",
        "# 추가 학습 설정\n",
        "epochs = 50  # 추가로 50 에폭\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "\n",
        "# 학습 시작\n",
        "model.train(X_train, y_train, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, save_path=model_save_path)\n",
        "\n",
        "# 최종 정확도 출력\n",
        "final_accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"\\n추가 학습 후 최종 학습 정확도: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUytAeVZXSyt",
        "outputId": "e42c63ba-a230-4053-b85b-3ff1ef736357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "저장된 모델을 불러옵니다...\n",
            "Epoch 1: Train Loss=3.2258, Val Loss=3.4038, Train Acc=0.2417, Val Acc=0.2030\n",
            "Epoch 2: Train Loss=3.2176, Val Loss=3.3965, Train Acc=0.2402, Val Acc=0.1992\n",
            "Epoch 3: Train Loss=3.2090, Val Loss=3.3888, Train Acc=0.2473, Val Acc=0.2018\n",
            "Epoch 4: Train Loss=3.1998, Val Loss=3.3791, Train Acc=0.2457, Val Acc=0.2016\n",
            "Epoch 5: Train Loss=3.1913, Val Loss=3.3744, Train Acc=0.2461, Val Acc=0.2002\n",
            "Epoch 6: Train Loss=3.1827, Val Loss=3.3803, Train Acc=0.2483, Val Acc=0.2022\n",
            "Epoch 7: Train Loss=3.1748, Val Loss=3.3738, Train Acc=0.2489, Val Acc=0.2014\n",
            "Epoch 8: Train Loss=3.1665, Val Loss=3.3587, Train Acc=0.2492, Val Acc=0.2094\n",
            "Epoch 9: Train Loss=3.1574, Val Loss=3.3601, Train Acc=0.2521, Val Acc=0.2046\n",
            "Epoch 10: Train Loss=3.1496, Val Loss=3.3581, Train Acc=0.2529, Val Acc=0.2064\n",
            "Epoch 11: Train Loss=3.1422, Val Loss=3.3469, Train Acc=0.2572, Val Acc=0.2086\n",
            "Epoch 12: Train Loss=3.1348, Val Loss=3.3452, Train Acc=0.2599, Val Acc=0.2076\n",
            "Epoch 13: Train Loss=3.1263, Val Loss=3.3411, Train Acc=0.2597, Val Acc=0.2098\n",
            "Epoch 14: Train Loss=3.1190, Val Loss=3.3393, Train Acc=0.2589, Val Acc=0.2150\n",
            "Epoch 15: Train Loss=3.1106, Val Loss=3.3298, Train Acc=0.2612, Val Acc=0.2118\n",
            "Epoch 16: Train Loss=3.1023, Val Loss=3.3268, Train Acc=0.2617, Val Acc=0.2148\n",
            "Epoch 17: Train Loss=3.0950, Val Loss=3.3209, Train Acc=0.2626, Val Acc=0.2092\n",
            "Epoch 18: Train Loss=3.0873, Val Loss=3.3142, Train Acc=0.2653, Val Acc=0.2104\n",
            "Epoch 19: Train Loss=3.0808, Val Loss=3.3127, Train Acc=0.2682, Val Acc=0.2172\n",
            "Epoch 20: Train Loss=3.0729, Val Loss=3.3093, Train Acc=0.2675, Val Acc=0.2194\n",
            "Epoch 21: Train Loss=3.0649, Val Loss=3.3039, Train Acc=0.2695, Val Acc=0.2174\n",
            "Epoch 22: Train Loss=3.0574, Val Loss=3.3044, Train Acc=0.2716, Val Acc=0.2152\n",
            "Epoch 23: Train Loss=3.0490, Val Loss=3.3061, Train Acc=0.2686, Val Acc=0.2164\n",
            "Epoch 24: Train Loss=3.0432, Val Loss=3.2903, Train Acc=0.2744, Val Acc=0.2202\n",
            "Epoch 25: Train Loss=3.0356, Val Loss=3.2902, Train Acc=0.2758, Val Acc=0.2192\n",
            "Epoch 26: Train Loss=3.0285, Val Loss=3.2967, Train Acc=0.2743, Val Acc=0.2186\n",
            "Epoch 27: Train Loss=3.0210, Val Loss=3.2804, Train Acc=0.2793, Val Acc=0.2218\n",
            "Epoch 28: Train Loss=3.0146, Val Loss=3.2802, Train Acc=0.2788, Val Acc=0.2226\n",
            "Epoch 29: Train Loss=3.0074, Val Loss=3.2803, Train Acc=0.2786, Val Acc=0.2186\n",
            "Epoch 30: Train Loss=2.9992, Val Loss=3.2774, Train Acc=0.2808, Val Acc=0.2210\n",
            "Epoch 31: Train Loss=2.9938, Val Loss=3.2758, Train Acc=0.2813, Val Acc=0.2260\n",
            "Epoch 32: Train Loss=2.9864, Val Loss=3.2727, Train Acc=0.2839, Val Acc=0.2186\n",
            "Epoch 33: Train Loss=2.9802, Val Loss=3.2707, Train Acc=0.2844, Val Acc=0.2214\n",
            "Epoch 34: Train Loss=2.9732, Val Loss=3.2664, Train Acc=0.2864, Val Acc=0.2276\n",
            "Epoch 35: Train Loss=2.9657, Val Loss=3.2681, Train Acc=0.2862, Val Acc=0.2230\n",
            "Epoch 36: Train Loss=2.9585, Val Loss=3.2631, Train Acc=0.2904, Val Acc=0.2254\n",
            "Epoch 37: Train Loss=2.9524, Val Loss=3.2498, Train Acc=0.2920, Val Acc=0.2292\n",
            "Epoch 38: Train Loss=2.9445, Val Loss=3.2584, Train Acc=0.2914, Val Acc=0.2284\n",
            "Epoch 39: Train Loss=2.9390, Val Loss=3.2604, Train Acc=0.2916, Val Acc=0.2228\n",
            "Epoch 40: Train Loss=2.9319, Val Loss=3.2528, Train Acc=0.2941, Val Acc=0.2240\n",
            "Epoch 41: Train Loss=2.9259, Val Loss=3.2461, Train Acc=0.2936, Val Acc=0.2264\n",
            "Epoch 42: Train Loss=2.9199, Val Loss=3.2405, Train Acc=0.2990, Val Acc=0.2312\n",
            "Epoch 43: Train Loss=2.9131, Val Loss=3.2404, Train Acc=0.2972, Val Acc=0.2248\n",
            "Epoch 44: Train Loss=2.9057, Val Loss=3.2319, Train Acc=0.3004, Val Acc=0.2290\n",
            "Epoch 45: Train Loss=2.8996, Val Loss=3.2419, Train Acc=0.3001, Val Acc=0.2252\n",
            "Epoch 46: Train Loss=2.8938, Val Loss=3.2331, Train Acc=0.3012, Val Acc=0.2266\n",
            "Epoch 47: Train Loss=2.8867, Val Loss=3.2371, Train Acc=0.3052, Val Acc=0.2322\n",
            "Epoch 48: Train Loss=2.8804, Val Loss=3.2301, Train Acc=0.3078, Val Acc=0.2220\n",
            "Epoch 49: Train Loss=2.8748, Val Loss=3.2350, Train Acc=0.3035, Val Acc=0.2266\n",
            "Epoch 50: Train Loss=2.8680, Val Loss=3.2237, Train Acc=0.3082, Val Acc=0.2302\n",
            "\n",
            "추가 학습 후 최종 학습 정확도: 0.3004\n"
          ]
        }
      ]
    }
  ]
}
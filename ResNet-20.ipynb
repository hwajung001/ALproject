{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32ff9d27-5f46-4c62-abb1-c567de5e1a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드 및 전처리\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "def download_cifar100(dest=\"./cifar-100-python\"):\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    \n",
    "    def is_within_directory(directory, target):\n",
    "        abs_directory = os.path.abspath(directory)\n",
    "        abs_target = os.path.abspath(target)\n",
    "        return os.path.commonprefix([abs_directory, abs_target]) == abs_directory\n",
    "\n",
    "    def safe_extract(tar, path=\".\", members=None):\n",
    "        for member in tar.getmembers():\n",
    "            member_path = os.path.join(path, member.name)\n",
    "            if not is_within_directory(path, member_path):\n",
    "                raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "        tar.extractall(path, members)\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest, exist_ok=True)\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        with tarfile.open(filename, \"r:gz\") as tar:\n",
    "            safe_extract(tar, path=\"./\")\n",
    "        print(\"CIFAR-100 downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "\n",
    "def load_cifar100(data_dir=\"./cifar-100-python\"):\n",
    "    def load_batch(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            dict = pickle.load(f, encoding='bytes')\n",
    "            data = dict[b'data']\n",
    "            labels = dict[b'fine_labels']\n",
    "            coarse_labels = dict[b'coarse_labels']\n",
    "            return data, labels, coarse_labels\n",
    "\n",
    "    x_train, y_train, y_train_coarse = load_batch(os.path.join(data_dir, \"train\"))\n",
    "    x_test, y_test, y_test_coarse = load_batch(os.path.join(data_dir, \"test\"))\n",
    "\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "    y_train_coarse = np.array(y_train_coarse)\n",
    "    y_test_coarse = np.array(y_test_coarse)\n",
    "\n",
    "    val_size = int(0.1 * len(x_train))\n",
    "    x_val = x_train[:val_size]\n",
    "    y_val = y_train[:val_size]\n",
    "    x_train = x_train[val_size:]\n",
    "    y_train = y_train[val_size:]\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test), (y_train_coarse, y_test_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d48f5f55-972e-4f9c-8ae2-546cc9c09edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 already downloaded.\n",
      " CIFAR-100 Dataset Loaded!\n",
      "Train X shape: (45000, 3, 32, 32), Train Y shape: (45000,)\n",
      "Val   X shape: (5000, 3, 32, 32), Val   Y shape: (5000,)\n",
      "Test  X shape: (10000, 3, 32, 32), Test  Y shape: (10000,)\n",
      "Coarse Labels - Train: (50000,), Test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 다운로드 및 로딩\n",
    "download_cifar100()\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test), (y_train_coarse, y_test_coarse) = load_cifar100()\n",
    "\n",
    "# 데이터셋 정보 출력\n",
    "print(\" CIFAR-100 Dataset Loaded!\")\n",
    "print(f\"Train X shape: {x_train.shape}, Train Y shape: {y_train.shape}\")\n",
    "print(f\"Val   X shape: {x_val.shape}, Val   Y shape: {y_val.shape}\")\n",
    "print(f\"Test  X shape: {x_test.shape}, Test  Y shape: {y_test.shape}\")\n",
    "print(f\"Coarse Labels - Train: {y_train_coarse.shape}, Test: {y_test_coarse.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2782b88-aaa3-4f5e-8a38-c0b506a53da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.layers import Convolution, Affine, Relu, BatchNormalization\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.util import im2col, col2im\n",
    "\n",
    "class ResidualBlock:\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        self.stride = stride\n",
    "        self.equal_in_out = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        self.conv1 = Convolution(\n",
    "            W=np.random.randn(out_channels, in_channels, 3, 3) * np.sqrt(2. / in_channels),\n",
    "            b=np.zeros(out_channels),\n",
    "            stride=stride,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn1 = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.conv2 = Convolution(\n",
    "            W=np.random.randn(out_channels, out_channels, 3, 3) * np.sqrt(2. / out_channels),\n",
    "            b=np.zeros(out_channels),\n",
    "            stride=1,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn2 = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "        self.relu2 = Relu()\n",
    "\n",
    "        if not self.equal_in_out:\n",
    "            self.shortcut = Convolution(\n",
    "                W=np.random.randn(out_channels, in_channels, 1, 1) * np.sqrt(2. / in_channels),\n",
    "                b=np.zeros(out_channels),\n",
    "                stride=stride,\n",
    "                pad=0\n",
    "            )\n",
    "            self.bn_shortcut = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.x = x\n",
    "\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out, train_flg)\n",
    "        out = self.relu1.forward(out)\n",
    "\n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out, train_flg)\n",
    "        self.out_main = out\n",
    "\n",
    "        if self.equal_in_out:\n",
    "            shortcut = x\n",
    "        else:\n",
    "            shortcut = self.shortcut.forward(x)\n",
    "            shortcut = self.bn_shortcut.forward(shortcut, train_flg)\n",
    "        self.out_shortcut = shortcut\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu2.forward(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.relu2.backward(dout)\n",
    "\n",
    "        dshortcut = dout.copy()\n",
    "        dmain = dout.copy()\n",
    "\n",
    "        dmain = self.bn2.backward(dmain)\n",
    "        dmain = self.conv2.backward(dmain)\n",
    "\n",
    "        dmain = self.relu1.backward(dmain)\n",
    "        dmain = self.bn1.backward(dmain)\n",
    "        dmain = self.conv1.backward(dmain)\n",
    "\n",
    "        if not self.equal_in_out:\n",
    "            dshortcut = self.bn_shortcut.backward(dshortcut)\n",
    "            dshortcut = self.shortcut.backward(dshortcut)\n",
    "\n",
    "        dx = dmain + dshortcut\n",
    "        return dx\n",
    "\n",
    "\n",
    "class ResNet20:\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_classes=100):\n",
    "        self.params = []\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        self.conv1 = Convolution(\n",
    "            W=np.random.randn(16, 3, 3, 3) * np.sqrt(2. / 3),\n",
    "            b=np.zeros(16),\n",
    "            stride=1,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn1 = BatchNormalization(gamma=np.ones(16), beta=np.zeros(16))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.layer1 = [ResidualBlock(16, 16, stride=1) for _ in range(3)]\n",
    "        self.layer2 = [ResidualBlock(16 if i == 0 else 32, 32, stride=2 if i == 0 else 1) for i in range(3)]\n",
    "        self.layer3 = [ResidualBlock(32 if i == 0 else 64, 64, stride=2 if i == 0 else 1) for i in range(3)]\n",
    "\n",
    "        self.fc = Affine(W=np.random.randn(64, num_classes) * np.sqrt(2. / 64), b=np.zeros(num_classes))\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input = x\n",
    "\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out, train_flg)\n",
    "        out = self.relu1.forward(out)\n",
    "\n",
    "        for block in self.layer1:\n",
    "            out = block.forward(out, train_flg)\n",
    "        for block in self.layer2:\n",
    "            out = block.forward(out, train_flg)\n",
    "        for block in self.layer3:\n",
    "            out = block.forward(out, train_flg)\n",
    "\n",
    "        self.feature_map = out\n",
    "\n",
    "        N, C, H, W = out.shape\n",
    "        out = out.mean(axis=(2, 3))\n",
    "\n",
    "        self.pooled = out\n",
    "        out = self.fc.forward(out)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x, batch_size=100):\n",
    "        y_list = []\n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            y_batch = self.forward(x_batch, train_flg=False)\n",
    "            y_list.append(y_batch)\n",
    "        return np.concatenate(y_list, axis=0)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.forward(x, train_flg=True)\n",
    "        return cross_entropy_error(softmax(y), t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        acc = 0.0\n",
    "        total = x.shape[0]\n",
    "        for i in range(0, total, batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "\n",
    "            y = self.predict(x_batch)\n",
    "            y = np.argmax(y, axis=1)\n",
    "\n",
    "            if t.ndim != 1:\n",
    "                t_batch = np.argmax(t_batch, axis=1)\n",
    "\n",
    "            acc += np.sum(y == t_batch)\n",
    "\n",
    "        return acc / total\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.fc.backward(dout)\n",
    "        dout = dout.reshape(self.feature_map.shape[0], self.feature_map.shape[1], 1, 1)\n",
    "        dout = dout.repeat(self.feature_map.shape[2], axis=2).repeat(self.feature_map.shape[3], axis=3)\n",
    "\n",
    "        for block in reversed(self.layer3):\n",
    "            dout = block.backward(dout)\n",
    "        for block in reversed(self.layer2):\n",
    "            dout = block.backward(dout)\n",
    "        for block in reversed(self.layer1):\n",
    "            dout = block.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        return dout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "178467ea-96d0-4b0a-bb56-9910beebaac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from common.optimizer import SGD, Adam\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_data, test_data, epochs=20, batch_size=64, optimizer_name='sgd', lr=0.01):\n",
    "        self.model = model\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "        self.max_iter = self.epochs * self.iter_per_epoch\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        # prepare optimizer (no param in constructor)\n",
    "        if optimizer_name == 'sgd':\n",
    "            self.optimizer = SGD(lr=lr)\n",
    "        elif optimizer_name == 'adam':\n",
    "            self.optimizer = Adam(lr=lr)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "        if hasattr(self.model.fc, 'W'):\n",
    "            param_dict['fc_W'] = self.model.fc.W\n",
    "            param_dict['fc_b'] = self.model.fc.b\n",
    "            grad_dict['fc_W'] = self.model.fc.dW\n",
    "            grad_dict['fc_b'] = self.model.fc.db\n",
    "\n",
    "        idx = 0\n",
    "        for layer in self.model.layer1 + self.model.layer2 + self.model.layer3:\n",
    "            for attr in ['conv1', 'conv2', 'shortcut']:\n",
    "                if hasattr(layer, attr):\n",
    "                    conv = getattr(layer, attr)\n",
    "                    param_dict[f'{idx}_W'] = conv.W\n",
    "                    param_dict[f'{idx}_b'] = conv.b\n",
    "                    grad_dict[f'{idx}_W'] = conv.dW\n",
    "                    grad_dict[f'{idx}_b'] = conv.db\n",
    "                    idx += 1\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        self.model.backward(self.loss_grad(x_batch, t_batch))\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            return (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            return dx / batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"[Epoch {epoch + 1}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"  Iter {i:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            test_acc = self.model.accuracy(self.test_x, self.test_t)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            print(f\"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\\n\", flush=True)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename, loss=self.train_loss_list, train_acc=self.train_acc_list, test_acc=self.test_acc_list)\n",
    "\n",
    "    def save_model(self, filename='model_and_opt.pkl'):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "        optimizer_state = self.optimizer.__dict__\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump({'model': model_state, 'optimizer': optimizer_state}, f)\n",
    "\n",
    "    def load_model(self, filename='model_and_opt.pkl'):\n",
    "        with open(filename, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        for k in params:\n",
    "            params[k][...] = state['model'][k]\n",
    "\n",
    "        self.optimizer.__dict__.update(state['optimizer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c02d5b-cefd-4302-9923-1d2870f9c7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1]\n",
      "  Iter   0/703: Loss 5.6104\n",
      "  Iter  10/703: Loss 5.1501\n",
      "  Iter  20/703: Loss 4.7382\n",
      "  Iter  30/703: Loss 4.5650\n",
      "  Iter  40/703: Loss 4.5359\n",
      "  Iter  50/703: Loss 4.4273\n",
      "  Iter  60/703: Loss 4.5535\n",
      "  Iter  70/703: Loss 4.4536\n",
      "  Iter  80/703: Loss 4.4036\n",
      "  Iter  90/703: Loss 4.4491\n",
      "  Iter 100/703: Loss 4.3655\n",
      "  Iter 110/703: Loss 4.2940\n",
      "  Iter 120/703: Loss 4.1066\n",
      "  Iter 130/703: Loss 4.4890\n",
      "  Iter 140/703: Loss 4.0506\n",
      "  Iter 150/703: Loss 4.3009\n",
      "  Iter 160/703: Loss 4.2286\n",
      "  Iter 170/703: Loss 4.0879\n",
      "  Iter 180/703: Loss 4.0784\n",
      "  Iter 190/703: Loss 4.1223\n",
      "  Iter 200/703: Loss 4.2249\n",
      "  Iter 210/703: Loss 4.1447\n",
      "  Iter 220/703: Loss 3.9159\n",
      "  Iter 230/703: Loss 3.8521\n",
      "  Iter 240/703: Loss 4.0446\n",
      "  Iter 250/703: Loss 4.0452\n",
      "  Iter 260/703: Loss 3.9598\n",
      "  Iter 270/703: Loss 4.2314\n",
      "  Iter 280/703: Loss 3.9445\n",
      "  Iter 290/703: Loss 3.9848\n",
      "  Iter 300/703: Loss 4.0521\n",
      "  Iter 310/703: Loss 4.1247\n",
      "  Iter 320/703: Loss 4.2770\n",
      "  Iter 330/703: Loss 4.0088\n",
      "  Iter 340/703: Loss 4.0171\n",
      "  Iter 350/703: Loss 3.8564\n",
      "  Iter 360/703: Loss 3.9877\n",
      "  Iter 370/703: Loss 3.9600\n",
      "  Iter 380/703: Loss 3.6457\n",
      "  Iter 390/703: Loss 4.0531\n",
      "  Iter 400/703: Loss 3.7066\n",
      "  Iter 410/703: Loss 3.9005\n",
      "  Iter 420/703: Loss 3.7528\n",
      "  Iter 430/703: Loss 3.9585\n",
      "  Iter 440/703: Loss 3.7384\n",
      "  Iter 450/703: Loss 3.7475\n",
      "  Iter 460/703: Loss 3.6475\n",
      "  Iter 470/703: Loss 3.5871\n",
      "  Iter 480/703: Loss 3.6683\n",
      "  Iter 490/703: Loss 4.0714\n",
      "  Iter 500/703: Loss 3.4846\n",
      "  Iter 510/703: Loss 3.5540\n",
      "  Iter 520/703: Loss 3.8444\n",
      "  Iter 530/703: Loss 3.6410\n",
      "  Iter 540/703: Loss 3.7531\n",
      "  Iter 550/703: Loss 3.7834\n",
      "  Iter 560/703: Loss 3.8770\n",
      "  Iter 570/703: Loss 3.6930\n",
      "  Iter 580/703: Loss 3.6507\n",
      "  Iter 590/703: Loss 3.4882\n",
      "  Iter 600/703: Loss 3.7113\n",
      "  Iter 610/703: Loss 3.4906\n",
      "  Iter 620/703: Loss 3.4448\n",
      "  Iter 630/703: Loss 3.6355\n",
      "  Iter 640/703: Loss 3.9016\n",
      "  Iter 650/703: Loss 3.5446\n",
      "  Iter 660/703: Loss 3.6578\n",
      "  Iter 670/703: Loss 3.6792\n",
      "  Iter 680/703: Loss 3.5931\n",
      "  Iter 690/703: Loss 3.3426\n",
      "  Iter 700/703: Loss 3.5346\n",
      "Train acc: 0.1570, Test acc: 0.1433\n",
      "\n",
      "[Epoch 2]\n",
      "  Iter   0/703: Loss 3.3882\n",
      "  Iter  10/703: Loss 3.5876\n",
      "  Iter  20/703: Loss 3.4866\n",
      "  Iter  30/703: Loss 3.5792\n",
      "  Iter  40/703: Loss 3.6127\n",
      "  Iter  50/703: Loss 3.5671\n",
      "  Iter  60/703: Loss 3.7090\n",
      "  Iter  70/703: Loss 3.3099\n",
      "  Iter  80/703: Loss 3.3427\n",
      "  Iter  90/703: Loss 3.7415\n",
      "  Iter 100/703: Loss 3.4330\n",
      "  Iter 110/703: Loss 3.6932\n",
      "  Iter 120/703: Loss 3.4138\n",
      "  Iter 130/703: Loss 3.4494\n",
      "  Iter 140/703: Loss 3.4137\n",
      "  Iter 150/703: Loss 3.3837\n",
      "  Iter 160/703: Loss 3.5401\n",
      "  Iter 170/703: Loss 3.5701\n",
      "  Iter 180/703: Loss 3.3128\n",
      "  Iter 190/703: Loss 3.6304\n",
      "  Iter 200/703: Loss 3.5668\n",
      "  Iter 210/703: Loss 3.5499\n",
      "  Iter 220/703: Loss 3.7397\n",
      "  Iter 230/703: Loss 3.6764\n",
      "  Iter 240/703: Loss 3.3951\n",
      "  Iter 250/703: Loss 3.1274\n",
      "  Iter 260/703: Loss 3.4173\n",
      "  Iter 270/703: Loss 3.5444\n",
      "  Iter 280/703: Loss 3.3755\n",
      "  Iter 290/703: Loss 3.4147\n",
      "  Iter 300/703: Loss 3.4824\n",
      "  Iter 310/703: Loss 3.1456\n",
      "  Iter 320/703: Loss 3.3766\n",
      "  Iter 330/703: Loss 3.3000\n",
      "  Iter 340/703: Loss 3.3904\n",
      "  Iter 350/703: Loss 3.1161\n",
      "  Iter 360/703: Loss 3.5841\n",
      "  Iter 370/703: Loss 3.2392\n",
      "  Iter 380/703: Loss 3.4727\n",
      "  Iter 390/703: Loss 3.2583\n",
      "  Iter 400/703: Loss 3.0067\n",
      "  Iter 410/703: Loss 3.0886\n",
      "  Iter 420/703: Loss 3.3895\n",
      "  Iter 430/703: Loss 3.4533\n",
      "  Iter 440/703: Loss 3.4171\n",
      "  Iter 450/703: Loss 3.0691\n",
      "  Iter 460/703: Loss 3.0623\n",
      "  Iter 470/703: Loss 3.2648\n",
      "  Iter 480/703: Loss 3.5392\n",
      "  Iter 490/703: Loss 3.2058\n",
      "  Iter 500/703: Loss 3.3807\n",
      "  Iter 510/703: Loss 3.0527\n",
      "  Iter 520/703: Loss 3.3070\n",
      "  Iter 530/703: Loss 3.1077\n",
      "  Iter 540/703: Loss 3.2359\n",
      "  Iter 550/703: Loss 3.1212\n",
      "  Iter 560/703: Loss 2.9972\n",
      "  Iter 570/703: Loss 3.2447\n",
      "  Iter 580/703: Loss 3.4731\n",
      "  Iter 590/703: Loss 3.6264\n",
      "  Iter 600/703: Loss 3.1731\n",
      "  Iter 610/703: Loss 3.3335\n",
      "  Iter 620/703: Loss 2.7994\n",
      "  Iter 630/703: Loss 2.8927\n",
      "  Iter 640/703: Loss 3.0843\n",
      "  Iter 650/703: Loss 2.9954\n",
      "  Iter 660/703: Loss 3.2751\n",
      "  Iter 670/703: Loss 3.2461\n",
      "  Iter 680/703: Loss 3.2215\n",
      "  Iter 690/703: Loss 2.7045\n",
      "  Iter 700/703: Loss 2.8852\n",
      "Train acc: 0.2370, Test acc: 0.2087\n",
      "\n",
      "[Epoch 3]\n",
      "  Iter   0/703: Loss 3.1196\n",
      "  Iter  10/703: Loss 3.0867\n",
      "  Iter  20/703: Loss 3.2768\n",
      "  Iter  30/703: Loss 3.2389\n",
      "  Iter  40/703: Loss 3.4492\n",
      "  Iter  50/703: Loss 2.7707\n",
      "  Iter  60/703: Loss 3.3173\n",
      "  Iter  70/703: Loss 3.0308\n",
      "  Iter  80/703: Loss 2.9428\n",
      "  Iter  90/703: Loss 3.2365\n",
      "  Iter 100/703: Loss 3.1767\n",
      "  Iter 110/703: Loss 2.8252\n",
      "  Iter 120/703: Loss 2.9813\n",
      "  Iter 130/703: Loss 3.1091\n",
      "  Iter 140/703: Loss 3.3766\n",
      "  Iter 150/703: Loss 3.1164\n",
      "  Iter 160/703: Loss 2.9294\n",
      "  Iter 170/703: Loss 3.0856\n",
      "  Iter 180/703: Loss 3.2065\n",
      "  Iter 190/703: Loss 3.3445\n",
      "  Iter 200/703: Loss 3.0504\n",
      "  Iter 210/703: Loss 2.9460\n",
      "  Iter 220/703: Loss 3.2252\n",
      "  Iter 230/703: Loss 3.2316\n",
      "  Iter 240/703: Loss 2.8851\n",
      "  Iter 250/703: Loss 3.1201\n",
      "  Iter 260/703: Loss 3.0215\n",
      "  Iter 270/703: Loss 2.9497\n",
      "  Iter 280/703: Loss 3.1840\n",
      "  Iter 290/703: Loss 3.0760\n",
      "  Iter 300/703: Loss 3.0985\n",
      "  Iter 310/703: Loss 3.0235\n",
      "  Iter 320/703: Loss 3.2330\n",
      "  Iter 330/703: Loss 2.9797\n",
      "  Iter 340/703: Loss 3.4289\n",
      "  Iter 350/703: Loss 3.2053\n",
      "  Iter 360/703: Loss 3.1262\n",
      "  Iter 370/703: Loss 3.0185\n",
      "  Iter 380/703: Loss 3.0562\n",
      "  Iter 390/703: Loss 3.1763\n",
      "  Iter 400/703: Loss 3.1297\n",
      "  Iter 410/703: Loss 3.2019\n",
      "  Iter 420/703: Loss 3.0099\n",
      "  Iter 430/703: Loss 2.7936\n",
      "  Iter 440/703: Loss 2.9948\n",
      "  Iter 450/703: Loss 3.2921\n",
      "  Iter 460/703: Loss 3.0268\n",
      "  Iter 470/703: Loss 3.0009\n",
      "  Iter 480/703: Loss 3.1490\n",
      "  Iter 490/703: Loss 3.0877\n",
      "  Iter 500/703: Loss 3.0577\n",
      "  Iter 510/703: Loss 3.1862\n",
      "  Iter 520/703: Loss 3.1253\n",
      "  Iter 530/703: Loss 2.7452\n",
      "  Iter 540/703: Loss 3.0599\n",
      "  Iter 550/703: Loss 3.1249\n",
      "  Iter 560/703: Loss 2.7980\n",
      "  Iter 570/703: Loss 2.8657\n",
      "  Iter 580/703: Loss 2.8809\n",
      "  Iter 590/703: Loss 3.0499\n",
      "  Iter 600/703: Loss 2.6917\n",
      "  Iter 610/703: Loss 2.8761\n",
      "  Iter 620/703: Loss 2.9959\n",
      "  Iter 630/703: Loss 2.8797\n",
      "  Iter 640/703: Loss 2.9024\n",
      "  Iter 650/703: Loss 2.9713\n",
      "  Iter 660/703: Loss 2.8022\n",
      "  Iter 670/703: Loss 2.8259\n",
      "  Iter 680/703: Loss 2.6447\n",
      "  Iter 690/703: Loss 2.8243\n",
      "  Iter 700/703: Loss 2.7817\n",
      "Train acc: 0.2730, Test acc: 0.2394\n",
      "\n",
      "[Epoch 4]\n",
      "  Iter   0/703: Loss 2.8660\n",
      "  Iter  10/703: Loss 2.8235\n",
      "  Iter  20/703: Loss 3.1457\n",
      "  Iter  30/703: Loss 2.3392\n",
      "  Iter  40/703: Loss 2.7921\n",
      "  Iter  50/703: Loss 2.9477\n",
      "  Iter  60/703: Loss 3.0127\n",
      "  Iter  70/703: Loss 2.8605\n",
      "  Iter  80/703: Loss 3.0286\n",
      "  Iter  90/703: Loss 3.0891\n",
      "  Iter 100/703: Loss 2.8601\n",
      "  Iter 110/703: Loss 2.6007\n",
      "  Iter 120/703: Loss 2.7793\n",
      "  Iter 130/703: Loss 2.7521\n",
      "  Iter 140/703: Loss 3.2261\n",
      "  Iter 150/703: Loss 2.6601\n",
      "  Iter 160/703: Loss 2.3736\n",
      "  Iter 170/703: Loss 2.6698\n",
      "  Iter 180/703: Loss 2.8399\n",
      "  Iter 190/703: Loss 2.6722\n",
      "  Iter 200/703: Loss 2.7069\n",
      "  Iter 210/703: Loss 2.9081\n",
      "  Iter 220/703: Loss 2.7858\n",
      "  Iter 230/703: Loss 2.8294\n",
      "  Iter 240/703: Loss 2.6201\n",
      "  Iter 250/703: Loss 2.6444\n",
      "  Iter 260/703: Loss 2.7914\n",
      "  Iter 270/703: Loss 2.6929\n",
      "  Iter 280/703: Loss 2.5872\n",
      "  Iter 290/703: Loss 2.8615\n",
      "  Iter 300/703: Loss 2.8059\n",
      "  Iter 310/703: Loss 2.4213\n",
      "  Iter 320/703: Loss 2.9149\n",
      "  Iter 330/703: Loss 2.8555\n",
      "  Iter 340/703: Loss 2.6738\n",
      "  Iter 350/703: Loss 2.8689\n",
      "  Iter 360/703: Loss 2.6964\n",
      "  Iter 370/703: Loss 2.6865\n",
      "  Iter 380/703: Loss 2.8706\n",
      "  Iter 390/703: Loss 2.5383\n",
      "  Iter 400/703: Loss 3.0299\n",
      "  Iter 410/703: Loss 2.9541\n",
      "  Iter 420/703: Loss 2.5149\n",
      "  Iter 430/703: Loss 2.5659\n",
      "  Iter 440/703: Loss 3.0854\n",
      "  Iter 450/703: Loss 2.6955\n",
      "  Iter 460/703: Loss 2.5719\n",
      "  Iter 470/703: Loss 3.3570\n",
      "  Iter 480/703: Loss 2.8876\n",
      "  Iter 490/703: Loss 2.6411\n",
      "  Iter 500/703: Loss 2.8750\n",
      "  Iter 510/703: Loss 2.4496\n",
      "  Iter 520/703: Loss 2.7440\n",
      "  Iter 530/703: Loss 2.6708\n",
      "  Iter 540/703: Loss 2.6487\n",
      "  Iter 550/703: Loss 2.9952\n",
      "  Iter 560/703: Loss 2.6089\n",
      "  Iter 570/703: Loss 2.7072\n",
      "  Iter 580/703: Loss 2.5276\n",
      "  Iter 590/703: Loss 2.6088\n",
      "  Iter 600/703: Loss 2.7676\n",
      "  Iter 610/703: Loss 2.4521\n",
      "  Iter 620/703: Loss 2.6710\n",
      "  Iter 630/703: Loss 2.6045\n",
      "  Iter 640/703: Loss 2.8209\n",
      "  Iter 650/703: Loss 2.8513\n",
      "  Iter 660/703: Loss 2.6914\n",
      "  Iter 670/703: Loss 2.8193\n",
      "  Iter 680/703: Loss 2.6754\n",
      "  Iter 690/703: Loss 2.8370\n",
      "  Iter 700/703: Loss 3.1328\n",
      "Train acc: 0.3250, Test acc: 0.2798\n",
      "\n",
      "[Epoch 5]\n",
      "  Iter   0/703: Loss 2.9721\n",
      "  Iter  10/703: Loss 2.6933\n",
      "  Iter  20/703: Loss 3.0653\n",
      "  Iter  30/703: Loss 2.9807\n",
      "  Iter  40/703: Loss 2.8331\n",
      "  Iter  50/703: Loss 2.6304\n",
      "  Iter  60/703: Loss 2.6267\n",
      "  Iter  70/703: Loss 2.6422\n",
      "  Iter  80/703: Loss 2.8827\n",
      "  Iter  90/703: Loss 2.8180\n",
      "  Iter 100/703: Loss 2.9301\n",
      "  Iter 110/703: Loss 2.5932\n",
      "  Iter 120/703: Loss 2.4814\n",
      "  Iter 130/703: Loss 2.8458\n",
      "  Iter 140/703: Loss 2.8191\n",
      "  Iter 150/703: Loss 2.7037\n",
      "  Iter 160/703: Loss 2.5625\n",
      "  Iter 170/703: Loss 2.7292\n",
      "  Iter 180/703: Loss 2.6314\n",
      "  Iter 190/703: Loss 2.7198\n",
      "  Iter 200/703: Loss 2.9736\n",
      "  Iter 210/703: Loss 2.7315\n",
      "  Iter 220/703: Loss 2.3768\n",
      "  Iter 230/703: Loss 2.7361\n",
      "  Iter 240/703: Loss 2.4349\n",
      "  Iter 250/703: Loss 2.9347\n",
      "  Iter 260/703: Loss 2.4373\n",
      "  Iter 270/703: Loss 2.5745\n",
      "  Iter 280/703: Loss 2.7668\n",
      "  Iter 290/703: Loss 3.0520\n",
      "  Iter 300/703: Loss 2.4749\n",
      "  Iter 310/703: Loss 2.6175\n",
      "  Iter 320/703: Loss 2.6769\n",
      "  Iter 330/703: Loss 2.6479\n",
      "  Iter 340/703: Loss 2.7202\n",
      "  Iter 350/703: Loss 2.7824\n",
      "  Iter 360/703: Loss 2.5097\n",
      "  Iter 370/703: Loss 2.5576\n",
      "  Iter 380/703: Loss 2.4534\n",
      "  Iter 390/703: Loss 2.7211\n",
      "  Iter 400/703: Loss 2.2450\n",
      "  Iter 410/703: Loss 2.4642\n",
      "  Iter 420/703: Loss 2.8951\n",
      "  Iter 430/703: Loss 2.5995\n",
      "  Iter 440/703: Loss 2.5356\n",
      "  Iter 450/703: Loss 2.4407\n",
      "  Iter 460/703: Loss 2.7490\n",
      "  Iter 470/703: Loss 2.7443\n",
      "  Iter 480/703: Loss 2.2173\n",
      "  Iter 490/703: Loss 2.9970\n",
      "  Iter 500/703: Loss 2.2047\n",
      "  Iter 510/703: Loss 2.4423\n",
      "  Iter 520/703: Loss 2.6044\n",
      "  Iter 530/703: Loss 2.3889\n",
      "  Iter 540/703: Loss 2.3021\n",
      "  Iter 550/703: Loss 2.7142\n",
      "  Iter 560/703: Loss 2.5865\n",
      "  Iter 570/703: Loss 2.2068\n",
      "  Iter 580/703: Loss 2.4807\n",
      "  Iter 590/703: Loss 2.8125\n",
      "  Iter 600/703: Loss 2.5894\n",
      "  Iter 610/703: Loss 2.4257\n",
      "  Iter 620/703: Loss 2.5454\n",
      "  Iter 630/703: Loss 2.7556\n",
      "  Iter 640/703: Loss 2.6508\n",
      "  Iter 650/703: Loss 2.6476\n",
      "  Iter 660/703: Loss 2.7468\n",
      "  Iter 670/703: Loss 2.7714\n",
      "  Iter 680/703: Loss 2.5190\n",
      "  Iter 690/703: Loss 2.5291\n",
      "  Iter 700/703: Loss 2.3737\n",
      "Train acc: 0.3680, Test acc: 0.3028\n",
      "\n",
      "[Epoch 6]\n",
      "  Iter   0/703: Loss 2.4870\n",
      "  Iter  10/703: Loss 2.5301\n",
      "  Iter  20/703: Loss 2.7691\n",
      "  Iter  30/703: Loss 2.3427\n",
      "  Iter  40/703: Loss 2.6655\n",
      "  Iter  50/703: Loss 2.4645\n",
      "  Iter  60/703: Loss 2.8470\n",
      "  Iter  70/703: Loss 2.4370\n",
      "  Iter  80/703: Loss 2.6143\n",
      "  Iter  90/703: Loss 2.5798\n",
      "  Iter 100/703: Loss 2.3476\n",
      "  Iter 110/703: Loss 2.5054\n",
      "  Iter 120/703: Loss 2.7040\n",
      "  Iter 130/703: Loss 2.7356\n",
      "  Iter 140/703: Loss 2.8356\n",
      "  Iter 150/703: Loss 2.4920\n",
      "  Iter 160/703: Loss 2.6173\n",
      "  Iter 170/703: Loss 2.5614\n",
      "  Iter 180/703: Loss 2.4988\n",
      "  Iter 190/703: Loss 2.4826\n",
      "  Iter 200/703: Loss 2.4730\n",
      "  Iter 210/703: Loss 2.3187\n",
      "  Iter 220/703: Loss 2.4382\n",
      "  Iter 230/703: Loss 2.5603\n",
      "  Iter 240/703: Loss 2.2810\n",
      "  Iter 250/703: Loss 2.3732\n",
      "  Iter 260/703: Loss 2.6285\n",
      "  Iter 270/703: Loss 2.4205\n",
      "  Iter 280/703: Loss 2.4779\n",
      "  Iter 290/703: Loss 2.4961\n",
      "  Iter 300/703: Loss 2.6383\n",
      "  Iter 310/703: Loss 2.4241\n",
      "  Iter 320/703: Loss 2.3624\n",
      "  Iter 330/703: Loss 2.3839\n",
      "  Iter 340/703: Loss 2.2957\n",
      "  Iter 350/703: Loss 2.5661\n",
      "  Iter 360/703: Loss 2.3558\n",
      "  Iter 370/703: Loss 2.5897\n",
      "  Iter 380/703: Loss 2.4049\n",
      "  Iter 390/703: Loss 2.5699\n",
      "  Iter 400/703: Loss 2.6408\n",
      "  Iter 410/703: Loss 3.2719\n",
      "  Iter 420/703: Loss 2.4308\n",
      "  Iter 430/703: Loss 2.3597\n",
      "  Iter 440/703: Loss 2.4895\n",
      "  Iter 450/703: Loss 2.2104\n",
      "  Iter 460/703: Loss 2.2272\n",
      "  Iter 470/703: Loss 2.3477\n",
      "  Iter 480/703: Loss 2.5620\n",
      "  Iter 490/703: Loss 2.4858\n",
      "  Iter 500/703: Loss 2.3029\n",
      "  Iter 510/703: Loss 2.5214\n",
      "  Iter 520/703: Loss 2.6003\n",
      "  Iter 530/703: Loss 2.7364\n",
      "  Iter 540/703: Loss 2.1436\n",
      "  Iter 550/703: Loss 2.2923\n",
      "  Iter 560/703: Loss 2.2960\n",
      "  Iter 570/703: Loss 2.4392\n",
      "  Iter 580/703: Loss 2.7687\n",
      "  Iter 590/703: Loss 2.2485\n",
      "  Iter 600/703: Loss 2.0538\n",
      "  Iter 610/703: Loss 2.5828\n",
      "  Iter 620/703: Loss 2.3395\n",
      "  Iter 630/703: Loss 2.3911\n",
      "  Iter 640/703: Loss 2.5357\n",
      "  Iter 650/703: Loss 2.6406\n",
      "  Iter 660/703: Loss 2.4827\n",
      "  Iter 670/703: Loss 2.6222\n",
      "  Iter 680/703: Loss 2.3007\n",
      "  Iter 690/703: Loss 2.3268\n",
      "  Iter 700/703: Loss 2.5410\n",
      "Train acc: 0.3870, Test acc: 0.3064\n",
      "\n",
      "[Epoch 7]\n",
      "  Iter   0/703: Loss 2.7178\n",
      "  Iter  10/703: Loss 2.2320\n",
      "  Iter  20/703: Loss 2.3439\n",
      "  Iter  30/703: Loss 2.4985\n",
      "  Iter  40/703: Loss 2.2734\n",
      "  Iter  50/703: Loss 2.7344\n",
      "  Iter  60/703: Loss 2.6047\n",
      "  Iter  70/703: Loss 2.1658\n",
      "  Iter  80/703: Loss 2.3446\n",
      "  Iter  90/703: Loss 2.3606\n",
      "  Iter 100/703: Loss 2.4177\n",
      "  Iter 110/703: Loss 2.6795\n",
      "  Iter 120/703: Loss 2.1718\n",
      "  Iter 130/703: Loss 2.3498\n",
      "  Iter 140/703: Loss 2.4564\n",
      "  Iter 150/703: Loss 2.3018\n",
      "  Iter 160/703: Loss 2.4407\n",
      "  Iter 170/703: Loss 2.5144\n",
      "  Iter 180/703: Loss 2.3735\n",
      "  Iter 190/703: Loss 2.2812\n",
      "  Iter 200/703: Loss 2.1277\n",
      "  Iter 210/703: Loss 2.4214\n",
      "  Iter 220/703: Loss 2.3334\n",
      "  Iter 230/703: Loss 2.2765\n",
      "  Iter 240/703: Loss 2.3203\n",
      "  Iter 250/703: Loss 2.3490\n",
      "  Iter 260/703: Loss 1.9534\n",
      "  Iter 270/703: Loss 2.4210\n",
      "  Iter 280/703: Loss 2.2202\n",
      "  Iter 290/703: Loss 2.1910\n",
      "  Iter 300/703: Loss 2.1901\n",
      "  Iter 310/703: Loss 2.3385\n",
      "  Iter 320/703: Loss 2.4430\n",
      "  Iter 330/703: Loss 2.4863\n",
      "  Iter 340/703: Loss 2.0812\n",
      "  Iter 350/703: Loss 2.2292\n",
      "  Iter 360/703: Loss 2.2308\n",
      "  Iter 370/703: Loss 2.6745\n",
      "  Iter 380/703: Loss 2.2460\n",
      "  Iter 390/703: Loss 2.3980\n",
      "  Iter 400/703: Loss 2.3796\n",
      "  Iter 410/703: Loss 2.7488\n",
      "  Iter 420/703: Loss 2.0776\n",
      "  Iter 430/703: Loss 2.1349\n",
      "  Iter 440/703: Loss 2.3994\n",
      "  Iter 450/703: Loss 2.6495\n",
      "  Iter 460/703: Loss 2.3271\n",
      "  Iter 470/703: Loss 2.2178\n",
      "  Iter 480/703: Loss 2.1104\n",
      "  Iter 490/703: Loss 2.4885\n",
      "  Iter 500/703: Loss 2.1772\n",
      "  Iter 510/703: Loss 2.4373\n",
      "  Iter 520/703: Loss 2.3870\n",
      "  Iter 530/703: Loss 2.2384\n",
      "  Iter 540/703: Loss 2.1485\n",
      "  Iter 550/703: Loss 2.1913\n",
      "  Iter 560/703: Loss 2.2398\n",
      "  Iter 570/703: Loss 2.2607\n",
      "  Iter 580/703: Loss 2.2252\n",
      "  Iter 590/703: Loss 2.5119\n",
      "  Iter 600/703: Loss 2.4633\n",
      "  Iter 610/703: Loss 2.2831\n",
      "  Iter 620/703: Loss 2.3175\n",
      "  Iter 630/703: Loss 2.0699\n",
      "  Iter 640/703: Loss 2.3788\n",
      "  Iter 650/703: Loss 2.4058\n",
      "  Iter 660/703: Loss 2.0542\n",
      "  Iter 670/703: Loss 2.3992\n",
      "  Iter 680/703: Loss 2.1869\n",
      "  Iter 690/703: Loss 2.1516\n",
      "  Iter 700/703: Loss 2.3818\n",
      "Train acc: 0.4250, Test acc: 0.3316\n",
      "\n",
      "[Epoch 8]\n",
      "  Iter   0/703: Loss 2.2510\n",
      "  Iter  10/703: Loss 2.2114\n",
      "  Iter  20/703: Loss 2.4328\n",
      "  Iter  30/703: Loss 2.2852\n",
      "  Iter  40/703: Loss 2.5598\n",
      "  Iter  50/703: Loss 2.4306\n",
      "  Iter  60/703: Loss 2.3139\n",
      "  Iter  70/703: Loss 2.2550\n",
      "  Iter  80/703: Loss 2.3188\n",
      "  Iter  90/703: Loss 2.2436\n",
      "  Iter 100/703: Loss 2.4213\n",
      "  Iter 110/703: Loss 2.5457\n",
      "  Iter 120/703: Loss 1.9588\n",
      "  Iter 130/703: Loss 1.9493\n",
      "  Iter 140/703: Loss 2.1998\n",
      "  Iter 150/703: Loss 1.6954\n",
      "  Iter 160/703: Loss 2.1781\n",
      "  Iter 170/703: Loss 2.1258\n",
      "  Iter 180/703: Loss 2.2241\n",
      "  Iter 190/703: Loss 2.4257\n",
      "  Iter 200/703: Loss 2.0190\n",
      "  Iter 210/703: Loss 2.1300\n",
      "  Iter 220/703: Loss 1.9344\n",
      "  Iter 230/703: Loss 2.4105\n",
      "  Iter 240/703: Loss 1.7991\n",
      "  Iter 250/703: Loss 2.3666\n",
      "  Iter 260/703: Loss 2.4350\n",
      "  Iter 270/703: Loss 2.3591\n",
      "  Iter 280/703: Loss 2.5317\n",
      "  Iter 290/703: Loss 2.1540\n",
      "  Iter 300/703: Loss 2.3171\n",
      "  Iter 310/703: Loss 2.4333\n",
      "  Iter 320/703: Loss 3.0257\n",
      "  Iter 330/703: Loss 2.3279\n",
      "  Iter 340/703: Loss 2.0609\n",
      "  Iter 350/703: Loss 2.3329\n",
      "  Iter 360/703: Loss 2.5608\n",
      "  Iter 370/703: Loss 2.3659\n",
      "  Iter 380/703: Loss 2.0142\n",
      "  Iter 390/703: Loss 2.5944\n",
      "  Iter 400/703: Loss 2.0535\n",
      "  Iter 410/703: Loss 1.8733\n",
      "  Iter 420/703: Loss 2.4149\n",
      "  Iter 430/703: Loss 2.3896\n",
      "  Iter 440/703: Loss 2.1997\n",
      "  Iter 450/703: Loss 2.2874\n",
      "  Iter 460/703: Loss 2.2132\n",
      "  Iter 470/703: Loss 2.2251\n",
      "  Iter 480/703: Loss 2.2152\n",
      "  Iter 490/703: Loss 2.2829\n",
      "  Iter 500/703: Loss 2.5615\n",
      "  Iter 510/703: Loss 2.0268\n",
      "  Iter 520/703: Loss 2.3266\n",
      "  Iter 530/703: Loss 2.2431\n",
      "  Iter 540/703: Loss 2.1281\n",
      "  Iter 550/703: Loss 2.4351\n",
      "  Iter 560/703: Loss 2.0331\n",
      "  Iter 570/703: Loss 2.1545\n",
      "  Iter 580/703: Loss 2.7573\n",
      "  Iter 590/703: Loss 2.4796\n",
      "  Iter 600/703: Loss 2.3517\n",
      "  Iter 610/703: Loss 1.9520\n",
      "  Iter 620/703: Loss 1.8262\n",
      "  Iter 630/703: Loss 2.0949\n",
      "  Iter 640/703: Loss 2.3517\n",
      "  Iter 650/703: Loss 2.1304\n",
      "  Iter 660/703: Loss 2.3513\n",
      "  Iter 670/703: Loss 2.2553\n",
      "  Iter 680/703: Loss 2.0667\n",
      "  Iter 690/703: Loss 2.0613\n",
      "  Iter 700/703: Loss 2.3404\n",
      "Train acc: 0.4320, Test acc: 0.3363\n",
      "\n",
      "[Epoch 9]\n",
      "  Iter   0/703: Loss 2.0806\n",
      "  Iter  10/703: Loss 1.8653\n",
      "  Iter  20/703: Loss 2.6047\n",
      "  Iter  30/703: Loss 2.0367\n",
      "  Iter  40/703: Loss 2.0464\n",
      "  Iter  50/703: Loss 2.0627\n",
      "  Iter  60/703: Loss 1.9425\n",
      "  Iter  70/703: Loss 2.0627\n",
      "  Iter  80/703: Loss 2.2848\n",
      "  Iter  90/703: Loss 2.1580\n",
      "  Iter 100/703: Loss 1.8104\n",
      "  Iter 110/703: Loss 1.9513\n",
      "  Iter 120/703: Loss 1.5680\n",
      "  Iter 130/703: Loss 1.9396\n",
      "  Iter 140/703: Loss 2.0810\n",
      "  Iter 150/703: Loss 1.9659\n",
      "  Iter 160/703: Loss 2.2243\n",
      "  Iter 170/703: Loss 2.1009\n",
      "  Iter 180/703: Loss 2.2770\n",
      "  Iter 190/703: Loss 2.3323\n",
      "  Iter 200/703: Loss 2.4861\n",
      "  Iter 210/703: Loss 2.3103\n",
      "  Iter 220/703: Loss 1.8127\n",
      "  Iter 230/703: Loss 1.9861\n",
      "  Iter 240/703: Loss 2.5075\n",
      "  Iter 250/703: Loss 2.0140\n",
      "  Iter 260/703: Loss 2.1103\n",
      "  Iter 270/703: Loss 1.5817\n",
      "  Iter 280/703: Loss 2.0723\n",
      "  Iter 290/703: Loss 2.4148\n",
      "  Iter 300/703: Loss 1.8828\n",
      "  Iter 310/703: Loss 2.3239\n",
      "  Iter 320/703: Loss 2.1787\n",
      "  Iter 330/703: Loss 2.2121\n",
      "  Iter 340/703: Loss 1.8866\n",
      "  Iter 350/703: Loss 1.9863\n",
      "  Iter 360/703: Loss 2.3338\n",
      "  Iter 370/703: Loss 1.9523\n",
      "  Iter 380/703: Loss 2.2208\n",
      "  Iter 390/703: Loss 2.2008\n",
      "  Iter 400/703: Loss 2.4016\n",
      "  Iter 410/703: Loss 1.8028\n",
      "  Iter 420/703: Loss 2.0539\n",
      "  Iter 430/703: Loss 1.9534\n",
      "  Iter 440/703: Loss 1.7756\n",
      "  Iter 450/703: Loss 2.3334\n",
      "  Iter 460/703: Loss 2.2916\n",
      "  Iter 470/703: Loss 2.0609\n",
      "  Iter 480/703: Loss 2.3174\n",
      "  Iter 490/703: Loss 1.9559\n",
      "  Iter 500/703: Loss 1.9756\n",
      "  Iter 510/703: Loss 2.1765\n",
      "  Iter 520/703: Loss 1.9438\n",
      "  Iter 530/703: Loss 2.0068\n",
      "  Iter 540/703: Loss 2.2037\n",
      "  Iter 550/703: Loss 1.8999\n",
      "  Iter 560/703: Loss 2.0113\n",
      "  Iter 570/703: Loss 1.8718\n",
      "  Iter 580/703: Loss 2.0259\n",
      "  Iter 590/703: Loss 2.2567\n",
      "  Iter 600/703: Loss 2.2484\n",
      "  Iter 610/703: Loss 2.0580\n",
      "  Iter 620/703: Loss 2.3402\n",
      "  Iter 630/703: Loss 1.7689\n",
      "  Iter 640/703: Loss 2.1546\n",
      "  Iter 650/703: Loss 2.1152\n",
      "  Iter 660/703: Loss 2.0118\n",
      "  Iter 670/703: Loss 2.1757\n",
      "  Iter 680/703: Loss 2.4481\n",
      "  Iter 690/703: Loss 1.9651\n",
      "  Iter 700/703: Loss 1.9359\n",
      "Train acc: 0.4430, Test acc: 0.3409\n",
      "\n",
      "[Epoch 10]\n",
      "  Iter   0/703: Loss 2.2952\n",
      "  Iter  10/703: Loss 2.2102\n",
      "  Iter  20/703: Loss 1.9872\n",
      "  Iter  30/703: Loss 2.3156\n",
      "  Iter  40/703: Loss 2.0532\n",
      "  Iter  50/703: Loss 1.8971\n",
      "  Iter  60/703: Loss 1.8423\n",
      "  Iter  70/703: Loss 2.4163\n",
      "  Iter  80/703: Loss 1.9894\n",
      "  Iter  90/703: Loss 2.1792\n",
      "  Iter 100/703: Loss 2.2234\n",
      "  Iter 110/703: Loss 1.9838\n",
      "  Iter 120/703: Loss 1.9353\n",
      "  Iter 130/703: Loss 2.7055\n",
      "  Iter 140/703: Loss 1.8853\n",
      "  Iter 150/703: Loss 2.3090\n",
      "  Iter 160/703: Loss 2.0797\n",
      "  Iter 170/703: Loss 1.9662\n",
      "  Iter 180/703: Loss 1.7454\n",
      "  Iter 190/703: Loss 1.9492\n",
      "  Iter 200/703: Loss 1.9907\n",
      "  Iter 210/703: Loss 1.9913\n",
      "  Iter 220/703: Loss 2.0517\n",
      "  Iter 230/703: Loss 2.0983\n",
      "  Iter 240/703: Loss 2.1750\n",
      "  Iter 250/703: Loss 1.8365\n",
      "  Iter 260/703: Loss 1.9939\n",
      "  Iter 270/703: Loss 2.0667\n",
      "  Iter 280/703: Loss 1.7933\n",
      "  Iter 290/703: Loss 1.9059\n",
      "  Iter 300/703: Loss 1.6585\n",
      "  Iter 310/703: Loss 1.8852\n",
      "  Iter 320/703: Loss 2.1981\n",
      "  Iter 330/703: Loss 2.2857\n",
      "  Iter 340/703: Loss 1.9770\n",
      "  Iter 350/703: Loss 1.7740\n",
      "  Iter 360/703: Loss 1.9743\n",
      "  Iter 370/703: Loss 1.9182\n",
      "  Iter 380/703: Loss 2.0417\n",
      "  Iter 390/703: Loss 1.7929\n",
      "  Iter 400/703: Loss 2.1128\n",
      "  Iter 410/703: Loss 2.0758\n",
      "  Iter 420/703: Loss 2.0191\n",
      "  Iter 430/703: Loss 2.2759\n",
      "  Iter 440/703: Loss 2.4147\n",
      "  Iter 450/703: Loss 1.9542\n",
      "  Iter 460/703: Loss 1.9190\n",
      "  Iter 470/703: Loss 2.1571\n",
      "  Iter 480/703: Loss 2.2208\n",
      "  Iter 490/703: Loss 2.3741\n",
      "  Iter 500/703: Loss 2.2678\n",
      "  Iter 510/703: Loss 1.9813\n",
      "  Iter 520/703: Loss 1.9804\n",
      "  Iter 530/703: Loss 1.9944\n",
      "  Iter 540/703: Loss 2.2665\n",
      "  Iter 550/703: Loss 1.9303\n",
      "  Iter 560/703: Loss 2.1002\n",
      "  Iter 570/703: Loss 1.6762\n",
      "  Iter 580/703: Loss 1.8466\n",
      "  Iter 590/703: Loss 2.1288\n",
      "  Iter 600/703: Loss 1.9891\n",
      "  Iter 610/703: Loss 1.9358\n",
      "  Iter 620/703: Loss 2.0326\n",
      "  Iter 630/703: Loss 1.9416\n",
      "  Iter 640/703: Loss 2.2120\n",
      "  Iter 650/703: Loss 2.0461\n",
      "  Iter 660/703: Loss 1.5716\n",
      "  Iter 670/703: Loss 2.3493\n",
      "  Iter 680/703: Loss 2.1957\n",
      "  Iter 690/703: Loss 2.1792\n",
      "  Iter 700/703: Loss 1.7503\n",
      "Train acc: 0.4930, Test acc: 0.3503\n",
      "\n",
      "[Epoch 11]\n",
      "  Iter   0/703: Loss 1.9934\n",
      "  Iter  10/703: Loss 1.9519\n",
      "  Iter  20/703: Loss 2.0965\n",
      "  Iter  30/703: Loss 2.2138\n",
      "  Iter  40/703: Loss 2.2770\n",
      "  Iter  50/703: Loss 1.9680\n",
      "  Iter  60/703: Loss 1.5758\n",
      "  Iter  70/703: Loss 1.6712\n",
      "  Iter  80/703: Loss 1.7467\n",
      "  Iter  90/703: Loss 1.7576\n",
      "  Iter 100/703: Loss 1.8259\n",
      "  Iter 110/703: Loss 2.3422\n",
      "  Iter 120/703: Loss 1.9170\n",
      "  Iter 130/703: Loss 2.2827\n",
      "  Iter 140/703: Loss 2.0553\n",
      "  Iter 150/703: Loss 2.2061\n",
      "  Iter 160/703: Loss 1.7765\n",
      "  Iter 170/703: Loss 1.8700\n",
      "  Iter 180/703: Loss 2.1874\n",
      "  Iter 190/703: Loss 2.2470\n",
      "  Iter 200/703: Loss 2.0594\n",
      "  Iter 210/703: Loss 2.3619\n",
      "  Iter 220/703: Loss 1.8390\n",
      "  Iter 230/703: Loss 1.9619\n",
      "  Iter 240/703: Loss 2.3610\n",
      "  Iter 250/703: Loss 1.6844\n",
      "  Iter 260/703: Loss 1.9040\n",
      "  Iter 270/703: Loss 2.0794\n",
      "  Iter 280/703: Loss 2.0306\n",
      "  Iter 290/703: Loss 2.3442\n",
      "  Iter 300/703: Loss 1.8889\n",
      "  Iter 310/703: Loss 1.8754\n",
      "  Iter 320/703: Loss 2.2628\n",
      "  Iter 330/703: Loss 2.1693\n",
      "  Iter 340/703: Loss 1.9513\n",
      "  Iter 350/703: Loss 2.0782\n",
      "  Iter 360/703: Loss 1.9695\n",
      "  Iter 370/703: Loss 1.8527\n",
      "  Iter 380/703: Loss 2.1042\n",
      "  Iter 390/703: Loss 1.9578\n",
      "  Iter 400/703: Loss 2.2480\n",
      "  Iter 410/703: Loss 2.3296\n",
      "  Iter 420/703: Loss 1.9646\n",
      "  Iter 430/703: Loss 1.8178\n",
      "  Iter 440/703: Loss 1.9471\n",
      "  Iter 450/703: Loss 1.6970\n",
      "  Iter 460/703: Loss 1.8170\n",
      "  Iter 470/703: Loss 2.1589\n",
      "  Iter 480/703: Loss 1.9379\n",
      "  Iter 490/703: Loss 2.1696\n",
      "  Iter 500/703: Loss 1.8217\n",
      "  Iter 510/703: Loss 1.8515\n",
      "  Iter 520/703: Loss 1.7807\n",
      "  Iter 530/703: Loss 1.7615\n",
      "  Iter 540/703: Loss 2.1993\n",
      "  Iter 550/703: Loss 1.6128\n",
      "  Iter 560/703: Loss 1.6063\n",
      "  Iter 570/703: Loss 1.7643\n",
      "  Iter 580/703: Loss 1.7220\n",
      "  Iter 590/703: Loss 1.5949\n",
      "  Iter 600/703: Loss 2.0518\n",
      "  Iter 610/703: Loss 1.8481\n",
      "  Iter 620/703: Loss 1.7774\n",
      "  Iter 630/703: Loss 2.1216\n",
      "  Iter 640/703: Loss 1.7841\n",
      "  Iter 650/703: Loss 2.1226\n",
      "  Iter 660/703: Loss 1.7578\n",
      "  Iter 670/703: Loss 1.5290\n",
      "  Iter 680/703: Loss 1.9673\n",
      "  Iter 690/703: Loss 1.8494\n",
      "  Iter 700/703: Loss 1.5605\n",
      "Train acc: 0.5140, Test acc: 0.3653\n",
      "\n",
      "[Epoch 12]\n",
      "  Iter   0/703: Loss 1.7914\n",
      "  Iter  10/703: Loss 1.8232\n",
      "  Iter  20/703: Loss 1.8262\n",
      "  Iter  30/703: Loss 2.0667\n",
      "  Iter  40/703: Loss 1.9892\n",
      "  Iter  50/703: Loss 1.9506\n",
      "  Iter  60/703: Loss 1.8435\n",
      "  Iter  70/703: Loss 1.9740\n",
      "  Iter  80/703: Loss 1.8374\n",
      "  Iter  90/703: Loss 1.8250\n",
      "  Iter 100/703: Loss 1.5196\n",
      "  Iter 110/703: Loss 1.5532\n",
      "  Iter 120/703: Loss 1.9606\n",
      "  Iter 130/703: Loss 2.0861\n",
      "  Iter 140/703: Loss 1.7686\n",
      "  Iter 150/703: Loss 1.9359\n",
      "  Iter 160/703: Loss 1.7421\n",
      "  Iter 170/703: Loss 1.4450\n",
      "  Iter 180/703: Loss 1.9753\n",
      "  Iter 190/703: Loss 2.0726\n",
      "  Iter 200/703: Loss 1.6098\n",
      "  Iter 210/703: Loss 2.4320\n",
      "  Iter 220/703: Loss 2.0291\n",
      "  Iter 230/703: Loss 1.9498\n",
      "  Iter 240/703: Loss 1.6516\n",
      "  Iter 250/703: Loss 1.7843\n",
      "  Iter 260/703: Loss 1.6682\n",
      "  Iter 270/703: Loss 2.1036\n",
      "  Iter 280/703: Loss 1.9886\n",
      "  Iter 290/703: Loss 1.9717\n",
      "  Iter 300/703: Loss 1.8144\n",
      "  Iter 310/703: Loss 2.0113\n",
      "  Iter 320/703: Loss 1.8654\n",
      "  Iter 330/703: Loss 1.7026\n",
      "  Iter 340/703: Loss 1.8203\n",
      "  Iter 350/703: Loss 1.7472\n",
      "  Iter 360/703: Loss 1.7559\n",
      "  Iter 370/703: Loss 1.6647\n",
      "  Iter 380/703: Loss 1.8519\n",
      "  Iter 390/703: Loss 1.8088\n",
      "  Iter 400/703: Loss 2.1047\n",
      "  Iter 410/703: Loss 1.8993\n",
      "  Iter 420/703: Loss 2.2600\n",
      "  Iter 430/703: Loss 2.0368\n",
      "  Iter 440/703: Loss 1.9716\n",
      "  Iter 450/703: Loss 1.6652\n",
      "  Iter 460/703: Loss 1.7895\n",
      "  Iter 470/703: Loss 1.7342\n",
      "  Iter 480/703: Loss 1.8565\n",
      "  Iter 490/703: Loss 1.9041\n",
      "  Iter 500/703: Loss 1.8775\n",
      "  Iter 510/703: Loss 1.9877\n",
      "  Iter 520/703: Loss 1.7561\n",
      "  Iter 530/703: Loss 1.9331\n",
      "  Iter 540/703: Loss 1.5801\n",
      "  Iter 550/703: Loss 2.0559\n",
      "  Iter 560/703: Loss 1.5820\n",
      "  Iter 570/703: Loss 1.7572\n",
      "  Iter 580/703: Loss 1.4918\n",
      "  Iter 590/703: Loss 1.5965\n",
      "  Iter 600/703: Loss 1.8174\n",
      "  Iter 610/703: Loss 1.9956\n",
      "  Iter 620/703: Loss 1.9980\n",
      "  Iter 630/703: Loss 2.0308\n",
      "  Iter 640/703: Loss 1.7080\n",
      "  Iter 650/703: Loss 1.6725\n",
      "  Iter 660/703: Loss 1.8950\n",
      "  Iter 670/703: Loss 1.8382\n",
      "  Iter 680/703: Loss 2.1171\n",
      "  Iter 690/703: Loss 1.8414\n",
      "  Iter 700/703: Loss 1.6303\n",
      "Train acc: 0.5230, Test acc: 0.3661\n",
      "\n",
      "[Epoch 13]\n",
      "  Iter   0/703: Loss 1.6965\n",
      "  Iter  10/703: Loss 1.6122\n",
      "  Iter  20/703: Loss 1.8155\n",
      "  Iter  30/703: Loss 1.4886\n",
      "  Iter  40/703: Loss 1.6147\n",
      "  Iter  50/703: Loss 1.4536\n",
      "  Iter  60/703: Loss 1.5184\n",
      "  Iter  70/703: Loss 1.8170\n",
      "  Iter  80/703: Loss 1.7800\n",
      "  Iter  90/703: Loss 1.8251\n",
      "  Iter 100/703: Loss 1.9624\n",
      "  Iter 110/703: Loss 1.7748\n",
      "  Iter 120/703: Loss 1.9709\n",
      "  Iter 130/703: Loss 1.7117\n",
      "  Iter 140/703: Loss 1.7818\n",
      "  Iter 150/703: Loss 2.1886\n",
      "  Iter 160/703: Loss 1.8364\n",
      "  Iter 170/703: Loss 1.9767\n",
      "  Iter 180/703: Loss 1.7739\n",
      "  Iter 190/703: Loss 1.3982\n",
      "  Iter 200/703: Loss 1.9794\n",
      "  Iter 210/703: Loss 1.4573\n",
      "  Iter 220/703: Loss 1.5650\n",
      "  Iter 230/703: Loss 1.5445\n",
      "  Iter 240/703: Loss 2.1506\n",
      "  Iter 250/703: Loss 1.9678\n",
      "  Iter 260/703: Loss 1.5915\n",
      "  Iter 270/703: Loss 1.8252\n",
      "  Iter 280/703: Loss 1.4927\n",
      "  Iter 290/703: Loss 1.9648\n",
      "  Iter 300/703: Loss 1.6163\n",
      "  Iter 310/703: Loss 2.0251\n",
      "  Iter 320/703: Loss 1.8772\n",
      "  Iter 330/703: Loss 1.8624\n",
      "  Iter 340/703: Loss 1.6406\n",
      "  Iter 350/703: Loss 1.7368\n",
      "  Iter 360/703: Loss 1.8013\n",
      "  Iter 370/703: Loss 1.6657\n",
      "  Iter 380/703: Loss 1.5876\n",
      "  Iter 390/703: Loss 1.6543\n",
      "  Iter 400/703: Loss 1.6986\n",
      "  Iter 410/703: Loss 1.9163\n",
      "  Iter 420/703: Loss 1.5438\n",
      "  Iter 430/703: Loss 1.8861\n",
      "  Iter 440/703: Loss 1.4252\n",
      "  Iter 450/703: Loss 2.0680\n",
      "  Iter 460/703: Loss 2.0095\n",
      "  Iter 470/703: Loss 1.6089\n",
      "  Iter 480/703: Loss 1.7589\n",
      "  Iter 490/703: Loss 1.7595\n",
      "  Iter 500/703: Loss 1.6198\n",
      "  Iter 510/703: Loss 1.9139\n",
      "  Iter 520/703: Loss 1.6935\n",
      "  Iter 530/703: Loss 2.1485\n",
      "  Iter 540/703: Loss 1.6784\n",
      "  Iter 550/703: Loss 1.8504\n",
      "  Iter 560/703: Loss 1.6540\n",
      "  Iter 570/703: Loss 1.6602\n",
      "  Iter 580/703: Loss 1.7522\n",
      "  Iter 590/703: Loss 1.6451\n",
      "  Iter 600/703: Loss 1.7290\n",
      "  Iter 610/703: Loss 1.8759\n",
      "  Iter 620/703: Loss 1.7250\n",
      "  Iter 630/703: Loss 1.9012\n",
      "  Iter 640/703: Loss 1.6632\n",
      "  Iter 650/703: Loss 1.6305\n",
      "  Iter 660/703: Loss 1.5368\n",
      "  Iter 670/703: Loss 1.5360\n",
      "  Iter 680/703: Loss 1.8189\n",
      "  Iter 690/703: Loss 1.9799\n",
      "  Iter 700/703: Loss 1.7566\n",
      "Train acc: 0.5340, Test acc: 0.3735\n",
      "\n",
      "[Epoch 14]\n",
      "  Iter   0/703: Loss 1.9850\n",
      "  Iter  10/703: Loss 1.8362\n",
      "  Iter  20/703: Loss 1.9096\n",
      "  Iter  30/703: Loss 1.7537\n",
      "  Iter  40/703: Loss 1.6175\n",
      "  Iter  50/703: Loss 1.8032\n",
      "  Iter  60/703: Loss 1.4990\n",
      "  Iter  70/703: Loss 1.8933\n",
      "  Iter  80/703: Loss 1.5968\n",
      "  Iter  90/703: Loss 1.7245\n",
      "  Iter 100/703: Loss 1.5368\n",
      "  Iter 110/703: Loss 2.0443\n",
      "  Iter 120/703: Loss 1.7783\n",
      "  Iter 130/703: Loss 1.6386\n",
      "  Iter 140/703: Loss 1.8284\n",
      "  Iter 150/703: Loss 1.4403\n",
      "  Iter 160/703: Loss 1.5259\n",
      "  Iter 170/703: Loss 1.6044\n",
      "  Iter 180/703: Loss 2.0156\n",
      "  Iter 190/703: Loss 1.8077\n",
      "  Iter 200/703: Loss 2.1229\n",
      "  Iter 210/703: Loss 1.7304\n",
      "  Iter 220/703: Loss 1.6455\n",
      "  Iter 230/703: Loss 1.6108\n",
      "  Iter 240/703: Loss 1.4223\n",
      "  Iter 250/703: Loss 1.7358\n",
      "  Iter 260/703: Loss 1.7699\n",
      "  Iter 270/703: Loss 1.5955\n",
      "  Iter 280/703: Loss 1.5901\n",
      "  Iter 290/703: Loss 1.7062\n",
      "  Iter 300/703: Loss 1.9375\n",
      "  Iter 310/703: Loss 1.5964\n",
      "  Iter 320/703: Loss 1.5971\n",
      "  Iter 330/703: Loss 1.6430\n",
      "  Iter 340/703: Loss 1.3159\n",
      "  Iter 350/703: Loss 1.8491\n",
      "  Iter 360/703: Loss 1.6782\n",
      "  Iter 370/703: Loss 1.7392\n",
      "  Iter 380/703: Loss 1.8668\n",
      "  Iter 390/703: Loss 1.7775\n",
      "  Iter 400/703: Loss 1.7185\n",
      "  Iter 410/703: Loss 1.8741\n",
      "  Iter 420/703: Loss 1.7261\n",
      "  Iter 430/703: Loss 1.5398\n",
      "  Iter 440/703: Loss 1.9127\n",
      "  Iter 450/703: Loss 1.4803\n",
      "  Iter 460/703: Loss 1.8381\n",
      "  Iter 470/703: Loss 1.1684\n",
      "  Iter 480/703: Loss 1.6826\n",
      "  Iter 490/703: Loss 1.9455\n",
      "  Iter 500/703: Loss 1.6896\n",
      "  Iter 510/703: Loss 1.6090\n",
      "  Iter 520/703: Loss 1.5614\n",
      "  Iter 530/703: Loss 2.0025\n",
      "  Iter 540/703: Loss 1.6794\n",
      "  Iter 550/703: Loss 1.7976\n",
      "  Iter 560/703: Loss 1.6826\n",
      "  Iter 570/703: Loss 1.5448\n",
      "  Iter 580/703: Loss 1.7660\n",
      "  Iter 590/703: Loss 1.6509\n",
      "  Iter 600/703: Loss 1.5424\n",
      "  Iter 610/703: Loss 1.6883\n",
      "  Iter 620/703: Loss 2.0494\n",
      "  Iter 630/703: Loss 1.5006\n",
      "  Iter 640/703: Loss 1.4964\n",
      "  Iter 650/703: Loss 1.6058\n",
      "  Iter 660/703: Loss 1.4576\n",
      "  Iter 670/703: Loss 2.0287\n",
      "  Iter 680/703: Loss 1.7226\n",
      "  Iter 690/703: Loss 1.7547\n",
      "  Iter 700/703: Loss 1.4382\n",
      "Train acc: 0.5430, Test acc: 0.3772\n",
      "\n",
      "[Epoch 15]\n",
      "  Iter   0/703: Loss 1.5374\n",
      "  Iter  10/703: Loss 1.8139\n",
      "  Iter  20/703: Loss 1.7745\n",
      "  Iter  30/703: Loss 1.5764\n",
      "  Iter  40/703: Loss 1.9302\n",
      "  Iter  50/703: Loss 1.3510\n",
      "  Iter  60/703: Loss 1.6606\n",
      "  Iter  70/703: Loss 1.5726\n",
      "  Iter  80/703: Loss 1.5261\n",
      "  Iter  90/703: Loss 1.7206\n",
      "  Iter 100/703: Loss 1.8510\n",
      "  Iter 110/703: Loss 1.5271\n",
      "  Iter 120/703: Loss 1.5880\n",
      "  Iter 130/703: Loss 1.7633\n",
      "  Iter 140/703: Loss 2.0440\n",
      "  Iter 150/703: Loss 1.7047\n",
      "  Iter 160/703: Loss 1.5451\n",
      "  Iter 170/703: Loss 1.5495\n",
      "  Iter 180/703: Loss 1.7946\n",
      "  Iter 190/703: Loss 1.5821\n",
      "  Iter 200/703: Loss 1.6456\n",
      "  Iter 210/703: Loss 1.6814\n",
      "  Iter 220/703: Loss 1.5587\n",
      "  Iter 230/703: Loss 1.4546\n",
      "  Iter 240/703: Loss 1.7033\n",
      "  Iter 250/703: Loss 1.8003\n",
      "  Iter 260/703: Loss 1.7068\n",
      "  Iter 270/703: Loss 1.5027\n",
      "  Iter 280/703: Loss 1.6458\n",
      "  Iter 290/703: Loss 1.5909\n",
      "  Iter 300/703: Loss 1.7001\n",
      "  Iter 310/703: Loss 1.5976\n",
      "  Iter 320/703: Loss 1.7483\n",
      "  Iter 330/703: Loss 1.1522\n",
      "  Iter 340/703: Loss 1.4156\n",
      "  Iter 350/703: Loss 1.9751\n",
      "  Iter 360/703: Loss 1.8146\n",
      "  Iter 370/703: Loss 1.4611\n",
      "  Iter 380/703: Loss 1.8865\n",
      "  Iter 390/703: Loss 1.4447\n",
      "  Iter 400/703: Loss 1.8027\n",
      "  Iter 410/703: Loss 1.9499\n",
      "  Iter 420/703: Loss 1.5811\n",
      "  Iter 430/703: Loss 1.9084\n",
      "  Iter 440/703: Loss 1.8312\n",
      "  Iter 450/703: Loss 2.0533\n",
      "  Iter 460/703: Loss 1.5552\n",
      "  Iter 470/703: Loss 1.6290\n",
      "  Iter 480/703: Loss 1.7314\n",
      "  Iter 490/703: Loss 1.7721\n",
      "  Iter 500/703: Loss 1.6677\n",
      "  Iter 510/703: Loss 1.5121\n",
      "  Iter 520/703: Loss 1.6052\n",
      "  Iter 530/703: Loss 1.5486\n",
      "  Iter 540/703: Loss 1.8213\n",
      "  Iter 550/703: Loss 1.6036\n",
      "  Iter 560/703: Loss 1.6377\n",
      "  Iter 570/703: Loss 1.6801\n",
      "  Iter 580/703: Loss 1.6439\n",
      "  Iter 590/703: Loss 1.6650\n",
      "  Iter 600/703: Loss 1.6730\n",
      "  Iter 610/703: Loss 1.6672\n",
      "  Iter 620/703: Loss 1.4932\n",
      "  Iter 630/703: Loss 1.3908\n",
      "  Iter 640/703: Loss 1.4219\n",
      "  Iter 650/703: Loss 1.5866\n",
      "  Iter 660/703: Loss 1.3725\n",
      "  Iter 670/703: Loss 1.3339\n",
      "  Iter 680/703: Loss 1.5698\n",
      "  Iter 690/703: Loss 1.6068\n",
      "  Iter 700/703: Loss 1.6646\n",
      "Train acc: 0.5810, Test acc: 0.3789\n",
      "\n",
      "[Epoch 16]\n",
      "  Iter   0/703: Loss 1.7381\n",
      "  Iter  10/703: Loss 1.5928\n",
      "  Iter  20/703: Loss 1.7704\n",
      "  Iter  30/703: Loss 1.3207\n",
      "  Iter  40/703: Loss 1.5562\n",
      "  Iter  50/703: Loss 1.7014\n",
      "  Iter  60/703: Loss 1.7284\n",
      "  Iter  70/703: Loss 1.5672\n",
      "  Iter  80/703: Loss 1.3524\n",
      "  Iter  90/703: Loss 1.4514\n",
      "  Iter 100/703: Loss 1.3767\n",
      "  Iter 110/703: Loss 1.7439\n",
      "  Iter 120/703: Loss 1.3600\n",
      "  Iter 130/703: Loss 1.6826\n",
      "  Iter 140/703: Loss 2.0719\n",
      "  Iter 150/703: Loss 1.7111\n",
      "  Iter 160/703: Loss 1.9182\n",
      "  Iter 170/703: Loss 1.6653\n",
      "  Iter 180/703: Loss 1.2618\n",
      "  Iter 190/703: Loss 1.6857\n",
      "  Iter 200/703: Loss 1.5531\n",
      "  Iter 210/703: Loss 1.7160\n",
      "  Iter 220/703: Loss 1.3945\n",
      "  Iter 230/703: Loss 1.2969\n",
      "  Iter 240/703: Loss 1.2451\n",
      "  Iter 250/703: Loss 1.7918\n",
      "  Iter 260/703: Loss 1.5541\n",
      "  Iter 270/703: Loss 1.8461\n",
      "  Iter 280/703: Loss 1.5622\n",
      "  Iter 290/703: Loss 1.4838\n",
      "  Iter 300/703: Loss 1.6963\n",
      "  Iter 310/703: Loss 1.6314\n",
      "  Iter 320/703: Loss 1.5872\n",
      "  Iter 330/703: Loss 1.6294\n",
      "  Iter 340/703: Loss 1.6685\n",
      "  Iter 350/703: Loss 1.8315\n",
      "  Iter 360/703: Loss 1.5749\n",
      "  Iter 370/703: Loss 1.1950\n",
      "  Iter 380/703: Loss 1.6879\n",
      "  Iter 390/703: Loss 1.6014\n",
      "  Iter 400/703: Loss 1.7948\n",
      "  Iter 410/703: Loss 1.6803\n",
      "  Iter 420/703: Loss 1.3904\n",
      "  Iter 430/703: Loss 1.4771\n",
      "  Iter 440/703: Loss 1.6716\n",
      "  Iter 450/703: Loss 1.7045\n",
      "  Iter 460/703: Loss 1.8742\n",
      "  Iter 470/703: Loss 1.3687\n",
      "  Iter 480/703: Loss 1.8819\n",
      "  Iter 490/703: Loss 1.4694\n",
      "  Iter 500/703: Loss 1.2693\n",
      "  Iter 510/703: Loss 1.4088\n",
      "  Iter 520/703: Loss 1.4448\n",
      "  Iter 530/703: Loss 1.7362\n",
      "  Iter 540/703: Loss 1.5434\n",
      "  Iter 550/703: Loss 1.7447\n",
      "  Iter 560/703: Loss 1.6723\n",
      "  Iter 570/703: Loss 1.3932\n",
      "  Iter 580/703: Loss 1.6043\n",
      "  Iter 590/703: Loss 1.8268\n",
      "  Iter 600/703: Loss 1.2425\n",
      "  Iter 610/703: Loss 1.5204\n",
      "  Iter 620/703: Loss 1.8202\n",
      "  Iter 630/703: Loss 1.2272\n",
      "  Iter 640/703: Loss 1.4149\n",
      "  Iter 650/703: Loss 1.7354\n",
      "  Iter 660/703: Loss 1.5477\n",
      "  Iter 670/703: Loss 1.3791\n",
      "  Iter 680/703: Loss 1.7323\n",
      "  Iter 690/703: Loss 1.8473\n",
      "  Iter 700/703: Loss 1.6878\n",
      "Train acc: 0.6040, Test acc: 0.3841\n",
      "\n",
      "[Epoch 17]\n",
      "  Iter   0/703: Loss 1.7526\n",
      "  Iter  10/703: Loss 1.2596\n",
      "  Iter  20/703: Loss 1.9603\n",
      "  Iter  30/703: Loss 1.2231\n",
      "  Iter  40/703: Loss 1.4930\n",
      "  Iter  50/703: Loss 1.6512\n",
      "  Iter  60/703: Loss 1.8258\n",
      "  Iter  70/703: Loss 1.7210\n",
      "  Iter  80/703: Loss 1.4720\n",
      "  Iter  90/703: Loss 1.4120\n",
      "  Iter 100/703: Loss 1.3234\n",
      "  Iter 110/703: Loss 1.6540\n",
      "  Iter 120/703: Loss 1.7837\n",
      "  Iter 130/703: Loss 1.4250\n",
      "  Iter 140/703: Loss 1.2899\n",
      "  Iter 150/703: Loss 1.4393\n",
      "  Iter 160/703: Loss 1.5762\n",
      "  Iter 170/703: Loss 1.8466\n",
      "  Iter 180/703: Loss 1.5968\n",
      "  Iter 190/703: Loss 1.9237\n",
      "  Iter 200/703: Loss 1.2904\n",
      "  Iter 210/703: Loss 1.7145\n",
      "  Iter 220/703: Loss 1.7035\n",
      "  Iter 230/703: Loss 1.6628\n",
      "  Iter 240/703: Loss 1.5367\n",
      "  Iter 250/703: Loss 1.5277\n",
      "  Iter 260/703: Loss 1.4853\n",
      "  Iter 270/703: Loss 2.0230\n",
      "  Iter 280/703: Loss 1.6162\n",
      "  Iter 290/703: Loss 1.3097\n",
      "  Iter 300/703: Loss 1.5196\n",
      "  Iter 310/703: Loss 1.2662\n",
      "  Iter 320/703: Loss 1.7341\n",
      "  Iter 330/703: Loss 1.4633\n",
      "  Iter 340/703: Loss 1.5715\n",
      "  Iter 350/703: Loss 1.8887\n",
      "  Iter 360/703: Loss 1.3140\n",
      "  Iter 370/703: Loss 1.7633\n",
      "  Iter 380/703: Loss 1.3496\n",
      "  Iter 390/703: Loss 1.5731\n",
      "  Iter 400/703: Loss 1.4879\n",
      "  Iter 410/703: Loss 1.0438\n",
      "  Iter 420/703: Loss 1.4602\n",
      "  Iter 430/703: Loss 1.4529\n",
      "  Iter 440/703: Loss 1.4889\n",
      "  Iter 450/703: Loss 1.4082\n",
      "  Iter 460/703: Loss 1.8222\n",
      "  Iter 470/703: Loss 1.2027\n",
      "  Iter 480/703: Loss 1.7489\n",
      "  Iter 490/703: Loss 1.4887\n",
      "  Iter 500/703: Loss 1.4105\n",
      "  Iter 510/703: Loss 1.4280\n",
      "  Iter 520/703: Loss 1.6600\n",
      "  Iter 530/703: Loss 1.4714\n",
      "  Iter 540/703: Loss 1.3453\n",
      "  Iter 550/703: Loss 1.5574\n",
      "  Iter 560/703: Loss 1.7123\n",
      "  Iter 570/703: Loss 1.5762\n",
      "  Iter 580/703: Loss 1.2932\n",
      "  Iter 590/703: Loss 1.3257\n",
      "  Iter 600/703: Loss 1.4585\n",
      "  Iter 610/703: Loss 1.6540\n",
      "  Iter 620/703: Loss 1.7194\n",
      "  Iter 630/703: Loss 1.3986\n",
      "  Iter 640/703: Loss 1.4898\n",
      "  Iter 650/703: Loss 1.7251\n",
      "  Iter 660/703: Loss 1.9013\n",
      "  Iter 670/703: Loss 1.3534\n",
      "  Iter 680/703: Loss 1.6438\n",
      "  Iter 690/703: Loss 1.5249\n",
      "  Iter 700/703: Loss 1.5701\n",
      "Train acc: 0.5980, Test acc: 0.3688\n",
      "\n",
      "[Epoch 18]\n",
      "  Iter   0/703: Loss 1.7632\n",
      "  Iter  10/703: Loss 1.3718\n",
      "  Iter  20/703: Loss 1.6298\n",
      "  Iter  30/703: Loss 1.3223\n",
      "  Iter  40/703: Loss 1.4364\n",
      "  Iter  50/703: Loss 1.3443\n",
      "  Iter  60/703: Loss 1.7369\n",
      "  Iter  70/703: Loss 1.2581\n",
      "  Iter  80/703: Loss 1.5007\n",
      "  Iter  90/703: Loss 1.2319\n",
      "  Iter 100/703: Loss 1.6909\n",
      "  Iter 110/703: Loss 1.2488\n",
      "  Iter 120/703: Loss 1.4067\n",
      "  Iter 130/703: Loss 1.2826\n",
      "  Iter 140/703: Loss 1.5462\n",
      "  Iter 150/703: Loss 1.2918\n",
      "  Iter 160/703: Loss 1.6212\n",
      "  Iter 170/703: Loss 1.3432\n",
      "  Iter 180/703: Loss 1.4851\n",
      "  Iter 190/703: Loss 1.3558\n",
      "  Iter 200/703: Loss 1.4989\n",
      "  Iter 210/703: Loss 1.4630\n",
      "  Iter 220/703: Loss 1.6396\n",
      "  Iter 230/703: Loss 1.2252\n",
      "  Iter 240/703: Loss 1.4181\n",
      "  Iter 250/703: Loss 1.4058\n",
      "  Iter 260/703: Loss 1.3557\n",
      "  Iter 270/703: Loss 1.7518\n",
      "  Iter 280/703: Loss 1.5309\n",
      "  Iter 290/703: Loss 1.6920\n",
      "  Iter 300/703: Loss 1.6145\n",
      "  Iter 310/703: Loss 1.2383\n",
      "  Iter 320/703: Loss 1.3940\n",
      "  Iter 330/703: Loss 1.6164\n",
      "  Iter 340/703: Loss 1.4876\n",
      "  Iter 350/703: Loss 1.2519\n",
      "  Iter 360/703: Loss 1.3427\n",
      "  Iter 370/703: Loss 1.4096\n",
      "  Iter 380/703: Loss 1.6715\n",
      "  Iter 390/703: Loss 1.6736\n",
      "  Iter 400/703: Loss 1.4166\n",
      "  Iter 410/703: Loss 1.6508\n",
      "  Iter 420/703: Loss 1.6195\n",
      "  Iter 430/703: Loss 1.7177\n",
      "  Iter 440/703: Loss 1.6454\n",
      "  Iter 450/703: Loss 1.5093\n",
      "  Iter 460/703: Loss 1.7327\n",
      "  Iter 470/703: Loss 1.5370\n",
      "  Iter 480/703: Loss 1.8009\n",
      "  Iter 490/703: Loss 1.7384\n",
      "  Iter 500/703: Loss 1.3507\n",
      "  Iter 510/703: Loss 1.3032\n",
      "  Iter 520/703: Loss 1.2210\n",
      "  Iter 530/703: Loss 1.5316\n",
      "  Iter 540/703: Loss 1.3835\n",
      "  Iter 550/703: Loss 1.6892\n",
      "  Iter 560/703: Loss 1.4136\n",
      "  Iter 570/703: Loss 1.2916\n",
      "  Iter 580/703: Loss 1.1317\n",
      "  Iter 590/703: Loss 0.9407\n",
      "  Iter 600/703: Loss 1.5491\n",
      "  Iter 610/703: Loss 1.1030\n",
      "  Iter 620/703: Loss 1.4314\n",
      "  Iter 630/703: Loss 1.8786\n",
      "  Iter 640/703: Loss 1.3314\n",
      "  Iter 650/703: Loss 1.4527\n",
      "  Iter 660/703: Loss 1.4098\n",
      "  Iter 670/703: Loss 1.3716\n",
      "  Iter 680/703: Loss 1.5065\n",
      "  Iter 690/703: Loss 1.4674\n",
      "  Iter 700/703: Loss 1.5931\n",
      "Train acc: 0.6160, Test acc: 0.3746\n",
      "\n",
      "[Epoch 19]\n",
      "  Iter   0/703: Loss 1.3724\n",
      "  Iter  10/703: Loss 1.7105\n",
      "  Iter  20/703: Loss 1.2820\n",
      "  Iter  30/703: Loss 1.5060\n",
      "  Iter  40/703: Loss 1.4905\n",
      "  Iter  50/703: Loss 1.3152\n",
      "  Iter  60/703: Loss 1.7827\n",
      "  Iter  70/703: Loss 1.2655\n",
      "  Iter  80/703: Loss 1.1628\n",
      "  Iter  90/703: Loss 1.3456\n",
      "  Iter 100/703: Loss 1.6088\n",
      "  Iter 110/703: Loss 1.5693\n",
      "  Iter 120/703: Loss 1.5048\n",
      "  Iter 130/703: Loss 1.4846\n",
      "  Iter 140/703: Loss 1.8550\n",
      "  Iter 150/703: Loss 1.4180\n",
      "  Iter 160/703: Loss 1.7232\n",
      "  Iter 170/703: Loss 1.7280\n",
      "  Iter 180/703: Loss 1.1686\n",
      "  Iter 190/703: Loss 1.4777\n",
      "  Iter 200/703: Loss 1.4350\n",
      "  Iter 210/703: Loss 1.1472\n",
      "  Iter 220/703: Loss 1.6790\n",
      "  Iter 230/703: Loss 1.3078\n",
      "  Iter 240/703: Loss 1.1057\n",
      "  Iter 250/703: Loss 1.5460\n",
      "  Iter 260/703: Loss 1.2580\n",
      "  Iter 270/703: Loss 1.7807\n",
      "  Iter 280/703: Loss 1.4092\n",
      "  Iter 290/703: Loss 1.5690\n",
      "  Iter 300/703: Loss 1.1608\n",
      "  Iter 310/703: Loss 1.6185\n",
      "  Iter 320/703: Loss 1.3382\n",
      "  Iter 330/703: Loss 1.2380\n",
      "  Iter 340/703: Loss 1.4210\n",
      "  Iter 350/703: Loss 1.2327\n",
      "  Iter 360/703: Loss 1.3746\n",
      "  Iter 370/703: Loss 1.5420\n",
      "  Iter 380/703: Loss 1.2031\n",
      "  Iter 390/703: Loss 1.2961\n",
      "  Iter 400/703: Loss 1.2056\n",
      "  Iter 410/703: Loss 1.5129\n",
      "  Iter 420/703: Loss 1.1550\n",
      "  Iter 430/703: Loss 1.2564\n",
      "  Iter 440/703: Loss 1.5273\n",
      "  Iter 450/703: Loss 1.4552\n",
      "  Iter 460/703: Loss 1.4921\n",
      "  Iter 470/703: Loss 1.3508\n",
      "  Iter 480/703: Loss 1.1105\n",
      "  Iter 490/703: Loss 1.0814\n",
      "  Iter 500/703: Loss 1.5284\n",
      "  Iter 510/703: Loss 1.3608\n",
      "  Iter 520/703: Loss 1.3427\n",
      "  Iter 530/703: Loss 1.4120\n",
      "  Iter 540/703: Loss 1.5860\n",
      "  Iter 550/703: Loss 1.2653\n",
      "  Iter 560/703: Loss 1.1962\n",
      "  Iter 570/703: Loss 1.7481\n",
      "  Iter 580/703: Loss 1.7371\n",
      "  Iter 590/703: Loss 1.4032\n",
      "  Iter 600/703: Loss 1.5048\n",
      "  Iter 610/703: Loss 1.3528\n",
      "  Iter 620/703: Loss 1.2634\n",
      "  Iter 630/703: Loss 1.5780\n",
      "  Iter 640/703: Loss 1.3330\n",
      "  Iter 650/703: Loss 1.4642\n",
      "  Iter 660/703: Loss 1.3790\n",
      "  Iter 670/703: Loss 1.1965\n",
      "  Iter 680/703: Loss 1.4274\n",
      "  Iter 690/703: Loss 1.2117\n",
      "  Iter 700/703: Loss 1.2142\n",
      "Train acc: 0.6190, Test acc: 0.3763\n",
      "\n",
      "[Epoch 20]\n",
      "  Iter   0/703: Loss 1.3543\n",
      "  Iter  10/703: Loss 1.1922\n",
      "  Iter  20/703: Loss 1.6581\n",
      "  Iter  30/703: Loss 1.2665\n",
      "  Iter  40/703: Loss 1.2774\n",
      "  Iter  50/703: Loss 1.4206\n",
      "  Iter  60/703: Loss 1.2070\n",
      "  Iter  70/703: Loss 1.1680\n",
      "  Iter  80/703: Loss 1.5398\n",
      "  Iter  90/703: Loss 1.5159\n",
      "  Iter 100/703: Loss 1.4875\n",
      "  Iter 110/703: Loss 1.3110\n",
      "  Iter 120/703: Loss 1.4297\n",
      "  Iter 130/703: Loss 1.5830\n",
      "  Iter 140/703: Loss 1.1398\n",
      "  Iter 150/703: Loss 1.3257\n",
      "  Iter 160/703: Loss 1.5620\n",
      "  Iter 170/703: Loss 1.2439\n",
      "  Iter 180/703: Loss 1.9133\n",
      "  Iter 190/703: Loss 1.0375\n",
      "  Iter 200/703: Loss 1.4593\n",
      "  Iter 210/703: Loss 1.2425\n",
      "  Iter 220/703: Loss 1.2367\n",
      "  Iter 230/703: Loss 1.4914\n",
      "  Iter 240/703: Loss 1.1983\n",
      "  Iter 250/703: Loss 1.3352\n",
      "  Iter 260/703: Loss 1.6234\n",
      "  Iter 270/703: Loss 1.3311\n",
      "  Iter 280/703: Loss 1.4941\n",
      "  Iter 290/703: Loss 1.4651\n",
      "  Iter 300/703: Loss 1.4310\n",
      "  Iter 310/703: Loss 1.1960\n",
      "  Iter 320/703: Loss 1.4535\n",
      "  Iter 330/703: Loss 1.4386\n",
      "  Iter 340/703: Loss 1.3846\n",
      "  Iter 350/703: Loss 1.2918\n",
      "  Iter 360/703: Loss 1.6257\n",
      "  Iter 370/703: Loss 1.3872\n",
      "  Iter 380/703: Loss 1.2619\n",
      "  Iter 390/703: Loss 1.4581\n",
      "  Iter 400/703: Loss 1.4309\n",
      "  Iter 410/703: Loss 1.2229\n",
      "  Iter 420/703: Loss 1.6737\n",
      "  Iter 430/703: Loss 1.1122\n",
      "  Iter 440/703: Loss 1.4489\n",
      "  Iter 450/703: Loss 1.2531\n",
      "  Iter 460/703: Loss 1.1649\n",
      "  Iter 470/703: Loss 1.3383\n",
      "  Iter 480/703: Loss 1.4072\n",
      "  Iter 490/703: Loss 1.4316\n",
      "  Iter 500/703: Loss 1.3915\n",
      "  Iter 510/703: Loss 1.0830\n",
      "  Iter 520/703: Loss 1.1632\n",
      "  Iter 530/703: Loss 1.2293\n",
      "  Iter 540/703: Loss 1.2110\n",
      "  Iter 550/703: Loss 1.3035\n",
      "  Iter 560/703: Loss 1.2845\n",
      "  Iter 570/703: Loss 1.2886\n",
      "  Iter 580/703: Loss 1.3687\n",
      "  Iter 590/703: Loss 0.9103\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 36.0 MiB for an array with shape (64, 16, 16, 32, 3, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet20()\n\u001b[0;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, (x_train, y_train), (x_test, y_test), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, optimizer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      5\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 80\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_per_epoch):\n\u001b[1;32m---> 80\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[0;32m     81\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[9], line 58\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_t[batch_mask]\n\u001b[0;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad(x_batch, t_batch))\n\u001b[0;32m     60\u001b[0m params, grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_param_dict_and_grad()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(params, grads)\n",
      "Cell \u001b[1;32mIn[9], line 66\u001b[0m, in \u001b[0;36mTrainer.loss_grad\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 66\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(x, train_flg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n",
      "Cell \u001b[1;32mIn[7], line 112\u001b[0m, in \u001b[0;36mResNet20.forward\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m    110\u001b[0m     out \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2:\n\u001b[1;32m--> 112\u001b[0m     out \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3:\n\u001b[0;32m    114\u001b[0m     out \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n",
      "Cell \u001b[1;32mIn[7], line 45\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m     42\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m     43\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1\u001b[38;5;241m.\u001b[39mforward(out)\n\u001b[1;32m---> 45\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mforward(out)\n\u001b[0;32m     46\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_main \u001b[38;5;241m=\u001b[39m out\n",
      "File \u001b[1;32m~\\project\\common\\layers.py:230\u001b[0m, in \u001b[0;36mConvolution.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    227\u001b[0m out_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m((H \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad \u001b[38;5;241m-\u001b[39m FH) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n\u001b[0;32m    228\u001b[0m out_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m((W \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad \u001b[38;5;241m-\u001b[39m FW) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride)\n\u001b[1;32m--> 230\u001b[0m col \u001b[38;5;241m=\u001b[39m im2col(x, FH, FW, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad)\n\u001b[0;32m    231\u001b[0m col_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mreshape(FN, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    233\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(col, col_W) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb\n",
      "File \u001b[1;32m~\\project\\common\\util.py:67\u001b[0m, in \u001b[0;36mim2col\u001b[1;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     64\u001b[0m         x_max \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m stride\u001b[38;5;241m*\u001b[39mout_w\n\u001b[0;32m     65\u001b[0m         col[:, :, y, x, :, :] \u001b[38;5;241m=\u001b[39m img[:, :, y:y_max:stride, x:x_max:stride]\n\u001b[1;32m---> 67\u001b[0m col \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(N\u001b[38;5;241m*\u001b[39mout_h\u001b[38;5;241m*\u001b[39mout_w, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m col\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 36.0 MiB for an array with shape (64, 16, 16, 32, 3, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "model = ResNet20()\n",
    "trainer = Trainer(model, (x_train, y_train), (x_test, y_test), epochs=30, optimizer_name='adam', lr=0.001)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"resnet_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ce491-32a9-4c75-889a-9ea18ca150e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c05202-a41b-4f8b-834b-a00e8edf75b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea039c8e-3764-4524-8a3d-ac15ec252407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from common.optimizer import SGD, Adam\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_data, test_data, epochs=20, batch_size=64, optimizer_name='sgd', lr=0.01):\n",
    "        self.model = model\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "        self.max_iter = self.epochs * self.iter_per_epoch\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        # prepare optimizer (no param in constructor)\n",
    "        if optimizer_name == 'sgd':\n",
    "            self.optimizer = SGD(lr=lr)\n",
    "        elif optimizer_name == 'adam':\n",
    "            self.optimizer = Adam(lr=lr)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "        if hasattr(self.model.fc, 'W'):\n",
    "            param_dict['fc_W'] = self.model.fc.W\n",
    "            param_dict['fc_b'] = self.model.fc.b\n",
    "            grad_dict['fc_W'] = self.model.fc.dW\n",
    "            grad_dict['fc_b'] = self.model.fc.db\n",
    "\n",
    "        idx = 0\n",
    "        for layer in self.model.layer1 + self.model.layer2 + self.model.layer3:\n",
    "            for attr in ['conv1', 'conv2', 'shortcut']:\n",
    "                if hasattr(layer, attr):\n",
    "                    conv = getattr(layer, attr)\n",
    "                    param_dict[f'{idx}_W'] = conv.W\n",
    "                    param_dict[f'{idx}_b'] = conv.b\n",
    "                    grad_dict[f'{idx}_W'] = conv.dW\n",
    "                    grad_dict[f'{idx}_b'] = conv.db\n",
    "                    idx += 1\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        self.model.backward(self.loss_grad(x_batch, t_batch))\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            return (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            return dx / batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"[Epoch {epoch + 1}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "    \n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"  Iter {i:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "    \n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "    \n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            test_acc = self.model.accuracy(self.test_x, self.test_t)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "    \n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f} (Time: {elapsed:.2f}s)\\n\", flush=True)\n",
    "    \n",
    "            # 10 에폭마다 모델 저장\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                model_filename = f\"checkpoint_epoch_{epoch+1}.pkl\"\n",
    "                self.save_model(model_filename)\n",
    "                print(f\">>> Saved model to {model_filename}\\n\", flush=True)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename, loss=self.train_loss_list, train_acc=self.train_acc_list, test_acc=self.test_acc_list)\n",
    "\n",
    "    def save_model(self, filename='model_and_opt.pkl'):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "        optimizer_state = self.optimizer.__dict__\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump({'model': model_state, 'optimizer': optimizer_state}, f)\n",
    "\n",
    "    def load_model(self, filename='model_and_opt.pkl'):\n",
    "        with open(filename, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        for k in params:\n",
    "            params[k][...] = state['model'][k]\n",
    "\n",
    "        self.optimizer.__dict__.update(state['optimizer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338a26e-3921-44ad-be8c-4085f50c58b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1]\n",
      "  Iter   0/1406: Loss 5.4615\n",
      "  Iter  10/1406: Loss 5.5349\n",
      "  Iter  20/1406: Loss 5.0462\n",
      "  Iter  30/1406: Loss 4.7900\n",
      "  Iter  40/1406: Loss 4.9043\n",
      "  Iter  50/1406: Loss 4.6091\n",
      "  Iter  60/1406: Loss 4.7827\n",
      "  Iter  70/1406: Loss 4.6386\n",
      "  Iter  80/1406: Loss 4.5744\n",
      "  Iter  90/1406: Loss 4.6074\n",
      "  Iter 100/1406: Loss 4.5606\n",
      "  Iter 110/1406: Loss 4.6323\n",
      "  Iter 120/1406: Loss 4.4650\n",
      "  Iter 130/1406: Loss 4.5583\n",
      "  Iter 140/1406: Loss 4.8702\n",
      "  Iter 150/1406: Loss 4.4570\n",
      "  Iter 160/1406: Loss 4.5190\n",
      "  Iter 170/1406: Loss 4.5044\n",
      "  Iter 180/1406: Loss 4.4036\n",
      "  Iter 190/1406: Loss 4.5127\n",
      "  Iter 200/1406: Loss 4.3210\n",
      "  Iter 210/1406: Loss 4.6055\n",
      "  Iter 220/1406: Loss 4.1428\n",
      "  Iter 230/1406: Loss 4.5418\n",
      "  Iter 240/1406: Loss 4.3695\n",
      "  Iter 250/1406: Loss 4.4769\n",
      "  Iter 260/1406: Loss 4.3187\n",
      "  Iter 270/1406: Loss 4.2911\n",
      "  Iter 280/1406: Loss 4.2545\n",
      "  Iter 290/1406: Loss 4.2445\n",
      "  Iter 300/1406: Loss 4.3788\n",
      "  Iter 310/1406: Loss 4.1878\n",
      "  Iter 320/1406: Loss 4.2775\n",
      "  Iter 330/1406: Loss 4.3252\n",
      "  Iter 340/1406: Loss 4.3125\n",
      "  Iter 350/1406: Loss 4.2665\n",
      "  Iter 360/1406: Loss 4.4824\n",
      "  Iter 370/1406: Loss 4.4769\n",
      "  Iter 380/1406: Loss 4.3808\n",
      "  Iter 390/1406: Loss 4.3959\n",
      "  Iter 400/1406: Loss 4.1621\n",
      "  Iter 410/1406: Loss 3.9908\n",
      "  Iter 420/1406: Loss 4.2840\n",
      "  Iter 430/1406: Loss 4.1914\n",
      "  Iter 440/1406: Loss 4.2572\n",
      "  Iter 450/1406: Loss 4.2103\n",
      "  Iter 460/1406: Loss 4.0499\n",
      "  Iter 470/1406: Loss 4.1313\n",
      "  Iter 480/1406: Loss 4.2282\n",
      "  Iter 490/1406: Loss 4.0487\n",
      "  Iter 500/1406: Loss 4.0359\n",
      "  Iter 510/1406: Loss 4.1640\n",
      "  Iter 520/1406: Loss 4.1571\n",
      "  Iter 530/1406: Loss 4.1517\n",
      "  Iter 540/1406: Loss 4.2342\n",
      "  Iter 550/1406: Loss 3.9745\n",
      "  Iter 560/1406: Loss 4.2212\n",
      "  Iter 570/1406: Loss 4.1457\n",
      "  Iter 580/1406: Loss 4.1907\n",
      "  Iter 590/1406: Loss 3.8941\n",
      "  Iter 600/1406: Loss 4.0065\n",
      "  Iter 610/1406: Loss 4.4095\n",
      "  Iter 620/1406: Loss 4.3271\n",
      "  Iter 630/1406: Loss 3.9834\n",
      "  Iter 640/1406: Loss 4.1523\n",
      "  Iter 650/1406: Loss 4.0693\n",
      "  Iter 660/1406: Loss 3.8432\n",
      "  Iter 670/1406: Loss 4.2294\n",
      "  Iter 680/1406: Loss 4.1896\n",
      "  Iter 690/1406: Loss 4.3656\n",
      "  Iter 700/1406: Loss 4.1114\n",
      "  Iter 710/1406: Loss 3.9613\n",
      "  Iter 720/1406: Loss 4.3863\n",
      "  Iter 730/1406: Loss 3.9439\n",
      "  Iter 740/1406: Loss 3.9905\n",
      "  Iter 750/1406: Loss 3.9894\n",
      "  Iter 760/1406: Loss 4.0324\n",
      "  Iter 770/1406: Loss 4.0032\n",
      "  Iter 780/1406: Loss 4.2044\n",
      "  Iter 790/1406: Loss 4.1894\n",
      "  Iter 800/1406: Loss 4.0570\n",
      "  Iter 810/1406: Loss 4.0655\n",
      "  Iter 820/1406: Loss 4.1965\n",
      "  Iter 830/1406: Loss 3.9402\n",
      "  Iter 840/1406: Loss 4.2143\n",
      "  Iter 850/1406: Loss 3.9169\n",
      "  Iter 860/1406: Loss 3.6982\n",
      "  Iter 870/1406: Loss 4.1187\n",
      "  Iter 880/1406: Loss 4.1747\n",
      "  Iter 890/1406: Loss 4.2547\n",
      "  Iter 900/1406: Loss 3.9485\n",
      "  Iter 910/1406: Loss 3.9674\n",
      "  Iter 920/1406: Loss 4.1057\n",
      "  Iter 930/1406: Loss 4.1107\n",
      "  Iter 940/1406: Loss 3.7492\n",
      "  Iter 950/1406: Loss 3.9324\n",
      "  Iter 960/1406: Loss 4.0341\n",
      "  Iter 970/1406: Loss 3.7583\n",
      "  Iter 980/1406: Loss 3.6638\n",
      "  Iter 990/1406: Loss 4.0904\n",
      "  Iter 1000/1406: Loss 4.0108\n",
      "  Iter 1010/1406: Loss 3.9158\n",
      "  Iter 1020/1406: Loss 4.1892\n",
      "  Iter 1030/1406: Loss 3.9309\n",
      "  Iter 1040/1406: Loss 3.7790\n",
      "  Iter 1050/1406: Loss 4.0301\n",
      "  Iter 1060/1406: Loss 4.1081\n",
      "  Iter 1070/1406: Loss 3.5813\n",
      "  Iter 1080/1406: Loss 3.8177\n",
      "  Iter 1090/1406: Loss 3.8274\n",
      "  Iter 1100/1406: Loss 3.7194\n",
      "  Iter 1110/1406: Loss 3.8914\n",
      "  Iter 1120/1406: Loss 4.0332\n",
      "  Iter 1130/1406: Loss 3.8271\n",
      "  Iter 1140/1406: Loss 3.3045\n",
      "  Iter 1150/1406: Loss 3.8114\n",
      "  Iter 1160/1406: Loss 3.8462\n",
      "  Iter 1170/1406: Loss 3.4268\n",
      "  Iter 1180/1406: Loss 3.5154\n",
      "  Iter 1190/1406: Loss 3.7690\n",
      "  Iter 1200/1406: Loss 3.6949\n",
      "  Iter 1210/1406: Loss 3.9453\n",
      "  Iter 1220/1406: Loss 3.6621\n",
      "  Iter 1230/1406: Loss 3.5320\n",
      "  Iter 1240/1406: Loss 3.8175\n",
      "  Iter 1250/1406: Loss 3.5665\n",
      "  Iter 1260/1406: Loss 3.8373\n",
      "  Iter 1270/1406: Loss 3.3872\n",
      "  Iter 1280/1406: Loss 3.6076\n",
      "  Iter 1290/1406: Loss 3.6049\n",
      "  Iter 1300/1406: Loss 3.7235\n",
      "  Iter 1310/1406: Loss 3.9103\n",
      "  Iter 1320/1406: Loss 3.8782\n",
      "  Iter 1330/1406: Loss 3.8654\n",
      "  Iter 1340/1406: Loss 3.6160\n",
      "  Iter 1350/1406: Loss 3.5430\n",
      "  Iter 1360/1406: Loss 3.7659\n",
      "  Iter 1370/1406: Loss 3.8803\n",
      "  Iter 1380/1406: Loss 3.6276\n",
      "  Iter 1390/1406: Loss 3.8421\n",
      "  Iter 1400/1406: Loss 3.2881\n",
      "Train acc: 0.1200, Test acc: 0.1289 (Time: 2750.08s)\n",
      "\n",
      "[Epoch 2]\n",
      "  Iter   0/1406: Loss 3.4615\n",
      "  Iter  10/1406: Loss 3.6573\n",
      "  Iter  20/1406: Loss 3.8719\n",
      "  Iter  30/1406: Loss 3.7578\n",
      "  Iter  40/1406: Loss 3.5190\n",
      "  Iter  50/1406: Loss 3.7299\n",
      "  Iter  60/1406: Loss 4.0020\n",
      "  Iter  70/1406: Loss 3.5902\n",
      "  Iter  80/1406: Loss 3.6747\n",
      "  Iter  90/1406: Loss 4.0325\n",
      "  Iter 100/1406: Loss 3.5457\n",
      "  Iter 110/1406: Loss 3.8296\n",
      "  Iter 120/1406: Loss 3.3947\n",
      "  Iter 130/1406: Loss 3.5329\n",
      "  Iter 140/1406: Loss 3.7384\n",
      "  Iter 150/1406: Loss 3.8180\n",
      "  Iter 160/1406: Loss 3.5233\n",
      "  Iter 170/1406: Loss 3.4576\n",
      "  Iter 180/1406: Loss 3.5231\n",
      "  Iter 190/1406: Loss 3.8353\n",
      "  Iter 200/1406: Loss 3.4634\n",
      "  Iter 210/1406: Loss 3.7577\n",
      "  Iter 220/1406: Loss 4.1106\n",
      "  Iter 230/1406: Loss 3.4497\n",
      "  Iter 240/1406: Loss 3.6502\n",
      "  Iter 250/1406: Loss 3.2633\n",
      "  Iter 260/1406: Loss 3.5469\n",
      "  Iter 270/1406: Loss 3.6099\n",
      "  Iter 280/1406: Loss 3.5590\n",
      "  Iter 290/1406: Loss 3.7320\n",
      "  Iter 300/1406: Loss 3.4225\n",
      "  Iter 310/1406: Loss 3.6334\n",
      "  Iter 320/1406: Loss 3.3956\n",
      "  Iter 330/1406: Loss 3.5052\n",
      "  Iter 340/1406: Loss 3.7833\n",
      "  Iter 350/1406: Loss 3.3309\n",
      "  Iter 360/1406: Loss 3.6516\n",
      "  Iter 370/1406: Loss 3.3465\n",
      "  Iter 380/1406: Loss 3.9653\n",
      "  Iter 390/1406: Loss 3.4896\n",
      "  Iter 400/1406: Loss 3.7428\n",
      "  Iter 410/1406: Loss 3.4714\n",
      "  Iter 420/1406: Loss 3.1822\n",
      "  Iter 430/1406: Loss 3.8055\n",
      "  Iter 440/1406: Loss 3.5985\n",
      "  Iter 450/1406: Loss 3.4593\n",
      "  Iter 460/1406: Loss 3.8570\n",
      "  Iter 470/1406: Loss 3.8171\n",
      "  Iter 480/1406: Loss 3.4391\n",
      "  Iter 490/1406: Loss 3.5641\n",
      "  Iter 500/1406: Loss 3.5837\n",
      "  Iter 510/1406: Loss 3.2179\n",
      "  Iter 520/1406: Loss 3.9791\n",
      "  Iter 530/1406: Loss 3.6671\n",
      "  Iter 540/1406: Loss 3.1769\n",
      "  Iter 550/1406: Loss 3.4559\n",
      "  Iter 560/1406: Loss 3.8643\n",
      "  Iter 570/1406: Loss 3.6435\n",
      "  Iter 580/1406: Loss 4.0553\n",
      "  Iter 590/1406: Loss 3.4772\n",
      "  Iter 600/1406: Loss 3.2487\n",
      "  Iter 610/1406: Loss 3.4106\n",
      "  Iter 620/1406: Loss 3.2437\n",
      "  Iter 630/1406: Loss 3.6724\n",
      "  Iter 640/1406: Loss 3.3364\n",
      "  Iter 650/1406: Loss 3.1053\n",
      "  Iter 660/1406: Loss 3.5570\n",
      "  Iter 670/1406: Loss 3.7585\n",
      "  Iter 680/1406: Loss 3.6389\n",
      "  Iter 690/1406: Loss 3.8043\n",
      "  Iter 700/1406: Loss 4.0353\n",
      "  Iter 710/1406: Loss 3.3333\n",
      "  Iter 720/1406: Loss 4.0798\n",
      "  Iter 730/1406: Loss 3.6829\n",
      "  Iter 740/1406: Loss 3.4554\n",
      "  Iter 750/1406: Loss 3.6457\n",
      "  Iter 760/1406: Loss 3.8842\n",
      "  Iter 770/1406: Loss 3.4758\n",
      "  Iter 780/1406: Loss 3.7567\n",
      "  Iter 790/1406: Loss 3.4260\n",
      "  Iter 800/1406: Loss 3.2568\n",
      "  Iter 810/1406: Loss 3.3236\n",
      "  Iter 820/1406: Loss 3.3573\n",
      "  Iter 830/1406: Loss 3.1550\n",
      "  Iter 840/1406: Loss 3.2128\n",
      "  Iter 850/1406: Loss 3.4254\n",
      "  Iter 860/1406: Loss 3.1056\n",
      "  Iter 870/1406: Loss 3.4229\n",
      "  Iter 880/1406: Loss 3.3023\n",
      "  Iter 890/1406: Loss 3.1520\n",
      "  Iter 900/1406: Loss 3.1107\n",
      "  Iter 910/1406: Loss 3.9765\n",
      "  Iter 920/1406: Loss 3.3559\n",
      "  Iter 930/1406: Loss 3.6397\n",
      "  Iter 940/1406: Loss 3.2770\n",
      "  Iter 950/1406: Loss 3.7082\n",
      "  Iter 960/1406: Loss 3.1410\n",
      "  Iter 970/1406: Loss 3.8116\n",
      "  Iter 980/1406: Loss 3.8404\n",
      "  Iter 990/1406: Loss 3.2312\n",
      "  Iter 1000/1406: Loss 3.7334\n",
      "  Iter 1010/1406: Loss 3.8037\n",
      "  Iter 1020/1406: Loss 3.5338\n",
      "  Iter 1030/1406: Loss 3.3785\n",
      "  Iter 1040/1406: Loss 3.3198\n",
      "  Iter 1050/1406: Loss 3.4625\n",
      "  Iter 1060/1406: Loss 3.6438\n",
      "  Iter 1070/1406: Loss 3.2863\n",
      "  Iter 1080/1406: Loss 3.6240\n",
      "  Iter 1090/1406: Loss 3.4479\n",
      "  Iter 1100/1406: Loss 3.3861\n",
      "  Iter 1110/1406: Loss 3.1894\n",
      "  Iter 1120/1406: Loss 3.1510\n",
      "  Iter 1130/1406: Loss 3.2078\n",
      "  Iter 1140/1406: Loss 3.4945\n",
      "  Iter 1150/1406: Loss 3.2918\n",
      "  Iter 1160/1406: Loss 2.9855\n",
      "  Iter 1170/1406: Loss 3.6130\n",
      "  Iter 1180/1406: Loss 3.4910\n",
      "  Iter 1190/1406: Loss 3.2602\n",
      "  Iter 1200/1406: Loss 3.6288\n",
      "  Iter 1210/1406: Loss 3.1366\n",
      "  Iter 1220/1406: Loss 3.0483\n",
      "  Iter 1230/1406: Loss 3.4270\n",
      "  Iter 1240/1406: Loss 3.2361\n",
      "  Iter 1250/1406: Loss 3.8178\n",
      "  Iter 1260/1406: Loss 3.1991\n",
      "  Iter 1270/1406: Loss 3.2517\n",
      "  Iter 1280/1406: Loss 3.3182\n",
      "  Iter 1290/1406: Loss 3.4152\n",
      "  Iter 1300/1406: Loss 3.1287\n",
      "  Iter 1310/1406: Loss 3.5021\n",
      "  Iter 1320/1406: Loss 3.5577\n",
      "  Iter 1330/1406: Loss 3.3002\n",
      "  Iter 1340/1406: Loss 3.4331\n",
      "  Iter 1350/1406: Loss 3.3347\n",
      "  Iter 1360/1406: Loss 3.6087\n",
      "  Iter 1370/1406: Loss 3.2316\n",
      "  Iter 1380/1406: Loss 3.5818\n",
      "  Iter 1390/1406: Loss 3.4825\n",
      "  Iter 1400/1406: Loss 3.3084\n",
      "Train acc: 0.1840, Test acc: 0.1875 (Time: 2719.82s)\n",
      "\n",
      "[Epoch 3]\n",
      "  Iter   0/1406: Loss 2.9930\n",
      "  Iter  10/1406: Loss 3.9036\n",
      "  Iter  20/1406: Loss 3.1660\n",
      "  Iter  30/1406: Loss 3.0761\n",
      "  Iter  40/1406: Loss 3.1354\n",
      "  Iter  50/1406: Loss 3.2678\n",
      "  Iter  60/1406: Loss 3.3201\n",
      "  Iter  70/1406: Loss 3.6391\n",
      "  Iter  80/1406: Loss 3.1936\n",
      "  Iter  90/1406: Loss 3.7491\n",
      "  Iter 100/1406: Loss 3.2471\n",
      "  Iter 110/1406: Loss 3.1649\n",
      "  Iter 120/1406: Loss 3.5876\n",
      "  Iter 130/1406: Loss 3.2988\n",
      "  Iter 140/1406: Loss 3.6211\n",
      "  Iter 150/1406: Loss 3.2419\n",
      "  Iter 160/1406: Loss 2.6290\n",
      "  Iter 170/1406: Loss 3.0007\n",
      "  Iter 180/1406: Loss 3.5129\n",
      "  Iter 190/1406: Loss 3.2428\n",
      "  Iter 200/1406: Loss 3.3575\n",
      "  Iter 210/1406: Loss 3.6742\n",
      "  Iter 220/1406: Loss 3.2708\n",
      "  Iter 230/1406: Loss 3.8735\n",
      "  Iter 240/1406: Loss 3.1706\n",
      "  Iter 250/1406: Loss 3.3580\n",
      "  Iter 260/1406: Loss 3.2638\n",
      "  Iter 270/1406: Loss 3.4016\n",
      "  Iter 280/1406: Loss 3.2667\n",
      "  Iter 290/1406: Loss 3.1069\n",
      "  Iter 300/1406: Loss 3.3194\n",
      "  Iter 310/1406: Loss 3.0812\n",
      "  Iter 320/1406: Loss 3.4125\n",
      "  Iter 330/1406: Loss 3.1886\n",
      "  Iter 340/1406: Loss 3.3940\n",
      "  Iter 350/1406: Loss 3.6041\n",
      "  Iter 360/1406: Loss 2.9074\n",
      "  Iter 370/1406: Loss 3.4011\n",
      "  Iter 380/1406: Loss 3.2083\n",
      "  Iter 390/1406: Loss 3.4616\n",
      "  Iter 400/1406: Loss 3.1280\n",
      "  Iter 410/1406: Loss 3.2183\n",
      "  Iter 420/1406: Loss 3.1371\n",
      "  Iter 430/1406: Loss 3.3407\n",
      "  Iter 440/1406: Loss 3.6003\n",
      "  Iter 450/1406: Loss 2.7801\n",
      "  Iter 460/1406: Loss 3.1927\n",
      "  Iter 470/1406: Loss 3.1766\n",
      "  Iter 480/1406: Loss 3.2628\n",
      "  Iter 490/1406: Loss 3.5206\n",
      "  Iter 500/1406: Loss 3.2688\n",
      "  Iter 510/1406: Loss 3.1289\n",
      "  Iter 520/1406: Loss 3.2377\n",
      "  Iter 530/1406: Loss 2.7577\n",
      "  Iter 540/1406: Loss 3.3162\n",
      "  Iter 550/1406: Loss 3.6133\n",
      "  Iter 560/1406: Loss 3.0712\n",
      "  Iter 570/1406: Loss 2.8531\n",
      "  Iter 580/1406: Loss 3.2296\n",
      "  Iter 590/1406: Loss 2.9345\n",
      "  Iter 600/1406: Loss 2.9126\n",
      "  Iter 610/1406: Loss 3.0093\n",
      "  Iter 620/1406: Loss 3.1318\n",
      "  Iter 630/1406: Loss 2.8537\n",
      "  Iter 640/1406: Loss 3.3764\n",
      "  Iter 650/1406: Loss 3.3890\n",
      "  Iter 660/1406: Loss 3.1845\n",
      "  Iter 670/1406: Loss 3.3663\n",
      "  Iter 680/1406: Loss 3.6062\n",
      "  Iter 690/1406: Loss 3.0450\n",
      "  Iter 700/1406: Loss 3.1142\n",
      "  Iter 710/1406: Loss 3.1259\n",
      "  Iter 720/1406: Loss 2.9789\n",
      "  Iter 730/1406: Loss 3.4403\n",
      "  Iter 740/1406: Loss 3.1970\n",
      "  Iter 750/1406: Loss 3.2801\n",
      "  Iter 760/1406: Loss 3.1293\n",
      "  Iter 770/1406: Loss 3.5058\n",
      "  Iter 780/1406: Loss 4.0355\n",
      "  Iter 790/1406: Loss 3.0328\n",
      "  Iter 800/1406: Loss 3.3103\n",
      "  Iter 810/1406: Loss 3.1239\n",
      "  Iter 820/1406: Loss 3.2917\n",
      "  Iter 830/1406: Loss 3.3228\n",
      "  Iter 840/1406: Loss 2.8673\n",
      "  Iter 850/1406: Loss 3.2624\n",
      "  Iter 860/1406: Loss 3.0516\n",
      "  Iter 870/1406: Loss 3.4522\n",
      "  Iter 880/1406: Loss 3.2854\n",
      "  Iter 890/1406: Loss 3.6053\n",
      "  Iter 900/1406: Loss 3.2854\n",
      "  Iter 910/1406: Loss 3.1491\n",
      "  Iter 920/1406: Loss 3.1557\n",
      "  Iter 930/1406: Loss 3.3402\n",
      "  Iter 940/1406: Loss 3.1198\n",
      "  Iter 950/1406: Loss 3.0536\n",
      "  Iter 960/1406: Loss 2.9418\n",
      "  Iter 970/1406: Loss 3.4228\n",
      "  Iter 980/1406: Loss 3.3793\n",
      "  Iter 990/1406: Loss 3.5405\n",
      "  Iter 1000/1406: Loss 2.7304\n",
      "  Iter 1010/1406: Loss 2.9572\n",
      "  Iter 1020/1406: Loss 2.8108\n",
      "  Iter 1030/1406: Loss 2.8094\n",
      "  Iter 1040/1406: Loss 3.5436\n",
      "  Iter 1050/1406: Loss 2.6141\n",
      "  Iter 1060/1406: Loss 3.3994\n",
      "  Iter 1070/1406: Loss 3.1660\n",
      "  Iter 1080/1406: Loss 3.0912\n",
      "  Iter 1090/1406: Loss 2.8762\n",
      "  Iter 1100/1406: Loss 3.3765\n",
      "  Iter 1110/1406: Loss 3.8578\n",
      "  Iter 1120/1406: Loss 3.1960\n",
      "  Iter 1130/1406: Loss 3.0275\n",
      "  Iter 1140/1406: Loss 3.3129\n",
      "  Iter 1150/1406: Loss 3.1352\n",
      "  Iter 1160/1406: Loss 3.1691\n",
      "  Iter 1170/1406: Loss 2.7241\n",
      "  Iter 1180/1406: Loss 3.1539\n",
      "  Iter 1190/1406: Loss 2.6943\n",
      "  Iter 1200/1406: Loss 3.3618\n",
      "  Iter 1210/1406: Loss 3.1926\n",
      "  Iter 1220/1406: Loss 2.9798\n",
      "  Iter 1230/1406: Loss 3.2840\n",
      "  Iter 1240/1406: Loss 2.9811\n",
      "  Iter 1250/1406: Loss 3.4338\n",
      "  Iter 1260/1406: Loss 3.3152\n",
      "  Iter 1270/1406: Loss 2.8991\n",
      "  Iter 1280/1406: Loss 3.0655\n",
      "  Iter 1290/1406: Loss 2.8858\n",
      "  Iter 1300/1406: Loss 3.5280\n",
      "  Iter 1310/1406: Loss 3.0866\n",
      "  Iter 1320/1406: Loss 3.3966\n",
      "  Iter 1330/1406: Loss 3.3910\n",
      "  Iter 1340/1406: Loss 3.1039\n",
      "  Iter 1350/1406: Loss 3.6231\n",
      "  Iter 1360/1406: Loss 3.2034\n",
      "  Iter 1370/1406: Loss 3.0341\n",
      "  Iter 1380/1406: Loss 3.2065\n",
      "  Iter 1390/1406: Loss 2.6405\n",
      "  Iter 1400/1406: Loss 3.5863\n",
      "Train acc: 0.2640, Test acc: 0.2276 (Time: 2710.49s)\n",
      "\n",
      "[Epoch 4]\n",
      "  Iter   0/1406: Loss 3.1198\n",
      "  Iter  10/1406: Loss 3.1594\n",
      "  Iter  20/1406: Loss 2.9452\n",
      "  Iter  30/1406: Loss 3.3267\n",
      "  Iter  40/1406: Loss 3.0429\n",
      "  Iter  50/1406: Loss 3.0022\n",
      "  Iter  60/1406: Loss 3.6448\n",
      "  Iter  70/1406: Loss 2.8906\n",
      "  Iter  80/1406: Loss 3.4369\n",
      "  Iter  90/1406: Loss 3.1265\n",
      "  Iter 100/1406: Loss 3.6723\n",
      "  Iter 110/1406: Loss 3.3971\n",
      "  Iter 120/1406: Loss 2.9402\n",
      "  Iter 130/1406: Loss 2.9266\n",
      "  Iter 140/1406: Loss 3.0230\n",
      "  Iter 150/1406: Loss 2.7557\n",
      "  Iter 160/1406: Loss 3.2961\n",
      "  Iter 170/1406: Loss 3.2800\n",
      "  Iter 180/1406: Loss 2.8660\n",
      "  Iter 190/1406: Loss 3.0057\n",
      "  Iter 200/1406: Loss 2.7546\n",
      "  Iter 210/1406: Loss 3.4227\n",
      "  Iter 220/1406: Loss 3.0771\n",
      "  Iter 230/1406: Loss 2.8468\n",
      "  Iter 240/1406: Loss 2.8936\n",
      "  Iter 250/1406: Loss 3.3135\n",
      "  Iter 260/1406: Loss 2.8104\n",
      "  Iter 270/1406: Loss 2.8551\n",
      "  Iter 280/1406: Loss 2.9426\n",
      "  Iter 290/1406: Loss 3.1034\n",
      "  Iter 300/1406: Loss 2.7936\n",
      "  Iter 310/1406: Loss 3.2278\n",
      "  Iter 320/1406: Loss 3.2895\n",
      "  Iter 330/1406: Loss 2.6421\n",
      "  Iter 340/1406: Loss 3.0460\n",
      "  Iter 350/1406: Loss 3.0665\n",
      "  Iter 360/1406: Loss 3.1483\n",
      "  Iter 370/1406: Loss 3.7176\n",
      "  Iter 380/1406: Loss 3.0835\n",
      "  Iter 390/1406: Loss 2.7327\n",
      "  Iter 400/1406: Loss 3.3824\n",
      "  Iter 410/1406: Loss 2.7535\n",
      "  Iter 420/1406: Loss 2.8253\n",
      "  Iter 430/1406: Loss 3.0379\n",
      "  Iter 440/1406: Loss 2.8182\n",
      "  Iter 450/1406: Loss 3.2177\n",
      "  Iter 460/1406: Loss 3.1304\n",
      "  Iter 470/1406: Loss 3.2554\n",
      "  Iter 480/1406: Loss 2.5101\n",
      "  Iter 490/1406: Loss 3.2702\n",
      "  Iter 500/1406: Loss 3.1109\n",
      "  Iter 510/1406: Loss 3.1238\n",
      "  Iter 520/1406: Loss 3.4244\n",
      "  Iter 530/1406: Loss 2.8073\n",
      "  Iter 540/1406: Loss 3.2443\n",
      "  Iter 550/1406: Loss 2.9741\n",
      "  Iter 560/1406: Loss 2.7405\n",
      "  Iter 570/1406: Loss 3.1126\n",
      "  Iter 580/1406: Loss 2.8462\n",
      "  Iter 590/1406: Loss 3.2223\n",
      "  Iter 600/1406: Loss 3.6004\n",
      "  Iter 610/1406: Loss 2.7835\n",
      "  Iter 620/1406: Loss 2.6810\n",
      "  Iter 630/1406: Loss 3.1416\n",
      "  Iter 640/1406: Loss 3.0557\n",
      "  Iter 650/1406: Loss 3.1849\n",
      "  Iter 660/1406: Loss 2.8648\n",
      "  Iter 670/1406: Loss 3.6416\n",
      "  Iter 680/1406: Loss 3.4466\n",
      "  Iter 690/1406: Loss 3.2655\n",
      "  Iter 700/1406: Loss 3.5171\n",
      "  Iter 710/1406: Loss 2.7739\n",
      "  Iter 720/1406: Loss 3.1228\n",
      "  Iter 730/1406: Loss 3.2380\n",
      "  Iter 740/1406: Loss 3.5132\n",
      "  Iter 750/1406: Loss 2.8394\n",
      "  Iter 760/1406: Loss 3.4068\n",
      "  Iter 770/1406: Loss 2.9325\n",
      "  Iter 780/1406: Loss 3.2442\n",
      "  Iter 790/1406: Loss 2.8928\n",
      "  Iter 800/1406: Loss 2.7172\n",
      "  Iter 810/1406: Loss 3.3311\n",
      "  Iter 820/1406: Loss 2.8246\n",
      "  Iter 830/1406: Loss 3.1888\n",
      "  Iter 840/1406: Loss 2.8261\n",
      "  Iter 850/1406: Loss 2.9808\n",
      "  Iter 860/1406: Loss 3.3556\n",
      "  Iter 870/1406: Loss 2.8155\n",
      "  Iter 880/1406: Loss 3.0301\n",
      "  Iter 890/1406: Loss 3.1726\n",
      "  Iter 900/1406: Loss 3.2239\n",
      "  Iter 910/1406: Loss 3.0106\n",
      "  Iter 920/1406: Loss 2.9174\n",
      "  Iter 930/1406: Loss 2.9440\n",
      "  Iter 940/1406: Loss 2.9447\n",
      "  Iter 950/1406: Loss 2.7944\n",
      "  Iter 960/1406: Loss 3.0607\n",
      "  Iter 970/1406: Loss 2.8596\n",
      "  Iter 980/1406: Loss 3.1189\n",
      "  Iter 990/1406: Loss 3.1779\n",
      "  Iter 1000/1406: Loss 3.0252\n",
      "  Iter 1010/1406: Loss 2.9231\n",
      "  Iter 1020/1406: Loss 2.7872\n",
      "  Iter 1030/1406: Loss 2.8030\n",
      "  Iter 1040/1406: Loss 3.0758\n",
      "  Iter 1050/1406: Loss 3.3750\n",
      "  Iter 1060/1406: Loss 3.1739\n",
      "  Iter 1070/1406: Loss 2.5249\n",
      "  Iter 1080/1406: Loss 2.4007\n",
      "  Iter 1090/1406: Loss 2.9675\n",
      "  Iter 1100/1406: Loss 2.8709\n",
      "  Iter 1110/1406: Loss 2.7916\n",
      "  Iter 1120/1406: Loss 2.8669\n",
      "  Iter 1130/1406: Loss 3.1656\n",
      "  Iter 1140/1406: Loss 3.1478\n",
      "  Iter 1150/1406: Loss 2.6671\n",
      "  Iter 1160/1406: Loss 3.0401\n",
      "  Iter 1170/1406: Loss 3.1600\n",
      "  Iter 1180/1406: Loss 3.2366\n",
      "  Iter 1190/1406: Loss 2.5799\n",
      "  Iter 1200/1406: Loss 3.0301\n",
      "  Iter 1210/1406: Loss 3.1940\n",
      "  Iter 1220/1406: Loss 3.1851\n",
      "  Iter 1230/1406: Loss 3.1782\n",
      "  Iter 1240/1406: Loss 2.8446\n",
      "  Iter 1250/1406: Loss 3.1028\n",
      "  Iter 1260/1406: Loss 2.8650\n",
      "  Iter 1270/1406: Loss 3.1925\n",
      "  Iter 1280/1406: Loss 2.9193\n",
      "  Iter 1290/1406: Loss 3.1842\n",
      "  Iter 1300/1406: Loss 2.6334\n",
      "  Iter 1310/1406: Loss 2.6988\n",
      "  Iter 1320/1406: Loss 3.3702\n",
      "  Iter 1330/1406: Loss 2.3272\n",
      "  Iter 1340/1406: Loss 2.7760\n",
      "  Iter 1350/1406: Loss 2.5777\n",
      "  Iter 1360/1406: Loss 2.7005\n",
      "  Iter 1370/1406: Loss 2.7974\n",
      "  Iter 1380/1406: Loss 2.4347\n",
      "  Iter 1390/1406: Loss 2.8363\n",
      "  Iter 1400/1406: Loss 3.3752\n",
      "Train acc: 0.2710, Test acc: 0.2484 (Time: 2711.60s)\n",
      "\n",
      "[Epoch 5]\n",
      "  Iter   0/1406: Loss 2.8045\n",
      "  Iter  10/1406: Loss 2.8112\n",
      "  Iter  20/1406: Loss 3.4238\n",
      "  Iter  30/1406: Loss 3.1639\n",
      "  Iter  40/1406: Loss 2.9953\n",
      "  Iter  50/1406: Loss 2.9692\n",
      "  Iter  60/1406: Loss 3.0652\n",
      "  Iter  70/1406: Loss 2.5316\n",
      "  Iter  80/1406: Loss 3.1296\n",
      "  Iter  90/1406: Loss 3.4402\n",
      "  Iter 100/1406: Loss 3.1591\n",
      "  Iter 110/1406: Loss 2.0927\n",
      "  Iter 120/1406: Loss 3.0429\n",
      "  Iter 130/1406: Loss 2.8477\n",
      "  Iter 140/1406: Loss 2.8670\n",
      "  Iter 150/1406: Loss 3.1363\n",
      "  Iter 160/1406: Loss 2.7743\n",
      "  Iter 170/1406: Loss 3.3884\n",
      "  Iter 180/1406: Loss 3.1501\n",
      "  Iter 190/1406: Loss 2.6879\n",
      "  Iter 200/1406: Loss 2.7896\n",
      "  Iter 210/1406: Loss 3.0179\n",
      "  Iter 220/1406: Loss 2.7382\n",
      "  Iter 230/1406: Loss 3.1665\n",
      "  Iter 240/1406: Loss 3.2017\n",
      "  Iter 250/1406: Loss 2.7073\n",
      "  Iter 260/1406: Loss 2.9288\n",
      "  Iter 270/1406: Loss 3.0298\n",
      "  Iter 280/1406: Loss 2.5573\n",
      "  Iter 290/1406: Loss 2.9877\n",
      "  Iter 300/1406: Loss 2.5490\n",
      "  Iter 310/1406: Loss 3.1513\n",
      "  Iter 320/1406: Loss 3.2992\n",
      "  Iter 330/1406: Loss 3.3230\n",
      "  Iter 340/1406: Loss 2.9698\n",
      "  Iter 350/1406: Loss 2.9068\n",
      "  Iter 360/1406: Loss 3.0987\n",
      "  Iter 370/1406: Loss 3.2553\n",
      "  Iter 380/1406: Loss 2.6965\n",
      "  Iter 390/1406: Loss 2.9493\n",
      "  Iter 400/1406: Loss 2.6784\n",
      "  Iter 410/1406: Loss 2.6830\n",
      "  Iter 420/1406: Loss 2.9620\n",
      "  Iter 430/1406: Loss 2.8646\n",
      "  Iter 440/1406: Loss 2.8873\n",
      "  Iter 450/1406: Loss 3.3404\n",
      "  Iter 460/1406: Loss 3.1516\n",
      "  Iter 470/1406: Loss 3.0204\n",
      "  Iter 480/1406: Loss 2.5852\n",
      "  Iter 490/1406: Loss 2.8066\n",
      "  Iter 500/1406: Loss 2.5186\n",
      "  Iter 510/1406: Loss 3.2271\n",
      "  Iter 520/1406: Loss 2.9872\n",
      "  Iter 530/1406: Loss 2.7776\n",
      "  Iter 540/1406: Loss 3.1809\n",
      "  Iter 550/1406: Loss 2.8131\n",
      "  Iter 560/1406: Loss 3.0304\n",
      "  Iter 570/1406: Loss 3.3270\n",
      "  Iter 580/1406: Loss 3.5840\n",
      "  Iter 590/1406: Loss 3.0989\n",
      "  Iter 600/1406: Loss 3.2750\n",
      "  Iter 610/1406: Loss 3.4757\n",
      "  Iter 620/1406: Loss 3.3170\n",
      "  Iter 630/1406: Loss 3.1145\n",
      "  Iter 640/1406: Loss 3.3519\n",
      "  Iter 650/1406: Loss 2.9425\n",
      "  Iter 660/1406: Loss 2.7054\n",
      "  Iter 670/1406: Loss 2.8332\n",
      "  Iter 680/1406: Loss 3.0513\n",
      "  Iter 690/1406: Loss 2.7063\n",
      "  Iter 700/1406: Loss 2.8542\n",
      "  Iter 710/1406: Loss 2.6104\n",
      "  Iter 720/1406: Loss 3.0074\n",
      "  Iter 730/1406: Loss 2.8200\n",
      "  Iter 740/1406: Loss 2.5640\n",
      "  Iter 750/1406: Loss 2.7762\n",
      "  Iter 760/1406: Loss 2.8228\n",
      "  Iter 770/1406: Loss 2.9689\n",
      "  Iter 780/1406: Loss 2.4872\n",
      "  Iter 790/1406: Loss 2.8960\n",
      "  Iter 800/1406: Loss 3.1139\n",
      "  Iter 810/1406: Loss 2.9325\n",
      "  Iter 820/1406: Loss 2.5565\n",
      "  Iter 830/1406: Loss 3.0302\n",
      "  Iter 840/1406: Loss 2.9575\n",
      "  Iter 850/1406: Loss 2.8076\n",
      "  Iter 860/1406: Loss 2.7692\n",
      "  Iter 870/1406: Loss 2.7882\n",
      "  Iter 880/1406: Loss 3.1015\n",
      "  Iter 890/1406: Loss 3.0840\n",
      "  Iter 900/1406: Loss 3.0775\n",
      "  Iter 910/1406: Loss 2.8475\n",
      "  Iter 920/1406: Loss 2.7405\n",
      "  Iter 930/1406: Loss 3.1824\n",
      "  Iter 940/1406: Loss 2.9466\n",
      "  Iter 950/1406: Loss 3.0237\n",
      "  Iter 960/1406: Loss 3.1710\n",
      "  Iter 970/1406: Loss 3.2684\n",
      "  Iter 980/1406: Loss 2.6159\n",
      "  Iter 990/1406: Loss 3.2425\n",
      "  Iter 1000/1406: Loss 3.4080\n",
      "  Iter 1010/1406: Loss 2.7514\n",
      "  Iter 1020/1406: Loss 2.8451\n",
      "  Iter 1030/1406: Loss 2.5980\n",
      "  Iter 1040/1406: Loss 2.9325\n",
      "  Iter 1050/1406: Loss 2.6280\n",
      "  Iter 1060/1406: Loss 2.6296\n",
      "  Iter 1070/1406: Loss 2.4680\n",
      "  Iter 1080/1406: Loss 2.7832\n",
      "  Iter 1090/1406: Loss 3.2634\n",
      "  Iter 1100/1406: Loss 2.8093\n",
      "  Iter 1110/1406: Loss 2.8411\n",
      "  Iter 1120/1406: Loss 2.8751\n",
      "  Iter 1130/1406: Loss 2.9835\n",
      "  Iter 1140/1406: Loss 3.0589\n",
      "  Iter 1150/1406: Loss 2.7969\n",
      "  Iter 1160/1406: Loss 2.7818\n",
      "  Iter 1170/1406: Loss 2.8095\n",
      "  Iter 1180/1406: Loss 2.7578\n",
      "  Iter 1190/1406: Loss 2.7552\n",
      "  Iter 1200/1406: Loss 2.8751\n",
      "  Iter 1210/1406: Loss 2.7624\n",
      "  Iter 1220/1406: Loss 2.9645\n",
      "  Iter 1230/1406: Loss 2.5039\n",
      "  Iter 1240/1406: Loss 2.7426\n",
      "  Iter 1250/1406: Loss 2.8831\n",
      "  Iter 1260/1406: Loss 3.0143\n",
      "  Iter 1270/1406: Loss 3.1535\n",
      "  Iter 1280/1406: Loss 3.1367\n",
      "  Iter 1290/1406: Loss 2.5998\n",
      "  Iter 1300/1406: Loss 2.2659\n",
      "  Iter 1310/1406: Loss 2.7420\n",
      "  Iter 1320/1406: Loss 2.8484\n",
      "  Iter 1330/1406: Loss 2.8211\n",
      "  Iter 1340/1406: Loss 2.3659\n",
      "  Iter 1350/1406: Loss 3.0530\n",
      "  Iter 1360/1406: Loss 2.6961\n",
      "  Iter 1370/1406: Loss 2.4978\n",
      "  Iter 1380/1406: Loss 2.6529\n",
      "  Iter 1390/1406: Loss 2.9973\n",
      "  Iter 1400/1406: Loss 2.2372\n",
      "Train acc: 0.3220, Test acc: 0.2754 (Time: 2708.68s)\n",
      "\n",
      "[Epoch 6]\n",
      "  Iter   0/1406: Loss 2.9573\n",
      "  Iter  10/1406: Loss 2.9564\n",
      "  Iter  20/1406: Loss 2.5473\n",
      "  Iter  30/1406: Loss 3.2845\n",
      "  Iter  40/1406: Loss 2.8322\n",
      "  Iter  50/1406: Loss 2.8840\n",
      "  Iter  60/1406: Loss 3.5569\n",
      "  Iter  70/1406: Loss 2.4659\n",
      "  Iter  80/1406: Loss 3.3026\n",
      "  Iter  90/1406: Loss 2.4456\n",
      "  Iter 100/1406: Loss 2.8464\n",
      "  Iter 110/1406: Loss 3.1311\n",
      "  Iter 120/1406: Loss 2.8302\n",
      "  Iter 130/1406: Loss 2.9358\n",
      "  Iter 140/1406: Loss 2.9990\n",
      "  Iter 150/1406: Loss 2.7357\n",
      "  Iter 160/1406: Loss 2.2250\n",
      "  Iter 170/1406: Loss 2.8339\n",
      "  Iter 180/1406: Loss 2.6082\n",
      "  Iter 190/1406: Loss 3.1930\n",
      "  Iter 200/1406: Loss 2.8586\n",
      "  Iter 210/1406: Loss 2.8445\n",
      "  Iter 220/1406: Loss 2.7106\n",
      "  Iter 230/1406: Loss 3.0519\n",
      "  Iter 240/1406: Loss 2.3925\n",
      "  Iter 250/1406: Loss 2.5899\n",
      "  Iter 260/1406: Loss 2.7893\n",
      "  Iter 270/1406: Loss 2.7583\n",
      "  Iter 280/1406: Loss 2.8019\n",
      "  Iter 290/1406: Loss 2.7395\n",
      "  Iter 300/1406: Loss 3.1571\n",
      "  Iter 310/1406: Loss 3.3029\n",
      "  Iter 320/1406: Loss 2.6653\n",
      "  Iter 330/1406: Loss 2.7091\n",
      "  Iter 340/1406: Loss 2.9696\n",
      "  Iter 350/1406: Loss 2.7067\n",
      "  Iter 360/1406: Loss 2.8987\n",
      "  Iter 370/1406: Loss 2.3286\n",
      "  Iter 380/1406: Loss 2.5918\n",
      "  Iter 390/1406: Loss 2.5705\n",
      "  Iter 400/1406: Loss 2.8541\n",
      "  Iter 410/1406: Loss 3.3518\n",
      "  Iter 420/1406: Loss 2.8089\n",
      "  Iter 430/1406: Loss 2.3795\n",
      "  Iter 440/1406: Loss 2.7629\n",
      "  Iter 450/1406: Loss 2.7885\n",
      "  Iter 460/1406: Loss 2.8516\n",
      "  Iter 470/1406: Loss 3.3856\n",
      "  Iter 480/1406: Loss 2.9355\n",
      "  Iter 490/1406: Loss 2.8882\n",
      "  Iter 500/1406: Loss 2.8209\n",
      "  Iter 510/1406: Loss 2.7817\n",
      "  Iter 520/1406: Loss 2.5896\n",
      "  Iter 530/1406: Loss 2.8189\n",
      "  Iter 540/1406: Loss 2.8819\n",
      "  Iter 550/1406: Loss 2.7297\n",
      "  Iter 560/1406: Loss 2.6070\n",
      "  Iter 570/1406: Loss 2.9039\n",
      "  Iter 580/1406: Loss 3.0951\n",
      "  Iter 590/1406: Loss 2.5159\n",
      "  Iter 600/1406: Loss 2.8588\n",
      "  Iter 610/1406: Loss 2.7102\n",
      "  Iter 620/1406: Loss 3.1494\n",
      "  Iter 630/1406: Loss 2.5829\n",
      "  Iter 640/1406: Loss 3.0633\n",
      "  Iter 650/1406: Loss 2.6481\n",
      "  Iter 660/1406: Loss 2.7871\n",
      "  Iter 670/1406: Loss 2.7334\n",
      "  Iter 680/1406: Loss 2.6613\n",
      "  Iter 690/1406: Loss 2.9134\n",
      "  Iter 700/1406: Loss 2.8794\n",
      "  Iter 710/1406: Loss 2.6896\n",
      "  Iter 720/1406: Loss 2.9009\n",
      "  Iter 730/1406: Loss 2.8776\n",
      "  Iter 740/1406: Loss 3.0252\n",
      "  Iter 750/1406: Loss 2.6213\n",
      "  Iter 760/1406: Loss 2.9245\n",
      "  Iter 770/1406: Loss 2.7120\n",
      "  Iter 780/1406: Loss 3.0890\n",
      "  Iter 790/1406: Loss 2.8364\n",
      "  Iter 800/1406: Loss 2.5939\n",
      "  Iter 810/1406: Loss 3.0576\n",
      "  Iter 820/1406: Loss 2.7619\n",
      "  Iter 830/1406: Loss 3.0792\n",
      "  Iter 840/1406: Loss 2.6925\n",
      "  Iter 850/1406: Loss 2.5686\n",
      "  Iter 860/1406: Loss 2.9283\n",
      "  Iter 870/1406: Loss 2.4939\n",
      "  Iter 880/1406: Loss 2.5289\n",
      "  Iter 890/1406: Loss 2.9052\n",
      "  Iter 900/1406: Loss 3.1412\n",
      "  Iter 910/1406: Loss 2.6317\n",
      "  Iter 920/1406: Loss 2.2483\n",
      "  Iter 930/1406: Loss 2.4417\n",
      "  Iter 940/1406: Loss 2.6944\n",
      "  Iter 950/1406: Loss 2.5605\n",
      "  Iter 960/1406: Loss 3.0463\n",
      "  Iter 970/1406: Loss 2.8258\n",
      "  Iter 980/1406: Loss 2.8346\n",
      "  Iter 990/1406: Loss 2.3300\n",
      "  Iter 1000/1406: Loss 2.7236\n",
      "  Iter 1010/1406: Loss 2.8997\n",
      "  Iter 1020/1406: Loss 3.3718\n",
      "  Iter 1030/1406: Loss 3.0837\n",
      "  Iter 1040/1406: Loss 2.6764\n",
      "  Iter 1050/1406: Loss 2.5173\n",
      "  Iter 1060/1406: Loss 2.8546\n",
      "  Iter 1070/1406: Loss 2.5304\n",
      "  Iter 1080/1406: Loss 2.7389\n",
      "  Iter 1090/1406: Loss 2.4764\n",
      "  Iter 1100/1406: Loss 2.7045\n",
      "  Iter 1110/1406: Loss 2.8168\n",
      "  Iter 1120/1406: Loss 2.5934\n",
      "  Iter 1130/1406: Loss 2.6372\n",
      "  Iter 1140/1406: Loss 3.1289\n",
      "  Iter 1150/1406: Loss 2.2550\n",
      "  Iter 1160/1406: Loss 2.8074\n",
      "  Iter 1170/1406: Loss 2.8571\n",
      "  Iter 1180/1406: Loss 2.5563\n",
      "  Iter 1190/1406: Loss 2.5242\n",
      "  Iter 1200/1406: Loss 2.5392\n",
      "  Iter 1210/1406: Loss 2.7518\n",
      "  Iter 1220/1406: Loss 2.5812\n",
      "  Iter 1230/1406: Loss 2.8280\n",
      "  Iter 1240/1406: Loss 2.9589\n",
      "  Iter 1250/1406: Loss 2.9129\n",
      "  Iter 1260/1406: Loss 2.8079\n",
      "  Iter 1270/1406: Loss 2.2687\n",
      "  Iter 1280/1406: Loss 2.9763\n",
      "  Iter 1290/1406: Loss 2.8472\n",
      "  Iter 1300/1406: Loss 2.5881\n",
      "  Iter 1310/1406: Loss 2.7316\n",
      "  Iter 1320/1406: Loss 2.7262\n",
      "  Iter 1330/1406: Loss 2.4456\n",
      "  Iter 1340/1406: Loss 3.2720\n",
      "  Iter 1350/1406: Loss 2.9114\n",
      "  Iter 1360/1406: Loss 2.5734\n",
      "  Iter 1370/1406: Loss 3.1518\n",
      "  Iter 1380/1406: Loss 2.3090\n",
      "  Iter 1390/1406: Loss 2.3732\n",
      "  Iter 1400/1406: Loss 3.1010\n",
      "Train acc: 0.3690, Test acc: 0.2925 (Time: 2713.68s)\n",
      "\n",
      "[Epoch 7]\n",
      "  Iter   0/1406: Loss 2.6307\n",
      "  Iter  10/1406: Loss 2.7334\n",
      "  Iter  20/1406: Loss 2.4969\n",
      "  Iter  30/1406: Loss 2.8696\n",
      "  Iter  40/1406: Loss 2.5405\n",
      "  Iter  50/1406: Loss 2.5350\n",
      "  Iter  60/1406: Loss 2.9737\n",
      "  Iter  70/1406: Loss 2.6770\n",
      "  Iter  80/1406: Loss 2.8981\n",
      "  Iter  90/1406: Loss 2.3445\n",
      "  Iter 100/1406: Loss 2.4985\n",
      "  Iter 110/1406: Loss 2.7268\n",
      "  Iter 120/1406: Loss 2.2740\n",
      "  Iter 130/1406: Loss 2.7701\n",
      "  Iter 140/1406: Loss 2.5570\n",
      "  Iter 150/1406: Loss 2.7538\n",
      "  Iter 160/1406: Loss 2.9321\n",
      "  Iter 170/1406: Loss 2.2893\n",
      "  Iter 180/1406: Loss 2.7170\n",
      "  Iter 190/1406: Loss 2.3620\n",
      "  Iter 200/1406: Loss 2.4360\n",
      "  Iter 210/1406: Loss 2.9903\n",
      "  Iter 220/1406: Loss 2.4614\n",
      "  Iter 230/1406: Loss 2.6947\n",
      "  Iter 240/1406: Loss 2.9182\n",
      "  Iter 250/1406: Loss 2.7477\n",
      "  Iter 260/1406: Loss 2.8586\n",
      "  Iter 270/1406: Loss 2.6579\n",
      "  Iter 280/1406: Loss 2.8692\n",
      "  Iter 290/1406: Loss 2.6759\n",
      "  Iter 300/1406: Loss 2.6972\n",
      "  Iter 310/1406: Loss 2.4559\n",
      "  Iter 320/1406: Loss 2.2676\n",
      "  Iter 330/1406: Loss 2.6901\n",
      "  Iter 340/1406: Loss 2.7152\n",
      "  Iter 350/1406: Loss 2.2713\n",
      "  Iter 360/1406: Loss 3.2979\n",
      "  Iter 370/1406: Loss 2.8417\n",
      "  Iter 380/1406: Loss 3.4658\n",
      "  Iter 390/1406: Loss 2.8054\n",
      "  Iter 400/1406: Loss 2.5369\n",
      "  Iter 410/1406: Loss 2.4800\n",
      "  Iter 420/1406: Loss 2.5748\n",
      "  Iter 430/1406: Loss 2.8047\n",
      "  Iter 440/1406: Loss 2.6531\n",
      "  Iter 450/1406: Loss 2.5604\n",
      "  Iter 460/1406: Loss 3.5351\n",
      "  Iter 470/1406: Loss 2.8066\n",
      "  Iter 480/1406: Loss 2.5218\n",
      "  Iter 490/1406: Loss 2.4504\n",
      "  Iter 500/1406: Loss 2.7816\n",
      "  Iter 510/1406: Loss 2.9505\n",
      "  Iter 520/1406: Loss 2.7517\n",
      "  Iter 530/1406: Loss 2.0468\n",
      "  Iter 540/1406: Loss 2.8696\n",
      "  Iter 550/1406: Loss 2.9928\n",
      "  Iter 560/1406: Loss 3.0351\n",
      "  Iter 570/1406: Loss 2.7514\n",
      "  Iter 580/1406: Loss 2.1170\n",
      "  Iter 590/1406: Loss 2.5092\n",
      "  Iter 600/1406: Loss 2.5572\n",
      "  Iter 610/1406: Loss 2.5284\n",
      "  Iter 620/1406: Loss 2.3132\n",
      "  Iter 630/1406: Loss 2.6701\n",
      "  Iter 640/1406: Loss 2.5452\n",
      "  Iter 650/1406: Loss 2.5543\n",
      "  Iter 660/1406: Loss 2.7441\n",
      "  Iter 670/1406: Loss 2.6616\n",
      "  Iter 680/1406: Loss 2.8818\n",
      "  Iter 690/1406: Loss 2.3418\n",
      "  Iter 700/1406: Loss 2.7455\n",
      "  Iter 710/1406: Loss 3.1271\n",
      "  Iter 720/1406: Loss 2.3481\n",
      "  Iter 730/1406: Loss 2.2772\n",
      "  Iter 740/1406: Loss 2.3169\n",
      "  Iter 750/1406: Loss 2.1677\n",
      "  Iter 760/1406: Loss 2.5227\n",
      "  Iter 770/1406: Loss 2.3481\n",
      "  Iter 780/1406: Loss 2.9158\n",
      "  Iter 790/1406: Loss 2.5618\n",
      "  Iter 800/1406: Loss 2.3822\n",
      "  Iter 810/1406: Loss 2.7081\n",
      "  Iter 820/1406: Loss 2.4752\n",
      "  Iter 830/1406: Loss 2.8522\n",
      "  Iter 840/1406: Loss 2.7339\n",
      "  Iter 850/1406: Loss 2.6559\n",
      "  Iter 860/1406: Loss 2.9811\n",
      "  Iter 870/1406: Loss 2.4806\n",
      "  Iter 880/1406: Loss 2.6226\n",
      "  Iter 890/1406: Loss 3.1509\n",
      "  Iter 900/1406: Loss 2.7108\n",
      "  Iter 910/1406: Loss 3.0263\n",
      "  Iter 920/1406: Loss 2.4928\n",
      "  Iter 930/1406: Loss 2.9001\n",
      "  Iter 940/1406: Loss 2.5071\n",
      "  Iter 950/1406: Loss 2.8755\n",
      "  Iter 960/1406: Loss 2.3276\n",
      "  Iter 970/1406: Loss 2.1848\n",
      "  Iter 980/1406: Loss 2.8147\n",
      "  Iter 990/1406: Loss 2.5089\n",
      "  Iter 1000/1406: Loss 2.6956\n",
      "  Iter 1010/1406: Loss 2.8151\n",
      "  Iter 1020/1406: Loss 2.3222\n",
      "  Iter 1030/1406: Loss 2.3668\n",
      "  Iter 1040/1406: Loss 2.6460\n",
      "  Iter 1050/1406: Loss 2.3692\n",
      "  Iter 1060/1406: Loss 2.3144\n",
      "  Iter 1070/1406: Loss 2.3680\n",
      "  Iter 1080/1406: Loss 2.0763\n",
      "  Iter 1090/1406: Loss 2.3670\n",
      "  Iter 1100/1406: Loss 2.8177\n",
      "  Iter 1110/1406: Loss 2.9062\n",
      "  Iter 1120/1406: Loss 2.4484\n",
      "  Iter 1130/1406: Loss 2.3700\n",
      "  Iter 1140/1406: Loss 2.7984\n",
      "  Iter 1150/1406: Loss 2.3236\n",
      "  Iter 1160/1406: Loss 2.8265\n",
      "  Iter 1170/1406: Loss 2.3000\n",
      "  Iter 1180/1406: Loss 3.1593\n",
      "  Iter 1190/1406: Loss 2.2019\n",
      "  Iter 1200/1406: Loss 2.6520\n",
      "  Iter 1210/1406: Loss 2.5449\n",
      "  Iter 1220/1406: Loss 2.8239\n",
      "  Iter 1230/1406: Loss 2.9320\n",
      "  Iter 1240/1406: Loss 2.8522\n",
      "  Iter 1250/1406: Loss 2.7145\n",
      "  Iter 1260/1406: Loss 2.3864\n",
      "  Iter 1270/1406: Loss 2.4785\n",
      "  Iter 1280/1406: Loss 2.5856\n",
      "  Iter 1290/1406: Loss 2.8081\n",
      "  Iter 1300/1406: Loss 2.6746\n",
      "  Iter 1310/1406: Loss 2.6584\n",
      "  Iter 1320/1406: Loss 2.7915\n",
      "  Iter 1330/1406: Loss 2.5373\n",
      "  Iter 1340/1406: Loss 2.4688\n",
      "  Iter 1350/1406: Loss 2.6943\n",
      "  Iter 1360/1406: Loss 2.7428\n",
      "  Iter 1370/1406: Loss 2.6325\n",
      "  Iter 1380/1406: Loss 2.5666\n",
      "  Iter 1390/1406: Loss 2.1841\n",
      "  Iter 1400/1406: Loss 2.6022\n",
      "Train acc: 0.3590, Test acc: 0.2993 (Time: 2713.39s)\n",
      "\n",
      "[Epoch 8]\n",
      "  Iter   0/1406: Loss 2.5074\n",
      "  Iter  10/1406: Loss 2.3342\n",
      "  Iter  20/1406: Loss 2.6254\n",
      "  Iter  30/1406: Loss 2.3568\n",
      "  Iter  40/1406: Loss 2.9848\n",
      "  Iter  50/1406: Loss 2.7691\n",
      "  Iter  60/1406: Loss 2.6288\n",
      "  Iter  70/1406: Loss 2.9236\n",
      "  Iter  80/1406: Loss 2.9641\n",
      "  Iter  90/1406: Loss 2.6133\n",
      "  Iter 100/1406: Loss 2.3060\n",
      "  Iter 110/1406: Loss 3.1692\n",
      "  Iter 120/1406: Loss 2.8276\n",
      "  Iter 130/1406: Loss 2.8335\n",
      "  Iter 140/1406: Loss 2.6080\n",
      "  Iter 150/1406: Loss 2.5225\n",
      "  Iter 160/1406: Loss 3.0771\n",
      "  Iter 170/1406: Loss 2.6760\n",
      "  Iter 180/1406: Loss 2.3913\n",
      "  Iter 190/1406: Loss 2.3686\n",
      "  Iter 200/1406: Loss 2.2949\n",
      "  Iter 210/1406: Loss 2.4367\n",
      "  Iter 220/1406: Loss 2.3086\n",
      "  Iter 230/1406: Loss 2.0904\n",
      "  Iter 240/1406: Loss 2.7075\n",
      "  Iter 250/1406: Loss 2.3488\n",
      "  Iter 260/1406: Loss 2.0950\n",
      "  Iter 270/1406: Loss 2.8230\n",
      "  Iter 280/1406: Loss 2.5360\n",
      "  Iter 290/1406: Loss 2.3047\n",
      "  Iter 300/1406: Loss 2.3163\n",
      "  Iter 310/1406: Loss 2.8697\n",
      "  Iter 320/1406: Loss 2.4341\n",
      "  Iter 330/1406: Loss 2.0807\n",
      "  Iter 340/1406: Loss 2.6473\n",
      "  Iter 350/1406: Loss 2.6032\n",
      "  Iter 360/1406: Loss 2.4021\n",
      "  Iter 370/1406: Loss 2.4400\n",
      "  Iter 380/1406: Loss 2.5527\n",
      "  Iter 390/1406: Loss 2.9324\n",
      "  Iter 400/1406: Loss 2.6180\n",
      "  Iter 410/1406: Loss 2.4244\n",
      "  Iter 420/1406: Loss 2.4006\n",
      "  Iter 430/1406: Loss 2.4699\n",
      "  Iter 440/1406: Loss 2.2762\n",
      "  Iter 450/1406: Loss 2.4940\n",
      "  Iter 460/1406: Loss 2.5478\n",
      "  Iter 470/1406: Loss 2.2858\n",
      "  Iter 480/1406: Loss 2.3214\n",
      "  Iter 490/1406: Loss 2.4167\n",
      "  Iter 500/1406: Loss 2.2828\n",
      "  Iter 510/1406: Loss 2.1074\n",
      "  Iter 520/1406: Loss 2.4320\n",
      "  Iter 530/1406: Loss 2.6040\n",
      "  Iter 540/1406: Loss 2.4200\n",
      "  Iter 550/1406: Loss 2.3440\n",
      "  Iter 560/1406: Loss 2.4196\n",
      "  Iter 570/1406: Loss 2.5299\n",
      "  Iter 580/1406: Loss 2.5636\n",
      "  Iter 590/1406: Loss 2.2834\n",
      "  Iter 600/1406: Loss 2.5327\n",
      "  Iter 610/1406: Loss 2.5900\n",
      "  Iter 620/1406: Loss 2.6852\n",
      "  Iter 630/1406: Loss 2.7077\n",
      "  Iter 640/1406: Loss 2.3107\n",
      "  Iter 650/1406: Loss 2.5045\n",
      "  Iter 660/1406: Loss 2.3347\n",
      "  Iter 670/1406: Loss 2.7240\n",
      "  Iter 680/1406: Loss 2.6864\n",
      "  Iter 690/1406: Loss 2.4424\n",
      "  Iter 700/1406: Loss 3.1324\n",
      "  Iter 710/1406: Loss 3.0736\n",
      "  Iter 720/1406: Loss 2.2539\n",
      "  Iter 730/1406: Loss 2.2720\n",
      "  Iter 740/1406: Loss 2.7355\n",
      "  Iter 750/1406: Loss 2.3189\n",
      "  Iter 760/1406: Loss 2.5057\n",
      "  Iter 770/1406: Loss 3.0640\n",
      "  Iter 780/1406: Loss 2.5835\n",
      "  Iter 790/1406: Loss 2.3842\n",
      "  Iter 800/1406: Loss 2.1971\n",
      "  Iter 810/1406: Loss 3.1000\n",
      "  Iter 820/1406: Loss 2.8997\n",
      "  Iter 830/1406: Loss 2.3054\n",
      "  Iter 840/1406: Loss 2.6293\n",
      "  Iter 850/1406: Loss 2.9623\n",
      "  Iter 860/1406: Loss 2.2882\n",
      "  Iter 870/1406: Loss 2.1765\n",
      "  Iter 880/1406: Loss 2.7208\n",
      "  Iter 890/1406: Loss 2.7387\n",
      "  Iter 900/1406: Loss 2.9580\n",
      "  Iter 910/1406: Loss 2.8170\n",
      "  Iter 920/1406: Loss 2.3321\n",
      "  Iter 930/1406: Loss 2.3549\n",
      "  Iter 940/1406: Loss 2.3712\n",
      "  Iter 950/1406: Loss 2.0232\n",
      "  Iter 960/1406: Loss 2.1834\n",
      "  Iter 970/1406: Loss 2.5016\n",
      "  Iter 980/1406: Loss 2.9267\n",
      "  Iter 990/1406: Loss 2.5928\n",
      "  Iter 1000/1406: Loss 2.3349\n",
      "  Iter 1010/1406: Loss 2.8824\n",
      "  Iter 1020/1406: Loss 2.6838\n",
      "  Iter 1030/1406: Loss 2.3475\n",
      "  Iter 1040/1406: Loss 2.6665\n",
      "  Iter 1050/1406: Loss 2.9595\n",
      "  Iter 1060/1406: Loss 2.7480\n",
      "  Iter 1070/1406: Loss 2.0328\n",
      "  Iter 1080/1406: Loss 2.5230\n",
      "  Iter 1090/1406: Loss 2.7504\n",
      "  Iter 1100/1406: Loss 1.7985\n",
      "  Iter 1110/1406: Loss 2.4332\n",
      "  Iter 1120/1406: Loss 2.7493\n",
      "  Iter 1130/1406: Loss 2.4350\n",
      "  Iter 1140/1406: Loss 2.8633\n",
      "  Iter 1150/1406: Loss 2.5597\n",
      "  Iter 1160/1406: Loss 2.7585\n",
      "  Iter 1170/1406: Loss 2.3188\n",
      "  Iter 1180/1406: Loss 2.3803\n",
      "  Iter 1190/1406: Loss 2.3802\n",
      "  Iter 1200/1406: Loss 2.8199\n",
      "  Iter 1210/1406: Loss 2.2612\n",
      "  Iter 1220/1406: Loss 2.4615\n",
      "  Iter 1230/1406: Loss 2.3558\n",
      "  Iter 1240/1406: Loss 2.3141\n",
      "  Iter 1250/1406: Loss 2.5450\n",
      "  Iter 1260/1406: Loss 3.3086\n",
      "  Iter 1270/1406: Loss 2.3599\n",
      "  Iter 1280/1406: Loss 2.7047\n",
      "  Iter 1290/1406: Loss 2.7731\n",
      "  Iter 1300/1406: Loss 2.6559\n",
      "  Iter 1310/1406: Loss 2.5130\n",
      "  Iter 1320/1406: Loss 2.6630\n",
      "  Iter 1330/1406: Loss 2.2573\n",
      "  Iter 1340/1406: Loss 2.0689\n",
      "  Iter 1350/1406: Loss 2.7818\n",
      "  Iter 1360/1406: Loss 2.6755\n",
      "  Iter 1370/1406: Loss 2.2425\n",
      "  Iter 1380/1406: Loss 1.9680\n",
      "  Iter 1390/1406: Loss 2.3036\n",
      "  Iter 1400/1406: Loss 2.6321\n",
      "Train acc: 0.3720, Test acc: 0.3202 (Time: 2714.77s)\n",
      "\n",
      "[Epoch 9]\n",
      "  Iter   0/1406: Loss 2.6044\n",
      "  Iter  10/1406: Loss 2.3880\n",
      "  Iter  20/1406: Loss 2.4056\n",
      "  Iter  30/1406: Loss 2.3452\n",
      "  Iter  40/1406: Loss 2.7756\n",
      "  Iter  50/1406: Loss 2.5638\n",
      "  Iter  60/1406: Loss 2.2940\n",
      "  Iter  70/1406: Loss 2.4308\n",
      "  Iter  80/1406: Loss 2.3245\n",
      "  Iter  90/1406: Loss 2.5233\n",
      "  Iter 100/1406: Loss 2.6427\n",
      "  Iter 110/1406: Loss 1.9919\n",
      "  Iter 120/1406: Loss 2.2230\n",
      "  Iter 130/1406: Loss 2.3485\n",
      "  Iter 140/1406: Loss 1.8724\n",
      "  Iter 150/1406: Loss 2.1564\n",
      "  Iter 160/1406: Loss 2.6676\n",
      "  Iter 170/1406: Loss 2.9028\n",
      "  Iter 180/1406: Loss 2.1174\n",
      "  Iter 190/1406: Loss 2.2322\n",
      "  Iter 200/1406: Loss 2.5080\n",
      "  Iter 210/1406: Loss 2.4797\n",
      "  Iter 220/1406: Loss 2.5889\n",
      "  Iter 230/1406: Loss 2.4896\n",
      "  Iter 240/1406: Loss 2.7764\n",
      "  Iter 250/1406: Loss 2.5422\n",
      "  Iter 260/1406: Loss 2.3884\n",
      "  Iter 270/1406: Loss 2.3590\n",
      "  Iter 280/1406: Loss 1.9521\n",
      "  Iter 290/1406: Loss 2.1121\n",
      "  Iter 300/1406: Loss 2.5305\n",
      "  Iter 310/1406: Loss 2.0127\n",
      "  Iter 320/1406: Loss 2.1960\n",
      "  Iter 330/1406: Loss 2.9355\n",
      "  Iter 340/1406: Loss 2.4829\n",
      "  Iter 350/1406: Loss 2.4852\n",
      "  Iter 360/1406: Loss 2.3841\n",
      "  Iter 370/1406: Loss 2.6916\n",
      "  Iter 380/1406: Loss 2.3029\n",
      "  Iter 390/1406: Loss 2.6032\n",
      "  Iter 400/1406: Loss 2.5243\n",
      "  Iter 410/1406: Loss 2.4579\n",
      "  Iter 420/1406: Loss 1.9843\n",
      "  Iter 430/1406: Loss 2.4926\n",
      "  Iter 440/1406: Loss 3.1618\n",
      "  Iter 450/1406: Loss 2.9642\n",
      "  Iter 460/1406: Loss 3.0516\n",
      "  Iter 470/1406: Loss 2.3422\n",
      "  Iter 480/1406: Loss 1.9115\n",
      "  Iter 490/1406: Loss 2.1694\n",
      "  Iter 500/1406: Loss 2.1672\n",
      "  Iter 510/1406: Loss 2.4774\n",
      "  Iter 520/1406: Loss 2.3725\n",
      "  Iter 530/1406: Loss 2.5233\n",
      "  Iter 540/1406: Loss 2.0021\n",
      "  Iter 550/1406: Loss 2.5486\n",
      "  Iter 560/1406: Loss 2.1369\n",
      "  Iter 570/1406: Loss 1.9272\n",
      "  Iter 580/1406: Loss 2.0679\n",
      "  Iter 590/1406: Loss 1.8063\n",
      "  Iter 600/1406: Loss 2.4619\n",
      "  Iter 610/1406: Loss 2.1991\n",
      "  Iter 620/1406: Loss 2.5890\n",
      "  Iter 630/1406: Loss 2.0301\n",
      "  Iter 640/1406: Loss 2.1849\n",
      "  Iter 650/1406: Loss 2.5702\n",
      "  Iter 660/1406: Loss 2.3476\n",
      "  Iter 670/1406: Loss 2.4496\n",
      "  Iter 680/1406: Loss 2.1772\n",
      "  Iter 690/1406: Loss 2.1531\n",
      "  Iter 700/1406: Loss 2.5677\n",
      "  Iter 710/1406: Loss 2.5606\n",
      "  Iter 720/1406: Loss 3.0332\n",
      "  Iter 730/1406: Loss 2.3134\n",
      "  Iter 740/1406: Loss 2.0202\n",
      "  Iter 750/1406: Loss 2.6186\n",
      "  Iter 760/1406: Loss 3.0725\n",
      "  Iter 770/1406: Loss 2.5185\n",
      "  Iter 780/1406: Loss 2.7536\n",
      "  Iter 790/1406: Loss 2.7552\n",
      "  Iter 800/1406: Loss 2.5172\n",
      "  Iter 810/1406: Loss 2.3820\n",
      "  Iter 820/1406: Loss 2.4453\n",
      "  Iter 830/1406: Loss 2.3717\n",
      "  Iter 840/1406: Loss 2.0960\n",
      "  Iter 850/1406: Loss 2.4078\n",
      "  Iter 860/1406: Loss 2.2718\n",
      "  Iter 870/1406: Loss 2.2432\n",
      "  Iter 880/1406: Loss 2.2380\n",
      "  Iter 890/1406: Loss 2.2970\n",
      "  Iter 900/1406: Loss 2.1206\n",
      "  Iter 910/1406: Loss 2.6041\n",
      "  Iter 920/1406: Loss 2.3485\n",
      "  Iter 930/1406: Loss 2.4817\n",
      "  Iter 940/1406: Loss 2.3719\n",
      "  Iter 950/1406: Loss 2.5241\n",
      "  Iter 960/1406: Loss 2.9779\n",
      "  Iter 970/1406: Loss 2.5480\n",
      "  Iter 980/1406: Loss 2.1787\n",
      "  Iter 990/1406: Loss 2.0317\n",
      "  Iter 1000/1406: Loss 2.2094\n",
      "  Iter 1010/1406: Loss 2.7176\n",
      "  Iter 1020/1406: Loss 2.4855\n",
      "  Iter 1030/1406: Loss 2.6892\n",
      "  Iter 1040/1406: Loss 2.4414\n",
      "  Iter 1050/1406: Loss 2.4857\n",
      "  Iter 1060/1406: Loss 2.4185\n",
      "  Iter 1070/1406: Loss 2.2238\n",
      "  Iter 1080/1406: Loss 2.5897\n",
      "  Iter 1090/1406: Loss 2.4561\n",
      "  Iter 1100/1406: Loss 2.1904\n",
      "  Iter 1110/1406: Loss 2.4985\n",
      "  Iter 1120/1406: Loss 2.0644\n",
      "  Iter 1130/1406: Loss 2.6009\n",
      "  Iter 1140/1406: Loss 2.6437\n",
      "  Iter 1150/1406: Loss 2.5801\n",
      "  Iter 1160/1406: Loss 2.5711\n",
      "  Iter 1170/1406: Loss 2.2416\n",
      "  Iter 1180/1406: Loss 2.1349\n",
      "  Iter 1190/1406: Loss 2.1613\n",
      "  Iter 1200/1406: Loss 2.1941\n",
      "  Iter 1210/1406: Loss 2.6542\n",
      "  Iter 1220/1406: Loss 1.8583\n",
      "  Iter 1230/1406: Loss 2.0698\n",
      "  Iter 1240/1406: Loss 2.1796\n",
      "  Iter 1250/1406: Loss 2.6363\n",
      "  Iter 1260/1406: Loss 2.5036\n",
      "  Iter 1270/1406: Loss 2.6502\n",
      "  Iter 1280/1406: Loss 2.3236\n",
      "  Iter 1290/1406: Loss 2.2150\n",
      "  Iter 1300/1406: Loss 1.8795\n",
      "  Iter 1310/1406: Loss 2.5864\n",
      "  Iter 1320/1406: Loss 2.3661\n",
      "  Iter 1330/1406: Loss 2.6232\n",
      "  Iter 1340/1406: Loss 2.0882\n",
      "  Iter 1350/1406: Loss 2.8472\n",
      "  Iter 1360/1406: Loss 2.3309\n",
      "  Iter 1370/1406: Loss 2.0753\n",
      "  Iter 1380/1406: Loss 2.3455\n",
      "  Iter 1390/1406: Loss 2.6102\n",
      "  Iter 1400/1406: Loss 2.2737\n",
      "Train acc: 0.4220, Test acc: 0.3196 (Time: 2713.85s)\n",
      "\n",
      "[Epoch 10]\n",
      "  Iter   0/1406: Loss 2.1872\n",
      "  Iter  10/1406: Loss 2.4670\n",
      "  Iter  20/1406: Loss 2.3449\n",
      "  Iter  30/1406: Loss 2.4681\n",
      "  Iter  40/1406: Loss 2.3608\n",
      "  Iter  50/1406: Loss 2.3328\n",
      "  Iter  60/1406: Loss 1.9965\n",
      "  Iter  70/1406: Loss 2.3951\n",
      "  Iter  80/1406: Loss 2.3330\n",
      "  Iter  90/1406: Loss 2.3050\n",
      "  Iter 100/1406: Loss 2.4710\n",
      "  Iter 110/1406: Loss 2.5230\n",
      "  Iter 120/1406: Loss 2.3105\n",
      "  Iter 130/1406: Loss 2.3011\n",
      "  Iter 140/1406: Loss 2.3044\n",
      "  Iter 150/1406: Loss 2.3647\n",
      "  Iter 160/1406: Loss 2.3724\n",
      "  Iter 170/1406: Loss 2.5117\n",
      "  Iter 180/1406: Loss 1.9365\n",
      "  Iter 190/1406: Loss 2.4526\n",
      "  Iter 200/1406: Loss 2.0038\n",
      "  Iter 210/1406: Loss 2.3642\n",
      "  Iter 220/1406: Loss 2.7437\n",
      "  Iter 230/1406: Loss 2.5965\n",
      "  Iter 240/1406: Loss 2.1419\n",
      "  Iter 250/1406: Loss 3.0486\n",
      "  Iter 260/1406: Loss 2.6735\n",
      "  Iter 270/1406: Loss 2.5617\n",
      "  Iter 280/1406: Loss 2.1721\n",
      "  Iter 290/1406: Loss 2.5316\n",
      "  Iter 300/1406: Loss 2.6263\n",
      "  Iter 310/1406: Loss 2.3093\n",
      "  Iter 320/1406: Loss 2.1794\n",
      "  Iter 330/1406: Loss 2.3517\n",
      "  Iter 340/1406: Loss 2.7670\n",
      "  Iter 350/1406: Loss 2.3194\n",
      "  Iter 360/1406: Loss 2.3940\n",
      "  Iter 370/1406: Loss 2.2052\n",
      "  Iter 380/1406: Loss 2.5680\n",
      "  Iter 390/1406: Loss 2.4532\n",
      "  Iter 400/1406: Loss 2.3069\n",
      "  Iter 410/1406: Loss 2.5658\n",
      "  Iter 420/1406: Loss 2.2640\n",
      "  Iter 430/1406: Loss 2.7409\n",
      "  Iter 440/1406: Loss 2.4319\n",
      "  Iter 450/1406: Loss 2.1511\n",
      "  Iter 460/1406: Loss 2.2384\n",
      "  Iter 470/1406: Loss 2.4589\n",
      "  Iter 480/1406: Loss 2.3925\n",
      "  Iter 490/1406: Loss 2.3190\n",
      "  Iter 500/1406: Loss 2.5591\n",
      "  Iter 510/1406: Loss 2.4452\n",
      "  Iter 520/1406: Loss 2.2160\n",
      "  Iter 530/1406: Loss 2.2484\n",
      "  Iter 540/1406: Loss 2.7228\n",
      "  Iter 550/1406: Loss 1.9558\n",
      "  Iter 560/1406: Loss 2.2586\n",
      "  Iter 570/1406: Loss 2.6862\n",
      "  Iter 580/1406: Loss 2.3949\n",
      "  Iter 590/1406: Loss 2.3680\n",
      "  Iter 600/1406: Loss 2.4676\n",
      "  Iter 610/1406: Loss 1.5808\n",
      "  Iter 620/1406: Loss 1.9822\n",
      "  Iter 630/1406: Loss 1.7724\n",
      "  Iter 640/1406: Loss 2.0366\n",
      "  Iter 650/1406: Loss 2.0383\n",
      "  Iter 660/1406: Loss 2.2487\n",
      "  Iter 670/1406: Loss 2.5051\n",
      "  Iter 680/1406: Loss 1.9890\n",
      "  Iter 690/1406: Loss 2.6071\n",
      "  Iter 700/1406: Loss 2.0608\n",
      "  Iter 710/1406: Loss 2.0125\n",
      "  Iter 720/1406: Loss 2.6814\n",
      "  Iter 730/1406: Loss 2.2285\n",
      "  Iter 740/1406: Loss 2.2950\n",
      "  Iter 750/1406: Loss 2.1398\n",
      "  Iter 760/1406: Loss 2.1806\n",
      "  Iter 770/1406: Loss 2.1866\n",
      "  Iter 780/1406: Loss 2.5865\n",
      "  Iter 790/1406: Loss 2.4152\n",
      "  Iter 800/1406: Loss 2.3478\n",
      "  Iter 810/1406: Loss 2.7835\n",
      "  Iter 820/1406: Loss 2.6069\n",
      "  Iter 830/1406: Loss 1.9016\n",
      "  Iter 840/1406: Loss 2.8178\n",
      "  Iter 850/1406: Loss 1.8371\n",
      "  Iter 860/1406: Loss 2.4346\n",
      "  Iter 870/1406: Loss 2.2904\n",
      "  Iter 880/1406: Loss 2.5366\n",
      "  Iter 890/1406: Loss 2.0755\n",
      "  Iter 900/1406: Loss 1.7410\n",
      "  Iter 910/1406: Loss 2.9641\n",
      "  Iter 920/1406: Loss 2.3929\n",
      "  Iter 930/1406: Loss 2.7921\n",
      "  Iter 940/1406: Loss 2.6120\n",
      "  Iter 950/1406: Loss 2.2784\n",
      "  Iter 960/1406: Loss 2.2763\n",
      "  Iter 970/1406: Loss 2.4642\n",
      "  Iter 980/1406: Loss 2.1263\n",
      "  Iter 990/1406: Loss 2.3720\n",
      "  Iter 1000/1406: Loss 2.4791\n",
      "  Iter 1010/1406: Loss 2.7491\n",
      "  Iter 1020/1406: Loss 2.1563\n",
      "  Iter 1030/1406: Loss 2.5816\n",
      "  Iter 1040/1406: Loss 2.0452\n",
      "  Iter 1050/1406: Loss 2.0562\n",
      "  Iter 1060/1406: Loss 2.1475\n",
      "  Iter 1070/1406: Loss 1.9721\n",
      "  Iter 1080/1406: Loss 2.3670\n",
      "  Iter 1090/1406: Loss 2.2205\n",
      "  Iter 1100/1406: Loss 2.3763\n",
      "  Iter 1110/1406: Loss 2.4826\n",
      "  Iter 1120/1406: Loss 2.3788\n",
      "  Iter 1130/1406: Loss 1.7965\n",
      "  Iter 1140/1406: Loss 1.9313\n",
      "  Iter 1150/1406: Loss 2.0951\n",
      "  Iter 1160/1406: Loss 2.3875\n",
      "  Iter 1170/1406: Loss 2.2314\n",
      "  Iter 1180/1406: Loss 2.2647\n",
      "  Iter 1190/1406: Loss 2.7533\n",
      "  Iter 1200/1406: Loss 2.6406\n",
      "  Iter 1210/1406: Loss 2.3606\n",
      "  Iter 1220/1406: Loss 2.4338\n",
      "  Iter 1230/1406: Loss 2.1045\n",
      "  Iter 1240/1406: Loss 2.5581\n",
      "  Iter 1250/1406: Loss 2.0657\n",
      "  Iter 1260/1406: Loss 2.0508\n",
      "  Iter 1270/1406: Loss 2.6095\n",
      "  Iter 1280/1406: Loss 2.0881\n",
      "  Iter 1290/1406: Loss 2.3544\n",
      "  Iter 1300/1406: Loss 2.0648\n",
      "  Iter 1310/1406: Loss 2.2863\n",
      "  Iter 1320/1406: Loss 3.0137\n",
      "  Iter 1330/1406: Loss 2.0914\n",
      "  Iter 1340/1406: Loss 2.1091\n",
      "  Iter 1350/1406: Loss 2.2750\n",
      "  Iter 1360/1406: Loss 2.5503\n",
      "  Iter 1370/1406: Loss 2.1604\n",
      "  Iter 1380/1406: Loss 2.3335\n",
      "  Iter 1390/1406: Loss 2.4681\n",
      "  Iter 1400/1406: Loss 2.3212\n",
      "Train acc: 0.4130, Test acc: 0.3283 (Time: 2722.27s)\n",
      "\n",
      ">>> Saved model to checkpoint_epoch_10.pkl\n",
      "\n",
      "[Epoch 11]\n",
      "  Iter   0/1406: Loss 2.7203\n",
      "  Iter  10/1406: Loss 2.6409\n",
      "  Iter  20/1406: Loss 2.5973\n",
      "  Iter  30/1406: Loss 2.5593\n",
      "  Iter  40/1406: Loss 2.3550\n",
      "  Iter  50/1406: Loss 2.0415\n",
      "  Iter  60/1406: Loss 2.1246\n",
      "  Iter  70/1406: Loss 2.2901\n",
      "  Iter  80/1406: Loss 2.6018\n",
      "  Iter  90/1406: Loss 2.3473\n",
      "  Iter 100/1406: Loss 2.0706\n",
      "  Iter 110/1406: Loss 2.4928\n",
      "  Iter 120/1406: Loss 2.2909\n",
      "  Iter 130/1406: Loss 2.3995\n",
      "  Iter 140/1406: Loss 2.4409\n",
      "  Iter 150/1406: Loss 2.5125\n",
      "  Iter 160/1406: Loss 2.8074\n",
      "  Iter 170/1406: Loss 2.1685\n",
      "  Iter 180/1406: Loss 2.5230\n",
      "  Iter 190/1406: Loss 1.9895\n",
      "  Iter 200/1406: Loss 3.2853\n",
      "  Iter 210/1406: Loss 2.6514\n",
      "  Iter 220/1406: Loss 1.8974\n",
      "  Iter 230/1406: Loss 1.8790\n",
      "  Iter 240/1406: Loss 2.3625\n",
      "  Iter 250/1406: Loss 2.7990\n",
      "  Iter 260/1406: Loss 2.4721\n",
      "  Iter 270/1406: Loss 2.1699\n",
      "  Iter 280/1406: Loss 2.2657\n",
      "  Iter 290/1406: Loss 2.8668\n",
      "  Iter 300/1406: Loss 2.1397\n",
      "  Iter 310/1406: Loss 1.9869\n",
      "  Iter 320/1406: Loss 2.0852\n",
      "  Iter 330/1406: Loss 2.9646\n",
      "  Iter 340/1406: Loss 2.0944\n",
      "  Iter 350/1406: Loss 2.1699\n",
      "  Iter 360/1406: Loss 1.6732\n",
      "  Iter 370/1406: Loss 2.2382\n",
      "  Iter 380/1406: Loss 2.4806\n",
      "  Iter 390/1406: Loss 1.8705\n",
      "  Iter 400/1406: Loss 2.3922\n",
      "  Iter 410/1406: Loss 1.9553\n",
      "  Iter 420/1406: Loss 2.2978\n",
      "  Iter 430/1406: Loss 1.9754\n",
      "  Iter 440/1406: Loss 2.3329\n",
      "  Iter 450/1406: Loss 2.2771\n",
      "  Iter 460/1406: Loss 2.2875\n",
      "  Iter 470/1406: Loss 2.3666\n",
      "  Iter 480/1406: Loss 2.1352\n",
      "  Iter 490/1406: Loss 2.0816\n",
      "  Iter 500/1406: Loss 2.2859\n",
      "  Iter 510/1406: Loss 2.2400\n",
      "  Iter 520/1406: Loss 2.4769\n",
      "  Iter 530/1406: Loss 1.8178\n",
      "  Iter 540/1406: Loss 2.3968\n",
      "  Iter 550/1406: Loss 2.6084\n",
      "  Iter 560/1406: Loss 2.3215\n",
      "  Iter 570/1406: Loss 2.3798\n",
      "  Iter 580/1406: Loss 2.3142\n",
      "  Iter 590/1406: Loss 2.4337\n",
      "  Iter 600/1406: Loss 1.9215\n",
      "  Iter 610/1406: Loss 2.2710\n",
      "  Iter 620/1406: Loss 2.0373\n",
      "  Iter 630/1406: Loss 2.7230\n",
      "  Iter 640/1406: Loss 2.0599\n",
      "  Iter 650/1406: Loss 2.1948\n",
      "  Iter 660/1406: Loss 2.2799\n",
      "  Iter 670/1406: Loss 2.2233\n",
      "  Iter 680/1406: Loss 2.2260\n",
      "  Iter 690/1406: Loss 1.9590\n",
      "  Iter 700/1406: Loss 1.8635\n",
      "  Iter 710/1406: Loss 2.7901\n",
      "  Iter 720/1406: Loss 2.0912\n",
      "  Iter 730/1406: Loss 1.9043\n",
      "  Iter 740/1406: Loss 2.6474\n",
      "  Iter 750/1406: Loss 2.3702\n",
      "  Iter 760/1406: Loss 2.5099\n",
      "  Iter 770/1406: Loss 2.3780\n",
      "  Iter 780/1406: Loss 2.6847\n",
      "  Iter 790/1406: Loss 2.3905\n",
      "  Iter 800/1406: Loss 2.4078\n",
      "  Iter 810/1406: Loss 2.6092\n",
      "  Iter 820/1406: Loss 2.1935\n",
      "  Iter 830/1406: Loss 2.2056\n",
      "  Iter 840/1406: Loss 2.3576\n",
      "  Iter 850/1406: Loss 1.7910\n",
      "  Iter 860/1406: Loss 2.4726\n",
      "  Iter 870/1406: Loss 1.8972\n",
      "  Iter 880/1406: Loss 2.4402\n",
      "  Iter 890/1406: Loss 2.0722\n",
      "  Iter 900/1406: Loss 2.1655\n",
      "  Iter 910/1406: Loss 2.7147\n",
      "  Iter 920/1406: Loss 1.8254\n",
      "  Iter 930/1406: Loss 1.8159\n",
      "  Iter 940/1406: Loss 2.4828\n",
      "  Iter 950/1406: Loss 2.8584\n",
      "  Iter 960/1406: Loss 2.3852\n",
      "  Iter 970/1406: Loss 2.3322\n",
      "  Iter 980/1406: Loss 2.1771\n",
      "  Iter 990/1406: Loss 2.4096\n",
      "  Iter 1000/1406: Loss 1.9171\n",
      "  Iter 1010/1406: Loss 2.3441\n",
      "  Iter 1020/1406: Loss 2.1689\n",
      "  Iter 1030/1406: Loss 1.7639\n",
      "  Iter 1040/1406: Loss 2.0669\n",
      "  Iter 1050/1406: Loss 2.7356\n",
      "  Iter 1060/1406: Loss 2.3337\n",
      "  Iter 1070/1406: Loss 2.3385\n",
      "  Iter 1080/1406: Loss 2.4419\n",
      "  Iter 1090/1406: Loss 1.8781\n",
      "  Iter 1100/1406: Loss 2.3947\n",
      "  Iter 1110/1406: Loss 2.8172\n",
      "  Iter 1120/1406: Loss 2.2490\n",
      "  Iter 1130/1406: Loss 1.7279\n",
      "  Iter 1140/1406: Loss 1.9842\n",
      "  Iter 1150/1406: Loss 2.2443\n",
      "  Iter 1160/1406: Loss 2.3617\n",
      "  Iter 1170/1406: Loss 2.1670\n",
      "  Iter 1180/1406: Loss 1.9678\n",
      "  Iter 1190/1406: Loss 2.5680\n",
      "  Iter 1200/1406: Loss 2.2358\n",
      "  Iter 1210/1406: Loss 2.6416\n",
      "  Iter 1220/1406: Loss 2.4109\n",
      "  Iter 1230/1406: Loss 2.0337\n",
      "  Iter 1240/1406: Loss 1.9435\n",
      "  Iter 1250/1406: Loss 2.5406\n",
      "  Iter 1260/1406: Loss 1.9581\n",
      "  Iter 1270/1406: Loss 2.1852\n",
      "  Iter 1280/1406: Loss 2.0909\n",
      "  Iter 1290/1406: Loss 2.0589\n",
      "  Iter 1300/1406: Loss 2.9016\n",
      "  Iter 1310/1406: Loss 2.4399\n",
      "  Iter 1320/1406: Loss 2.2172\n",
      "  Iter 1330/1406: Loss 2.1998\n",
      "  Iter 1340/1406: Loss 1.9890\n",
      "  Iter 1350/1406: Loss 2.5736\n",
      "  Iter 1360/1406: Loss 2.2718\n",
      "  Iter 1370/1406: Loss 2.3577\n",
      "  Iter 1380/1406: Loss 2.8213\n",
      "  Iter 1390/1406: Loss 1.9178\n",
      "  Iter 1400/1406: Loss 1.7900\n",
      "Train acc: 0.4400, Test acc: 0.3359 (Time: 2736.13s)\n",
      "\n",
      "[Epoch 12]\n",
      "  Iter   0/1406: Loss 2.7395\n",
      "  Iter  10/1406: Loss 2.4578\n",
      "  Iter  20/1406: Loss 2.3843\n",
      "  Iter  30/1406: Loss 1.6832\n",
      "  Iter  40/1406: Loss 2.3202\n",
      "  Iter  50/1406: Loss 2.3304\n",
      "  Iter  60/1406: Loss 2.2055\n",
      "  Iter  70/1406: Loss 1.7703\n",
      "  Iter  80/1406: Loss 2.8449\n",
      "  Iter  90/1406: Loss 2.2381\n",
      "  Iter 100/1406: Loss 2.2703\n",
      "  Iter 110/1406: Loss 2.2883\n",
      "  Iter 120/1406: Loss 1.8606\n",
      "  Iter 130/1406: Loss 2.2614\n",
      "  Iter 140/1406: Loss 2.2553\n",
      "  Iter 150/1406: Loss 2.0649\n",
      "  Iter 160/1406: Loss 2.4957\n",
      "  Iter 170/1406: Loss 2.1798\n",
      "  Iter 180/1406: Loss 1.8723\n",
      "  Iter 190/1406: Loss 2.0400\n",
      "  Iter 200/1406: Loss 1.8590\n",
      "  Iter 210/1406: Loss 2.4643\n",
      "  Iter 220/1406: Loss 1.9974\n",
      "  Iter 230/1406: Loss 2.1096\n",
      "  Iter 240/1406: Loss 2.1103\n",
      "  Iter 250/1406: Loss 2.1294\n",
      "  Iter 260/1406: Loss 1.9215\n",
      "  Iter 270/1406: Loss 1.6846\n",
      "  Iter 280/1406: Loss 2.3439\n",
      "  Iter 290/1406: Loss 2.3428\n",
      "  Iter 300/1406: Loss 1.9810\n",
      "  Iter 310/1406: Loss 1.9645\n",
      "  Iter 320/1406: Loss 2.2981\n",
      "  Iter 330/1406: Loss 2.2714\n",
      "  Iter 340/1406: Loss 2.2240\n",
      "  Iter 350/1406: Loss 1.7576\n",
      "  Iter 360/1406: Loss 2.0796\n",
      "  Iter 370/1406: Loss 2.5460\n",
      "  Iter 380/1406: Loss 2.8313\n",
      "  Iter 390/1406: Loss 2.2170\n",
      "  Iter 400/1406: Loss 2.1000\n",
      "  Iter 410/1406: Loss 1.9907\n",
      "  Iter 420/1406: Loss 2.0253\n",
      "  Iter 430/1406: Loss 2.0344\n",
      "  Iter 440/1406: Loss 1.9459\n",
      "  Iter 450/1406: Loss 2.7341\n",
      "  Iter 460/1406: Loss 2.5737\n",
      "  Iter 470/1406: Loss 2.7825\n",
      "  Iter 480/1406: Loss 2.3977\n",
      "  Iter 490/1406: Loss 2.4174\n",
      "  Iter 500/1406: Loss 1.9211\n",
      "  Iter 510/1406: Loss 2.6575\n",
      "  Iter 520/1406: Loss 2.5500\n",
      "  Iter 530/1406: Loss 2.0529\n",
      "  Iter 540/1406: Loss 2.6988\n",
      "  Iter 550/1406: Loss 2.7519\n",
      "  Iter 560/1406: Loss 2.4540\n",
      "  Iter 570/1406: Loss 1.9134\n",
      "  Iter 580/1406: Loss 1.7752\n",
      "  Iter 590/1406: Loss 1.7181\n",
      "  Iter 600/1406: Loss 1.7059\n",
      "  Iter 610/1406: Loss 2.4073\n",
      "  Iter 620/1406: Loss 2.2323\n",
      "  Iter 630/1406: Loss 2.2041\n",
      "  Iter 640/1406: Loss 1.9978\n",
      "  Iter 650/1406: Loss 2.0693\n",
      "  Iter 660/1406: Loss 2.4157\n",
      "  Iter 670/1406: Loss 2.6001\n",
      "  Iter 680/1406: Loss 2.1360\n",
      "  Iter 690/1406: Loss 1.9501\n",
      "  Iter 700/1406: Loss 2.2646\n",
      "  Iter 710/1406: Loss 2.4026\n",
      "  Iter 720/1406: Loss 1.8606\n",
      "  Iter 730/1406: Loss 3.0079\n",
      "  Iter 740/1406: Loss 2.1820\n",
      "  Iter 750/1406: Loss 2.3366\n",
      "  Iter 760/1406: Loss 1.8854\n",
      "  Iter 770/1406: Loss 1.7654\n",
      "  Iter 780/1406: Loss 1.9940\n",
      "  Iter 790/1406: Loss 2.4546\n",
      "  Iter 800/1406: Loss 2.3785\n",
      "  Iter 810/1406: Loss 2.2156\n",
      "  Iter 820/1406: Loss 2.4660\n",
      "  Iter 830/1406: Loss 2.6141\n",
      "  Iter 840/1406: Loss 2.1078\n",
      "  Iter 850/1406: Loss 2.2702\n",
      "  Iter 860/1406: Loss 2.2923\n",
      "  Iter 870/1406: Loss 2.3441\n",
      "  Iter 880/1406: Loss 2.1762\n",
      "  Iter 890/1406: Loss 1.9871\n",
      "  Iter 900/1406: Loss 2.4471\n",
      "  Iter 910/1406: Loss 1.9143\n",
      "  Iter 920/1406: Loss 1.9271\n",
      "  Iter 930/1406: Loss 2.0902\n",
      "  Iter 940/1406: Loss 2.2536\n",
      "  Iter 950/1406: Loss 1.6932\n",
      "  Iter 960/1406: Loss 2.3405\n",
      "  Iter 970/1406: Loss 1.5380\n",
      "  Iter 980/1406: Loss 2.3664\n",
      "  Iter 990/1406: Loss 2.4397\n",
      "  Iter 1000/1406: Loss 2.1601\n",
      "  Iter 1010/1406: Loss 1.8005\n",
      "  Iter 1020/1406: Loss 2.1090\n",
      "  Iter 1030/1406: Loss 1.9699\n",
      "  Iter 1040/1406: Loss 1.9792\n",
      "  Iter 1050/1406: Loss 2.1766\n",
      "  Iter 1060/1406: Loss 2.4972\n",
      "  Iter 1070/1406: Loss 2.0490\n",
      "  Iter 1080/1406: Loss 1.7401\n",
      "  Iter 1090/1406: Loss 2.7049\n",
      "  Iter 1100/1406: Loss 1.9911\n",
      "  Iter 1110/1406: Loss 2.3031\n",
      "  Iter 1120/1406: Loss 2.4112\n",
      "  Iter 1130/1406: Loss 2.2536\n",
      "  Iter 1140/1406: Loss 2.2053\n",
      "  Iter 1150/1406: Loss 1.9897\n",
      "  Iter 1160/1406: Loss 2.1676\n",
      "  Iter 1170/1406: Loss 1.9109\n",
      "  Iter 1180/1406: Loss 2.1041\n",
      "  Iter 1190/1406: Loss 2.2729\n",
      "  Iter 1200/1406: Loss 2.3130\n",
      "  Iter 1210/1406: Loss 2.1896\n",
      "  Iter 1220/1406: Loss 2.4014\n",
      "  Iter 1230/1406: Loss 2.2870\n",
      "  Iter 1240/1406: Loss 1.9297\n",
      "  Iter 1250/1406: Loss 2.3773\n",
      "  Iter 1260/1406: Loss 2.9675\n",
      "  Iter 1270/1406: Loss 2.4953\n",
      "  Iter 1280/1406: Loss 2.8002\n",
      "  Iter 1290/1406: Loss 2.0206\n",
      "  Iter 1300/1406: Loss 2.0706\n",
      "  Iter 1310/1406: Loss 2.5684\n",
      "  Iter 1320/1406: Loss 2.0753\n",
      "  Iter 1330/1406: Loss 2.3390\n",
      "  Iter 1340/1406: Loss 2.1327\n",
      "  Iter 1350/1406: Loss 2.0574\n",
      "  Iter 1360/1406: Loss 2.0719\n",
      "  Iter 1370/1406: Loss 1.8065\n",
      "  Iter 1380/1406: Loss 2.0284\n",
      "  Iter 1390/1406: Loss 2.7267\n",
      "  Iter 1400/1406: Loss 2.3284\n",
      "Train acc: 0.4650, Test acc: 0.3504 (Time: 2722.66s)\n",
      "\n",
      "[Epoch 13]\n",
      "  Iter   0/1406: Loss 2.2799\n",
      "  Iter  10/1406: Loss 1.8942\n",
      "  Iter  20/1406: Loss 1.9741\n",
      "  Iter  30/1406: Loss 2.1599\n",
      "  Iter  40/1406: Loss 2.1510\n",
      "  Iter  50/1406: Loss 1.5609\n",
      "  Iter  60/1406: Loss 2.0493\n",
      "  Iter  70/1406: Loss 2.4273\n",
      "  Iter  80/1406: Loss 1.8895\n",
      "  Iter  90/1406: Loss 2.0821\n",
      "  Iter 100/1406: Loss 2.0990\n",
      "  Iter 110/1406: Loss 2.5509\n",
      "  Iter 120/1406: Loss 2.2623\n",
      "  Iter 130/1406: Loss 1.9368\n",
      "  Iter 140/1406: Loss 2.1555\n",
      "  Iter 150/1406: Loss 2.6799\n",
      "  Iter 160/1406: Loss 1.6922\n",
      "  Iter 170/1406: Loss 2.1628\n",
      "  Iter 180/1406: Loss 2.4736\n",
      "  Iter 190/1406: Loss 2.6387\n",
      "  Iter 200/1406: Loss 1.8523\n",
      "  Iter 210/1406: Loss 2.4080\n",
      "  Iter 220/1406: Loss 2.3463\n",
      "  Iter 230/1406: Loss 2.7554\n",
      "  Iter 240/1406: Loss 2.3778\n",
      "  Iter 250/1406: Loss 2.1758\n",
      "  Iter 260/1406: Loss 2.1222\n",
      "  Iter 270/1406: Loss 2.1825\n",
      "  Iter 280/1406: Loss 2.2337\n",
      "  Iter 290/1406: Loss 2.5694\n",
      "  Iter 300/1406: Loss 2.3883\n",
      "  Iter 310/1406: Loss 2.3775\n",
      "  Iter 320/1406: Loss 2.7404\n",
      "  Iter 330/1406: Loss 2.9843\n",
      "  Iter 340/1406: Loss 1.9650\n",
      "  Iter 350/1406: Loss 2.4240\n",
      "  Iter 360/1406: Loss 1.9808\n",
      "  Iter 370/1406: Loss 1.7036\n",
      "  Iter 380/1406: Loss 2.3194\n",
      "  Iter 390/1406: Loss 2.4401\n",
      "  Iter 400/1406: Loss 1.8824\n",
      "  Iter 410/1406: Loss 2.2557\n",
      "  Iter 420/1406: Loss 1.9818\n",
      "  Iter 430/1406: Loss 1.7075\n",
      "  Iter 440/1406: Loss 1.9852\n",
      "  Iter 450/1406: Loss 1.9208\n",
      "  Iter 460/1406: Loss 1.8575\n",
      "  Iter 470/1406: Loss 2.0856\n",
      "  Iter 480/1406: Loss 2.1253\n",
      "  Iter 490/1406: Loss 2.2009\n",
      "  Iter 500/1406: Loss 2.6041\n",
      "  Iter 510/1406: Loss 1.8399\n",
      "  Iter 520/1406: Loss 2.3813\n",
      "  Iter 530/1406: Loss 1.5941\n",
      "  Iter 540/1406: Loss 1.9683\n",
      "  Iter 550/1406: Loss 2.9574\n",
      "  Iter 560/1406: Loss 2.2498\n",
      "  Iter 570/1406: Loss 2.4839\n",
      "  Iter 580/1406: Loss 2.1306\n",
      "  Iter 590/1406: Loss 1.8658\n",
      "  Iter 600/1406: Loss 2.3062\n",
      "  Iter 610/1406: Loss 2.2069\n",
      "  Iter 620/1406: Loss 2.2147\n",
      "  Iter 630/1406: Loss 2.5108\n",
      "  Iter 640/1406: Loss 2.2078\n",
      "  Iter 650/1406: Loss 2.2412\n",
      "  Iter 660/1406: Loss 2.5342\n",
      "  Iter 670/1406: Loss 2.1103\n",
      "  Iter 680/1406: Loss 2.2231\n",
      "  Iter 690/1406: Loss 2.0683\n",
      "  Iter 700/1406: Loss 1.9726\n",
      "  Iter 710/1406: Loss 2.4099\n",
      "  Iter 720/1406: Loss 2.0705\n",
      "  Iter 730/1406: Loss 2.0206\n",
      "  Iter 740/1406: Loss 2.4160\n",
      "  Iter 750/1406: Loss 2.5086\n",
      "  Iter 760/1406: Loss 2.2797\n",
      "  Iter 770/1406: Loss 1.9372\n",
      "  Iter 780/1406: Loss 1.9364\n",
      "  Iter 790/1406: Loss 1.8053\n",
      "  Iter 800/1406: Loss 1.9405\n",
      "  Iter 810/1406: Loss 1.9784\n",
      "  Iter 820/1406: Loss 2.0778\n",
      "  Iter 830/1406: Loss 2.1977\n",
      "  Iter 840/1406: Loss 1.6527\n",
      "  Iter 850/1406: Loss 2.1816\n",
      "  Iter 860/1406: Loss 2.2710\n",
      "  Iter 870/1406: Loss 2.3365\n",
      "  Iter 880/1406: Loss 2.1429\n",
      "  Iter 890/1406: Loss 2.0461\n",
      "  Iter 900/1406: Loss 2.2936\n",
      "  Iter 910/1406: Loss 1.9423\n",
      "  Iter 920/1406: Loss 2.1872\n",
      "  Iter 930/1406: Loss 2.3195\n",
      "  Iter 940/1406: Loss 1.8462\n",
      "  Iter 950/1406: Loss 1.9663\n",
      "  Iter 960/1406: Loss 2.9446\n",
      "  Iter 970/1406: Loss 2.1379\n",
      "  Iter 980/1406: Loss 2.3360\n",
      "  Iter 990/1406: Loss 1.7612\n",
      "  Iter 1000/1406: Loss 2.3738\n",
      "  Iter 1010/1406: Loss 2.1717\n",
      "  Iter 1020/1406: Loss 1.8033\n",
      "  Iter 1030/1406: Loss 2.2713\n",
      "  Iter 1040/1406: Loss 2.1985\n",
      "  Iter 1050/1406: Loss 2.4707\n",
      "  Iter 1060/1406: Loss 2.5030\n",
      "  Iter 1070/1406: Loss 1.8395\n",
      "  Iter 1080/1406: Loss 1.9850\n",
      "  Iter 1090/1406: Loss 2.1216\n",
      "  Iter 1100/1406: Loss 1.7970\n",
      "  Iter 1110/1406: Loss 1.7561\n",
      "  Iter 1120/1406: Loss 2.3287\n",
      "  Iter 1130/1406: Loss 2.1042\n",
      "  Iter 1140/1406: Loss 2.4878\n",
      "  Iter 1150/1406: Loss 2.0155\n",
      "  Iter 1160/1406: Loss 2.8814\n",
      "  Iter 1170/1406: Loss 2.3533\n",
      "  Iter 1180/1406: Loss 2.2169\n",
      "  Iter 1190/1406: Loss 2.3661\n",
      "  Iter 1200/1406: Loss 2.1046\n",
      "  Iter 1210/1406: Loss 1.6374\n",
      "  Iter 1220/1406: Loss 2.2149\n",
      "  Iter 1230/1406: Loss 2.3672\n",
      "  Iter 1240/1406: Loss 2.0788\n",
      "  Iter 1250/1406: Loss 2.2740\n",
      "  Iter 1260/1406: Loss 2.0942\n",
      "  Iter 1270/1406: Loss 2.1109\n",
      "  Iter 1280/1406: Loss 2.2453\n",
      "  Iter 1290/1406: Loss 2.3021\n",
      "  Iter 1300/1406: Loss 2.3515\n",
      "  Iter 1310/1406: Loss 2.7604\n",
      "  Iter 1320/1406: Loss 2.2319\n",
      "  Iter 1330/1406: Loss 1.8427\n",
      "  Iter 1340/1406: Loss 1.6819\n",
      "  Iter 1350/1406: Loss 2.0784\n",
      "  Iter 1360/1406: Loss 2.4372\n",
      "  Iter 1370/1406: Loss 2.1234\n",
      "  Iter 1380/1406: Loss 2.1159\n",
      "  Iter 1390/1406: Loss 2.5151\n",
      "  Iter 1400/1406: Loss 1.5920\n",
      "Train acc: 0.4690, Test acc: 0.3533 (Time: 2722.42s)\n",
      "\n",
      "[Epoch 14]\n",
      "  Iter   0/1406: Loss 2.1216\n",
      "  Iter  10/1406: Loss 2.5068\n",
      "  Iter  20/1406: Loss 2.2683\n",
      "  Iter  30/1406: Loss 2.1991\n",
      "  Iter  40/1406: Loss 1.8393\n",
      "  Iter  50/1406: Loss 2.5446\n",
      "  Iter  60/1406: Loss 2.3988\n",
      "  Iter  70/1406: Loss 2.1750\n",
      "  Iter  80/1406: Loss 2.0106\n",
      "  Iter  90/1406: Loss 1.9274\n",
      "  Iter 100/1406: Loss 2.4423\n",
      "  Iter 110/1406: Loss 2.3513\n",
      "  Iter 120/1406: Loss 1.9849\n",
      "  Iter 130/1406: Loss 2.0659\n",
      "  Iter 140/1406: Loss 1.8714\n",
      "  Iter 150/1406: Loss 1.6726\n",
      "  Iter 160/1406: Loss 2.3892\n",
      "  Iter 170/1406: Loss 2.0078\n",
      "  Iter 180/1406: Loss 2.4496\n",
      "  Iter 190/1406: Loss 2.5590\n",
      "  Iter 200/1406: Loss 2.0438\n",
      "  Iter 210/1406: Loss 2.0602\n",
      "  Iter 220/1406: Loss 1.7351\n",
      "  Iter 230/1406: Loss 2.0926\n",
      "  Iter 240/1406: Loss 1.8313\n",
      "  Iter 250/1406: Loss 2.5142\n",
      "  Iter 260/1406: Loss 1.8150\n",
      "  Iter 270/1406: Loss 1.9096\n",
      "  Iter 280/1406: Loss 2.0493\n",
      "  Iter 290/1406: Loss 1.8734\n",
      "  Iter 300/1406: Loss 1.8708\n",
      "  Iter 310/1406: Loss 2.1073\n",
      "  Iter 320/1406: Loss 1.8477\n",
      "  Iter 330/1406: Loss 1.8178\n",
      "  Iter 340/1406: Loss 1.9360\n",
      "  Iter 350/1406: Loss 2.1498\n",
      "  Iter 360/1406: Loss 2.2048\n",
      "  Iter 370/1406: Loss 1.9365\n",
      "  Iter 380/1406: Loss 2.1386\n",
      "  Iter 390/1406: Loss 2.3290\n",
      "  Iter 400/1406: Loss 1.6664\n",
      "  Iter 410/1406: Loss 2.3392\n",
      "  Iter 420/1406: Loss 2.0577\n",
      "  Iter 430/1406: Loss 2.0300\n",
      "  Iter 440/1406: Loss 2.2587\n",
      "  Iter 450/1406: Loss 1.6570\n",
      "  Iter 460/1406: Loss 2.2814\n",
      "  Iter 470/1406: Loss 1.9269\n",
      "  Iter 480/1406: Loss 1.8686\n",
      "  Iter 490/1406: Loss 1.9959\n",
      "  Iter 500/1406: Loss 1.8720\n",
      "  Iter 510/1406: Loss 2.2179\n",
      "  Iter 520/1406: Loss 1.8074\n",
      "  Iter 530/1406: Loss 2.3034\n",
      "  Iter 540/1406: Loss 2.3963\n",
      "  Iter 550/1406: Loss 2.5959\n",
      "  Iter 560/1406: Loss 1.6579\n",
      "  Iter 570/1406: Loss 2.1477\n",
      "  Iter 580/1406: Loss 2.2783\n",
      "  Iter 590/1406: Loss 2.1604\n",
      "  Iter 600/1406: Loss 2.6809\n",
      "  Iter 610/1406: Loss 1.9368\n",
      "  Iter 620/1406: Loss 2.4734\n",
      "  Iter 630/1406: Loss 1.8293\n",
      "  Iter 640/1406: Loss 1.6042\n",
      "  Iter 650/1406: Loss 2.0809\n",
      "  Iter 660/1406: Loss 2.4668\n",
      "  Iter 670/1406: Loss 1.5118\n",
      "  Iter 680/1406: Loss 2.4762\n",
      "  Iter 690/1406: Loss 1.8530\n",
      "  Iter 700/1406: Loss 2.0851\n",
      "  Iter 710/1406: Loss 1.8627\n",
      "  Iter 720/1406: Loss 2.5011\n",
      "  Iter 730/1406: Loss 1.7395\n",
      "  Iter 740/1406: Loss 2.2276\n",
      "  Iter 750/1406: Loss 1.9646\n",
      "  Iter 760/1406: Loss 2.3934\n",
      "  Iter 770/1406: Loss 2.1524\n",
      "  Iter 780/1406: Loss 1.5578\n",
      "  Iter 790/1406: Loss 2.4745\n",
      "  Iter 800/1406: Loss 1.8201\n",
      "  Iter 810/1406: Loss 1.8703\n",
      "  Iter 820/1406: Loss 1.9259\n",
      "  Iter 830/1406: Loss 1.9960\n",
      "  Iter 840/1406: Loss 2.4082\n",
      "  Iter 850/1406: Loss 1.9285\n",
      "  Iter 860/1406: Loss 2.2453\n",
      "  Iter 870/1406: Loss 2.5691\n",
      "  Iter 880/1406: Loss 2.0841\n",
      "  Iter 890/1406: Loss 1.7502\n",
      "  Iter 900/1406: Loss 2.1168\n",
      "  Iter 910/1406: Loss 1.6551\n",
      "  Iter 920/1406: Loss 2.1387\n",
      "  Iter 930/1406: Loss 2.2407\n",
      "  Iter 940/1406: Loss 1.9918\n",
      "  Iter 950/1406: Loss 1.6545\n",
      "  Iter 960/1406: Loss 2.3351\n",
      "  Iter 970/1406: Loss 1.8651\n",
      "  Iter 980/1406: Loss 1.9951\n",
      "  Iter 990/1406: Loss 1.9683\n",
      "  Iter 1000/1406: Loss 2.4209\n",
      "  Iter 1010/1406: Loss 2.2326\n",
      "  Iter 1020/1406: Loss 2.2642\n",
      "  Iter 1030/1406: Loss 2.0591\n",
      "  Iter 1040/1406: Loss 1.9621\n",
      "  Iter 1050/1406: Loss 2.0503\n",
      "  Iter 1060/1406: Loss 2.0669\n",
      "  Iter 1070/1406: Loss 1.7726\n",
      "  Iter 1080/1406: Loss 1.9933\n",
      "  Iter 1090/1406: Loss 1.9501\n",
      "  Iter 1100/1406: Loss 1.9744\n",
      "  Iter 1110/1406: Loss 1.5613\n",
      "  Iter 1120/1406: Loss 1.6981\n",
      "  Iter 1130/1406: Loss 2.1726\n",
      "  Iter 1140/1406: Loss 2.7172\n",
      "  Iter 1150/1406: Loss 2.1896\n",
      "  Iter 1160/1406: Loss 2.1841\n",
      "  Iter 1170/1406: Loss 2.4357\n",
      "  Iter 1180/1406: Loss 1.6936\n",
      "  Iter 1190/1406: Loss 2.2398\n",
      "  Iter 1200/1406: Loss 2.2827\n",
      "  Iter 1210/1406: Loss 1.8593\n",
      "  Iter 1220/1406: Loss 2.5442\n",
      "  Iter 1230/1406: Loss 2.5616\n",
      "  Iter 1240/1406: Loss 1.7666\n",
      "  Iter 1250/1406: Loss 1.9276\n",
      "  Iter 1260/1406: Loss 2.7040\n",
      "  Iter 1270/1406: Loss 2.2173\n",
      "  Iter 1280/1406: Loss 2.5715\n",
      "  Iter 1290/1406: Loss 2.1520\n",
      "  Iter 1300/1406: Loss 1.6905\n",
      "  Iter 1310/1406: Loss 2.1794\n",
      "  Iter 1320/1406: Loss 2.0421\n",
      "  Iter 1330/1406: Loss 2.5083\n",
      "  Iter 1340/1406: Loss 1.9438\n",
      "  Iter 1350/1406: Loss 1.5283\n",
      "  Iter 1360/1406: Loss 1.8516\n",
      "  Iter 1370/1406: Loss 1.9298\n",
      "  Iter 1380/1406: Loss 2.4226\n",
      "  Iter 1390/1406: Loss 2.1206\n",
      "  Iter 1400/1406: Loss 1.9382\n",
      "Train acc: 0.4850, Test acc: 0.3549 (Time: 2721.24s)\n",
      "\n",
      "[Epoch 15]\n",
      "  Iter   0/1406: Loss 1.7693\n",
      "  Iter  10/1406: Loss 2.0111\n",
      "  Iter  20/1406: Loss 1.8370\n",
      "  Iter  30/1406: Loss 2.4893\n",
      "  Iter  40/1406: Loss 1.7315\n",
      "  Iter  50/1406: Loss 2.3511\n",
      "  Iter  60/1406: Loss 2.3902\n",
      "  Iter  70/1406: Loss 1.9209\n",
      "  Iter  80/1406: Loss 2.2690\n",
      "  Iter  90/1406: Loss 2.5855\n",
      "  Iter 100/1406: Loss 1.7304\n",
      "  Iter 110/1406: Loss 2.2292\n",
      "  Iter 120/1406: Loss 1.7459\n",
      "  Iter 130/1406: Loss 2.0007\n",
      "  Iter 140/1406: Loss 1.7003\n",
      "  Iter 150/1406: Loss 2.3298\n",
      "  Iter 160/1406: Loss 2.3060\n",
      "  Iter 170/1406: Loss 2.0297\n",
      "  Iter 180/1406: Loss 1.9435\n",
      "  Iter 190/1406: Loss 2.0429\n",
      "  Iter 200/1406: Loss 2.0576\n",
      "  Iter 210/1406: Loss 1.9501\n",
      "  Iter 220/1406: Loss 1.6688\n",
      "  Iter 230/1406: Loss 1.8789\n",
      "  Iter 240/1406: Loss 1.9818\n",
      "  Iter 250/1406: Loss 1.9019\n",
      "  Iter 260/1406: Loss 1.9848\n",
      "  Iter 270/1406: Loss 2.6480\n",
      "  Iter 280/1406: Loss 1.7778\n",
      "  Iter 290/1406: Loss 2.0214\n",
      "  Iter 300/1406: Loss 1.8936\n",
      "  Iter 310/1406: Loss 2.2584\n",
      "  Iter 320/1406: Loss 2.3701\n",
      "  Iter 330/1406: Loss 2.2218\n",
      "  Iter 340/1406: Loss 1.8296\n",
      "  Iter 350/1406: Loss 1.9869\n",
      "  Iter 360/1406: Loss 1.7600\n",
      "  Iter 370/1406: Loss 2.0203\n",
      "  Iter 380/1406: Loss 2.3479\n",
      "  Iter 390/1406: Loss 2.2198\n",
      "  Iter 400/1406: Loss 1.8196\n",
      "  Iter 410/1406: Loss 2.1443\n",
      "  Iter 420/1406: Loss 1.5360\n",
      "  Iter 430/1406: Loss 2.0768\n",
      "  Iter 440/1406: Loss 2.1742\n",
      "  Iter 450/1406: Loss 1.9649\n",
      "  Iter 460/1406: Loss 1.8379\n",
      "  Iter 470/1406: Loss 1.7126\n",
      "  Iter 480/1406: Loss 2.0850\n",
      "  Iter 490/1406: Loss 2.1235\n",
      "  Iter 500/1406: Loss 2.5361\n",
      "  Iter 510/1406: Loss 1.9905\n",
      "  Iter 520/1406: Loss 1.9469\n",
      "  Iter 530/1406: Loss 2.1215\n",
      "  Iter 540/1406: Loss 2.0509\n",
      "  Iter 550/1406: Loss 1.7610\n",
      "  Iter 560/1406: Loss 2.4510\n",
      "  Iter 570/1406: Loss 2.4988\n",
      "  Iter 580/1406: Loss 2.1486\n",
      "  Iter 590/1406: Loss 1.9543\n",
      "  Iter 600/1406: Loss 1.9623\n",
      "  Iter 610/1406: Loss 2.1397\n",
      "  Iter 620/1406: Loss 1.5842\n",
      "  Iter 630/1406: Loss 1.9969\n",
      "  Iter 640/1406: Loss 2.0841\n",
      "  Iter 650/1406: Loss 2.2914\n",
      "  Iter 660/1406: Loss 2.1232\n",
      "  Iter 670/1406: Loss 1.8385\n",
      "  Iter 680/1406: Loss 2.1025\n",
      "  Iter 690/1406: Loss 2.0933\n",
      "  Iter 700/1406: Loss 2.1386\n",
      "  Iter 710/1406: Loss 2.2054\n",
      "  Iter 720/1406: Loss 2.0915\n",
      "  Iter 730/1406: Loss 1.8807\n",
      "  Iter 740/1406: Loss 1.6833\n",
      "  Iter 750/1406: Loss 1.6807\n",
      "  Iter 760/1406: Loss 2.1421\n",
      "  Iter 770/1406: Loss 1.9818\n",
      "  Iter 780/1406: Loss 1.7403\n",
      "  Iter 790/1406: Loss 2.1462\n",
      "  Iter 800/1406: Loss 2.0939\n",
      "  Iter 810/1406: Loss 2.4168\n",
      "  Iter 820/1406: Loss 1.7707\n",
      "  Iter 830/1406: Loss 1.9447\n",
      "  Iter 840/1406: Loss 2.5517\n",
      "  Iter 850/1406: Loss 1.7725\n",
      "  Iter 860/1406: Loss 1.6413\n",
      "  Iter 870/1406: Loss 2.0542\n",
      "  Iter 880/1406: Loss 2.3012\n",
      "  Iter 890/1406: Loss 2.1749\n",
      "  Iter 900/1406: Loss 2.3408\n",
      "  Iter 910/1406: Loss 1.8389\n",
      "  Iter 920/1406: Loss 1.7079\n",
      "  Iter 930/1406: Loss 1.8429\n",
      "  Iter 940/1406: Loss 1.6431\n",
      "  Iter 950/1406: Loss 2.0367\n",
      "  Iter 960/1406: Loss 2.1899\n",
      "  Iter 970/1406: Loss 1.7957\n",
      "  Iter 980/1406: Loss 2.5501\n",
      "  Iter 990/1406: Loss 1.7325\n",
      "  Iter 1000/1406: Loss 1.8548\n",
      "  Iter 1010/1406: Loss 2.1059\n",
      "  Iter 1020/1406: Loss 1.8015\n",
      "  Iter 1030/1406: Loss 1.9474\n",
      "  Iter 1040/1406: Loss 1.9253\n",
      "  Iter 1050/1406: Loss 2.1015\n",
      "  Iter 1060/1406: Loss 1.7434\n",
      "  Iter 1070/1406: Loss 2.6507\n",
      "  Iter 1080/1406: Loss 2.0343\n",
      "  Iter 1090/1406: Loss 2.3953\n",
      "  Iter 1100/1406: Loss 2.0031\n",
      "  Iter 1110/1406: Loss 1.9474\n",
      "  Iter 1120/1406: Loss 2.2567\n",
      "  Iter 1130/1406: Loss 2.2001\n",
      "  Iter 1140/1406: Loss 2.3088\n",
      "  Iter 1150/1406: Loss 2.1384\n",
      "  Iter 1160/1406: Loss 1.7557\n",
      "  Iter 1170/1406: Loss 1.6722\n",
      "  Iter 1180/1406: Loss 1.9834\n",
      "  Iter 1190/1406: Loss 2.0883\n",
      "  Iter 1200/1406: Loss 2.0801\n",
      "  Iter 1210/1406: Loss 2.0423\n",
      "  Iter 1220/1406: Loss 2.0169\n",
      "  Iter 1230/1406: Loss 1.9900\n",
      "  Iter 1240/1406: Loss 1.6175\n",
      "  Iter 1250/1406: Loss 2.0420\n",
      "  Iter 1260/1406: Loss 2.0952\n",
      "  Iter 1270/1406: Loss 2.0917\n",
      "  Iter 1280/1406: Loss 2.1108\n",
      "  Iter 1290/1406: Loss 2.1000\n",
      "  Iter 1300/1406: Loss 1.7506\n",
      "  Iter 1310/1406: Loss 1.9535\n",
      "  Iter 1320/1406: Loss 2.1761\n",
      "  Iter 1330/1406: Loss 1.8418\n",
      "  Iter 1340/1406: Loss 2.1033\n",
      "  Iter 1350/1406: Loss 2.1287\n",
      "  Iter 1360/1406: Loss 2.0796\n",
      "  Iter 1370/1406: Loss 1.6332\n",
      "  Iter 1380/1406: Loss 2.4183\n",
      "  Iter 1390/1406: Loss 1.7397\n",
      "  Iter 1400/1406: Loss 2.1315\n",
      "Train acc: 0.4870, Test acc: 0.3566 (Time: 2719.17s)\n",
      "\n",
      "[Epoch 16]\n",
      "  Iter   0/1406: Loss 2.0775\n",
      "  Iter  10/1406: Loss 2.1327\n",
      "  Iter  20/1406: Loss 2.1214\n",
      "  Iter  30/1406: Loss 1.6570\n",
      "  Iter  40/1406: Loss 1.8819\n",
      "  Iter  50/1406: Loss 2.0207\n",
      "  Iter  60/1406: Loss 2.5949\n",
      "  Iter  70/1406: Loss 1.8611\n",
      "  Iter  80/1406: Loss 1.4717\n",
      "  Iter  90/1406: Loss 1.9969\n",
      "  Iter 100/1406: Loss 2.1407\n",
      "  Iter 110/1406: Loss 2.0147\n",
      "  Iter 120/1406: Loss 1.8754\n",
      "  Iter 130/1406: Loss 1.7595\n",
      "  Iter 140/1406: Loss 1.8333\n",
      "  Iter 150/1406: Loss 2.1518\n",
      "  Iter 160/1406: Loss 2.0667\n",
      "  Iter 170/1406: Loss 1.8493\n",
      "  Iter 180/1406: Loss 2.0791\n",
      "  Iter 190/1406: Loss 1.8809\n",
      "  Iter 200/1406: Loss 1.8037\n",
      "  Iter 210/1406: Loss 1.9756\n",
      "  Iter 220/1406: Loss 1.9672\n",
      "  Iter 230/1406: Loss 2.3909\n",
      "  Iter 240/1406: Loss 1.9684\n",
      "  Iter 250/1406: Loss 1.6671\n",
      "  Iter 260/1406: Loss 1.9443\n",
      "  Iter 270/1406: Loss 2.1183\n",
      "  Iter 280/1406: Loss 2.3386\n",
      "  Iter 290/1406: Loss 1.6480\n",
      "  Iter 300/1406: Loss 2.2167\n",
      "  Iter 310/1406: Loss 2.0917\n",
      "  Iter 320/1406: Loss 1.7290\n",
      "  Iter 330/1406: Loss 1.8696\n",
      "  Iter 340/1406: Loss 2.7819\n",
      "  Iter 350/1406: Loss 2.1062\n",
      "  Iter 360/1406: Loss 2.0111\n",
      "  Iter 370/1406: Loss 1.8819\n",
      "  Iter 380/1406: Loss 1.7684\n",
      "  Iter 390/1406: Loss 2.1984\n",
      "  Iter 400/1406: Loss 2.2449\n",
      "  Iter 410/1406: Loss 2.0084\n",
      "  Iter 420/1406: Loss 1.7777\n",
      "  Iter 430/1406: Loss 2.0106\n",
      "  Iter 440/1406: Loss 2.0031\n",
      "  Iter 450/1406: Loss 1.5660\n",
      "  Iter 460/1406: Loss 2.2946\n",
      "  Iter 470/1406: Loss 2.5471\n",
      "  Iter 480/1406: Loss 2.1229\n",
      "  Iter 490/1406: Loss 2.0995\n",
      "  Iter 500/1406: Loss 1.8865\n",
      "  Iter 510/1406: Loss 1.9695\n",
      "  Iter 520/1406: Loss 1.7679\n",
      "  Iter 530/1406: Loss 2.0427\n",
      "  Iter 540/1406: Loss 1.9721\n",
      "  Iter 550/1406: Loss 1.6290\n",
      "  Iter 560/1406: Loss 1.8657\n",
      "  Iter 570/1406: Loss 2.0436\n",
      "  Iter 580/1406: Loss 1.8990\n",
      "  Iter 590/1406: Loss 1.9167\n",
      "  Iter 600/1406: Loss 1.9255\n",
      "  Iter 610/1406: Loss 2.2062\n",
      "  Iter 620/1406: Loss 2.3099\n",
      "  Iter 630/1406: Loss 2.4581\n",
      "  Iter 640/1406: Loss 1.8715\n",
      "  Iter 650/1406: Loss 1.8660\n",
      "  Iter 660/1406: Loss 1.7672\n",
      "  Iter 670/1406: Loss 2.3270\n",
      "  Iter 680/1406: Loss 2.0512\n",
      "  Iter 690/1406: Loss 1.7776\n",
      "  Iter 700/1406: Loss 2.0624\n",
      "  Iter 710/1406: Loss 2.0511\n",
      "  Iter 720/1406: Loss 2.2607\n",
      "  Iter 730/1406: Loss 1.5967\n",
      "  Iter 740/1406: Loss 1.7158\n",
      "  Iter 750/1406: Loss 2.2023\n",
      "  Iter 760/1406: Loss 1.6951\n",
      "  Iter 770/1406: Loss 1.7609\n",
      "  Iter 780/1406: Loss 2.0104\n",
      "  Iter 790/1406: Loss 2.0184\n",
      "  Iter 800/1406: Loss 1.8870\n",
      "  Iter 810/1406: Loss 1.5097\n",
      "  Iter 820/1406: Loss 1.6955\n",
      "  Iter 830/1406: Loss 1.6766\n",
      "  Iter 840/1406: Loss 2.1123\n",
      "  Iter 850/1406: Loss 2.0449\n",
      "  Iter 860/1406: Loss 2.3638\n",
      "  Iter 870/1406: Loss 2.1748\n",
      "  Iter 880/1406: Loss 1.8408\n",
      "  Iter 890/1406: Loss 1.7156\n",
      "  Iter 900/1406: Loss 2.5162\n",
      "  Iter 910/1406: Loss 1.9074\n",
      "  Iter 920/1406: Loss 2.0980\n",
      "  Iter 930/1406: Loss 2.3993\n",
      "  Iter 940/1406: Loss 1.9292\n",
      "  Iter 950/1406: Loss 1.8682\n",
      "  Iter 960/1406: Loss 2.0650\n",
      "  Iter 970/1406: Loss 2.3783\n",
      "  Iter 980/1406: Loss 1.9468\n",
      "  Iter 990/1406: Loss 1.9771\n",
      "  Iter 1000/1406: Loss 2.3225\n",
      "  Iter 1010/1406: Loss 1.3685\n",
      "  Iter 1020/1406: Loss 1.9060\n",
      "  Iter 1030/1406: Loss 1.5148\n",
      "  Iter 1040/1406: Loss 2.0676\n",
      "  Iter 1050/1406: Loss 1.8818\n",
      "  Iter 1060/1406: Loss 1.7857\n",
      "  Iter 1070/1406: Loss 2.1430\n",
      "  Iter 1080/1406: Loss 2.5127\n",
      "  Iter 1090/1406: Loss 1.8059\n",
      "  Iter 1100/1406: Loss 2.1285\n",
      "  Iter 1110/1406: Loss 2.3547\n",
      "  Iter 1120/1406: Loss 2.2569\n",
      "  Iter 1130/1406: Loss 2.0840\n",
      "  Iter 1140/1406: Loss 1.6395\n",
      "  Iter 1150/1406: Loss 1.4074\n",
      "  Iter 1160/1406: Loss 1.7647\n",
      "  Iter 1170/1406: Loss 2.2722\n",
      "  Iter 1180/1406: Loss 2.1675\n",
      "  Iter 1190/1406: Loss 1.7283\n",
      "  Iter 1200/1406: Loss 2.0158\n",
      "  Iter 1210/1406: Loss 1.8009\n",
      "  Iter 1220/1406: Loss 1.4476\n",
      "  Iter 1230/1406: Loss 1.9382\n",
      "  Iter 1240/1406: Loss 2.0618\n",
      "  Iter 1250/1406: Loss 1.5963\n",
      "  Iter 1260/1406: Loss 2.4701\n",
      "  Iter 1270/1406: Loss 1.9285\n",
      "  Iter 1280/1406: Loss 2.3983\n",
      "  Iter 1290/1406: Loss 1.9387\n",
      "  Iter 1300/1406: Loss 1.8604\n",
      "  Iter 1310/1406: Loss 2.4505\n",
      "  Iter 1320/1406: Loss 1.9581\n",
      "  Iter 1330/1406: Loss 2.1952\n",
      "  Iter 1340/1406: Loss 2.0902\n",
      "  Iter 1350/1406: Loss 1.9968\n",
      "  Iter 1360/1406: Loss 1.7334\n",
      "  Iter 1370/1406: Loss 2.0452\n",
      "  Iter 1380/1406: Loss 1.6236\n",
      "  Iter 1390/1406: Loss 1.8071\n",
      "  Iter 1400/1406: Loss 1.9141\n",
      "Train acc: 0.5160, Test acc: 0.3638 (Time: 2718.45s)\n",
      "\n",
      "[Epoch 17]\n",
      "  Iter   0/1406: Loss 1.7168\n",
      "  Iter  10/1406: Loss 2.1876\n",
      "  Iter  20/1406: Loss 1.9769\n",
      "  Iter  30/1406: Loss 1.8834\n",
      "  Iter  40/1406: Loss 1.8624\n",
      "  Iter  50/1406: Loss 1.8557\n",
      "  Iter  60/1406: Loss 1.9845\n",
      "  Iter  70/1406: Loss 2.0019\n",
      "  Iter  80/1406: Loss 2.3547\n",
      "  Iter  90/1406: Loss 2.0485\n",
      "  Iter 100/1406: Loss 1.5345\n",
      "  Iter 110/1406: Loss 2.1295\n",
      "  Iter 120/1406: Loss 2.1264\n",
      "  Iter 130/1406: Loss 1.4993\n",
      "  Iter 140/1406: Loss 2.0866\n",
      "  Iter 150/1406: Loss 1.6676\n",
      "  Iter 160/1406: Loss 2.0530\n",
      "  Iter 170/1406: Loss 1.6456\n",
      "  Iter 180/1406: Loss 2.6172\n",
      "  Iter 190/1406: Loss 1.7773\n",
      "  Iter 200/1406: Loss 1.3184\n",
      "  Iter 210/1406: Loss 1.7248\n",
      "  Iter 220/1406: Loss 1.3572\n",
      "  Iter 230/1406: Loss 1.6616\n",
      "  Iter 240/1406: Loss 2.1028\n",
      "  Iter 250/1406: Loss 1.5053\n",
      "  Iter 260/1406: Loss 1.9833\n",
      "  Iter 270/1406: Loss 1.3415\n",
      "  Iter 280/1406: Loss 1.7406\n",
      "  Iter 290/1406: Loss 2.0987\n",
      "  Iter 300/1406: Loss 1.7805\n",
      "  Iter 310/1406: Loss 2.2994\n",
      "  Iter 320/1406: Loss 1.5282\n",
      "  Iter 330/1406: Loss 2.3767\n",
      "  Iter 340/1406: Loss 1.8422\n",
      "  Iter 350/1406: Loss 1.8859\n",
      "  Iter 360/1406: Loss 2.3978\n",
      "  Iter 370/1406: Loss 1.7522\n",
      "  Iter 380/1406: Loss 2.1210\n",
      "  Iter 390/1406: Loss 1.5942\n",
      "  Iter 400/1406: Loss 2.3938\n",
      "  Iter 410/1406: Loss 2.0024\n",
      "  Iter 420/1406: Loss 1.9749\n",
      "  Iter 430/1406: Loss 2.1541\n",
      "  Iter 440/1406: Loss 1.3001\n",
      "  Iter 450/1406: Loss 2.0192\n",
      "  Iter 460/1406: Loss 2.3090\n",
      "  Iter 470/1406: Loss 1.7091\n",
      "  Iter 480/1406: Loss 1.5674\n",
      "  Iter 490/1406: Loss 1.9645\n",
      "  Iter 500/1406: Loss 2.0062\n",
      "  Iter 510/1406: Loss 1.6931\n",
      "  Iter 520/1406: Loss 1.9512\n",
      "  Iter 530/1406: Loss 1.6977\n",
      "  Iter 540/1406: Loss 1.9439\n",
      "  Iter 550/1406: Loss 1.2565\n",
      "  Iter 560/1406: Loss 2.1333\n",
      "  Iter 570/1406: Loss 2.0013\n",
      "  Iter 580/1406: Loss 1.9096\n",
      "  Iter 590/1406: Loss 2.0623\n",
      "  Iter 600/1406: Loss 1.8383\n",
      "  Iter 610/1406: Loss 1.4770\n",
      "  Iter 620/1406: Loss 1.5224\n",
      "  Iter 630/1406: Loss 2.0096\n",
      "  Iter 640/1406: Loss 2.2385\n",
      "  Iter 650/1406: Loss 1.7460\n",
      "  Iter 660/1406: Loss 2.2358\n",
      "  Iter 670/1406: Loss 1.9958\n",
      "  Iter 680/1406: Loss 2.2501\n",
      "  Iter 690/1406: Loss 1.6529\n",
      "  Iter 700/1406: Loss 1.6486\n",
      "  Iter 710/1406: Loss 1.9809\n",
      "  Iter 720/1406: Loss 2.0686\n",
      "  Iter 730/1406: Loss 2.0980\n",
      "  Iter 740/1406: Loss 1.8180\n",
      "  Iter 750/1406: Loss 1.7477\n",
      "  Iter 760/1406: Loss 1.7050\n",
      "  Iter 770/1406: Loss 1.7956\n",
      "  Iter 780/1406: Loss 1.7985\n",
      "  Iter 790/1406: Loss 1.6621\n",
      "  Iter 800/1406: Loss 2.1951\n",
      "  Iter 810/1406: Loss 1.6340\n",
      "  Iter 820/1406: Loss 2.0331\n",
      "  Iter 830/1406: Loss 1.7247\n",
      "  Iter 840/1406: Loss 1.6786\n",
      "  Iter 850/1406: Loss 1.7856\n",
      "  Iter 860/1406: Loss 2.1438\n",
      "  Iter 870/1406: Loss 1.8094\n",
      "  Iter 880/1406: Loss 1.9680\n",
      "  Iter 890/1406: Loss 1.7419\n",
      "  Iter 900/1406: Loss 1.6931\n",
      "  Iter 910/1406: Loss 1.6403\n",
      "  Iter 920/1406: Loss 1.9262\n",
      "  Iter 930/1406: Loss 1.6177\n",
      "  Iter 940/1406: Loss 1.8163\n",
      "  Iter 950/1406: Loss 2.2578\n",
      "  Iter 960/1406: Loss 2.4531\n",
      "  Iter 970/1406: Loss 1.9303\n",
      "  Iter 980/1406: Loss 2.0948\n",
      "  Iter 990/1406: Loss 1.8088\n",
      "  Iter 1000/1406: Loss 2.6588\n",
      "  Iter 1010/1406: Loss 2.0094\n",
      "  Iter 1020/1406: Loss 1.5355\n",
      "  Iter 1030/1406: Loss 1.7879\n",
      "  Iter 1040/1406: Loss 2.0378\n",
      "  Iter 1050/1406: Loss 1.3653\n",
      "  Iter 1060/1406: Loss 1.7692\n",
      "  Iter 1070/1406: Loss 1.7246\n",
      "  Iter 1080/1406: Loss 1.8158\n",
      "  Iter 1090/1406: Loss 1.8501\n",
      "  Iter 1100/1406: Loss 1.6685\n",
      "  Iter 1110/1406: Loss 1.8806\n",
      "  Iter 1120/1406: Loss 1.5352\n",
      "  Iter 1130/1406: Loss 1.6644\n",
      "  Iter 1140/1406: Loss 2.1693\n",
      "  Iter 1150/1406: Loss 1.8462\n",
      "  Iter 1160/1406: Loss 2.4801\n",
      "  Iter 1170/1406: Loss 1.3470\n",
      "  Iter 1180/1406: Loss 1.9416\n",
      "  Iter 1190/1406: Loss 1.8865\n",
      "  Iter 1200/1406: Loss 2.1108\n",
      "  Iter 1210/1406: Loss 1.5325\n",
      "  Iter 1220/1406: Loss 2.2603\n",
      "  Iter 1230/1406: Loss 2.1995\n",
      "  Iter 1240/1406: Loss 1.6210\n",
      "  Iter 1250/1406: Loss 1.7243\n",
      "  Iter 1260/1406: Loss 1.8854\n",
      "  Iter 1270/1406: Loss 1.4435\n",
      "  Iter 1280/1406: Loss 1.7453\n",
      "  Iter 1290/1406: Loss 2.4130\n",
      "  Iter 1300/1406: Loss 1.7363\n",
      "  Iter 1310/1406: Loss 1.5351\n",
      "  Iter 1320/1406: Loss 1.7858\n",
      "  Iter 1330/1406: Loss 2.1313\n",
      "  Iter 1340/1406: Loss 1.6747\n",
      "  Iter 1350/1406: Loss 1.9298\n",
      "  Iter 1360/1406: Loss 1.8676\n",
      "  Iter 1370/1406: Loss 1.7211\n",
      "  Iter 1380/1406: Loss 2.0881\n",
      "  Iter 1390/1406: Loss 2.0449\n",
      "  Iter 1400/1406: Loss 2.2986\n",
      "Train acc: 0.5260, Test acc: 0.3635 (Time: 2714.35s)\n",
      "\n",
      "[Epoch 18]\n",
      "  Iter   0/1406: Loss 1.6450\n",
      "  Iter  10/1406: Loss 1.4566\n",
      "  Iter  20/1406: Loss 1.9873\n",
      "  Iter  30/1406: Loss 2.3893\n",
      "  Iter  40/1406: Loss 2.2360\n",
      "  Iter  50/1406: Loss 1.3692\n",
      "  Iter  60/1406: Loss 1.6824\n",
      "  Iter  70/1406: Loss 1.9944\n",
      "  Iter  80/1406: Loss 1.5586\n",
      "  Iter  90/1406: Loss 2.1597\n",
      "  Iter 100/1406: Loss 2.2398\n",
      "  Iter 110/1406: Loss 1.8376\n",
      "  Iter 120/1406: Loss 2.2236\n",
      "  Iter 130/1406: Loss 1.5006\n",
      "  Iter 140/1406: Loss 1.5179\n",
      "  Iter 150/1406: Loss 1.8998\n",
      "  Iter 160/1406: Loss 2.1688\n",
      "  Iter 170/1406: Loss 1.7122\n",
      "  Iter 180/1406: Loss 2.3998\n",
      "  Iter 190/1406: Loss 1.7445\n",
      "  Iter 200/1406: Loss 1.7961\n",
      "  Iter 210/1406: Loss 1.7737\n",
      "  Iter 220/1406: Loss 1.9152\n",
      "  Iter 230/1406: Loss 1.6478\n",
      "  Iter 240/1406: Loss 2.0142\n",
      "  Iter 250/1406: Loss 1.7196\n",
      "  Iter 260/1406: Loss 1.5370\n",
      "  Iter 270/1406: Loss 2.1346\n",
      "  Iter 280/1406: Loss 1.2916\n",
      "  Iter 290/1406: Loss 2.0602\n",
      "  Iter 300/1406: Loss 1.6101\n",
      "  Iter 310/1406: Loss 1.6100\n",
      "  Iter 320/1406: Loss 2.3003\n",
      "  Iter 330/1406: Loss 1.8357\n",
      "  Iter 340/1406: Loss 2.4863\n",
      "  Iter 350/1406: Loss 2.0156\n",
      "  Iter 360/1406: Loss 1.8964\n",
      "  Iter 370/1406: Loss 2.1647\n",
      "  Iter 380/1406: Loss 1.7849\n",
      "  Iter 390/1406: Loss 1.9212\n",
      "  Iter 400/1406: Loss 1.7334\n",
      "  Iter 410/1406: Loss 1.8354\n",
      "  Iter 420/1406: Loss 1.7688\n",
      "  Iter 430/1406: Loss 1.8924\n",
      "  Iter 440/1406: Loss 1.5409\n",
      "  Iter 450/1406: Loss 1.8226\n",
      "  Iter 460/1406: Loss 1.9670\n",
      "  Iter 470/1406: Loss 2.0496\n",
      "  Iter 480/1406: Loss 1.7282\n",
      "  Iter 490/1406: Loss 1.5735\n",
      "  Iter 500/1406: Loss 2.4549\n",
      "  Iter 510/1406: Loss 2.0672\n",
      "  Iter 520/1406: Loss 1.4499\n",
      "  Iter 530/1406: Loss 1.8834\n",
      "  Iter 540/1406: Loss 1.8694\n",
      "  Iter 550/1406: Loss 1.8668\n",
      "  Iter 560/1406: Loss 1.7797\n",
      "  Iter 570/1406: Loss 1.5155\n",
      "  Iter 580/1406: Loss 1.8799\n",
      "  Iter 590/1406: Loss 1.7094\n",
      "  Iter 600/1406: Loss 1.5521\n",
      "  Iter 610/1406: Loss 1.5183\n",
      "  Iter 620/1406: Loss 1.7667\n",
      "  Iter 630/1406: Loss 2.3289\n",
      "  Iter 640/1406: Loss 1.8749\n",
      "  Iter 650/1406: Loss 1.9521\n",
      "  Iter 660/1406: Loss 2.0985\n",
      "  Iter 670/1406: Loss 2.0050\n",
      "  Iter 680/1406: Loss 1.4757\n",
      "  Iter 690/1406: Loss 1.6030\n",
      "  Iter 700/1406: Loss 1.7158\n",
      "  Iter 710/1406: Loss 1.6318\n",
      "  Iter 720/1406: Loss 1.8139\n",
      "  Iter 730/1406: Loss 2.0451\n",
      "  Iter 740/1406: Loss 1.9169\n",
      "  Iter 750/1406: Loss 2.7626\n",
      "  Iter 760/1406: Loss 1.4582\n",
      "  Iter 770/1406: Loss 2.0879\n",
      "  Iter 780/1406: Loss 1.6495\n",
      "  Iter 790/1406: Loss 2.0971\n",
      "  Iter 800/1406: Loss 2.0371\n",
      "  Iter 810/1406: Loss 1.7574\n",
      "  Iter 820/1406: Loss 1.5475\n",
      "  Iter 830/1406: Loss 1.7598\n",
      "  Iter 840/1406: Loss 1.8720\n",
      "  Iter 850/1406: Loss 1.9867\n",
      "  Iter 860/1406: Loss 1.9130\n",
      "  Iter 870/1406: Loss 2.1223\n",
      "  Iter 880/1406: Loss 2.1870\n",
      "  Iter 890/1406: Loss 2.2732\n",
      "  Iter 900/1406: Loss 2.0233\n",
      "  Iter 910/1406: Loss 1.8175\n",
      "  Iter 920/1406: Loss 1.6275\n",
      "  Iter 930/1406: Loss 1.8313\n",
      "  Iter 940/1406: Loss 2.2094\n",
      "  Iter 950/1406: Loss 1.9946\n",
      "  Iter 960/1406: Loss 1.6673\n",
      "  Iter 970/1406: Loss 1.9695\n",
      "  Iter 980/1406: Loss 1.9559\n",
      "  Iter 990/1406: Loss 1.8920\n",
      "  Iter 1000/1406: Loss 1.7592\n",
      "  Iter 1010/1406: Loss 1.8467\n",
      "  Iter 1020/1406: Loss 1.6016\n",
      "  Iter 1030/1406: Loss 1.9697\n",
      "  Iter 1040/1406: Loss 1.9178\n",
      "  Iter 1050/1406: Loss 2.0123\n",
      "  Iter 1060/1406: Loss 1.7103\n",
      "  Iter 1070/1406: Loss 2.0068\n",
      "  Iter 1080/1406: Loss 1.4290\n",
      "  Iter 1090/1406: Loss 2.1440\n",
      "  Iter 1100/1406: Loss 2.2742\n",
      "  Iter 1110/1406: Loss 1.6161\n",
      "  Iter 1120/1406: Loss 1.7314\n",
      "  Iter 1130/1406: Loss 2.0014\n",
      "  Iter 1140/1406: Loss 2.0646\n",
      "  Iter 1150/1406: Loss 1.6258\n",
      "  Iter 1160/1406: Loss 2.2291\n",
      "  Iter 1170/1406: Loss 2.3830\n",
      "  Iter 1180/1406: Loss 1.4570\n",
      "  Iter 1190/1406: Loss 2.3240\n",
      "  Iter 1200/1406: Loss 1.9738\n",
      "  Iter 1210/1406: Loss 1.5765\n",
      "  Iter 1220/1406: Loss 1.8012\n",
      "  Iter 1230/1406: Loss 1.7816\n",
      "  Iter 1240/1406: Loss 1.8784\n",
      "  Iter 1250/1406: Loss 1.7357\n",
      "  Iter 1260/1406: Loss 2.0328\n",
      "  Iter 1270/1406: Loss 1.8036\n",
      "  Iter 1280/1406: Loss 2.0102\n",
      "  Iter 1290/1406: Loss 1.8371\n",
      "  Iter 1300/1406: Loss 1.8393\n",
      "  Iter 1310/1406: Loss 1.9998\n",
      "  Iter 1320/1406: Loss 1.7512\n",
      "  Iter 1330/1406: Loss 1.9424\n",
      "  Iter 1340/1406: Loss 1.5330\n",
      "  Iter 1350/1406: Loss 2.0170\n",
      "  Iter 1360/1406: Loss 1.8265\n",
      "  Iter 1370/1406: Loss 2.1045\n",
      "  Iter 1380/1406: Loss 1.4390\n",
      "  Iter 1390/1406: Loss 1.8673\n",
      "  Iter 1400/1406: Loss 1.8087\n",
      "Train acc: 0.5430, Test acc: 0.3642 (Time: 2700.09s)\n",
      "\n",
      "[Epoch 19]\n",
      "  Iter   0/1406: Loss 2.3438\n",
      "  Iter  10/1406: Loss 1.7634\n",
      "  Iter  20/1406: Loss 2.4285\n",
      "  Iter  30/1406: Loss 1.9151\n",
      "  Iter  40/1406: Loss 1.9556\n",
      "  Iter  50/1406: Loss 2.2003\n",
      "  Iter  60/1406: Loss 2.2743\n",
      "  Iter  70/1406: Loss 2.9274\n",
      "  Iter  80/1406: Loss 1.5698\n",
      "  Iter  90/1406: Loss 1.4306\n",
      "  Iter 100/1406: Loss 1.9480\n",
      "  Iter 110/1406: Loss 1.7948\n",
      "  Iter 120/1406: Loss 1.8874\n",
      "  Iter 130/1406: Loss 2.1592\n",
      "  Iter 140/1406: Loss 2.2740\n",
      "  Iter 150/1406: Loss 1.6009\n",
      "  Iter 160/1406: Loss 2.0953\n",
      "  Iter 170/1406: Loss 2.0183\n",
      "  Iter 180/1406: Loss 1.7018\n",
      "  Iter 190/1406: Loss 2.1251\n",
      "  Iter 200/1406: Loss 2.1261\n",
      "  Iter 210/1406: Loss 2.0294\n",
      "  Iter 220/1406: Loss 1.6115\n",
      "  Iter 230/1406: Loss 1.3639\n",
      "  Iter 240/1406: Loss 1.3746\n",
      "  Iter 250/1406: Loss 1.8434\n",
      "  Iter 260/1406: Loss 1.9193\n",
      "  Iter 270/1406: Loss 1.6601\n",
      "  Iter 280/1406: Loss 2.0179\n",
      "  Iter 290/1406: Loss 2.0542\n",
      "  Iter 300/1406: Loss 2.0414\n",
      "  Iter 310/1406: Loss 2.2520\n",
      "  Iter 320/1406: Loss 1.6343\n",
      "  Iter 330/1406: Loss 1.9139\n",
      "  Iter 340/1406: Loss 1.9378\n",
      "  Iter 350/1406: Loss 1.8393\n",
      "  Iter 360/1406: Loss 1.6540\n",
      "  Iter 370/1406: Loss 1.8501\n",
      "  Iter 380/1406: Loss 1.5766\n",
      "  Iter 390/1406: Loss 1.6314\n",
      "  Iter 400/1406: Loss 1.9301\n",
      "  Iter 410/1406: Loss 2.1657\n",
      "  Iter 420/1406: Loss 1.9646\n",
      "  Iter 430/1406: Loss 2.2781\n",
      "  Iter 440/1406: Loss 1.5997\n",
      "  Iter 450/1406: Loss 1.9901\n",
      "  Iter 460/1406: Loss 1.3989\n",
      "  Iter 470/1406: Loss 2.1519\n",
      "  Iter 480/1406: Loss 1.8624\n",
      "  Iter 490/1406: Loss 1.8536\n",
      "  Iter 500/1406: Loss 2.7117\n",
      "  Iter 510/1406: Loss 1.7617\n",
      "  Iter 520/1406: Loss 1.9514\n",
      "  Iter 530/1406: Loss 1.6912\n",
      "  Iter 540/1406: Loss 1.7836\n",
      "  Iter 550/1406: Loss 2.0197\n",
      "  Iter 560/1406: Loss 2.2002\n",
      "  Iter 570/1406: Loss 1.7008\n",
      "  Iter 580/1406: Loss 1.8219\n",
      "  Iter 590/1406: Loss 1.9683\n",
      "  Iter 600/1406: Loss 1.4077\n",
      "  Iter 610/1406: Loss 1.5452\n",
      "  Iter 620/1406: Loss 1.8292\n",
      "  Iter 630/1406: Loss 1.8827\n",
      "  Iter 640/1406: Loss 1.7423\n",
      "  Iter 650/1406: Loss 1.8232\n",
      "  Iter 660/1406: Loss 1.4733\n",
      "  Iter 670/1406: Loss 2.2220\n",
      "  Iter 680/1406: Loss 1.8262\n",
      "  Iter 690/1406: Loss 1.6073\n",
      "  Iter 700/1406: Loss 2.2404\n",
      "  Iter 710/1406: Loss 1.4850\n",
      "  Iter 720/1406: Loss 1.3142\n",
      "  Iter 730/1406: Loss 1.7236\n",
      "  Iter 740/1406: Loss 1.5763\n",
      "  Iter 750/1406: Loss 1.9072\n",
      "  Iter 760/1406: Loss 2.0868\n",
      "  Iter 770/1406: Loss 1.7378\n",
      "  Iter 780/1406: Loss 1.6348\n",
      "  Iter 790/1406: Loss 1.7831\n",
      "  Iter 800/1406: Loss 1.8011\n",
      "  Iter 810/1406: Loss 1.7813\n",
      "  Iter 820/1406: Loss 2.1301\n",
      "  Iter 830/1406: Loss 1.9782\n",
      "  Iter 840/1406: Loss 1.9873\n",
      "  Iter 850/1406: Loss 1.8621\n",
      "  Iter 860/1406: Loss 1.9428\n",
      "  Iter 870/1406: Loss 1.7231\n",
      "  Iter 880/1406: Loss 2.1633\n",
      "  Iter 890/1406: Loss 1.8970\n",
      "  Iter 900/1406: Loss 2.1181\n",
      "  Iter 910/1406: Loss 2.0092\n",
      "  Iter 920/1406: Loss 1.8076\n",
      "  Iter 930/1406: Loss 1.6585\n",
      "  Iter 940/1406: Loss 1.3874\n",
      "  Iter 950/1406: Loss 1.6404\n",
      "  Iter 960/1406: Loss 2.4152\n",
      "  Iter 970/1406: Loss 1.9609\n",
      "  Iter 980/1406: Loss 1.6419\n",
      "  Iter 990/1406: Loss 1.8077\n",
      "  Iter 1000/1406: Loss 1.7000\n",
      "  Iter 1010/1406: Loss 1.8011\n",
      "  Iter 1020/1406: Loss 1.4484\n",
      "  Iter 1030/1406: Loss 1.8805\n",
      "  Iter 1040/1406: Loss 1.6842\n",
      "  Iter 1050/1406: Loss 1.8697\n",
      "  Iter 1060/1406: Loss 1.3445\n",
      "  Iter 1070/1406: Loss 2.1879\n",
      "  Iter 1080/1406: Loss 1.6760\n",
      "  Iter 1090/1406: Loss 1.5832\n",
      "  Iter 1100/1406: Loss 2.0685\n",
      "  Iter 1110/1406: Loss 1.8389\n",
      "  Iter 1120/1406: Loss 1.5579\n",
      "  Iter 1130/1406: Loss 1.8849\n",
      "  Iter 1140/1406: Loss 1.9574\n",
      "  Iter 1150/1406: Loss 1.8574\n",
      "  Iter 1160/1406: Loss 1.8179\n",
      "  Iter 1170/1406: Loss 2.0518\n",
      "  Iter 1180/1406: Loss 2.0649\n",
      "  Iter 1190/1406: Loss 1.8710\n",
      "  Iter 1200/1406: Loss 1.5281\n",
      "  Iter 1210/1406: Loss 1.7175\n",
      "  Iter 1220/1406: Loss 2.5007\n",
      "  Iter 1230/1406: Loss 1.6899\n",
      "  Iter 1240/1406: Loss 1.7684\n",
      "  Iter 1250/1406: Loss 1.7128\n",
      "  Iter 1260/1406: Loss 1.8427\n",
      "  Iter 1270/1406: Loss 1.5702\n",
      "  Iter 1280/1406: Loss 1.9761\n",
      "  Iter 1290/1406: Loss 1.9380\n",
      "  Iter 1300/1406: Loss 1.5055\n",
      "  Iter 1310/1406: Loss 1.5609\n",
      "  Iter 1320/1406: Loss 1.9574\n",
      "  Iter 1330/1406: Loss 1.8717\n",
      "  Iter 1340/1406: Loss 2.0283\n",
      "  Iter 1350/1406: Loss 1.9736\n",
      "  Iter 1360/1406: Loss 1.6657\n",
      "  Iter 1370/1406: Loss 1.4123\n",
      "  Iter 1380/1406: Loss 1.8603\n",
      "  Iter 1390/1406: Loss 1.9548\n",
      "  Iter 1400/1406: Loss 2.1413\n",
      "Train acc: 0.5570, Test acc: 0.3690 (Time: 2699.60s)\n",
      "\n",
      "[Epoch 20]\n",
      "  Iter   0/1406: Loss 1.5313\n",
      "  Iter  10/1406: Loss 1.3070\n",
      "  Iter  20/1406: Loss 2.0080\n",
      "  Iter  30/1406: Loss 1.8176\n",
      "  Iter  40/1406: Loss 2.1165\n",
      "  Iter  50/1406: Loss 1.9541\n",
      "  Iter  60/1406: Loss 1.6465\n",
      "  Iter  70/1406: Loss 1.7507\n",
      "  Iter  80/1406: Loss 1.2433\n",
      "  Iter  90/1406: Loss 1.8301\n",
      "  Iter 100/1406: Loss 1.6250\n",
      "  Iter 110/1406: Loss 1.8997\n",
      "  Iter 120/1406: Loss 1.7886\n",
      "  Iter 130/1406: Loss 2.0627\n",
      "  Iter 140/1406: Loss 1.7727\n",
      "  Iter 150/1406: Loss 1.3541\n",
      "  Iter 160/1406: Loss 1.7970\n",
      "  Iter 170/1406: Loss 2.0038\n",
      "  Iter 180/1406: Loss 1.8785\n",
      "  Iter 190/1406: Loss 1.9302\n",
      "  Iter 200/1406: Loss 1.4125\n",
      "  Iter 210/1406: Loss 1.6621\n",
      "  Iter 220/1406: Loss 1.9125\n",
      "  Iter 230/1406: Loss 1.5432\n",
      "  Iter 240/1406: Loss 2.2906\n",
      "  Iter 250/1406: Loss 1.9488\n",
      "  Iter 260/1406: Loss 1.5786\n",
      "  Iter 270/1406: Loss 1.4646\n",
      "  Iter 280/1406: Loss 2.2487\n",
      "  Iter 290/1406: Loss 1.2554\n",
      "  Iter 300/1406: Loss 1.6897\n",
      "  Iter 310/1406: Loss 1.8201\n",
      "  Iter 320/1406: Loss 2.3926\n",
      "  Iter 330/1406: Loss 1.8463\n",
      "  Iter 340/1406: Loss 1.5195\n",
      "  Iter 350/1406: Loss 1.8380\n",
      "  Iter 360/1406: Loss 1.8799\n",
      "  Iter 370/1406: Loss 1.8972\n",
      "  Iter 380/1406: Loss 1.7012\n",
      "  Iter 390/1406: Loss 1.4732\n",
      "  Iter 400/1406: Loss 1.9258\n",
      "  Iter 410/1406: Loss 1.5419\n",
      "  Iter 420/1406: Loss 2.2570\n",
      "  Iter 430/1406: Loss 1.6073\n",
      "  Iter 440/1406: Loss 1.5934\n",
      "  Iter 450/1406: Loss 1.6360\n",
      "  Iter 460/1406: Loss 2.0151\n",
      "  Iter 470/1406: Loss 1.8413\n",
      "  Iter 480/1406: Loss 1.8379\n",
      "  Iter 490/1406: Loss 1.6032\n",
      "  Iter 500/1406: Loss 1.4011\n",
      "  Iter 510/1406: Loss 1.6257\n",
      "  Iter 520/1406: Loss 1.9219\n",
      "  Iter 530/1406: Loss 1.9690\n",
      "  Iter 540/1406: Loss 1.7187\n",
      "  Iter 550/1406: Loss 1.3498\n",
      "  Iter 560/1406: Loss 1.8251\n",
      "  Iter 570/1406: Loss 1.5521\n",
      "  Iter 580/1406: Loss 1.6175\n",
      "  Iter 590/1406: Loss 2.3390\n",
      "  Iter 600/1406: Loss 2.0127\n",
      "  Iter 610/1406: Loss 1.8508\n",
      "  Iter 620/1406: Loss 1.5751\n",
      "  Iter 630/1406: Loss 2.1813\n",
      "  Iter 640/1406: Loss 1.7634\n",
      "  Iter 650/1406: Loss 1.4720\n",
      "  Iter 660/1406: Loss 2.3022\n",
      "  Iter 670/1406: Loss 1.8748\n",
      "  Iter 680/1406: Loss 2.2534\n",
      "  Iter 690/1406: Loss 1.8850\n",
      "  Iter 700/1406: Loss 1.5155\n",
      "  Iter 710/1406: Loss 1.6303\n",
      "  Iter 720/1406: Loss 1.7968\n",
      "  Iter 730/1406: Loss 1.5804\n",
      "  Iter 740/1406: Loss 1.9210\n",
      "  Iter 750/1406: Loss 1.6822\n",
      "  Iter 760/1406: Loss 1.8939\n",
      "  Iter 770/1406: Loss 1.8576\n",
      "  Iter 780/1406: Loss 2.3881\n",
      "  Iter 790/1406: Loss 1.6986\n",
      "  Iter 800/1406: Loss 1.9730\n",
      "  Iter 810/1406: Loss 1.7348\n",
      "  Iter 820/1406: Loss 1.3699\n",
      "  Iter 830/1406: Loss 1.6268\n",
      "  Iter 840/1406: Loss 1.9820\n",
      "  Iter 850/1406: Loss 1.5922\n",
      "  Iter 860/1406: Loss 1.8925\n",
      "  Iter 870/1406: Loss 1.8882\n",
      "  Iter 880/1406: Loss 1.2093\n",
      "  Iter 890/1406: Loss 1.7131\n",
      "  Iter 900/1406: Loss 1.8522\n",
      "  Iter 910/1406: Loss 2.1937\n",
      "  Iter 920/1406: Loss 1.6461\n",
      "  Iter 930/1406: Loss 1.7568\n",
      "  Iter 940/1406: Loss 1.7353\n",
      "  Iter 950/1406: Loss 1.8949\n",
      "  Iter 960/1406: Loss 1.8338\n",
      "  Iter 970/1406: Loss 2.2156\n",
      "  Iter 980/1406: Loss 2.3020\n",
      "  Iter 990/1406: Loss 1.8622\n",
      "  Iter 1000/1406: Loss 2.0846\n",
      "  Iter 1010/1406: Loss 1.7067\n",
      "  Iter 1020/1406: Loss 1.4113\n",
      "  Iter 1030/1406: Loss 1.4521\n",
      "  Iter 1040/1406: Loss 1.6980\n",
      "  Iter 1050/1406: Loss 1.7691\n",
      "  Iter 1060/1406: Loss 1.5484\n",
      "  Iter 1070/1406: Loss 1.6706\n",
      "  Iter 1080/1406: Loss 1.5075\n",
      "  Iter 1090/1406: Loss 2.0240\n",
      "  Iter 1100/1406: Loss 2.1924\n",
      "  Iter 1110/1406: Loss 1.4874\n",
      "  Iter 1120/1406: Loss 1.5099\n",
      "  Iter 1130/1406: Loss 1.7005\n",
      "  Iter 1140/1406: Loss 1.4484\n",
      "  Iter 1150/1406: Loss 1.7708\n",
      "  Iter 1160/1406: Loss 1.4320\n",
      "  Iter 1170/1406: Loss 1.6098\n",
      "  Iter 1180/1406: Loss 1.7056\n",
      "  Iter 1190/1406: Loss 1.4942\n",
      "  Iter 1200/1406: Loss 1.7514\n",
      "  Iter 1210/1406: Loss 1.6039\n",
      "  Iter 1220/1406: Loss 1.7327\n",
      "  Iter 1230/1406: Loss 2.3989\n",
      "  Iter 1240/1406: Loss 1.9403\n",
      "  Iter 1250/1406: Loss 1.5455\n",
      "  Iter 1260/1406: Loss 1.8886\n",
      "  Iter 1270/1406: Loss 2.3314\n",
      "  Iter 1280/1406: Loss 1.5318\n",
      "  Iter 1290/1406: Loss 1.9965\n",
      "  Iter 1300/1406: Loss 1.7285\n",
      "  Iter 1310/1406: Loss 1.6119\n"
     ]
    }
   ],
   "source": [
    "model = ResNet20()\n",
    "trainer = Trainer(model,\n",
    "    (x_train, y_train),\n",
    "    (x_test, y_test),\n",
    "    epochs=20,\n",
    "    batch_size=32,           \n",
    "    optimizer_name='adam',\n",
    "    lr=0.0005                \n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_log(\"final_log.npz\")\n",
    "trainer.save_model(\"final_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55497158-d0ba-4044-91b9-f1ae42db07ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59130902-6305-4113-930c-4c0ea732ae39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49c47712-681f-42a5-8171-ae8d8dc874e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: -0.003361580557562571\n",
      "After:  -0.029291441978111253\n"
     ]
    }
   ],
   "source": [
    "model = ResNet20()\n",
    "trainer = Trainer(model,\n",
    "    (x_train, y_train),\n",
    "    (x_test, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=32,           \n",
    "    optimizer_name='adam',\n",
    "    lr=0.0005                \n",
    ")\n",
    "\n",
    "print(\"Before:\", np.mean(trainer.model.fc.W))  # or any param\n",
    "trainer.load_model(\"checkpoint_epoch_10.pkl\")\n",
    "print(\"After: \", np.mean(trainer.model.fc.W))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc478d1-b278-42a0-9412-92e03b417c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2a59b07-8e4a-492b-b6f3-df8387f6ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장 로드 수정해서 다시 학습..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "29b6146a-dc39-44e9-91b6-287326e1240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from common.optimizer import SGD, Adam\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_data, test_data, epochs=20, batch_size=64, optimizer_name='sgd', lr=0.01):\n",
    "        self.model = model\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "        self.max_iter = self.epochs * self.iter_per_epoch\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        # prepare optimizer\n",
    "        if optimizer_name == 'sgd':\n",
    "            self.optimizer = SGD(lr=lr)\n",
    "        elif optimizer_name == 'adam':\n",
    "            self.optimizer = Adam(lr=lr)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "        if hasattr(self.model.fc, 'W'):\n",
    "            param_dict['fc_W'] = self.model.fc.W\n",
    "            param_dict['fc_b'] = self.model.fc.b\n",
    "            grad_dict['fc_W'] = self.model.fc.dW\n",
    "            grad_dict['fc_b'] = self.model.fc.db\n",
    "\n",
    "        idx = 0\n",
    "        for layer in self.model.layer1 + self.model.layer2 + self.model.layer3:\n",
    "            for attr in ['conv1', 'conv2', 'shortcut']:\n",
    "                if hasattr(layer, attr):\n",
    "                    conv = getattr(layer, attr)\n",
    "                    param_dict[f'{idx}_W'] = conv.W\n",
    "                    param_dict[f'{idx}_b'] = conv.b\n",
    "                    grad_dict[f'{idx}_W'] = conv.dW\n",
    "                    grad_dict[f'{idx}_b'] = conv.db\n",
    "                    idx += 1\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        self.model.backward(self.loss_grad(x_batch, t_batch))\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            return (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            return dx / batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"[Epoch {epoch + 1}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"  Iter {i:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            test_acc = self.model.accuracy(self.test_x, self.test_t)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f} (Time: {elapsed:.2f}s)\\n\", flush=True)\n",
    "\n",
    "            # 5 에폭마다 모델 저장\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                model_filename = f\"checkpoint_epoch_{epoch+1}.pkl\"\n",
    "                self.save_model(model_filename)\n",
    "                print(f\">>> Saved model to {model_filename}\\n\", flush=True)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename, loss=self.train_loss_list, train_acc=self.train_acc_list, test_acc=self.test_acc_list)\n",
    "\n",
    "    def save_model(self, filename='model_and_opt.pkl'):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "        \n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': getattr(self.optimizer, 'beta1', None),\n",
    "            'beta2': getattr(self.optimizer, 'beta2', None),\n",
    "            'eps': getattr(self.optimizer, 'eps', None),\n",
    "            'm': getattr(self.optimizer, 'm', {}),\n",
    "            'v': getattr(self.optimizer, 'v', {}),\n",
    "            't': getattr(self.optimizer, 't', 0),\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump({'model': model_state, 'optimizer': optimizer_state}, f)\n",
    "\n",
    "    def load_model(self, filename='model_and_opt.pkl'):\n",
    "        with open(filename, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        for k in params:\n",
    "            if k in state['model']:\n",
    "                params[k][...] = state['model'][k]\n",
    "            else:\n",
    "                print(f\"[WARN] Key {k} not found in checkpoint!\")\n",
    "\n",
    "        opt = state['optimizer']\n",
    "        self.optimizer.lr = opt['lr']\n",
    "        self.optimizer.beta1 = opt['beta1']\n",
    "        self.optimizer.beta2 = opt['beta2']\n",
    "        self.optimizer.eps = opt['eps']\n",
    "        self.optimizer.m = opt['m']\n",
    "        self.optimizer.v = opt['v']\n",
    "        self.optimizer.t = opt['t']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "400400c9-4267-464c-8a9d-bd5bb1d3e9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1]\n",
      "  Iter   0/1406: Loss 5.4616\n",
      "  Iter  10/1406: Loss 5.2300\n",
      "  Iter  20/1406: Loss 5.1731\n",
      "  Iter  30/1406: Loss 4.6551\n",
      "  Iter  40/1406: Loss 4.7463\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 14\u001b[0m\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      4\u001b[0m     model,\n\u001b[0;32m      5\u001b[0m     (x_train, y_train),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m               \u001b[38;5;66;03m# 안정적인 학습을 위한 낮은 학습률\u001b[39;00m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 학습 시작\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 최종 모델 및 로그 저장\u001b[39;00m\n\u001b[0;32m     17\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_log.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[36], line 81\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_per_epoch):\n\u001b[1;32m---> 81\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[0;32m     82\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[36], line 57\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_t[batch_mask]\n\u001b[0;32m     56\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad(x_batch, t_batch))\n\u001b[0;32m     59\u001b[0m params, grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_param_dict_and_grad()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate(params, grads)\n",
      "Cell \u001b[1;32mIn[36], line 65\u001b[0m, in \u001b[0;36mTrainer.loss_grad\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 65\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(x, train_flg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n",
      "Cell \u001b[1;32mIn[8], line 110\u001b[0m, in \u001b[0;36mResNet20.forward\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m    107\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1\u001b[38;5;241m.\u001b[39mforward(out)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1:\n\u001b[1;32m--> 110\u001b[0m     out \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2:\n\u001b[0;32m    112\u001b[0m     out \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n",
      "Cell \u001b[1;32mIn[8], line 46\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m     43\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1\u001b[38;5;241m.\u001b[39mforward(out)\n\u001b[0;32m     45\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mforward(out)\n\u001b[1;32m---> 46\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_main \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mequal_in_out:\n",
      "File \u001b[1;32m~\\project\\common\\layers.py:145\u001b[0m, in \u001b[0;36mBatchNormalization.forward\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__forward(x, train_flg)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_mode:\n\u001b[0;32m    148\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mreshape(N, H, W, C)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\project\\common\\layers.py:159\u001b[0m, in \u001b[0;36mBatchNormalization.__forward\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(D)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_flg:\n\u001b[1;32m--> 159\u001b[0m     mu \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    160\u001b[0m     xc \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m mu\n\u001b[0;32m    161\u001b[0m     var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(xc \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델 및 트레이너 초기화\n",
    "model = ResNet20()\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    (x_train, y_train),\n",
    "    (x_test, y_test),\n",
    "    epochs=20,              # 전체 에폭 수\n",
    "    batch_size=32,          # 작은 배치로 메모리 절약\n",
    "    optimizer_name='adam',\n",
    "    lr=0.0005               # 안정적인 학습을 위한 낮은 학습률\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "trainer.train()\n",
    "\n",
    "# 최종 모델 및 로그 저장\n",
    "trainer.save_log(\"final_log.npz\")\n",
    "trainer.save_model(\"checkpoint_epoch_20.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d271143-8482-4561-8459-61983fdf3960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

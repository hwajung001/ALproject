{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxBpO017aYxQ",
        "outputId": "45e3c9fa-91aa-418d-a9d4-4465659b4ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images: (50000, 32, 32, 3)\n",
            "Train fine labels: (50000,)\n",
            "Test images: (10000, 32, 32, 3)\n",
            "Test fine labels: (10000,)\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-100 Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# 1. Îç∞Ïù¥ÌÑ∞ Îã§Ïö¥Î°úÎìú Î∞è ÏïïÏ∂ï ÌíÄÍ∏∞\n",
        "!wget -q https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar -xzf cifar-100-python.tar.gz\n",
        "\n",
        "# 2. Îç∞Ïù¥ÌÑ∞ Î°úÎî© Ìï®Ïàò\n",
        "def load_cifar100(data_dir='/content/cifar-100-python'):\n",
        "    def load_file(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            data = pickle.load(f, encoding='latin1')\n",
        "        images = data['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
        "        images = images.astype('float32') / 255.0  # Ï†ïÍ∑úÌôî\n",
        "        fine_labels = np.array(data['fine_labels'])\n",
        "        coarse_labels = np.array(data['coarse_labels'])\n",
        "        return images, fine_labels, coarse_labels\n",
        "\n",
        "    train_images, train_fine_labels, train_coarse_labels = load_file(os.path.join(data_dir, 'train'))\n",
        "    test_images, test_fine_labels, test_coarse_labels = load_file(os.path.join(data_dir, 'test'))\n",
        "    return (train_images, train_fine_labels, train_coarse_labels), (test_images, test_fine_labels, test_coarse_labels)\n",
        "\n",
        "# 3. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
        "(train_images, train_fine_labels, train_coarse_labels), (test_images, test_fine_labels, test_coarse_labels) = load_cifar100()\n",
        "\n",
        "# 4. ÌôïÏù∏\n",
        "print(\"Train images:\", train_images.shape)\n",
        "print(\"Train fine labels:\", train_fine_labels.shape)\n",
        "print(\"Test images:\", test_images.shape)\n",
        "print(\"Test fine labels:\", test_fine_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class OptimizedFeatureMLP:\n",
        "    def __init__(self, input_size=768, hidden_sizes=[1024, 512], output_size=100, dropout_rate=0.5):\n",
        "        self.params = {}\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.init_weights(input_size, hidden_sizes, output_size)\n",
        "\n",
        "    def init_weights(self, input_size, hidden_sizes, output_size):\n",
        "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            fan_in, fan_out = layer_sizes[i], layer_sizes[i+1]\n",
        "            limit = np.sqrt(6 / (fan_in + fan_out))  # Xavier Ï¥àÍ∏∞Ìôî\n",
        "            self.params[f'W{i+1}'] = np.random.uniform(-limit, limit, (fan_in, fan_out)).astype(np.float32)\n",
        "            self.params[f'b{i+1}'] = np.zeros((1, fan_out), dtype=np.float32)\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X, training=True):\n",
        "        cache = {}\n",
        "        A = X\n",
        "        num_layers = len(self.params) // 2\n",
        "\n",
        "        for i in range(1, num_layers):\n",
        "            Z = np.dot(A, self.params[f'W{i}']) + self.params[f'b{i}']\n",
        "            A = self.relu(Z)\n",
        "            if training:\n",
        "                dropout_mask = (np.random.rand(*A.shape) > self.dropout_rate).astype(np.float32)\n",
        "                A *= dropout_mask\n",
        "                A /= (1.0 - self.dropout_rate)\n",
        "                cache[f'dropout_mask{i}'] = dropout_mask\n",
        "            cache[f'Z{i}'] = Z\n",
        "            cache[f'A{i}'] = A\n",
        "\n",
        "        Z_final = np.dot(A, self.params[f'W{num_layers}']) + self.params[f'b{num_layers}']\n",
        "        A_final = self.softmax(Z_final)\n",
        "        cache[f'Z{num_layers}'] = Z_final\n",
        "        cache[f'A{num_layers}'] = A_final\n",
        "\n",
        "        return A_final, cache\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        n_samples = y_true.shape[0]\n",
        "        log_probs = -np.log(y_pred[np.arange(n_samples), y_true] + 1e-8)\n",
        "        return np.sum(log_probs) / n_samples\n",
        "\n",
        "    def backward(self, X, y_true, cache):\n",
        "        grads = {}\n",
        "        num_layers = len(self.params) // 2\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        A_final = cache[f'A{num_layers}']\n",
        "        delta = A_final\n",
        "        delta[np.arange(n_samples), y_true] -= 1\n",
        "        delta /= n_samples\n",
        "\n",
        "        grads[f'W{num_layers}'] = np.dot(cache[f'A{num_layers-1}'].T, delta)\n",
        "        grads[f'b{num_layers}'] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        for i in reversed(range(1, num_layers)):\n",
        "            delta = np.dot(delta, self.params[f'W{i+1}'].T) * self.relu_derivative(cache[f'Z{i}'])\n",
        "            if f'dropout_mask{i}' in cache:\n",
        "                delta *= cache[f'dropout_mask{i}']\n",
        "                delta /= (1.0 - self.dropout_rate)\n",
        "            A_prev = X if i == 1 else cache[f'A{i-1}']\n",
        "            grads[f'W{i}'] = np.dot(A_prev.T, delta)\n",
        "            grads[f'b{i}'] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update_params(self, grads, learning_rate):\n",
        "        num_layers = len(self.params) // 2\n",
        "        for i in range(1, num_layers + 1):\n",
        "            self.params[f'W{i}'] -= learning_rate * grads[f'W{i}']\n",
        "            self.params[f'b{i}'] -= learning_rate * grads[f'b{i}']\n",
        "\n",
        "    def save(self, filename):\n",
        "        np.savez(filename, **self.params)\n",
        "\n",
        "    def load(self, filename):\n",
        "        data = np.load(filename)\n",
        "        for key in self.params.keys():\n",
        "            self.params[key] = data[key]\n",
        "\n",
        "    def train(self, X_train_full, y_train_full, epochs, learning_rate, batch_size=128, save_path=None, validation_split=0.1):\n",
        "        n_samples = X_train_full.shape[0]\n",
        "        n_train = int(n_samples * (1 - validation_split))\n",
        "\n",
        "        X_train = X_train_full[:n_train]\n",
        "        y_train = y_train_full[:n_train]\n",
        "        X_val = X_train_full[n_train:]\n",
        "        y_val = y_train_full[n_train:]\n",
        "\n",
        "        num_batches = n_train // batch_size\n",
        "        initial_lr = learning_rate\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            if epoch % 20 == 0:\n",
        "                learning_rate *= 0.5  # üî• 20 epochÎßàÎã§ lr Ï§ÑÏù¥Í∏∞\n",
        "\n",
        "            indices = np.arange(n_train)\n",
        "            np.random.shuffle(indices)\n",
        "            X_train = X_train[indices]\n",
        "            y_train = y_train[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for batch_idx in range(num_batches):\n",
        "                start = batch_idx * batch_size\n",
        "                end = start + batch_size\n",
        "                X_batch = X_train[start:end]\n",
        "                y_batch = y_train[start:end]\n",
        "\n",
        "                y_pred, cache = self.forward(X_batch, training=True)\n",
        "                loss = self.compute_loss(y_batch, y_pred)\n",
        "                grads = self.backward(X_batch, y_batch, cache)\n",
        "                self.update_params(grads, learning_rate)\n",
        "\n",
        "                epoch_loss += loss\n",
        "\n",
        "            avg_train_loss = epoch_loss / num_batches\n",
        "            train_acc = self.evaluate(X_train, y_train)\n",
        "            val_acc = self.evaluate(X_val, y_val)\n",
        "\n",
        "            y_val_pred, _ = self.forward(X_val, training=False)\n",
        "            val_loss = self.compute_loss(y_val, y_val_pred)\n",
        "\n",
        "            if save_path is not None:\n",
        "                self.save(save_path)\n",
        "\n",
        "            print(f\"Epoch {epoch}: Train Loss={avg_train_loss:.4f}, Val Loss={val_loss:.4f}, Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
        "\n",
        "    def evaluate(self, X, y_true):\n",
        "        y_pred, _ = self.forward(X, training=False)\n",
        "        predictions = np.argmax(y_pred, axis=1)\n",
        "        accuracy = np.mean(predictions == y_true)\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "eXVkVObDatHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ‚úÖ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
        "# Îã§Ïö¥ÏÉòÌîåÎßÅ (32x32x3 ‚Üí 16x16x3)\n",
        "def downsample_images(images):\n",
        "    return images[:, ::2, ::2, :]  # 2ÌîΩÏÖÄ Í∞ÑÍ≤©ÏúºÎ°ú Ï§ÑÏù¥Í∏∞\n",
        "\n",
        "X_train = downsample_images(train_images)\n",
        "X_train = X_train.reshape(-1, 16*16*3).astype(np.float32)  # (50000, 768)\n",
        "y_train = train_fine_labels\n",
        "\n",
        "# ‚úÖ Î™®Îç∏ ÏÉùÏÑ±\n",
        "model = OptimizedFeatureMLP(input_size=16*16*3, hidden_sizes=[1024, 512], output_size=100, dropout_rate=0.5)\n",
        "\n",
        "# ‚úÖ Ï†ÄÏû• Í≤ΩÎ°ú\n",
        "model_save_path = '/content/optimized_feature_mlp_checkpoint.npz'\n",
        "\n",
        "# ‚úÖ Ï†ÄÏû•Îêú Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
        "if os.path.exists(model_save_path):\n",
        "    print(\"Ï†ÄÏû•Îêú Î™®Îç∏ÏùÑ Î∂àÎü¨ÏòµÎãàÎã§...\")\n",
        "    model.load(model_save_path)\n",
        "else:\n",
        "    print(\"ÏÉà Î™®Îç∏Î°ú ÌïôÏäµÏùÑ ÏãúÏûëÌï©ÎãàÎã§...\")\n",
        "\n",
        "# ‚úÖ ÌïôÏäµ ÏÑ§Ï†ï\n",
        "epochs = 50\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "\n",
        "# ‚úÖ ÌïôÏäµ ÏãúÏûë\n",
        "model.train(X_train, y_train, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, save_path=model_save_path)\n",
        "\n",
        "# ‚úÖ ÏµúÏ¢Ö Ï†ïÌôïÎèÑ Ï∂úÎ†•\n",
        "final_accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"\\nÏµúÏ¢Ö ÌïôÏäµ Ï†ïÌôïÎèÑ: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1GIl0ULbD_m",
        "outputId": "ba3b103b-4bc9-49d9-b90f-83b5cd1ed18e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÏÉà Î™®Îç∏Î°ú ÌïôÏäµÏùÑ ÏãúÏûëÌï©ÎãàÎã§...\n",
            "Epoch 1: Train Loss=4.6371, Val Loss=4.5577, Train Acc=0.0390, Val Acc=0.0398\n",
            "Epoch 2: Train Loss=4.5614, Val Loss=4.5029, Train Acc=0.0496, Val Acc=0.0492\n",
            "Epoch 3: Train Loss=4.5081, Val Loss=4.4250, Train Acc=0.0604, Val Acc=0.0598\n",
            "Epoch 4: Train Loss=4.4396, Val Loss=4.3379, Train Acc=0.0647, Val Acc=0.0634\n",
            "Epoch 5: Train Loss=4.3619, Val Loss=4.2577, Train Acc=0.0718, Val Acc=0.0684\n",
            "Epoch 6: Train Loss=4.3006, Val Loss=4.1947, Train Acc=0.0790, Val Acc=0.0756\n",
            "Epoch 7: Train Loss=4.2483, Val Loss=4.1427, Train Acc=0.0855, Val Acc=0.0800\n",
            "Epoch 8: Train Loss=4.2086, Val Loss=4.1052, Train Acc=0.0880, Val Acc=0.0838\n",
            "Epoch 9: Train Loss=4.1727, Val Loss=4.0702, Train Acc=0.0969, Val Acc=0.0904\n",
            "Epoch 10: Train Loss=4.1441, Val Loss=4.0413, Train Acc=0.1032, Val Acc=0.0948\n",
            "Epoch 11: Train Loss=4.1165, Val Loss=4.0107, Train Acc=0.1057, Val Acc=0.0994\n",
            "Epoch 12: Train Loss=4.0915, Val Loss=3.9870, Train Acc=0.1113, Val Acc=0.1048\n",
            "Epoch 13: Train Loss=4.0634, Val Loss=3.9609, Train Acc=0.1170, Val Acc=0.1106\n",
            "Epoch 14: Train Loss=4.0394, Val Loss=3.9377, Train Acc=0.1206, Val Acc=0.1196\n",
            "Epoch 15: Train Loss=4.0169, Val Loss=3.9125, Train Acc=0.1258, Val Acc=0.1200\n",
            "Epoch 16: Train Loss=3.9939, Val Loss=3.8907, Train Acc=0.1295, Val Acc=0.1244\n",
            "Epoch 17: Train Loss=3.9681, Val Loss=3.8697, Train Acc=0.1336, Val Acc=0.1270\n",
            "Epoch 18: Train Loss=3.9504, Val Loss=3.8472, Train Acc=0.1355, Val Acc=0.1322\n",
            "Epoch 19: Train Loss=3.9293, Val Loss=3.8273, Train Acc=0.1393, Val Acc=0.1340\n",
            "Epoch 20: Train Loss=3.9069, Val Loss=3.8173, Train Acc=0.1420, Val Acc=0.1382\n",
            "Epoch 21: Train Loss=3.9009, Val Loss=3.8059, Train Acc=0.1446, Val Acc=0.1374\n",
            "Epoch 22: Train Loss=3.8953, Val Loss=3.7983, Train Acc=0.1466, Val Acc=0.1396\n",
            "Epoch 23: Train Loss=3.8786, Val Loss=3.7854, Train Acc=0.1487, Val Acc=0.1428\n",
            "Epoch 24: Train Loss=3.8775, Val Loss=3.7802, Train Acc=0.1496, Val Acc=0.1442\n",
            "Epoch 25: Train Loss=3.8649, Val Loss=3.7745, Train Acc=0.1523, Val Acc=0.1448\n",
            "Epoch 26: Train Loss=3.8613, Val Loss=3.7651, Train Acc=0.1518, Val Acc=0.1450\n",
            "Epoch 27: Train Loss=3.8488, Val Loss=3.7595, Train Acc=0.1535, Val Acc=0.1434\n",
            "Epoch 28: Train Loss=3.8461, Val Loss=3.7515, Train Acc=0.1546, Val Acc=0.1500\n",
            "Epoch 29: Train Loss=3.8327, Val Loss=3.7447, Train Acc=0.1559, Val Acc=0.1480\n",
            "Epoch 30: Train Loss=3.8286, Val Loss=3.7358, Train Acc=0.1572, Val Acc=0.1482\n",
            "Epoch 31: Train Loss=3.8213, Val Loss=3.7302, Train Acc=0.1575, Val Acc=0.1508\n",
            "Epoch 32: Train Loss=3.8108, Val Loss=3.7256, Train Acc=0.1613, Val Acc=0.1530\n",
            "Epoch 33: Train Loss=3.8065, Val Loss=3.7174, Train Acc=0.1607, Val Acc=0.1530\n",
            "Epoch 34: Train Loss=3.8041, Val Loss=3.7130, Train Acc=0.1604, Val Acc=0.1524\n",
            "Epoch 35: Train Loss=3.7989, Val Loss=3.7062, Train Acc=0.1627, Val Acc=0.1550\n",
            "Epoch 36: Train Loss=3.7932, Val Loss=3.7009, Train Acc=0.1622, Val Acc=0.1526\n",
            "Epoch 37: Train Loss=3.7827, Val Loss=3.6952, Train Acc=0.1656, Val Acc=0.1538\n",
            "Epoch 38: Train Loss=3.7770, Val Loss=3.6906, Train Acc=0.1664, Val Acc=0.1566\n",
            "Epoch 39: Train Loss=3.7681, Val Loss=3.6821, Train Acc=0.1678, Val Acc=0.1572\n",
            "Epoch 40: Train Loss=3.7639, Val Loss=3.6815, Train Acc=0.1684, Val Acc=0.1576\n",
            "Epoch 41: Train Loss=3.7610, Val Loss=3.6773, Train Acc=0.1690, Val Acc=0.1580\n",
            "Epoch 42: Train Loss=3.7600, Val Loss=3.6772, Train Acc=0.1696, Val Acc=0.1584\n",
            "Epoch 43: Train Loss=3.7562, Val Loss=3.6721, Train Acc=0.1705, Val Acc=0.1582\n",
            "Epoch 44: Train Loss=3.7517, Val Loss=3.6699, Train Acc=0.1711, Val Acc=0.1602\n",
            "Epoch 45: Train Loss=3.7531, Val Loss=3.6677, Train Acc=0.1711, Val Acc=0.1612\n",
            "Epoch 46: Train Loss=3.7455, Val Loss=3.6641, Train Acc=0.1712, Val Acc=0.1602\n",
            "Epoch 47: Train Loss=3.7494, Val Loss=3.6658, Train Acc=0.1720, Val Acc=0.1604\n",
            "Epoch 48: Train Loss=3.7483, Val Loss=3.6605, Train Acc=0.1736, Val Acc=0.1612\n",
            "Epoch 49: Train Loss=3.7366, Val Loss=3.6569, Train Acc=0.1733, Val Acc=0.1616\n",
            "Epoch 50: Train Loss=3.7392, Val Loss=3.6547, Train Acc=0.1740, Val Acc=0.1614\n",
            "\n",
            "ÏµúÏ¢Ö ÌïôÏäµ Ï†ïÌôïÎèÑ: 0.1728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Î™®Îç∏ ÏÉùÏÑ±\n",
        "model = OptimizedFeatureMLP(input_size=16*16*3, hidden_sizes=[1024, 512], output_size=100, dropout_rate=0.5)\n",
        "\n",
        "# Ï†ÄÏû•Îêú Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞\n",
        "model_save_path = '/content/optimized_feature_mlp_checkpoint.npz'\n",
        "\n",
        "if os.path.exists(model_save_path):\n",
        "    print(\"Ï†ÄÏû•Îêú Î™®Îç∏ÏùÑ Î∂àÎü¨ÏòµÎãàÎã§...\")\n",
        "    model.load(model_save_path)\n",
        "else:\n",
        "    print(\"Î™®Îç∏ÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. ÏÉàÎ°ú ÌïôÏäµÏùÑ ÏãúÏûëÌï©ÎãàÎã§.\")\n",
        "\n",
        "# Ï∂îÍ∞Ä ÌïôÏäµ ÏÑ§Ï†ï\n",
        "epochs = 50  # Ï∂îÍ∞Ä 50 epoch\n",
        "learning_rate = 0.005  # üî• ÏÇ¥Ïßù Ï§ÑÏù¥Í∏∞\n",
        "batch_size = 128\n",
        "\n",
        "# Ï∂îÍ∞Ä ÌïôÏäµ ÏãúÏûë\n",
        "model.train(X_train, y_train, epochs=epochs, learning_rate=learning_rate, batch_size=batch_size, save_path=model_save_path)\n",
        "\n",
        "# Ï∂îÍ∞Ä ÌïôÏäµ ÌõÑ ÏµúÏ¢Ö Ï†ïÌôïÎèÑ Ï∂úÎ†•\n",
        "final_accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"\\nÏ∂îÍ∞Ä ÌïôÏäµ ÌõÑ ÏµúÏ¢Ö ÌïôÏäµ Ï†ïÌôïÎèÑ: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSwWMelRefen",
        "outputId": "2785ebce-0d0c-473d-8d24-8f2547e6f6f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ï†ÄÏû•Îêú Î™®Îç∏ÏùÑ Î∂àÎü¨ÏòµÎãàÎã§...\n",
            "Epoch 1: Train Loss=3.7349, Val Loss=3.6533, Train Acc=0.1748, Val Acc=0.1630\n",
            "Epoch 2: Train Loss=3.7266, Val Loss=3.6484, Train Acc=0.1754, Val Acc=0.1676\n",
            "Epoch 3: Train Loss=3.7237, Val Loss=3.6430, Train Acc=0.1770, Val Acc=0.1672\n",
            "Epoch 4: Train Loss=3.7236, Val Loss=3.6400, Train Acc=0.1764, Val Acc=0.1628\n",
            "Epoch 5: Train Loss=3.7189, Val Loss=3.6391, Train Acc=0.1794, Val Acc=0.1636\n",
            "Epoch 6: Train Loss=3.7091, Val Loss=3.6292, Train Acc=0.1796, Val Acc=0.1680\n",
            "Epoch 7: Train Loss=3.7116, Val Loss=3.6261, Train Acc=0.1800, Val Acc=0.1668\n",
            "Epoch 8: Train Loss=3.6974, Val Loss=3.6215, Train Acc=0.1820, Val Acc=0.1674\n",
            "Epoch 9: Train Loss=3.6978, Val Loss=3.6191, Train Acc=0.1829, Val Acc=0.1706\n",
            "Epoch 10: Train Loss=3.6969, Val Loss=3.6168, Train Acc=0.1830, Val Acc=0.1670\n",
            "Epoch 11: Train Loss=3.6947, Val Loss=3.6107, Train Acc=0.1833, Val Acc=0.1698\n",
            "Epoch 12: Train Loss=3.6825, Val Loss=3.6038, Train Acc=0.1848, Val Acc=0.1710\n",
            "Epoch 13: Train Loss=3.6794, Val Loss=3.6019, Train Acc=0.1852, Val Acc=0.1706\n",
            "Epoch 14: Train Loss=3.6798, Val Loss=3.6004, Train Acc=0.1859, Val Acc=0.1718\n",
            "Epoch 15: Train Loss=3.6691, Val Loss=3.5921, Train Acc=0.1866, Val Acc=0.1748\n",
            "Epoch 16: Train Loss=3.6713, Val Loss=3.5896, Train Acc=0.1877, Val Acc=0.1736\n",
            "Epoch 17: Train Loss=3.6632, Val Loss=3.5866, Train Acc=0.1893, Val Acc=0.1738\n",
            "Epoch 18: Train Loss=3.6594, Val Loss=3.5835, Train Acc=0.1894, Val Acc=0.1722\n",
            "Epoch 19: Train Loss=3.6559, Val Loss=3.5813, Train Acc=0.1903, Val Acc=0.1740\n",
            "Epoch 20: Train Loss=3.6438, Val Loss=3.5754, Train Acc=0.1914, Val Acc=0.1744\n",
            "Epoch 21: Train Loss=3.6484, Val Loss=3.5745, Train Acc=0.1923, Val Acc=0.1748\n",
            "Epoch 22: Train Loss=3.6467, Val Loss=3.5716, Train Acc=0.1928, Val Acc=0.1758\n",
            "Epoch 23: Train Loss=3.6354, Val Loss=3.5683, Train Acc=0.1921, Val Acc=0.1748\n",
            "Epoch 24: Train Loss=3.6388, Val Loss=3.5687, Train Acc=0.1926, Val Acc=0.1760\n",
            "Epoch 25: Train Loss=3.6404, Val Loss=3.5676, Train Acc=0.1935, Val Acc=0.1744\n",
            "Epoch 26: Train Loss=3.6347, Val Loss=3.5645, Train Acc=0.1926, Val Acc=0.1762\n",
            "Epoch 27: Train Loss=3.6294, Val Loss=3.5623, Train Acc=0.1941, Val Acc=0.1768\n",
            "Epoch 28: Train Loss=3.6301, Val Loss=3.5586, Train Acc=0.1950, Val Acc=0.1782\n",
            "Epoch 29: Train Loss=3.6299, Val Loss=3.5584, Train Acc=0.1956, Val Acc=0.1780\n",
            "Epoch 30: Train Loss=3.6274, Val Loss=3.5568, Train Acc=0.1964, Val Acc=0.1788\n",
            "Epoch 31: Train Loss=3.6235, Val Loss=3.5545, Train Acc=0.1952, Val Acc=0.1780\n",
            "Epoch 32: Train Loss=3.6221, Val Loss=3.5540, Train Acc=0.1966, Val Acc=0.1766\n",
            "Epoch 33: Train Loss=3.6227, Val Loss=3.5535, Train Acc=0.1972, Val Acc=0.1786\n",
            "Epoch 34: Train Loss=3.6186, Val Loss=3.5497, Train Acc=0.1972, Val Acc=0.1794\n",
            "Epoch 35: Train Loss=3.6167, Val Loss=3.5486, Train Acc=0.1977, Val Acc=0.1796\n",
            "Epoch 36: Train Loss=3.6165, Val Loss=3.5460, Train Acc=0.1976, Val Acc=0.1788\n",
            "Epoch 37: Train Loss=3.6135, Val Loss=3.5454, Train Acc=0.1976, Val Acc=0.1802\n",
            "Epoch 38: Train Loss=3.6099, Val Loss=3.5438, Train Acc=0.1979, Val Acc=0.1796\n",
            "Epoch 39: Train Loss=3.6070, Val Loss=3.5427, Train Acc=0.1987, Val Acc=0.1818\n",
            "Epoch 40: Train Loss=3.6078, Val Loss=3.5399, Train Acc=0.1996, Val Acc=0.1812\n",
            "Epoch 41: Train Loss=3.6044, Val Loss=3.5396, Train Acc=0.1999, Val Acc=0.1820\n",
            "Epoch 42: Train Loss=3.6023, Val Loss=3.5381, Train Acc=0.2000, Val Acc=0.1802\n",
            "Epoch 43: Train Loss=3.6016, Val Loss=3.5383, Train Acc=0.2000, Val Acc=0.1820\n",
            "Epoch 44: Train Loss=3.5999, Val Loss=3.5369, Train Acc=0.2003, Val Acc=0.1826\n",
            "Epoch 45: Train Loss=3.6023, Val Loss=3.5366, Train Acc=0.2004, Val Acc=0.1812\n",
            "Epoch 46: Train Loss=3.5981, Val Loss=3.5341, Train Acc=0.2006, Val Acc=0.1816\n",
            "Epoch 47: Train Loss=3.5938, Val Loss=3.5329, Train Acc=0.2007, Val Acc=0.1814\n",
            "Epoch 48: Train Loss=3.5983, Val Loss=3.5330, Train Acc=0.2007, Val Acc=0.1820\n",
            "Epoch 49: Train Loss=3.5952, Val Loss=3.5326, Train Acc=0.2015, Val Acc=0.1814\n",
            "Epoch 50: Train Loss=3.5934, Val Loss=3.5315, Train Acc=0.2015, Val Acc=0.1816\n",
            "\n",
            "Ï∂îÍ∞Ä ÌïôÏäµ ÌõÑ ÏµúÏ¢Ö ÌïôÏäµ Ï†ïÌôïÎèÑ: 0.1995\n"
          ]
        }
      ]
    }
  ]
}
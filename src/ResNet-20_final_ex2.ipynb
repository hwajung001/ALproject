{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-20 final Train code\n",
    "\n",
    "### ex 2 : train dataset 100,000 = original + random crop\n",
    "##### (random seed = 3181)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - CIFAR-100 데이터 다운로드 및 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 already downloaded.\n",
      "Generating augmented dataset with crop only...\n",
      "train_crop: [(90000, 3, 32, 32), (90000,)]\n",
      "val_crop: [(10000, 3, 32, 32), (10000,)]\n",
      "test: [(10000, 3, 32, 32), (10000,)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(3181)  # ex2 random seed\n",
    "\n",
    "def download_cifar100(save_path='cifar-100-python'):\n",
    "    if os.path.exists(save_path):\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "        return\n",
    "\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
    "    filename = 'cifar-100-python.tar.gz'\n",
    "    print(\"Downloading CIFAR-100...\")\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "    with tarfile.open(filename, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "    os.remove(filename)\n",
    "    print(\"Download and extraction completed.\")\n",
    "\n",
    "def load_batch(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "    data = data_dict[b'data']\n",
    "    fine_labels = np.array(data_dict[b'fine_labels'])\n",
    "    data = data.reshape(-1, 3, 32, 32)\n",
    "    return data, fine_labels\n",
    "\n",
    "def normalize_images(images):\n",
    "    return images.astype(np.float32) / 255.0\n",
    "\n",
    "def split_validation(images, labels, val_ratio=0.1):\n",
    "    num_samples = images.shape[0]\n",
    "    val_size = int(num_samples * val_ratio)\n",
    "\n",
    "    idx = np.random.permutation(num_samples)\n",
    "    images = images[idx]\n",
    "    labels = labels[idx]\n",
    "\n",
    "    val_images = images[:val_size]\n",
    "    val_labels = labels[:val_size]\n",
    "    train_images = images[val_size:]\n",
    "    train_labels = labels[val_size:]\n",
    "\n",
    "    return (train_images, train_labels), (val_images, val_labels)\n",
    "\n",
    "def random_crop(x, crop_size=32, padding=4):\n",
    "    n, c, h, w = x.shape\n",
    "    padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='reflect')\n",
    "    cropped = np.empty((n, c, crop_size, crop_size), dtype=x.dtype)\n",
    "    for i in range(n):\n",
    "        top = np.random.randint(0, padding * 2 + 1)\n",
    "        left = np.random.randint(0, padding * 2 + 1)\n",
    "        cropped[i] = padded[i, :, top:top+crop_size, left:left+crop_size]\n",
    "    return cropped\n",
    "\n",
    "def horizontal_flip(x):\n",
    "    return x[:, :, :, ::-1]\n",
    "\n",
    "def load_cifar100_dataset():\n",
    "    download_cifar100()\n",
    "    train_data, train_fine = load_batch('cifar-100-python/train')\n",
    "    test_data, test_fine = load_batch('cifar-100-python/test')\n",
    "    train_data = normalize_images(train_data)\n",
    "    test_data = normalize_images(test_data)\n",
    "    return (train_data, train_fine), (test_data, test_fine)\n",
    "\n",
    "def generate_augmented_dataset(images, labels, target_size):\n",
    "    N = images.shape[0]\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    repeat = target_size // N + 1  # only random crop \n",
    "\n",
    "    for _ in range(repeat):\n",
    "        imgs_crop = random_crop(images.copy())\n",
    "        augmented_images.append(imgs_crop)\n",
    "        augmented_labels.append(labels.copy())\n",
    "        if sum(x.shape[0] for x in augmented_images) >= target_size:\n",
    "            break\n",
    "\n",
    "    X = np.concatenate(augmented_images, axis=0)[:target_size]\n",
    "    y = np.concatenate(augmented_labels, axis=0)[:target_size]\n",
    "    return X, y\n",
    "\n",
    "def prepare_dataset():\n",
    "    (full_train_images, full_train_labels), (test_images, test_labels) = load_cifar100_dataset()\n",
    "    print(\"Generating augmented dataset with crop only...\")\n",
    "\n",
    "    X_aug, y_aug = generate_augmented_dataset(full_train_images, full_train_labels, target_size=100000)\n",
    "    train_aug, val_aug = split_validation(X_aug, y_aug)\n",
    "\n",
    "    return {\n",
    "        'train_crop': train_aug,\n",
    "        'val_crop': val_aug,\n",
    "        'test': (test_images, test_labels)\n",
    "    }\n",
    "\n",
    "data = prepare_dataset()\n",
    "for k, v in data.items():\n",
    "    if isinstance(v, tuple):\n",
    "        print(f\"{k}: {[x.shape for x in v]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - ResNet-20 모델 구조 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type)                       Output Shape                Param #\n",
      "===========================================================================\n",
      " 1. Conv1                           (1, 16, 32, 32)                 448\n",
      " 2. Block[1-1]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 3. Block[1-1]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 4. Block[1-2]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 5. Block[1-2]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 6. Block[1-3]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 7. Block[1-3]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 8. Block[2-1]_Conv1                (1, 32, 16, 16)               4,640\n",
      " 9. Block[2-1]_Conv2                (1, 32, 16, 16)               9,248\n",
      "    └─ Shortcut[2-1]                (1, 32, 16, 16)                 544\n",
      "10. Block[2-2]_Conv1                (1, 32, 16, 16)               9,248\n",
      "11. Block[2-2]_Conv2                (1, 32, 16, 16)               9,248\n",
      "12. Block[2-3]_Conv1                (1, 32, 16, 16)               9,248\n",
      "13. Block[2-3]_Conv2                (1, 32, 16, 16)               9,248\n",
      "14. Block[3-1]_Conv1                (1, 64, 8, 8)                18,496\n",
      "15. Block[3-1]_Conv2                (1, 64, 8, 8)                36,928\n",
      "    └─ Shortcut[3-1]                (1, 64, 8, 8)                 2,112\n",
      "16. Block[3-2]_Conv1                (1, 64, 8, 8)                36,928\n",
      "17. Block[3-2]_Conv2                (1, 64, 8, 8)                36,928\n",
      "18. Block[3-3]_Conv1                (1, 64, 8, 8)                36,928\n",
      "19. Block[3-3]_Conv2                (1, 64, 8, 8)                36,928\n",
      "    GlobalAvgPool                   (1, 64)                           0\n",
      "20. FC                              (1, 100)                      6,500\n",
      "===========================================================================\n",
      "Total weight layers:                                        20\n",
      "Total params:                                               277,540\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "from common.ResNet20 import ResNet20\n",
    "\n",
    "def count_params(layer):\n",
    "    count = 0\n",
    "    if hasattr(layer, 'W'):\n",
    "        count += np.prod(layer.W.shape)\n",
    "    if hasattr(layer, 'b'):\n",
    "        count += np.prod(layer.b.shape)\n",
    "    return count\n",
    "\n",
    "def print_resnet20_summary(model, input_shape=(1, 3, 32, 32)):\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Layer (type)':<35}{'Output Shape':<25}{'Param #':>10}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "    x = np.zeros(input_shape)\n",
    "    total_params = 0\n",
    "    layer_idx = 1\n",
    "\n",
    "    x = model.conv1.forward(x)\n",
    "    p = count_params(model.conv1)\n",
    "    print(f\"{layer_idx:>2}. {'Conv1':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "    layer_idx += 1\n",
    "\n",
    "    x = model.bn1.forward(x, train_flg=False)\n",
    "    x = model.relu1.forward(x)\n",
    "\n",
    "    for i, layer_block in enumerate([model.layer1, model.layer2, model.layer3]):\n",
    "        for j, block in enumerate(layer_block):\n",
    "            residual = x.copy()\n",
    "\n",
    "            # Conv1\n",
    "            x = block.conv1.forward(x)\n",
    "            p = count_params(block.conv1)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv1\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn1.forward(x, train_flg=False)\n",
    "            x = block.relu1.forward(x)\n",
    "\n",
    "            x = block.conv2.forward(x)\n",
    "            p = count_params(block.conv2)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv2\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn2.forward(x, train_flg=False)\n",
    "\n",
    "            if not block.equal_in_out:\n",
    "                x_sc = block.shortcut.forward(residual)\n",
    "                p = count_params(block.shortcut)\n",
    "                name = f\"└─ Shortcut[{i+1}-{j+1}]\"\n",
    "                print(f\"{'':>3} {name:<32}{str(x_sc.shape):<25}{p:>10,}\", flush=True)\n",
    "                total_params += p\n",
    "                x = x + x_sc\n",
    "                x = block.bn_shortcut.forward(x, train_flg=False)\n",
    "            else:\n",
    "                x = x + residual\n",
    "\n",
    "            x = block.relu2.forward(x)\n",
    "\n",
    "    x = x.mean(axis=(2, 3))\n",
    "    print(f\"{'':>3} {'GlobalAvgPool':<32}{str(x.shape):<25}{'0':>10}\", flush=True)\n",
    "\n",
    "    x = model.fc.forward(x)\n",
    "    p = count_params(model.fc)\n",
    "    print(f\"{layer_idx:>2}. {'FC':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Total weight layers:':<60}{'20'}\", flush=True)\n",
    "    print(f\"{'Total params:':<60}{total_params:,}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "model = ResNet20()\n",
    "print_resnet20_summary(model, input_shape=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - ResNet-20 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from common.optimizer import Adam\n",
    "from common.functions import softmax\n",
    "\n",
    "# label smoothing\n",
    "def smooth_labels(y, smoothing=0.1, num_classes=100):\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = (y.shape[0], num_classes)\n",
    "    smooth = np.full(label_shape, smoothing / (num_classes - 1))\n",
    "    smooth[np.arange(y.shape[0]), y] = confidence\n",
    "    return smooth\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name,\n",
    "                 train_data, val_data, test_data,\n",
    "                 epochs=20, batch_size=64, lr=0.01,\n",
    "                 smoothing=0.15):\n",
    "\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.val_x, self.val_t = val_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "\n",
    "        self.optimizer = Adam(lr=lr)\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.val_acc_list = []\n",
    "\n",
    "    def smooth_labels(self, y, num_classes=100):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        label_shape = (y.shape[0], num_classes)\n",
    "        smooth = np.full(label_shape, self.smoothing / (num_classes - 1), dtype=np.float32)\n",
    "        smooth[np.arange(y.shape[0]), y] = confidence\n",
    "        return smooth\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            dx = (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            dx /= batch_size\n",
    "        return dx, y\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "        if hasattr(self.model.fc, 'W'):\n",
    "            param_dict['fc_W'] = self.model.fc.W\n",
    "            param_dict['fc_b'] = self.model.fc.b\n",
    "            grad_dict['fc_W'] = self.model.fc.dW\n",
    "            grad_dict['fc_b'] = self.model.fc.db\n",
    "\n",
    "        idx = 0\n",
    "        for layer in self.model.layer1 + self.model.layer2 + self.model.layer3:\n",
    "            for attr in ['conv1', 'conv2', 'shortcut']:\n",
    "                if hasattr(layer, attr):\n",
    "                    conv = getattr(layer, attr)\n",
    "                    param_dict[f'{idx}_W'] = conv.W\n",
    "                    param_dict[f'{idx}_b'] = conv.b\n",
    "                    grad_dict[f'{idx}_W'] = conv.dW\n",
    "                    grad_dict[f'{idx}_b'] = conv.db\n",
    "                    idx += 1\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        if t_batch.ndim == 1:\n",
    "            t_batch = self.smooth_labels(t_batch)\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        dx, y = self.loss_grad(x_batch, t_batch)\n",
    "        self.model.backward(dx)\n",
    "\n",
    "        if hasattr(self.model, 'clip_weights'):\n",
    "            self.model.clip_weights(clip_value=1.0)\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        patience = 10\n",
    "        best_val_loss = float('inf')\n",
    "        no_improve_count = 0\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\n[Epoch {epoch + 1}/{self.epochs}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0 or i == self.iter_per_epoch - 1:\n",
    "                    print(f\"  Iter {i+1:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            val_acc = self.model.accuracy(self.val_x, self.val_t)\n",
    "\n",
    "            val_loss = self.batched_loss(self.val_x, self.val_t, batch_size=128)\n",
    "\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.val_acc_list.append(val_acc)\n",
    "            self.val_loss_list.append(val_loss)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Fine Train Loss: {avg_loss:.4f}, Fine Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Val Loss: {val_loss:.4f}\", flush=True)\n",
    "            print(f\"Time: {elapsed:.2f}s\", flush=True)\n",
    "\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                model_filename = f\"{self.model_name}_epoch{epoch+1}.pkl\"\n",
    "                self.save_model(model_filename)\n",
    "                print(f\">>> Model saved to {model_filename}\", flush=True)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improve_count = 0\n",
    "                self.save_model(f\"{self.model_name}_best.pkl\")\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "                if no_improve_count >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "    def batched_loss(self, x, t, batch_size=128):\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "            loss = self.model.loss(x_batch, t_batch)\n",
    "            total_loss += loss * len(x_batch)\n",
    "            total_count += len(x_batch)\n",
    "        return total_loss / total_count\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "\n",
    "        model_state['conv1_W'] = self.model.conv1.W.copy()\n",
    "        model_state['conv1_b'] = self.model.conv1.b.copy()\n",
    "\n",
    "        def extract_bn_params(model):\n",
    "            bn_params = {}\n",
    "            bn_count = 0\n",
    "            for layer in model.layer1 + model.layer2 + model.layer3:\n",
    "                for bn_attr in ['bn1', 'bn2']:\n",
    "                    if hasattr(layer, bn_attr):\n",
    "                        bn = getattr(layer, bn_attr)\n",
    "                        bn_params[f'{bn_count}_gamma'] = bn.gamma.copy()\n",
    "                        bn_params[f'{bn_count}_beta'] = bn.beta.copy()\n",
    "                        bn_params[f'{bn_count}_running_mean'] = bn.running_mean.copy()\n",
    "                        bn_params[f'{bn_count}_running_var'] = bn.running_var.copy()\n",
    "                        bn_count += 1\n",
    "                if hasattr(layer, 'bn_shortcut'):\n",
    "                    bn = layer.bn_shortcut\n",
    "                    bn_params[f'{bn_count}_gamma'] = bn.gamma.copy()\n",
    "                    bn_params[f'{bn_count}_beta'] = bn.beta.copy()\n",
    "                    bn_params[f'{bn_count}_running_mean'] = bn.running_mean.copy()\n",
    "                    bn_params[f'{bn_count}_running_var'] = bn.running_var.copy()\n",
    "                    bn_count += 1\n",
    "            bn = model.bn1\n",
    "            bn_params[f'{bn_count}_gamma'] = bn.gamma.copy()\n",
    "            bn_params[f'{bn_count}_beta'] = bn.beta.copy()\n",
    "            bn_params[f'{bn_count}_running_mean'] = bn.running_mean.copy()\n",
    "            bn_params[f'{bn_count}_running_var'] = bn.running_var.copy()\n",
    "            return bn_params\n",
    "\n",
    "        model_state.update(extract_bn_params(self.model))\n",
    "\n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': self.optimizer.beta1,\n",
    "            'beta2': self.optimizer.beta2,\n",
    "            'm': self.optimizer.m,\n",
    "            'v': self.optimizer.v,\n",
    "            't': self.optimizer.iter\n",
    "        }\n",
    "\n",
    "        save_data = {\n",
    "            'model': model_state,\n",
    "            'optimizer': optimizer_state,\n",
    "            'train_loss_list': self.train_loss_list,\n",
    "            'train_acc_list': self.train_acc_list,\n",
    "            'val_acc_list': self.val_acc_list,\n",
    "            'val_loss_list': self.val_loss_list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename,\n",
    "                 loss=np.array(self.train_loss_list),\n",
    "                 train_acc=np.array(self.train_acc_list),\n",
    "                 val_acc=np.array(self.val_acc_list),\n",
    "                 val_loss=np.array(self.val_loss_list))\n",
    "        print(f\"Log saved to {filename}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - ResNet-20_ex1 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Running ex2 : train dataset 100,000 = original + random crop ====\n",
      "\n",
      "[Epoch 1/100]\n",
      "  Iter   1/1406: Loss 5.4516\n",
      "  Iter  11/1406: Loss 4.9904\n",
      "  Iter  21/1406: Loss 5.0278\n",
      "  Iter  31/1406: Loss 4.5874\n",
      "  Iter  41/1406: Loss 4.3951\n",
      "  Iter  51/1406: Loss 4.2855\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==== Running ex2 : train dataset 100,000 = original + random crop ====\")\n",
    "model = ResNet20()\n",
    "\n",
    "x_train, y_train = data['train_crop']\n",
    "x_val, y_val = data['val_crop']\n",
    "x_test, y_test = data['test']\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    model_name='ResNet-20_ex2',\n",
    "    train_data=(x_train, y_train),\n",
    "    val_data=(x_val, y_val),\n",
    "    test_data=(x_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    lr=0.01,\n",
    "    smoothing=0.15\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_log(\"ResNet-20_ex2_log.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from common.util import smooth_curve\n",
    "\n",
    "log = np.load(\"ResNet-20_ex2_log.npz\")\n",
    "train_loss = log[\"loss\"]\n",
    "train_acc = log[\"train_acc\"]\n",
    "val_acc = log[\"val_acc\"]\n",
    "val_loss = log[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "smoothed_epochs = range(1, len(smooth_curve(train_loss)) + 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_loss), label=\"Train Loss (smoothed)\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_loss), label=\"Val Loss (smoothed)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(smoothed_epochs, smooth_curve(train_acc), label=\"Train Acc (smoothed)\")\n",
    "plt.plot(smoothed_epochs, smooth_curve(val_acc), label=\"Val Acc (smoothed)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

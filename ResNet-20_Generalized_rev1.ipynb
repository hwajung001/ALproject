{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 already downloaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "from common.util import shuffle_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CIFAR-100 다운로드 및 압축 해제\n",
    "def download_cifar100(dest=\"./cifar-100-python\"):\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "\n",
    "    def is_within_directory(directory, target):\n",
    "        abs_directory = os.path.abspath(directory)\n",
    "        abs_target = os.path.abspath(target)\n",
    "        return os.path.commonprefix([abs_directory, abs_target]) == abs_directory\n",
    "\n",
    "    def safe_extract(tar, path=\".\", members=None):\n",
    "        for member in tar.getmembers():\n",
    "            member_path = os.path.join(path, member.name)\n",
    "            if not is_within_directory(path, member_path):\n",
    "                raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "        tar.extractall(path, members)\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest, exist_ok=True)\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        with tarfile.open(filename, \"r:gz\") as tar:\n",
    "            safe_extract(tar, path=\"./\")\n",
    "        print(\"CIFAR-100 downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "\n",
    "# 데이터 배치 로딩\n",
    "def load_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "        data = data_dict[b'data']\n",
    "        fine_labels = np.array(data_dict[b'fine_labels'])\n",
    "        coarse_labels = np.array(data_dict[b'coarse_labels'])\n",
    "        return data, fine_labels, coarse_labels\n",
    "\n",
    "# 메타데이터 로딩\n",
    "def load_meta(data_dir=\"./cifar-100-python\"):\n",
    "    with open(os.path.join(data_dir, \"meta\"), 'rb') as f:\n",
    "        meta_dict = pickle.load(f, encoding='bytes')\n",
    "        fine_label_names = [name.decode('utf-8') for name in meta_dict[b'fine_label_names']]\n",
    "        coarse_label_names = [name.decode('utf-8') for name in meta_dict[b'coarse_label_names']]\n",
    "        return {\"fine_label_names\": fine_label_names, \"coarse_label_names\": coarse_label_names}\n",
    "\n",
    "# 정규화 함수\n",
    "def normalize(x):\n",
    "    mean = np.array([0.507, 0.487, 0.441]).reshape(1, 3, 1, 1)\n",
    "    std = np.array([0.267, 0.256, 0.276]).reshape(1, 3, 1, 1)\n",
    "    return (x - mean) / std\n",
    "\n",
    "# 전체 데이터 로딩\n",
    "def load_cifar100(data_dir=\"./cifar-100-python\"):\n",
    "    x_train, y_train_fine, y_train_coarse = load_batch(os.path.join(data_dir, \"train\"))\n",
    "    x_test, y_test_fine, y_test_coarse = load_batch(os.path.join(data_dir, \"test\"))\n",
    "\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "\n",
    "    x_train = normalize(x_train)\n",
    "    x_test = normalize(x_test)\n",
    "\n",
    "    val_size = int(0.1 * len(x_train))\n",
    "    x_val, y_val_fine, y_val_coarse = (\n",
    "        x_train[:val_size], y_train_fine[:val_size], y_train_coarse[:val_size]\n",
    "    )\n",
    "    x_train, y_train_fine, y_train_coarse = (\n",
    "        x_train[val_size:], y_train_fine[val_size:], y_train_coarse[val_size:]\n",
    "    )\n",
    "\n",
    "    x_train, y_train_fine = shuffle_dataset(x_train, y_train_fine)\n",
    "    x_train, y_train_coarse = shuffle_dataset(x_train, y_train_coarse)\n",
    "\n",
    "    return (x_train, y_train_fine, y_train_coarse), (x_val, y_val_fine, y_val_coarse), (x_test, y_test_fine, y_test_coarse)\n",
    "\n",
    "download_cifar100()\n",
    "(x_train, y_train_fine, y_train_coarse), (x_val, y_val_fine, y_val_coarse), (x_test, y_test_fine, y_test_coarse) = load_cifar100()\n",
    "meta = load_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강 함수\n",
    "def random_crop(x, crop_size=32, padding=4):\n",
    "    n, c, h, w = x.shape\n",
    "    padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='reflect')\n",
    "    cropped = np.empty((n, c, crop_size, crop_size), dtype=x.dtype)\n",
    "    for i in range(n):\n",
    "        top = np.random.randint(0, padding * 2 + 1)\n",
    "        left = np.random.randint(0, padding * 2 + 1)\n",
    "        cropped[i] = padded[i, :, top:top+crop_size, left:left+crop_size]\n",
    "    return cropped\n",
    "\n",
    "def horizontal_flip(x):\n",
    "    return x[:, :, :, ::-1]\n",
    "\n",
    "def cutout(x, size=16):  \n",
    "    x_cut = x.copy()\n",
    "    n, c, h, w = x.shape\n",
    "    for i in range(n):\n",
    "        cy, cx = np.random.randint(h), np.random.randint(w)\n",
    "        y1 = np.clip(cy - size // 2, 0, h)\n",
    "        y2 = np.clip(cy + size // 2, 0, h)\n",
    "        x1 = np.clip(cx - size // 2, 0, w)\n",
    "        x2 = np.clip(cx + size // 2, 0, w)\n",
    "        x_cut[i, :, y1:y2, x1:x2] = 0\n",
    "    return x_cut\n",
    "\n",
    "def color_jitter(x, brightness=0.3, contrast=0.3):\n",
    "    x_jittered = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        b = 1 + np.random.uniform(-brightness, brightness)\n",
    "        c = 1 + np.random.uniform(-contrast, contrast)\n",
    "        mean = x_jittered[i].mean(axis=(1, 2), keepdims=True)\n",
    "        x_jittered[i] = (x_jittered[i] - mean) * c + mean\n",
    "        x_jittered[i] = np.clip(x_jittered[i] * b, 0, 1)\n",
    "    return x_jittered\n",
    "\n",
    "# 증강 적용 함수 (on-the-fly)\n",
    "def apply_augmentations(x, crop_size=32, padding=4, cutout_size=16):\n",
    "    # random crop\n",
    "    n, c, h, w = x.shape\n",
    "    padded = np.pad(x, ((0, 0), (0, 0), (padding, padding), (padding, padding)), mode='reflect')\n",
    "    cropped = np.empty((n, c, crop_size, crop_size), dtype=x.dtype)\n",
    "    for i in range(n):\n",
    "        top = np.random.randint(0, padding * 2 + 1)\n",
    "        left = np.random.randint(0, padding * 2 + 1)\n",
    "        cropped[i] = padded[i, :, top:top+crop_size, left:left+crop_size]\n",
    "\n",
    "    # horizontal flip\n",
    "    if np.random.rand() < 0.5:\n",
    "        cropped = cropped[:, :, :, ::-1]\n",
    "\n",
    "    # cutout\n",
    "    for i in range(n):\n",
    "        cy, cx = np.random.randint(crop_size), np.random.randint(crop_size)\n",
    "        y1 = np.clip(cy - cutout_size // 2, 0, crop_size)\n",
    "        y2 = np.clip(cy + cutout_size // 2, 0, crop_size)\n",
    "        x1 = np.clip(cx - cutout_size // 2, 0, crop_size)\n",
    "        x2 = np.clip(cx + cutout_size // 2, 0, crop_size)\n",
    "        cropped[i, :, y1:y2, x1:x2] = 0\n",
    "\n",
    "    return cropped\n",
    "\n",
    "def smooth_labels(y, smoothing=0.1, num_classes=100):\n",
    "    confidence = 1.0 - smoothing\n",
    "    label_shape = (y.shape[0], num_classes)\n",
    "    smooth = np.full(label_shape, smoothing / (num_classes - 1))\n",
    "    smooth[np.arange(y.shape[0]), y] = confidence\n",
    "    return smooth\n",
    "\n",
    "meta = load_meta()\n",
    "label_names = meta['fine_label_names']\n",
    "\n",
    "# 원본 x_train 로드\n",
    "x_train, y_train_fine, _ = load_batch(\"./cifar-100-python/train\")\n",
    "x_train = x_train.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "x_train = normalize(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake Quantization 함수\n",
    "def fake_quantize(x, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    \n",
    "    if x_max == x_min:\n",
    "        return x  # avoid divide by zero\n",
    "    \n",
    "    scale = (x_max - x_min) / (qmax - qmin)\n",
    "    zero_point = qmin - x_min / scale\n",
    "    zero_point = np.clip(np.round(zero_point), qmin, qmax)\n",
    "\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x = np.clip(np.round(q_x), qmin, qmax)\n",
    "    fq_x = scale * (q_x - zero_point)\n",
    "    return fq_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 레이어 및 ResNet-20 정의\n",
    "from common.layers import Convolution, Affine, Relu, BatchNormalization\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.util import im2col, col2im\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        # Fake Quantization\n",
    "        W_q = fake_quantize(self.W)\n",
    "        b_q = fake_quantize(self.b)\n",
    "        x_q = fake_quantize(self.x)\n",
    "\n",
    "        out = np.dot(x_q, W_q) + b_q\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)\n",
    "        return dx\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, _, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        # Fake Quantization\n",
    "        W_q = fake_quantize(self.W)\n",
    "        b_q = fake_quantize(self.b)\n",
    "        x_q = fake_quantize(x)\n",
    "\n",
    "        col = im2col(x_q, FH, FW, self.stride, self.pad)\n",
    "        col_W = W_q.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + b_q\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout).transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class ResidualBlock:\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        self.stride = stride\n",
    "        self.equal_in_out = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        self.conv1 = Convolution(\n",
    "            W=np.random.randn(out_channels, in_channels, 3, 3) * np.sqrt(2. / in_channels),\n",
    "            b=np.zeros(out_channels),\n",
    "            stride=stride,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn1 = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.conv2 = Convolution(\n",
    "            W=np.random.randn(out_channels, out_channels, 3, 3) * np.sqrt(2. / out_channels),\n",
    "            b=np.zeros(out_channels),\n",
    "            stride=1,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn2 = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "        self.relu2 = Relu()\n",
    "\n",
    "        if not self.equal_in_out:\n",
    "            self.shortcut = Convolution(\n",
    "                W=np.random.randn(out_channels, in_channels, 1, 1) * np.sqrt(2. / in_channels),\n",
    "                b=np.zeros(out_channels),\n",
    "                stride=stride,\n",
    "                pad=0\n",
    "            )\n",
    "            self.bn_shortcut = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "\n",
    "    def forward(self, x, train_flg=True, skip_prob=0.0):\n",
    "        self.x = x\n",
    "\n",
    "        if train_flg and np.random.rand() < skip_prob:\n",
    "            return x  # skip this residual block\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out, train_flg)\n",
    "        out = self.relu1.forward(out)\n",
    "\n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out, train_flg)\n",
    "        self.out_main = out\n",
    "\n",
    "        if self.equal_in_out:\n",
    "            shortcut = x\n",
    "        else:\n",
    "            shortcut = self.shortcut.forward(x)\n",
    "            shortcut = self.bn_shortcut.forward(shortcut, train_flg)\n",
    "        self.out_shortcut = shortcut\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu2.forward(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.relu2.backward(dout)\n",
    "\n",
    "        dshortcut = dout.copy()\n",
    "        dmain = dout.copy()\n",
    "\n",
    "        dmain = self.bn2.backward(dmain)\n",
    "        dmain = self.conv2.backward(dmain)\n",
    "\n",
    "        dmain = self.relu1.backward(dmain)\n",
    "        dmain = self.bn1.backward(dmain)\n",
    "        dmain = self.conv1.backward(dmain)\n",
    "\n",
    "        if not self.equal_in_out:\n",
    "            dshortcut = self.bn_shortcut.backward(dshortcut)\n",
    "            dshortcut = self.shortcut.backward(dshortcut)\n",
    "\n",
    "        dx = dmain + dshortcut\n",
    "        return dx\n",
    "\n",
    "class ResNet20:\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_classes=100):\n",
    "        self.params = []\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        self.conv1 = Convolution(\n",
    "            W=np.random.randn(16, 3, 3, 3) * np.sqrt(2. / 3),\n",
    "            b=np.zeros(16),\n",
    "            stride=1,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn1 = BatchNormalization(gamma=np.ones(16), beta=np.zeros(16))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.layer1 = [ResidualBlock(16, 16, stride=1) for _ in range(3)]\n",
    "        self.layer2 = [ResidualBlock(16 if i == 0 else 32, 32, stride=2 if i == 0 else 1) for i in range(3)]\n",
    "        self.layer3 = [ResidualBlock(32 if i == 0 else 64, 64, stride=2 if i == 0 else 1) for i in range(3)]\n",
    "\n",
    "        self.fc = Affine(W=np.random.randn(64, num_classes) * np.sqrt(2. / 64), b=np.zeros(num_classes))\n",
    "\n",
    "    def clip_weights(self, clip_value=1.0):\n",
    "    # 개별 레이어의 weight들을 [-clip_value, clip_value]로 제한\n",
    "        self.conv1.W = np.clip(self.conv1.W, -clip_value, clip_value)\n",
    "        self.fc.W = np.clip(self.fc.W, -clip_value, clip_value)\n",
    "\n",
    "        for block in self.layer1 + self.layer2 + self.layer3:\n",
    "            block.conv1.W = np.clip(block.conv1.W, -clip_value, clip_value)\n",
    "            block.conv2.W = np.clip(block.conv2.W, -clip_value, clip_value)\n",
    "            if not block.equal_in_out:\n",
    "                block.shortcut.W = np.clip(block.shortcut.W, -clip_value, clip_value)\n",
    "\n",
    "    def forward(self, x, train_flg=True, skip_prob=0.0):\n",
    "        self.input = x\n",
    "\n",
    "        if train_flg and np.random.rand() < skip_prob:\n",
    "            return x  # skip this residual block\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out, train_flg)\n",
    "        out = self.relu1.forward(out)\n",
    "\n",
    "        for block in self.layer1:\n",
    "            out = block.forward(out, train_flg)\n",
    "        for block in self.layer2:\n",
    "            out = block.forward(out, train_flg)\n",
    "        for block in self.layer3:\n",
    "            out = block.forward(out, train_flg)\n",
    "\n",
    "        self.feature_map = out\n",
    "\n",
    "        N, C, H, W = out.shape\n",
    "        out = out.mean(axis=(2, 3))\n",
    "\n",
    "        self.pooled = out\n",
    "        out = self.fc.forward(out)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x, batch_size=100):\n",
    "        y_list = []\n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            y_batch = self.forward(x_batch, train_flg=False)\n",
    "            y_list.append(y_batch)\n",
    "        return np.concatenate(y_list, axis=0)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.forward(x, train_flg=True)\n",
    "        return cross_entropy_error(softmax(y), t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        acc = 0.0\n",
    "        total = x.shape[0]\n",
    "        for i in range(0, total, batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "\n",
    "            y = self.predict(x_batch)\n",
    "            y = np.argmax(y, axis=1)\n",
    "\n",
    "            if t.ndim != 1:\n",
    "                t_batch = np.argmax(t_batch, axis=1)\n",
    "\n",
    "            acc += np.sum(y == t_batch)\n",
    "\n",
    "        return acc / total\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.fc.backward(dout)\n",
    "        dout = dout.reshape(self.feature_map.shape[0], self.feature_map.shape[1], 1, 1)\n",
    "        dout = dout.repeat(self.feature_map.shape[2], axis=2).repeat(self.feature_map.shape[3], axis=3)\n",
    "\n",
    "        for block in reversed(self.layer3):\n",
    "            dout = block.backward(dout)\n",
    "        for block in reversed(self.layer2):\n",
    "            dout = block.backward(dout)\n",
    "        for block in reversed(self.layer1):\n",
    "            dout = block.backward(dout)\n",
    "\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type)                       Output Shape                Param #\n",
      "===========================================================================\n",
      " 1. Conv1                           (1, 16, 32, 32)                 448\n",
      " 2. Block[1-1]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 3. Block[1-1]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 4. Block[1-2]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 5. Block[1-2]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 6. Block[1-3]_Conv1                (1, 16, 32, 32)               2,320\n",
      " 7. Block[1-3]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 8. Block[2-1]_Conv1                (1, 32, 16, 16)               4,640\n",
      " 9. Block[2-1]_Conv2                (1, 32, 16, 16)               9,248\n",
      "    └─ Shortcut[2-1]                (1, 32, 16, 16)                 544\n",
      "10. Block[2-2]_Conv1                (1, 32, 16, 16)               9,248\n",
      "11. Block[2-2]_Conv2                (1, 32, 16, 16)               9,248\n",
      "12. Block[2-3]_Conv1                (1, 32, 16, 16)               9,248\n",
      "13. Block[2-3]_Conv2                (1, 32, 16, 16)               9,248\n",
      "14. Block[3-1]_Conv1                (1, 64, 8, 8)                18,496\n",
      "15. Block[3-1]_Conv2                (1, 64, 8, 8)                36,928\n",
      "    └─ Shortcut[3-1]                (1, 64, 8, 8)                 2,112\n",
      "16. Block[3-2]_Conv1                (1, 64, 8, 8)                36,928\n",
      "17. Block[3-2]_Conv2                (1, 64, 8, 8)                36,928\n",
      "18. Block[3-3]_Conv1                (1, 64, 8, 8)                36,928\n",
      "19. Block[3-3]_Conv2                (1, 64, 8, 8)                36,928\n",
      "    GlobalAvgPool                   (1, 64)                           0\n",
      "20. FC                              (1, 100)                      6,500\n",
      "===========================================================================\n",
      "Total weight layers:                                        20\n",
      "Total params:                                               277,540\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 출력\n",
    "\n",
    "def count_params(layer):\n",
    "    count = 0\n",
    "    if hasattr(layer, 'W'):\n",
    "        count += np.prod(layer.W.shape)\n",
    "    if hasattr(layer, 'b'):\n",
    "        count += np.prod(layer.b.shape)\n",
    "    return count\n",
    "\n",
    "def print_resnet20_summary(model, input_shape=(1, 3, 32, 32)):\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Layer (type)':<35}{'Output Shape':<25}{'Param #':>10}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "    x = np.zeros(input_shape)\n",
    "    total_params = 0\n",
    "    layer_idx = 1\n",
    "\n",
    "    # Conv1\n",
    "    x = model.conv1.forward(x)\n",
    "    p = count_params(model.conv1)\n",
    "    print(f\"{layer_idx:>2}. {'Conv1':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "    layer_idx += 1\n",
    "\n",
    "    x = model.bn1.forward(x, train_flg=False)\n",
    "    x = model.relu1.forward(x)\n",
    "\n",
    "    # Residual Blocks\n",
    "    for i, layer_block in enumerate([model.layer1, model.layer2, model.layer3]):\n",
    "        for j, block in enumerate(layer_block):\n",
    "            residual = x.copy()\n",
    "\n",
    "            # Conv1\n",
    "            x = block.conv1.forward(x)\n",
    "            p = count_params(block.conv1)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv1\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn1.forward(x, train_flg=False)\n",
    "            x = block.relu1.forward(x)\n",
    "\n",
    "            # Conv2\n",
    "            x = block.conv2.forward(x)\n",
    "            p = count_params(block.conv2)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv2\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn2.forward(x, train_flg=False)\n",
    "\n",
    "            # Shortcut (optional)\n",
    "            if not block.equal_in_out:\n",
    "                x_sc = block.shortcut.forward(residual)\n",
    "                p = count_params(block.shortcut)\n",
    "                name = f\"└─ Shortcut[{i+1}-{j+1}]\"\n",
    "                print(f\"{'':>3} {name:<32}{str(x_sc.shape):<25}{p:>10,}\", flush=True)\n",
    "                total_params += p\n",
    "                x = x + x_sc\n",
    "                x = block.bn_shortcut.forward(x, train_flg=False)\n",
    "            else:\n",
    "                x = x + residual\n",
    "\n",
    "            x = block.relu2.forward(x)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = x.mean(axis=(2, 3))\n",
    "    print(f\"{'':>3} {'GlobalAvgPool':<32}{str(x.shape):<25}{'0':>10}\", flush=True)\n",
    "\n",
    "    # FC\n",
    "    x = model.fc.forward(x)\n",
    "    p = count_params(model.fc)\n",
    "    print(f\"{layer_idx:>2}. {'FC':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Total weight layers:':<60}{'20'}\", flush=True)\n",
    "    print(f\"{'Total params:':<60}{total_params:,}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "model = ResNet20()\n",
    "print_resnet20_summary(model, input_shape=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 복제 및 EMA 업데이트 함수\n",
    "import math\n",
    "\n",
    "def clone_model(model):\n",
    "    return copy.deepcopy(model)\n",
    "\n",
    "def update_ema_model(ema_model, model, decay=0.999):\n",
    "    for ema_layer, model_layer in zip(ema_model.layer1 + ema_model.layer2 + ema_model.layer3,\n",
    "                                      model.layer1 + model.layer2 + model.layer3):\n",
    "        for attr in ['conv1', 'conv2', 'shortcut']:\n",
    "            if hasattr(model_layer, attr):\n",
    "                model_conv = getattr(model_layer, attr)\n",
    "                ema_conv = getattr(ema_layer, attr)\n",
    "                ema_conv.W = decay * ema_conv.W + (1 - decay) * model_conv.W\n",
    "                ema_conv.b = decay * ema_conv.b + (1 - decay) * model_conv.b\n",
    "\n",
    "    ema_model.fc.W = decay * ema_model.fc.W + (1 - decay) * model.fc.W\n",
    "    ema_model.fc.b = decay * ema_model.fc.b + (1 - decay) * model.fc.b\n",
    "\n",
    "\n",
    "def cosine_annealing_with_warmup(epoch, total_epochs, base_lr, warmup_epochs=5):\n",
    "    if epoch < warmup_epochs:\n",
    "        return base_lr * (epoch + 1) / warmup_epochs\n",
    "    else:\n",
    "        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "        return 0.5 * base_lr * (1 + math.cos(math.pi * progress))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from common.optimizer import Adam\n",
    "from common.functions import softmax\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name, train_data, val_data, test_data, epochs=20, batch_size=64, optimizer_name='sgd', lr=0.01):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.val_x, self.val_t = val_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "        self.max_iter = self.epochs * self.iter_per_epoch\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        if optimizer_name == 'adam':\n",
    "            self.optimizer = Adam(lr=lr)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "        self.ema_model = clone_model(self.model)\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "        if hasattr(self.model.fc, 'W'):\n",
    "            param_dict['fc_W'] = self.model.fc.W\n",
    "            param_dict['fc_b'] = self.model.fc.b\n",
    "            grad_dict['fc_W'] = self.model.fc.dW\n",
    "            grad_dict['fc_b'] = self.model.fc.db\n",
    "\n",
    "        idx = 0\n",
    "        for layer in self.model.layer1 + self.model.layer2 + self.model.layer3:\n",
    "            for attr in ['conv1', 'conv2', 'shortcut']:\n",
    "                if hasattr(layer, attr):\n",
    "                    conv = getattr(layer, attr)\n",
    "                    param_dict[f'{idx}_W'] = conv.W\n",
    "                    param_dict[f'{idx}_b'] = conv.b\n",
    "                    grad_dict[f'{idx}_W'] = conv.dW\n",
    "                    grad_dict[f'{idx}_b'] = conv.db\n",
    "                    idx += 1\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            return (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            return dx / batch_size\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        x_batch = apply_augmentations(x_batch)\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "        if t_batch.ndim == 1:\n",
    "            t_batch = smooth_labels(t_batch, smoothing=0.1, num_classes=100)\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        self.model.backward(self.loss_grad(x_batch, t_batch))\n",
    "\n",
    "        if hasattr(self.model, 'clip_weights'):\n",
    "            self.model.clip_weights(clip_value=1.0)\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "        update_ema_model(self.ema_model, self.model)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.optimizer.lr = cosine_annealing_with_warmup(epoch, self.epochs, base_lr=0.01)\n",
    "            print(f\"[Epoch {epoch + 1}]\", flush=True)\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"  Iter {i:3d}/{self.iter_per_epoch}: Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            val_acc = self.ema_model.accuracy(self.val_x, self.val_t)\n",
    "            val_loss = self.batched_loss(self.val_x, self.val_t, batch_size=128)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(val_acc)\n",
    "            self.val_loss_list.append(val_loss)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Train acc: {train_acc:.4f}, Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f} (Time: {elapsed:.2f}s)\\n\", flush=True)\n",
    "\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                model_filename = f\"{self.model_name}_epoch_{epoch+1}.pkl\"\n",
    "                self.save_model(model_filename)\n",
    "                print(f\">>> Saved model to {model_filename}\", flush=True)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "\n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': getattr(self.optimizer, 'beta1', None),\n",
    "            'beta2': getattr(self.optimizer, 'beta2', None),\n",
    "            'eps': getattr(self.optimizer, 'eps', None),\n",
    "            'm': getattr(self.optimizer, 'm', {}),\n",
    "            'v': getattr(self.optimizer, 'v', {}),\n",
    "            't': getattr(self.optimizer, 't', 0),\n",
    "        }\n",
    "\n",
    "        save_data = {\n",
    "            'model': model_state,\n",
    "            'optimizer': optimizer_state,\n",
    "            'train_loss_list': self.train_loss_list,\n",
    "            'train_acc_list': self.train_acc_list,\n",
    "            'test_acc_list': self.test_acc_list,\n",
    "            'val_loss_list': self.val_loss_list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename,\n",
    "                 loss=np.array(self.train_loss_list),\n",
    "                 train_acc=np.array(self.train_acc_list),\n",
    "                 test_acc=np.array(self.test_acc_list),\n",
    "                 val_loss=np.array(self.val_loss_list))\n",
    "        print(f\"Log saved to {filename}\", flush=True)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        for k in params:\n",
    "            if k in state['model']:\n",
    "                params[k][...] = state['model'][k]\n",
    "            else:\n",
    "                print(f\"[WARN] Key {k} not found in checkpoint!\", flush=True)\n",
    "\n",
    "        opt = state['optimizer']\n",
    "        self.optimizer.lr = opt['lr']\n",
    "        self.optimizer.beta1 = opt['beta1']\n",
    "        self.optimizer.beta2 = opt['beta2']\n",
    "        self.optimizer.eps = opt['eps']\n",
    "        self.optimizer.m = opt['m']\n",
    "        self.optimizer.v = opt['v']\n",
    "        self.optimizer.t = opt['t']\n",
    "\n",
    "        # 복원된 로그\n",
    "        self.train_loss_list = state.get('train_loss_list', [])\n",
    "        self.train_acc_list = state.get('train_acc_list', [])\n",
    "        self.test_acc_list = state.get('test_acc_list', [])\n",
    "        self.val_loss_list = state.get('val_loss_list', [])\n",
    "\n",
    "    def batched_loss(self, x, t, batch_size=128):\n",
    "        total_loss = 0.0\n",
    "        total_count = 0\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "            loss = self.model.loss(x_batch, t_batch)\n",
    "            total_loss += loss * len(x_batch)\n",
    "            total_count += len(x_batch)\n",
    "        return total_loss / total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== [모델 설정 1] LR=0.01, BS=64 ===\n",
      "[Epoch 1]\n",
      "  Iter   0/781: Loss 5.4997\n",
      "  Iter  10/781: Loss 5.0370\n",
      "  Iter  20/781: Loss 4.7453\n",
      "  Iter  30/781: Loss 4.5141\n",
      "  Iter  40/781: Loss 4.4888\n",
      "  Iter  50/781: Loss 4.6494\n",
      "  Iter  60/781: Loss 4.4305\n",
      "  Iter  70/781: Loss 4.5015\n",
      "  Iter  80/781: Loss 4.3894\n",
      "  Iter  90/781: Loss 4.3830\n",
      "  Iter 100/781: Loss 4.3469\n",
      "  Iter 110/781: Loss 4.1664\n",
      "  Iter 120/781: Loss 4.1860\n",
      "  Iter 130/781: Loss 4.3391\n",
      "  Iter 140/781: Loss 4.2119\n",
      "  Iter 150/781: Loss 4.2044\n",
      "  Iter 160/781: Loss 4.3661\n",
      "  Iter 170/781: Loss 4.2445\n",
      "  Iter 180/781: Loss 4.1306\n",
      "  Iter 190/781: Loss 4.3741\n",
      "  Iter 200/781: Loss 4.2067\n",
      "  Iter 210/781: Loss 4.1877\n",
      "  Iter 220/781: Loss 4.1536\n",
      "  Iter 230/781: Loss 3.8896\n",
      "  Iter 240/781: Loss 3.9387\n",
      "  Iter 250/781: Loss 4.2109\n",
      "  Iter 260/781: Loss 4.0273\n",
      "  Iter 270/781: Loss 3.9002\n",
      "  Iter 280/781: Loss 4.1515\n",
      "  Iter 290/781: Loss 3.9806\n",
      "  Iter 300/781: Loss 3.9005\n",
      "  Iter 310/781: Loss 3.9369\n",
      "  Iter 320/781: Loss 4.0719\n",
      "  Iter 330/781: Loss 4.0763\n",
      "  Iter 340/781: Loss 4.0772\n",
      "  Iter 350/781: Loss 3.9509\n",
      "  Iter 360/781: Loss 3.8055\n",
      "  Iter 370/781: Loss 3.9348\n",
      "  Iter 380/781: Loss 4.1088\n",
      "  Iter 390/781: Loss 3.8627\n",
      "  Iter 400/781: Loss 3.8975\n",
      "  Iter 410/781: Loss 4.0359\n",
      "  Iter 420/781: Loss 4.0591\n",
      "  Iter 430/781: Loss 4.1446\n",
      "  Iter 440/781: Loss 3.7203\n",
      "  Iter 450/781: Loss 3.7755\n",
      "  Iter 460/781: Loss 3.9674\n",
      "  Iter 470/781: Loss 3.8809\n",
      "  Iter 480/781: Loss 3.8062\n",
      "  Iter 490/781: Loss 3.6682\n",
      "  Iter 500/781: Loss 4.1524\n",
      "  Iter 510/781: Loss 3.8236\n",
      "  Iter 520/781: Loss 3.8956\n",
      "  Iter 530/781: Loss 3.9481\n",
      "  Iter 540/781: Loss 3.6858\n",
      "  Iter 550/781: Loss 3.7030\n",
      "  Iter 560/781: Loss 3.7449\n",
      "  Iter 570/781: Loss 3.7614\n",
      "  Iter 580/781: Loss 3.5849\n",
      "  Iter 590/781: Loss 4.0059\n",
      "  Iter 600/781: Loss 3.8718\n",
      "  Iter 610/781: Loss 3.7522\n",
      "  Iter 620/781: Loss 3.9425\n",
      "  Iter 630/781: Loss 3.8537\n",
      "  Iter 640/781: Loss 3.6980\n",
      "  Iter 650/781: Loss 3.6518\n",
      "  Iter 660/781: Loss 3.7114\n",
      "  Iter 670/781: Loss 3.5324\n",
      "  Iter 680/781: Loss 3.7245\n",
      "  Iter 690/781: Loss 3.6597\n",
      "  Iter 700/781: Loss 3.7260\n",
      "  Iter 710/781: Loss 3.9222\n",
      "  Iter 720/781: Loss 3.5339\n",
      "  Iter 730/781: Loss 3.7897\n",
      "  Iter 740/781: Loss 3.4661\n",
      "  Iter 750/781: Loss 3.7285\n",
      "  Iter 760/781: Loss 3.6770\n",
      "  Iter 770/781: Loss 3.7779\n",
      "  Iter 780/781: Loss 3.8218\n",
      "Train acc: 0.1180, Val loss: 3.6449, Val acc: 0.0116 (Time: 3459.28s)\n",
      "\n",
      "[Epoch 2]\n",
      "  Iter   0/781: Loss 3.5904\n",
      "  Iter  10/781: Loss 3.6959\n",
      "  Iter  20/781: Loss 3.9168\n",
      "  Iter  30/781: Loss 3.7761\n",
      "  Iter  40/781: Loss 3.6676\n",
      "  Iter  50/781: Loss 3.8445\n",
      "  Iter  60/781: Loss 3.7557\n",
      "  Iter  70/781: Loss 3.9635\n",
      "  Iter  80/781: Loss 3.9201\n",
      "  Iter  90/781: Loss 3.9136\n",
      "  Iter 100/781: Loss 3.6744\n",
      "  Iter 110/781: Loss 3.5695\n",
      "  Iter 120/781: Loss 3.7045\n",
      "  Iter 130/781: Loss 3.5765\n",
      "  Iter 140/781: Loss 3.7391\n",
      "  Iter 150/781: Loss 3.8167\n",
      "  Iter 160/781: Loss 3.9232\n",
      "  Iter 170/781: Loss 3.8041\n",
      "  Iter 180/781: Loss 3.7032\n",
      "  Iter 190/781: Loss 3.7936\n",
      "  Iter 200/781: Loss 3.7255\n",
      "  Iter 210/781: Loss 3.6680\n",
      "  Iter 220/781: Loss 3.4144\n",
      "  Iter 230/781: Loss 3.5927\n",
      "  Iter 240/781: Loss 3.5313\n",
      "  Iter 250/781: Loss 3.7823\n",
      "  Iter 260/781: Loss 3.5976\n",
      "  Iter 270/781: Loss 3.9070\n",
      "  Iter 280/781: Loss 3.8958\n",
      "  Iter 290/781: Loss 3.7126\n",
      "  Iter 300/781: Loss 3.7560\n",
      "  Iter 310/781: Loss 3.4162\n",
      "  Iter 320/781: Loss 3.3087\n",
      "  Iter 330/781: Loss 3.5359\n",
      "  Iter 340/781: Loss 3.7568\n",
      "  Iter 350/781: Loss 3.7138\n",
      "  Iter 360/781: Loss 3.5086\n",
      "  Iter 370/781: Loss 3.6801\n",
      "  Iter 380/781: Loss 3.6626\n",
      "  Iter 390/781: Loss 3.6398\n",
      "  Iter 400/781: Loss 3.4833\n",
      "  Iter 410/781: Loss 3.3679\n",
      "  Iter 420/781: Loss 3.5882\n",
      "  Iter 430/781: Loss 3.4966\n",
      "  Iter 440/781: Loss 3.4693\n",
      "  Iter 450/781: Loss 3.1451\n",
      "  Iter 460/781: Loss 3.5220\n",
      "  Iter 470/781: Loss 3.3242\n",
      "  Iter 480/781: Loss 3.4034\n",
      "  Iter 490/781: Loss 3.4842\n",
      "  Iter 500/781: Loss 3.4616\n",
      "  Iter 510/781: Loss 3.6601\n",
      "  Iter 520/781: Loss 3.3907\n",
      "  Iter 530/781: Loss 3.4077\n",
      "  Iter 540/781: Loss 3.7030\n",
      "  Iter 550/781: Loss 3.6337\n",
      "  Iter 560/781: Loss 3.6201\n",
      "  Iter 570/781: Loss 3.5220\n",
      "  Iter 580/781: Loss 3.5711\n",
      "  Iter 590/781: Loss 3.6895\n",
      "  Iter 600/781: Loss 3.5226\n",
      "  Iter 610/781: Loss 3.5780\n",
      "  Iter 620/781: Loss 3.3296\n",
      "  Iter 630/781: Loss 3.7095\n",
      "  Iter 640/781: Loss 3.4276\n",
      "  Iter 650/781: Loss 3.4286\n",
      "  Iter 660/781: Loss 3.3261\n",
      "  Iter 670/781: Loss 3.4575\n",
      "  Iter 680/781: Loss 3.3759\n",
      "  Iter 690/781: Loss 3.4289\n",
      "  Iter 700/781: Loss 3.5439\n",
      "  Iter 710/781: Loss 3.4291\n",
      "  Iter 720/781: Loss 3.5382\n",
      "  Iter 730/781: Loss 3.6140\n",
      "  Iter 740/781: Loss 3.6859\n",
      "  Iter 750/781: Loss 3.3265\n",
      "  Iter 760/781: Loss 3.1834\n",
      "  Iter 770/781: Loss 3.5654\n",
      "  Iter 780/781: Loss 3.4570\n",
      "Train acc: 0.1970, Val loss: 3.2023, Val acc: 0.0126 (Time: 3283.27s)\n",
      "\n",
      "[Epoch 3]\n",
      "  Iter   0/781: Loss 3.3668\n",
      "  Iter  10/781: Loss 3.6185\n",
      "  Iter  20/781: Loss 3.7159\n",
      "  Iter  30/781: Loss 3.5977\n",
      "  Iter  40/781: Loss 3.3384\n",
      "  Iter  50/781: Loss 3.4393\n",
      "  Iter  60/781: Loss 3.5339\n",
      "  Iter  70/781: Loss 3.5326\n",
      "  Iter  80/781: Loss 3.5894\n",
      "  Iter  90/781: Loss 3.8047\n",
      "  Iter 100/781: Loss 3.2150\n",
      "  Iter 110/781: Loss 3.3749\n",
      "  Iter 120/781: Loss 3.3481\n",
      "  Iter 130/781: Loss 3.6060\n",
      "  Iter 140/781: Loss 3.7167\n",
      "  Iter 150/781: Loss 3.2491\n",
      "  Iter 160/781: Loss 3.4831\n",
      "  Iter 170/781: Loss 3.6000\n",
      "  Iter 180/781: Loss 3.2105\n",
      "  Iter 190/781: Loss 3.4442\n",
      "  Iter 200/781: Loss 3.2209\n",
      "  Iter 210/781: Loss 3.5412\n",
      "  Iter 220/781: Loss 3.1577\n",
      "  Iter 230/781: Loss 3.4689\n",
      "  Iter 240/781: Loss 3.3864\n",
      "  Iter 250/781: Loss 3.6008\n",
      "  Iter 260/781: Loss 3.1619\n",
      "  Iter 270/781: Loss 3.1541\n",
      "  Iter 280/781: Loss 3.1804\n",
      "  Iter 290/781: Loss 3.1015\n",
      "  Iter 300/781: Loss 3.0484\n",
      "  Iter 310/781: Loss 3.1568\n",
      "  Iter 320/781: Loss 3.3071\n",
      "  Iter 330/781: Loss 3.2192\n",
      "  Iter 340/781: Loss 3.3312\n",
      "  Iter 350/781: Loss 3.1658\n",
      "  Iter 360/781: Loss 3.8231\n",
      "  Iter 370/781: Loss 3.5717\n",
      "  Iter 380/781: Loss 3.4284\n",
      "  Iter 390/781: Loss 2.7785\n",
      "  Iter 400/781: Loss 3.2403\n",
      "  Iter 410/781: Loss 3.6489\n",
      "  Iter 420/781: Loss 3.1279\n",
      "  Iter 430/781: Loss 3.1772\n",
      "  Iter 440/781: Loss 3.1976\n",
      "  Iter 450/781: Loss 3.2641\n",
      "  Iter 460/781: Loss 3.2218\n",
      "  Iter 470/781: Loss 3.0256\n",
      "  Iter 480/781: Loss 3.1801\n",
      "  Iter 490/781: Loss 3.0959\n",
      "  Iter 500/781: Loss 3.2400\n",
      "  Iter 510/781: Loss 3.2164\n",
      "  Iter 520/781: Loss 3.4410\n",
      "  Iter 530/781: Loss 3.2100\n",
      "  Iter 540/781: Loss 3.3548\n",
      "  Iter 550/781: Loss 3.3912\n",
      "  Iter 560/781: Loss 2.9628\n",
      "  Iter 570/781: Loss 3.0066\n",
      "  Iter 580/781: Loss 3.0372\n",
      "  Iter 590/781: Loss 3.0978\n",
      "  Iter 600/781: Loss 3.3725\n",
      "  Iter 610/781: Loss 3.2158\n",
      "  Iter 620/781: Loss 3.0608\n",
      "  Iter 630/781: Loss 3.4155\n",
      "  Iter 640/781: Loss 3.3831\n",
      "  Iter 650/781: Loss 3.0005\n",
      "  Iter 660/781: Loss 2.9871\n",
      "  Iter 670/781: Loss 3.5034\n",
      "  Iter 680/781: Loss 3.2150\n",
      "  Iter 690/781: Loss 3.1871\n",
      "  Iter 700/781: Loss 3.2557\n",
      "  Iter 710/781: Loss 3.4778\n",
      "  Iter 720/781: Loss 3.3587\n",
      "  Iter 730/781: Loss 3.0403\n",
      "  Iter 740/781: Loss 3.1271\n",
      "  Iter 750/781: Loss 2.9900\n",
      "  Iter 760/781: Loss 3.5727\n",
      "  Iter 770/781: Loss 3.0817\n",
      "  Iter 780/781: Loss 3.0550\n",
      "Train acc: 0.2390, Val loss: 2.9201, Val acc: 0.0122 (Time: 3256.50s)\n",
      "\n",
      "[Epoch 4]\n",
      "  Iter   0/781: Loss 3.2549\n",
      "  Iter  10/781: Loss 3.2749\n",
      "  Iter  20/781: Loss 3.3750\n",
      "  Iter  30/781: Loss 3.0390\n",
      "  Iter  40/781: Loss 3.3833\n",
      "  Iter  50/781: Loss 3.4802\n",
      "  Iter  60/781: Loss 3.4186\n",
      "  Iter  70/781: Loss 2.9693\n",
      "  Iter  80/781: Loss 3.2797\n",
      "  Iter  90/781: Loss 3.1408\n",
      "  Iter 100/781: Loss 3.2414\n",
      "  Iter 110/781: Loss 3.4992\n",
      "  Iter 120/781: Loss 3.3235\n",
      "  Iter 130/781: Loss 3.2382\n",
      "  Iter 140/781: Loss 3.2101\n",
      "  Iter 150/781: Loss 3.1539\n",
      "  Iter 160/781: Loss 3.1531\n",
      "  Iter 170/781: Loss 2.9631\n",
      "  Iter 180/781: Loss 3.2188\n",
      "  Iter 190/781: Loss 2.9446\n",
      "  Iter 200/781: Loss 2.9338\n",
      "  Iter 210/781: Loss 3.3133\n",
      "  Iter 220/781: Loss 3.1672\n",
      "  Iter 230/781: Loss 3.1552\n",
      "  Iter 240/781: Loss 3.0830\n",
      "  Iter 250/781: Loss 3.1895\n",
      "  Iter 260/781: Loss 3.0241\n",
      "  Iter 270/781: Loss 3.1307\n",
      "  Iter 280/781: Loss 3.2334\n",
      "  Iter 290/781: Loss 3.0543\n",
      "  Iter 300/781: Loss 3.1056\n",
      "  Iter 310/781: Loss 2.8310\n",
      "  Iter 320/781: Loss 2.8213\n",
      "  Iter 330/781: Loss 3.0637\n",
      "  Iter 340/781: Loss 3.3376\n",
      "  Iter 350/781: Loss 3.2027\n",
      "  Iter 360/781: Loss 3.0573\n",
      "  Iter 370/781: Loss 2.9729\n",
      "  Iter 380/781: Loss 3.1601\n",
      "  Iter 390/781: Loss 3.1227\n",
      "  Iter 400/781: Loss 3.1696\n",
      "  Iter 410/781: Loss 3.1332\n",
      "  Iter 420/781: Loss 3.6064\n",
      "  Iter 430/781: Loss 3.2293\n",
      "  Iter 440/781: Loss 2.9139\n",
      "  Iter 450/781: Loss 2.9521\n",
      "  Iter 460/781: Loss 3.0923\n",
      "  Iter 470/781: Loss 3.1666\n",
      "  Iter 480/781: Loss 3.3671\n",
      "  Iter 490/781: Loss 3.1215\n",
      "  Iter 500/781: Loss 3.3144\n",
      "  Iter 510/781: Loss 3.0912\n",
      "  Iter 520/781: Loss 3.0372\n",
      "  Iter 530/781: Loss 3.0527\n",
      "  Iter 540/781: Loss 3.0124\n",
      "  Iter 550/781: Loss 3.0611\n",
      "  Iter 560/781: Loss 3.0149\n",
      "  Iter 570/781: Loss 3.2226\n",
      "  Iter 580/781: Loss 3.0774\n",
      "  Iter 590/781: Loss 2.8511\n",
      "  Iter 600/781: Loss 3.0151\n",
      "  Iter 610/781: Loss 3.3376\n",
      "  Iter 620/781: Loss 2.9881\n",
      "  Iter 630/781: Loss 3.0783\n",
      "  Iter 640/781: Loss 3.3575\n",
      "  Iter 650/781: Loss 2.8271\n",
      "  Iter 660/781: Loss 3.0568\n",
      "  Iter 670/781: Loss 3.0024\n",
      "  Iter 680/781: Loss 3.1246\n",
      "  Iter 690/781: Loss 3.1242\n",
      "  Iter 700/781: Loss 3.1836\n",
      "  Iter 710/781: Loss 2.6648\n",
      "  Iter 720/781: Loss 3.1249\n",
      "  Iter 730/781: Loss 2.9266\n",
      "  Iter 740/781: Loss 3.1256\n",
      "  Iter 750/781: Loss 2.9284\n",
      "  Iter 760/781: Loss 3.3053\n",
      "  Iter 770/781: Loss 2.7565\n",
      "  Iter 780/781: Loss 3.0503\n",
      "Train acc: 0.2490, Val loss: 2.8318, Val acc: 0.0080 (Time: 3255.64s)\n",
      "\n",
      "[Epoch 5]\n",
      "  Iter   0/781: Loss 2.8758\n",
      "  Iter  10/781: Loss 2.7345\n",
      "  Iter  20/781: Loss 2.8348\n",
      "  Iter  30/781: Loss 2.7741\n",
      "  Iter  40/781: Loss 2.8595\n",
      "  Iter  50/781: Loss 2.8398\n",
      "  Iter  60/781: Loss 2.8445\n",
      "  Iter  70/781: Loss 2.8809\n",
      "  Iter  80/781: Loss 3.0458\n",
      "  Iter  90/781: Loss 2.9351\n",
      "  Iter 100/781: Loss 3.2682\n",
      "  Iter 110/781: Loss 2.8610\n",
      "  Iter 120/781: Loss 3.0098\n",
      "  Iter 130/781: Loss 3.0915\n",
      "  Iter 140/781: Loss 2.9160\n",
      "  Iter 150/781: Loss 3.3136\n",
      "  Iter 160/781: Loss 3.2817\n",
      "  Iter 170/781: Loss 2.9910\n",
      "  Iter 180/781: Loss 3.1129\n",
      "  Iter 190/781: Loss 2.7505\n",
      "  Iter 200/781: Loss 3.0653\n",
      "  Iter 210/781: Loss 2.7263\n",
      "  Iter 220/781: Loss 2.8839\n",
      "  Iter 230/781: Loss 3.0002\n",
      "  Iter 240/781: Loss 2.9578\n",
      "  Iter 250/781: Loss 2.9266\n",
      "  Iter 260/781: Loss 3.1415\n",
      "  Iter 270/781: Loss 2.9476\n",
      "  Iter 280/781: Loss 2.7114\n",
      "  Iter 290/781: Loss 2.9089\n",
      "  Iter 300/781: Loss 2.9783\n",
      "  Iter 310/781: Loss 3.0141\n",
      "  Iter 320/781: Loss 2.8264\n",
      "  Iter 330/781: Loss 2.6873\n",
      "  Iter 340/781: Loss 2.9815\n",
      "  Iter 350/781: Loss 3.3286\n",
      "  Iter 360/781: Loss 2.9472\n",
      "  Iter 370/781: Loss 2.7269\n",
      "  Iter 380/781: Loss 3.0925\n",
      "  Iter 390/781: Loss 2.7528\n",
      "  Iter 400/781: Loss 2.8718\n",
      "  Iter 410/781: Loss 2.7901\n",
      "  Iter 420/781: Loss 2.8988\n",
      "  Iter 430/781: Loss 3.2759\n",
      "  Iter 440/781: Loss 2.8893\n",
      "  Iter 450/781: Loss 2.7492\n",
      "  Iter 460/781: Loss 3.1253\n",
      "  Iter 470/781: Loss 2.7131\n",
      "  Iter 480/781: Loss 2.4713\n",
      "  Iter 490/781: Loss 2.9347\n",
      "  Iter 500/781: Loss 2.7370\n",
      "  Iter 510/781: Loss 2.7882\n",
      "  Iter 520/781: Loss 2.9447\n",
      "  Iter 530/781: Loss 2.9535\n",
      "  Iter 540/781: Loss 3.1026\n",
      "  Iter 550/781: Loss 2.7650\n",
      "  Iter 560/781: Loss 2.8532\n",
      "  Iter 570/781: Loss 2.7266\n",
      "  Iter 580/781: Loss 2.9735\n",
      "  Iter 590/781: Loss 2.6333\n",
      "  Iter 600/781: Loss 3.1269\n",
      "  Iter 610/781: Loss 3.0815\n",
      "  Iter 620/781: Loss 2.6789\n",
      "  Iter 630/781: Loss 2.9871\n",
      "  Iter 640/781: Loss 3.0679\n",
      "  Iter 650/781: Loss 2.7123\n",
      "  Iter 660/781: Loss 2.7224\n",
      "  Iter 670/781: Loss 2.8409\n",
      "  Iter 680/781: Loss 2.6945\n",
      "  Iter 690/781: Loss 2.7763\n",
      "  Iter 700/781: Loss 2.8738\n",
      "  Iter 710/781: Loss 3.0040\n",
      "  Iter 720/781: Loss 2.8605\n",
      "  Iter 730/781: Loss 2.6172\n",
      "  Iter 740/781: Loss 3.0615\n",
      "  Iter 750/781: Loss 2.8700\n",
      "  Iter 760/781: Loss 2.8514\n",
      "  Iter 770/781: Loss 2.6538\n",
      "  Iter 780/781: Loss 2.5938\n",
      "Train acc: 0.2950, Val loss: 2.6326, Val acc: 0.0076 (Time: 3258.49s)\n",
      "\n",
      "[Epoch 6]\n",
      "  Iter   0/781: Loss 3.0273\n",
      "  Iter  10/781: Loss 2.9266\n",
      "  Iter  20/781: Loss 2.6996\n",
      "  Iter  30/781: Loss 2.9526\n",
      "  Iter  40/781: Loss 2.8689\n",
      "  Iter  50/781: Loss 2.7766\n",
      "  Iter  60/781: Loss 2.7071\n",
      "  Iter  70/781: Loss 3.0324\n",
      "  Iter  80/781: Loss 3.2262\n",
      "  Iter  90/781: Loss 3.0047\n",
      "  Iter 100/781: Loss 2.8202\n",
      "  Iter 110/781: Loss 2.9193\n",
      "  Iter 120/781: Loss 2.7383\n",
      "  Iter 130/781: Loss 2.6852\n",
      "  Iter 140/781: Loss 2.5319\n",
      "  Iter 150/781: Loss 2.5899\n",
      "  Iter 160/781: Loss 2.9836\n",
      "  Iter 170/781: Loss 2.8028\n",
      "  Iter 180/781: Loss 2.4748\n",
      "  Iter 190/781: Loss 2.8066\n",
      "  Iter 200/781: Loss 3.1676\n",
      "  Iter 210/781: Loss 2.7654\n",
      "  Iter 220/781: Loss 2.8374\n",
      "  Iter 230/781: Loss 2.8796\n",
      "  Iter 240/781: Loss 2.6287\n",
      "  Iter 250/781: Loss 2.7565\n",
      "  Iter 260/781: Loss 2.8761\n",
      "  Iter 270/781: Loss 2.9542\n",
      "  Iter 280/781: Loss 2.8211\n",
      "  Iter 290/781: Loss 2.9765\n",
      "  Iter 300/781: Loss 3.2270\n",
      "  Iter 310/781: Loss 2.7325\n",
      "  Iter 320/781: Loss 2.8571\n",
      "  Iter 330/781: Loss 2.7089\n",
      "  Iter 340/781: Loss 2.8232\n",
      "  Iter 350/781: Loss 2.8176\n",
      "  Iter 360/781: Loss 3.1034\n",
      "  Iter 370/781: Loss 2.7500\n",
      "  Iter 380/781: Loss 2.6087\n",
      "  Iter 390/781: Loss 3.0066\n",
      "  Iter 400/781: Loss 3.0922\n",
      "  Iter 410/781: Loss 2.9611\n",
      "  Iter 420/781: Loss 2.7232\n",
      "  Iter 430/781: Loss 3.0393\n",
      "  Iter 440/781: Loss 2.8024\n",
      "  Iter 450/781: Loss 2.8085\n",
      "  Iter 460/781: Loss 2.7663\n",
      "  Iter 470/781: Loss 3.2445\n",
      "  Iter 480/781: Loss 2.9581\n",
      "  Iter 490/781: Loss 2.8777\n",
      "  Iter 500/781: Loss 2.6729\n",
      "  Iter 510/781: Loss 2.8890\n",
      "  Iter 520/781: Loss 2.6355\n",
      "  Iter 530/781: Loss 2.8066\n",
      "  Iter 540/781: Loss 2.6192\n",
      "  Iter 550/781: Loss 2.5474\n",
      "  Iter 560/781: Loss 2.6575\n",
      "  Iter 570/781: Loss 2.8391\n",
      "  Iter 580/781: Loss 2.7352\n",
      "  Iter 590/781: Loss 2.8275\n",
      "  Iter 600/781: Loss 2.4205\n",
      "  Iter 610/781: Loss 2.4621\n",
      "  Iter 620/781: Loss 2.9544\n",
      "  Iter 630/781: Loss 2.5084\n",
      "  Iter 640/781: Loss 2.5633\n",
      "  Iter 650/781: Loss 2.7743\n",
      "  Iter 660/781: Loss 2.7846\n",
      "  Iter 670/781: Loss 2.6387\n",
      "  Iter 680/781: Loss 2.8841\n",
      "  Iter 690/781: Loss 2.7772\n",
      "  Iter 700/781: Loss 2.6566\n",
      "  Iter 710/781: Loss 2.8041\n",
      "  Iter 720/781: Loss 2.6168\n",
      "  Iter 730/781: Loss 2.7239\n",
      "  Iter 740/781: Loss 2.5975\n",
      "  Iter 750/781: Loss 2.8780\n",
      "  Iter 760/781: Loss 2.9121\n",
      "  Iter 770/781: Loss 2.5585\n",
      "  Iter 780/781: Loss 2.6324\n",
      "Train acc: 0.3310, Val loss: 2.4539, Val acc: 0.0076 (Time: 3265.40s)\n",
      "\n",
      "[Epoch 7]\n",
      "  Iter   0/781: Loss 2.4083\n",
      "  Iter  10/781: Loss 2.7486\n",
      "  Iter  20/781: Loss 2.5530\n",
      "  Iter  30/781: Loss 2.7203\n",
      "  Iter  40/781: Loss 2.4336\n",
      "  Iter  50/781: Loss 2.5752\n",
      "  Iter  60/781: Loss 2.7382\n",
      "  Iter  70/781: Loss 3.0062\n",
      "  Iter  80/781: Loss 2.8595\n",
      "  Iter  90/781: Loss 2.6177\n",
      "  Iter 100/781: Loss 2.7403\n",
      "  Iter 110/781: Loss 2.6243\n",
      "  Iter 120/781: Loss 2.6712\n",
      "  Iter 130/781: Loss 2.7290\n",
      "  Iter 140/781: Loss 2.5990\n",
      "  Iter 150/781: Loss 2.6964\n",
      "  Iter 160/781: Loss 2.6178\n",
      "  Iter 170/781: Loss 2.5719\n",
      "  Iter 180/781: Loss 2.1901\n",
      "  Iter 190/781: Loss 2.3620\n",
      "  Iter 200/781: Loss 2.9460\n",
      "  Iter 210/781: Loss 2.7207\n",
      "  Iter 220/781: Loss 2.7985\n",
      "  Iter 230/781: Loss 2.7447\n",
      "  Iter 240/781: Loss 2.7551\n",
      "  Iter 250/781: Loss 2.4712\n",
      "  Iter 260/781: Loss 2.2235\n",
      "  Iter 270/781: Loss 2.6939\n",
      "  Iter 280/781: Loss 2.6767\n",
      "  Iter 290/781: Loss 2.7463\n",
      "  Iter 300/781: Loss 3.1041\n",
      "  Iter 310/781: Loss 2.6288\n",
      "  Iter 320/781: Loss 2.5580\n",
      "  Iter 330/781: Loss 2.7650\n",
      "  Iter 340/781: Loss 2.7981\n",
      "  Iter 350/781: Loss 3.0258\n",
      "  Iter 360/781: Loss 2.2706\n",
      "  Iter 370/781: Loss 2.7810\n",
      "  Iter 380/781: Loss 2.5875\n",
      "  Iter 390/781: Loss 2.4785\n",
      "  Iter 400/781: Loss 2.2535\n",
      "  Iter 410/781: Loss 2.3744\n",
      "  Iter 420/781: Loss 2.9017\n",
      "  Iter 430/781: Loss 2.5724\n",
      "  Iter 440/781: Loss 2.4649\n",
      "  Iter 450/781: Loss 2.7708\n",
      "  Iter 460/781: Loss 2.4169\n",
      "  Iter 470/781: Loss 2.4818\n",
      "  Iter 480/781: Loss 2.2946\n",
      "  Iter 490/781: Loss 2.5264\n",
      "  Iter 500/781: Loss 2.8847\n",
      "  Iter 510/781: Loss 2.4300\n",
      "  Iter 520/781: Loss 2.6665\n",
      "  Iter 530/781: Loss 2.6068\n",
      "  Iter 540/781: Loss 2.8380\n",
      "  Iter 550/781: Loss 2.9201\n",
      "  Iter 560/781: Loss 2.6706\n",
      "  Iter 570/781: Loss 2.8155\n",
      "  Iter 580/781: Loss 2.8945\n",
      "  Iter 590/781: Loss 2.4133\n",
      "  Iter 600/781: Loss 2.5618\n",
      "  Iter 610/781: Loss 2.4007\n",
      "  Iter 620/781: Loss 2.3940\n",
      "  Iter 630/781: Loss 2.5471\n",
      "  Iter 640/781: Loss 2.6801\n",
      "  Iter 650/781: Loss 2.8866\n",
      "  Iter 660/781: Loss 2.3417\n",
      "  Iter 670/781: Loss 2.6814\n",
      "  Iter 680/781: Loss 2.7921\n",
      "  Iter 690/781: Loss 2.6153\n",
      "  Iter 700/781: Loss 2.3162\n",
      "  Iter 710/781: Loss 2.8590\n",
      "  Iter 720/781: Loss 2.5880\n",
      "  Iter 730/781: Loss 2.5383\n",
      "  Iter 740/781: Loss 2.8192\n",
      "  Iter 750/781: Loss 2.4511\n",
      "  Iter 760/781: Loss 2.4626\n",
      "  Iter 770/781: Loss 2.4614\n",
      "  Iter 780/781: Loss 2.6446\n",
      "Train acc: 0.3790, Val loss: 2.3171, Val acc: 0.0116 (Time: 3277.39s)\n",
      "\n",
      "[Epoch 8]\n",
      "  Iter   0/781: Loss 2.4599\n",
      "  Iter  10/781: Loss 2.5538\n",
      "  Iter  20/781: Loss 2.3782\n",
      "  Iter  30/781: Loss 2.4785\n",
      "  Iter  40/781: Loss 2.4520\n",
      "  Iter  50/781: Loss 2.3093\n",
      "  Iter  60/781: Loss 2.3973\n",
      "  Iter  70/781: Loss 2.6163\n",
      "  Iter  80/781: Loss 2.4276\n",
      "  Iter  90/781: Loss 2.6629\n",
      "  Iter 100/781: Loss 2.2152\n",
      "  Iter 110/781: Loss 2.5556\n",
      "  Iter 120/781: Loss 2.5745\n",
      "  Iter 130/781: Loss 2.6129\n",
      "  Iter 140/781: Loss 2.5397\n",
      "  Iter 150/781: Loss 2.5853\n",
      "  Iter 160/781: Loss 2.4656\n",
      "  Iter 170/781: Loss 2.3673\n",
      "  Iter 180/781: Loss 2.2333\n",
      "  Iter 190/781: Loss 2.5117\n",
      "  Iter 200/781: Loss 2.4955\n",
      "  Iter 210/781: Loss 2.6910\n",
      "  Iter 220/781: Loss 2.5837\n",
      "  Iter 230/781: Loss 2.5634\n",
      "  Iter 240/781: Loss 2.8240\n",
      "  Iter 250/781: Loss 2.3341\n",
      "  Iter 260/781: Loss 2.1416\n",
      "  Iter 270/781: Loss 2.6423\n",
      "  Iter 280/781: Loss 2.5806\n",
      "  Iter 290/781: Loss 2.4523\n",
      "  Iter 300/781: Loss 2.3967\n",
      "  Iter 310/781: Loss 2.4588\n",
      "  Iter 320/781: Loss 2.2809\n",
      "  Iter 330/781: Loss 2.5414\n",
      "  Iter 340/781: Loss 2.3133\n",
      "  Iter 350/781: Loss 2.4462\n",
      "  Iter 360/781: Loss 2.4132\n",
      "  Iter 370/781: Loss 2.6465\n",
      "  Iter 380/781: Loss 2.1904\n",
      "  Iter 390/781: Loss 2.4090\n",
      "  Iter 400/781: Loss 2.6151\n",
      "  Iter 410/781: Loss 2.2098\n",
      "  Iter 420/781: Loss 2.6879\n",
      "  Iter 430/781: Loss 2.6386\n",
      "  Iter 440/781: Loss 2.3756\n",
      "  Iter 450/781: Loss 2.5371\n",
      "  Iter 460/781: Loss 2.4400\n",
      "  Iter 470/781: Loss 2.3773\n",
      "  Iter 480/781: Loss 2.3242\n",
      "  Iter 490/781: Loss 2.0261\n",
      "  Iter 500/781: Loss 2.4479\n",
      "  Iter 510/781: Loss 2.1050\n",
      "  Iter 520/781: Loss 2.2400\n",
      "  Iter 530/781: Loss 2.6830\n",
      "  Iter 540/781: Loss 2.1039\n",
      "  Iter 550/781: Loss 2.3460\n",
      "  Iter 560/781: Loss 2.5538\n",
      "  Iter 570/781: Loss 2.2963\n",
      "  Iter 580/781: Loss 2.4477\n",
      "  Iter 590/781: Loss 2.3431\n",
      "  Iter 600/781: Loss 2.3153\n",
      "  Iter 610/781: Loss 2.6142\n",
      "  Iter 620/781: Loss 2.2923\n",
      "  Iter 630/781: Loss 2.6342\n",
      "  Iter 640/781: Loss 2.3732\n",
      "  Iter 650/781: Loss 2.2549\n",
      "  Iter 660/781: Loss 2.5357\n",
      "  Iter 670/781: Loss 2.4310\n",
      "  Iter 680/781: Loss 2.4107\n",
      "  Iter 690/781: Loss 2.4540\n",
      "  Iter 700/781: Loss 2.2336\n",
      "  Iter 710/781: Loss 2.3362\n",
      "  Iter 720/781: Loss 2.3184\n",
      "  Iter 730/781: Loss 2.2585\n",
      "  Iter 740/781: Loss 2.4223\n",
      "  Iter 750/781: Loss 2.5114\n",
      "  Iter 760/781: Loss 2.2546\n",
      "  Iter 770/781: Loss 2.6465\n",
      "  Iter 780/781: Loss 2.4829\n",
      "Train acc: 0.4190, Val loss: 2.1149, Val acc: 0.0120 (Time: 3306.55s)\n",
      "\n",
      "[Epoch 9]\n",
      "  Iter   0/781: Loss 2.0571\n",
      "  Iter  10/781: Loss 2.2655\n",
      "  Iter  20/781: Loss 2.3331\n",
      "  Iter  30/781: Loss 2.7775\n",
      "  Iter  40/781: Loss 2.3866\n",
      "  Iter  50/781: Loss 2.5853\n",
      "  Iter  60/781: Loss 2.2752\n",
      "  Iter  70/781: Loss 2.2573\n",
      "  Iter  80/781: Loss 2.2988\n",
      "  Iter  90/781: Loss 2.4447\n",
      "  Iter 100/781: Loss 2.3639\n",
      "  Iter 110/781: Loss 2.1953\n",
      "  Iter 120/781: Loss 2.2918\n",
      "  Iter 130/781: Loss 2.3325\n",
      "  Iter 140/781: Loss 2.3413\n",
      "  Iter 150/781: Loss 2.6463\n",
      "  Iter 160/781: Loss 1.8566\n",
      "  Iter 170/781: Loss 2.4404\n",
      "  Iter 180/781: Loss 2.1744\n",
      "  Iter 190/781: Loss 2.2378\n",
      "  Iter 200/781: Loss 2.4388\n",
      "  Iter 210/781: Loss 2.2845\n",
      "  Iter 220/781: Loss 2.2078\n",
      "  Iter 230/781: Loss 2.4848\n",
      "  Iter 240/781: Loss 2.3033\n",
      "  Iter 250/781: Loss 2.2001\n",
      "  Iter 260/781: Loss 2.2806\n",
      "  Iter 270/781: Loss 2.1367\n",
      "  Iter 280/781: Loss 2.2538\n",
      "  Iter 290/781: Loss 2.5516\n",
      "  Iter 300/781: Loss 2.2501\n",
      "  Iter 310/781: Loss 2.3336\n",
      "  Iter 320/781: Loss 1.9916\n",
      "  Iter 330/781: Loss 2.1767\n",
      "  Iter 340/781: Loss 2.3608\n",
      "  Iter 350/781: Loss 2.2781\n",
      "  Iter 360/781: Loss 1.9542\n",
      "  Iter 370/781: Loss 2.0887\n",
      "  Iter 380/781: Loss 2.2725\n",
      "  Iter 390/781: Loss 1.8522\n",
      "  Iter 400/781: Loss 2.0986\n",
      "  Iter 410/781: Loss 2.0523\n",
      "  Iter 420/781: Loss 2.2702\n",
      "  Iter 430/781: Loss 2.0521\n",
      "  Iter 440/781: Loss 2.3356\n",
      "  Iter 450/781: Loss 2.1855\n",
      "  Iter 460/781: Loss 2.5237\n",
      "  Iter 470/781: Loss 2.0263\n",
      "  Iter 480/781: Loss 2.1534\n",
      "  Iter 490/781: Loss 1.6028\n",
      "  Iter 500/781: Loss 2.1514\n",
      "  Iter 510/781: Loss 2.2687\n",
      "  Iter 520/781: Loss 2.4570\n",
      "  Iter 530/781: Loss 2.2032\n",
      "  Iter 540/781: Loss 2.0605\n",
      "  Iter 550/781: Loss 2.4379\n",
      "  Iter 560/781: Loss 2.1439\n",
      "  Iter 570/781: Loss 2.0250\n",
      "  Iter 580/781: Loss 2.2285\n",
      "  Iter 590/781: Loss 2.0038\n",
      "  Iter 600/781: Loss 2.4611\n",
      "  Iter 610/781: Loss 2.5139\n",
      "  Iter 620/781: Loss 2.5210\n",
      "  Iter 630/781: Loss 2.2434\n",
      "  Iter 640/781: Loss 2.2953\n",
      "  Iter 650/781: Loss 2.0604\n",
      "  Iter 660/781: Loss 2.1623\n",
      "  Iter 670/781: Loss 2.3442\n",
      "  Iter 680/781: Loss 2.3756\n",
      "  Iter 690/781: Loss 2.1561\n",
      "  Iter 700/781: Loss 2.1834\n",
      "  Iter 710/781: Loss 2.2124\n",
      "  Iter 720/781: Loss 2.1946\n",
      "  Iter 730/781: Loss 2.4471\n",
      "  Iter 740/781: Loss 2.0933\n",
      "  Iter 750/781: Loss 2.1594\n",
      "  Iter 760/781: Loss 2.0805\n",
      "  Iter 770/781: Loss 2.3580\n",
      "  Iter 780/781: Loss 2.1496\n",
      "Train acc: 0.4510, Val loss: 2.0139, Val acc: 0.0126 (Time: 3258.31s)\n",
      "\n",
      "[Epoch 10]\n",
      "  Iter   0/781: Loss 2.3487\n",
      "  Iter  10/781: Loss 2.2810\n",
      "  Iter  20/781: Loss 2.5524\n",
      "  Iter  30/781: Loss 2.0323\n",
      "  Iter  40/781: Loss 2.0715\n",
      "  Iter  50/781: Loss 2.0907\n",
      "  Iter  60/781: Loss 2.0901\n",
      "  Iter  70/781: Loss 2.1874\n",
      "  Iter  80/781: Loss 1.9809\n",
      "  Iter  90/781: Loss 2.3854\n",
      "  Iter 100/781: Loss 2.2555\n",
      "  Iter 110/781: Loss 2.2625\n",
      "  Iter 120/781: Loss 1.7407\n",
      "  Iter 130/781: Loss 1.8029\n",
      "  Iter 140/781: Loss 1.8772\n",
      "  Iter 150/781: Loss 1.9287\n",
      "  Iter 160/781: Loss 2.0668\n",
      "  Iter 170/781: Loss 2.2040\n",
      "  Iter 180/781: Loss 2.2475\n",
      "  Iter 190/781: Loss 2.0138\n",
      "  Iter 200/781: Loss 2.2665\n",
      "  Iter 210/781: Loss 2.1938\n",
      "  Iter 220/781: Loss 1.9980\n",
      "  Iter 230/781: Loss 2.1897\n",
      "  Iter 240/781: Loss 2.5516\n",
      "  Iter 250/781: Loss 1.8429\n",
      "  Iter 260/781: Loss 2.5787\n",
      "  Iter 270/781: Loss 2.2941\n",
      "  Iter 280/781: Loss 2.1183\n",
      "  Iter 290/781: Loss 2.5154\n",
      "  Iter 300/781: Loss 2.1705\n",
      "  Iter 310/781: Loss 2.1559\n",
      "  Iter 320/781: Loss 2.2534\n",
      "  Iter 330/781: Loss 2.0643\n",
      "  Iter 340/781: Loss 2.2035\n",
      "  Iter 350/781: Loss 2.3667\n",
      "  Iter 360/781: Loss 1.7891\n",
      "  Iter 370/781: Loss 2.2049\n",
      "  Iter 380/781: Loss 2.1735\n",
      "  Iter 390/781: Loss 2.0146\n",
      "  Iter 400/781: Loss 2.2077\n",
      "  Iter 410/781: Loss 1.9932\n",
      "  Iter 420/781: Loss 2.3726\n",
      "  Iter 430/781: Loss 2.2079\n",
      "  Iter 440/781: Loss 2.2265\n",
      "  Iter 450/781: Loss 1.9892\n",
      "  Iter 460/781: Loss 2.3396\n",
      "  Iter 470/781: Loss 2.2190\n",
      "  Iter 480/781: Loss 2.2642\n",
      "  Iter 490/781: Loss 2.0021\n",
      "  Iter 500/781: Loss 1.7694\n",
      "  Iter 510/781: Loss 2.3640\n",
      "  Iter 520/781: Loss 2.2195\n",
      "  Iter 530/781: Loss 2.3583\n",
      "  Iter 540/781: Loss 2.3161\n",
      "  Iter 550/781: Loss 2.0721\n",
      "  Iter 560/781: Loss 2.1778\n",
      "  Iter 570/781: Loss 2.5273\n",
      "  Iter 580/781: Loss 2.2797\n",
      "  Iter 590/781: Loss 2.2600\n",
      "  Iter 600/781: Loss 1.9673\n",
      "  Iter 610/781: Loss 2.2260\n",
      "  Iter 620/781: Loss 2.3329\n",
      "  Iter 630/781: Loss 2.2429\n",
      "  Iter 640/781: Loss 2.1191\n",
      "  Iter 650/781: Loss 2.0923\n",
      "  Iter 660/781: Loss 1.9355\n",
      "  Iter 670/781: Loss 2.2038\n",
      "  Iter 680/781: Loss 2.4814\n",
      "  Iter 690/781: Loss 2.2850\n",
      "  Iter 700/781: Loss 2.1855\n",
      "  Iter 710/781: Loss 2.2365\n",
      "  Iter 720/781: Loss 2.0567\n",
      "  Iter 730/781: Loss 2.1158\n",
      "  Iter 740/781: Loss 2.3879\n",
      "  Iter 750/781: Loss 2.2507\n",
      "  Iter 760/781: Loss 1.9124\n",
      "  Iter 770/781: Loss 1.9687\n",
      "  Iter 780/781: Loss 2.2162\n",
      "Train acc: 0.4950, Val loss: 1.8895, Val acc: 0.0138 (Time: 3298.30s)\n",
      "\n",
      ">>> Saved model to ResNet20_cfg1_smooth0.05_epoch_10.pkl\n",
      "Log saved to log_cfg1_smooth0.05.npz\n",
      "[Epoch 1]\n",
      "  Iter   0/781: Loss 5.5987\n",
      "  Iter  10/781: Loss 4.5183\n",
      "  Iter  20/781: Loss 4.6474\n",
      "  Iter  30/781: Loss 4.5904\n",
      "  Iter  40/781: Loss 4.6336\n",
      "  Iter  50/781: Loss 4.3743\n",
      "  Iter  60/781: Loss 4.3658\n",
      "  Iter  70/781: Loss 4.2902\n",
      "  Iter  80/781: Loss 4.4558\n",
      "  Iter  90/781: Loss 4.3810\n",
      "  Iter 100/781: Loss 4.1482\n",
      "  Iter 110/781: Loss 4.2379\n",
      "  Iter 120/781: Loss 4.2243\n",
      "  Iter 130/781: Loss 4.0759\n",
      "  Iter 140/781: Loss 4.3164\n",
      "  Iter 150/781: Loss 4.3247\n",
      "  Iter 160/781: Loss 4.1741\n",
      "  Iter 170/781: Loss 3.8792\n",
      "  Iter 180/781: Loss 4.0296\n",
      "  Iter 190/781: Loss 4.1658\n",
      "  Iter 200/781: Loss 4.0937\n",
      "  Iter 210/781: Loss 4.1229\n",
      "  Iter 220/781: Loss 4.1499\n",
      "  Iter 230/781: Loss 4.0887\n",
      "  Iter 240/781: Loss 3.8696\n",
      "  Iter 250/781: Loss 4.3163\n",
      "  Iter 260/781: Loss 3.9910\n",
      "  Iter 270/781: Loss 4.1068\n",
      "  Iter 280/781: Loss 3.9396\n",
      "  Iter 290/781: Loss 3.8800\n",
      "  Iter 300/781: Loss 4.0797\n",
      "  Iter 310/781: Loss 4.0047\n",
      "  Iter 320/781: Loss 3.8987\n",
      "  Iter 330/781: Loss 3.7563\n",
      "  Iter 340/781: Loss 4.0351\n",
      "  Iter 350/781: Loss 4.0827\n",
      "  Iter 360/781: Loss 3.7448\n",
      "  Iter 370/781: Loss 3.8537\n",
      "  Iter 380/781: Loss 4.0295\n",
      "  Iter 390/781: Loss 4.0609\n",
      "  Iter 400/781: Loss 3.9399\n",
      "  Iter 410/781: Loss 3.9508\n",
      "  Iter 420/781: Loss 4.0791\n",
      "  Iter 430/781: Loss 3.7905\n",
      "  Iter 440/781: Loss 3.8438\n",
      "  Iter 450/781: Loss 4.0015\n",
      "  Iter 460/781: Loss 3.7432\n",
      "  Iter 470/781: Loss 3.8626\n",
      "  Iter 480/781: Loss 3.9617\n",
      "  Iter 490/781: Loss 3.8572\n",
      "  Iter 500/781: Loss 3.8419\n",
      "  Iter 510/781: Loss 3.8085\n",
      "  Iter 520/781: Loss 3.7059\n",
      "  Iter 530/781: Loss 3.7829\n",
      "  Iter 540/781: Loss 3.6862\n",
      "  Iter 550/781: Loss 3.9759\n",
      "  Iter 560/781: Loss 3.6882\n",
      "  Iter 570/781: Loss 3.7326\n",
      "  Iter 580/781: Loss 3.6570\n",
      "  Iter 590/781: Loss 3.5814\n",
      "  Iter 600/781: Loss 3.8649\n",
      "  Iter 610/781: Loss 3.8855\n",
      "  Iter 620/781: Loss 3.7771\n",
      "  Iter 630/781: Loss 3.7384\n",
      "  Iter 640/781: Loss 3.7626\n",
      "  Iter 650/781: Loss 3.5583\n",
      "  Iter 660/781: Loss 3.5196\n",
      "  Iter 670/781: Loss 3.9033\n",
      "  Iter 680/781: Loss 3.6165\n",
      "  Iter 690/781: Loss 3.9870\n",
      "  Iter 700/781: Loss 3.8054\n",
      "  Iter 710/781: Loss 3.8084\n",
      "  Iter 720/781: Loss 3.5686\n",
      "  Iter 730/781: Loss 3.5970\n",
      "  Iter 740/781: Loss 3.9038\n",
      "  Iter 750/781: Loss 3.7836\n",
      "  Iter 760/781: Loss 4.0162\n",
      "  Iter 770/781: Loss 3.6209\n",
      "  Iter 780/781: Loss 3.5120\n",
      "Train acc: 0.1520, Val loss: 3.5457, Val acc: 0.0084 (Time: 3283.03s)\n",
      "\n",
      "[Epoch 2]\n",
      "  Iter   0/781: Loss 3.6922\n",
      "  Iter  10/781: Loss 3.3849\n",
      "  Iter  20/781: Loss 3.7785\n",
      "  Iter  30/781: Loss 3.5644\n",
      "  Iter  40/781: Loss 4.0838\n",
      "  Iter  50/781: Loss 3.7024\n",
      "  Iter  60/781: Loss 4.0412\n",
      "  Iter  70/781: Loss 3.5422\n",
      "  Iter  80/781: Loss 3.7952\n",
      "  Iter  90/781: Loss 3.9864\n",
      "  Iter 100/781: Loss 3.6707\n",
      "  Iter 110/781: Loss 3.7659\n",
      "  Iter 120/781: Loss 3.8904\n",
      "  Iter 130/781: Loss 3.5822\n",
      "  Iter 140/781: Loss 3.6021\n",
      "  Iter 150/781: Loss 3.7890\n",
      "  Iter 160/781: Loss 3.8984\n",
      "  Iter 170/781: Loss 3.5395\n",
      "  Iter 180/781: Loss 3.7243\n",
      "  Iter 190/781: Loss 3.5053\n",
      "  Iter 200/781: Loss 3.8267\n",
      "  Iter 210/781: Loss 3.3808\n",
      "  Iter 220/781: Loss 3.7178\n",
      "  Iter 230/781: Loss 3.8178\n",
      "  Iter 240/781: Loss 3.4831\n",
      "  Iter 250/781: Loss 3.5987\n",
      "  Iter 260/781: Loss 3.7720\n",
      "  Iter 270/781: Loss 3.6299\n",
      "  Iter 280/781: Loss 3.7833\n",
      "  Iter 290/781: Loss 3.5833\n",
      "  Iter 300/781: Loss 3.5836\n",
      "  Iter 310/781: Loss 3.3609\n",
      "  Iter 320/781: Loss 3.3130\n",
      "  Iter 330/781: Loss 3.3411\n",
      "  Iter 340/781: Loss 3.7949\n",
      "  Iter 350/781: Loss 3.6210\n",
      "  Iter 360/781: Loss 3.3269\n",
      "  Iter 370/781: Loss 3.6049\n",
      "  Iter 380/781: Loss 3.7362\n",
      "  Iter 390/781: Loss 3.3765\n",
      "  Iter 400/781: Loss 3.3397\n",
      "  Iter 410/781: Loss 3.4336\n",
      "  Iter 420/781: Loss 3.4213\n",
      "  Iter 430/781: Loss 3.4306\n",
      "  Iter 440/781: Loss 3.4445\n",
      "  Iter 450/781: Loss 3.7236\n",
      "  Iter 460/781: Loss 3.4635\n",
      "  Iter 470/781: Loss 3.5162\n",
      "  Iter 480/781: Loss 3.3499\n",
      "  Iter 490/781: Loss 3.3442\n",
      "  Iter 500/781: Loss 3.3424\n",
      "  Iter 510/781: Loss 3.4234\n",
      "  Iter 520/781: Loss 3.2619\n",
      "  Iter 530/781: Loss 3.2898\n",
      "  Iter 540/781: Loss 3.8122\n",
      "  Iter 550/781: Loss 3.3242\n",
      "  Iter 560/781: Loss 3.1474\n",
      "  Iter 570/781: Loss 3.2685\n",
      "  Iter 580/781: Loss 3.2209\n",
      "  Iter 590/781: Loss 3.3688\n",
      "  Iter 600/781: Loss 3.6229\n",
      "  Iter 610/781: Loss 3.4591\n",
      "  Iter 620/781: Loss 3.4590\n",
      "  Iter 630/781: Loss 3.5733\n",
      "  Iter 640/781: Loss 3.5412\n",
      "  Iter 650/781: Loss 3.1998\n",
      "  Iter 660/781: Loss 3.3203\n",
      "  Iter 670/781: Loss 3.7467\n",
      "  Iter 680/781: Loss 3.3760\n",
      "  Iter 690/781: Loss 3.5519\n",
      "  Iter 700/781: Loss 3.1932\n",
      "  Iter 710/781: Loss 3.4765\n",
      "  Iter 720/781: Loss 3.2637\n",
      "  Iter 730/781: Loss 3.2280\n",
      "  Iter 740/781: Loss 3.2701\n",
      "  Iter 750/781: Loss 3.3384\n",
      "  Iter 760/781: Loss 3.6290\n",
      "  Iter 770/781: Loss 3.0475\n",
      "  Iter 780/781: Loss 3.1472\n",
      "Train acc: 0.2050, Val loss: 3.1452, Val acc: 0.0112 (Time: 3241.76s)\n",
      "\n",
      "[Epoch 3]\n",
      "  Iter   0/781: Loss 3.1072\n",
      "  Iter  10/781: Loss 3.5625\n",
      "  Iter  20/781: Loss 3.4247\n",
      "  Iter  30/781: Loss 3.6530\n",
      "  Iter  40/781: Loss 3.6911\n",
      "  Iter  50/781: Loss 3.4088\n",
      "  Iter  60/781: Loss 3.6025\n",
      "  Iter  70/781: Loss 3.7409\n",
      "  Iter  80/781: Loss 3.5129\n",
      "  Iter  90/781: Loss 3.5037\n",
      "  Iter 100/781: Loss 3.3935\n",
      "  Iter 110/781: Loss 3.3916\n",
      "  Iter 120/781: Loss 3.3225\n",
      "  Iter 130/781: Loss 3.2979\n",
      "  Iter 140/781: Loss 3.5320\n",
      "  Iter 150/781: Loss 3.4964\n",
      "  Iter 160/781: Loss 3.4405\n",
      "  Iter 170/781: Loss 3.1964\n",
      "  Iter 180/781: Loss 3.4692\n",
      "  Iter 190/781: Loss 3.4189\n",
      "  Iter 200/781: Loss 3.4706\n",
      "  Iter 210/781: Loss 3.3978\n",
      "  Iter 220/781: Loss 3.3898\n",
      "  Iter 230/781: Loss 3.3389\n",
      "  Iter 240/781: Loss 3.1923\n",
      "  Iter 250/781: Loss 3.3318\n",
      "  Iter 260/781: Loss 3.4186\n",
      "  Iter 270/781: Loss 3.4628\n",
      "  Iter 280/781: Loss 3.0570\n",
      "  Iter 290/781: Loss 3.1064\n",
      "  Iter 300/781: Loss 3.4320\n",
      "  Iter 310/781: Loss 3.2587\n",
      "  Iter 320/781: Loss 3.1448\n",
      "  Iter 330/781: Loss 3.1915\n",
      "  Iter 340/781: Loss 3.3282\n",
      "  Iter 350/781: Loss 3.3822\n",
      "  Iter 360/781: Loss 3.0693\n",
      "  Iter 370/781: Loss 3.3895\n",
      "  Iter 380/781: Loss 3.2262\n",
      "  Iter 390/781: Loss 3.2854\n",
      "  Iter 400/781: Loss 3.2141\n",
      "  Iter 410/781: Loss 2.9650\n",
      "  Iter 420/781: Loss 3.3179\n",
      "  Iter 430/781: Loss 3.4072\n",
      "  Iter 440/781: Loss 3.3105\n",
      "  Iter 450/781: Loss 3.4041\n",
      "  Iter 460/781: Loss 3.3875\n",
      "  Iter 470/781: Loss 3.2678\n",
      "  Iter 480/781: Loss 3.1718\n",
      "  Iter 490/781: Loss 3.1425\n",
      "  Iter 500/781: Loss 3.2443\n",
      "  Iter 510/781: Loss 3.4358\n",
      "  Iter 520/781: Loss 3.0148\n",
      "  Iter 530/781: Loss 3.2321\n",
      "  Iter 540/781: Loss 3.3795\n",
      "  Iter 550/781: Loss 3.4028\n",
      "  Iter 560/781: Loss 3.2625\n",
      "  Iter 570/781: Loss 2.9866\n",
      "  Iter 580/781: Loss 2.9304\n",
      "  Iter 590/781: Loss 2.9366\n",
      "  Iter 600/781: Loss 3.3588\n",
      "  Iter 610/781: Loss 3.1646\n",
      "  Iter 620/781: Loss 2.9664\n",
      "  Iter 630/781: Loss 2.8223\n",
      "  Iter 640/781: Loss 3.1125\n",
      "  Iter 650/781: Loss 3.1377\n",
      "  Iter 660/781: Loss 2.8295\n",
      "  Iter 670/781: Loss 3.2334\n",
      "  Iter 680/781: Loss 3.1098\n",
      "  Iter 690/781: Loss 3.1123\n",
      "  Iter 700/781: Loss 3.2834\n",
      "  Iter 710/781: Loss 3.5277\n",
      "  Iter 720/781: Loss 2.9554\n",
      "  Iter 730/781: Loss 2.8863\n",
      "  Iter 740/781: Loss 2.9251\n",
      "  Iter 750/781: Loss 3.1584\n",
      "  Iter 760/781: Loss 2.9907\n",
      "  Iter 770/781: Loss 2.9053\n",
      "  Iter 780/781: Loss 3.1113\n",
      "Train acc: 0.2530, Val loss: 2.9537, Val acc: 0.0112 (Time: 3248.01s)\n",
      "\n",
      "[Epoch 4]\n",
      "  Iter   0/781: Loss 3.2426\n",
      "  Iter  10/781: Loss 2.8052\n",
      "  Iter  20/781: Loss 3.4336\n",
      "  Iter  30/781: Loss 3.3632\n",
      "  Iter  40/781: Loss 3.2718\n",
      "  Iter  50/781: Loss 3.1998\n",
      "  Iter  60/781: Loss 2.9448\n",
      "  Iter  70/781: Loss 3.2531\n",
      "  Iter  80/781: Loss 3.2386\n",
      "  Iter  90/781: Loss 3.7135\n",
      "  Iter 100/781: Loss 3.2951\n",
      "  Iter 110/781: Loss 3.1083\n",
      "  Iter 120/781: Loss 3.2191\n",
      "  Iter 130/781: Loss 3.2544\n",
      "  Iter 140/781: Loss 3.5282\n",
      "  Iter 150/781: Loss 2.9561\n",
      "  Iter 160/781: Loss 3.0824\n",
      "  Iter 170/781: Loss 3.3744\n",
      "  Iter 180/781: Loss 3.0210\n",
      "  Iter 190/781: Loss 3.6028\n",
      "  Iter 200/781: Loss 3.1366\n",
      "  Iter 210/781: Loss 2.9382\n",
      "  Iter 220/781: Loss 3.1511\n",
      "  Iter 230/781: Loss 3.2510\n",
      "  Iter 240/781: Loss 2.9405\n",
      "  Iter 250/781: Loss 2.9726\n",
      "  Iter 260/781: Loss 3.1950\n",
      "  Iter 270/781: Loss 3.1316\n",
      "  Iter 280/781: Loss 3.0883\n",
      "  Iter 290/781: Loss 2.7928\n",
      "  Iter 300/781: Loss 3.3964\n",
      "  Iter 310/781: Loss 3.0686\n",
      "  Iter 320/781: Loss 3.1876\n",
      "  Iter 330/781: Loss 3.2279\n",
      "  Iter 340/781: Loss 3.0178\n",
      "  Iter 350/781: Loss 3.1244\n",
      "  Iter 360/781: Loss 3.0554\n",
      "  Iter 370/781: Loss 2.9676\n",
      "  Iter 380/781: Loss 2.9165\n",
      "  Iter 390/781: Loss 2.7073\n",
      "  Iter 400/781: Loss 2.9192\n",
      "  Iter 410/781: Loss 3.2165\n",
      "  Iter 420/781: Loss 3.1266\n",
      "  Iter 430/781: Loss 2.9669\n",
      "  Iter 440/781: Loss 3.3775\n",
      "  Iter 450/781: Loss 3.1654\n",
      "  Iter 460/781: Loss 3.0037\n",
      "  Iter 470/781: Loss 3.1760\n",
      "  Iter 480/781: Loss 3.3205\n",
      "  Iter 490/781: Loss 3.1802\n",
      "  Iter 500/781: Loss 3.2551\n",
      "  Iter 510/781: Loss 2.8559\n",
      "  Iter 520/781: Loss 2.7962\n",
      "  Iter 530/781: Loss 3.1549\n",
      "  Iter 540/781: Loss 2.9748\n",
      "  Iter 550/781: Loss 3.0027\n",
      "  Iter 560/781: Loss 3.0969\n",
      "  Iter 570/781: Loss 3.0494\n",
      "  Iter 580/781: Loss 3.3385\n",
      "  Iter 590/781: Loss 2.6260\n",
      "  Iter 600/781: Loss 2.8308\n",
      "  Iter 610/781: Loss 3.1805\n",
      "  Iter 620/781: Loss 3.0224\n",
      "  Iter 630/781: Loss 3.0141\n",
      "  Iter 640/781: Loss 2.8880\n",
      "  Iter 650/781: Loss 2.6940\n",
      "  Iter 660/781: Loss 2.9319\n",
      "  Iter 670/781: Loss 3.0339\n",
      "  Iter 680/781: Loss 2.8154\n",
      "  Iter 690/781: Loss 2.9073\n",
      "  Iter 700/781: Loss 3.2290\n",
      "  Iter 710/781: Loss 3.1018\n",
      "  Iter 720/781: Loss 2.9668\n",
      "  Iter 730/781: Loss 3.0657\n",
      "  Iter 740/781: Loss 3.1794\n",
      "  Iter 750/781: Loss 2.8951\n",
      "  Iter 760/781: Loss 2.8276\n",
      "  Iter 770/781: Loss 2.9466\n",
      "  Iter 780/781: Loss 3.3857\n",
      "Train acc: 0.2660, Val loss: 2.7558, Val acc: 0.0112 (Time: 3245.01s)\n",
      "\n",
      "[Epoch 5]\n",
      "  Iter   0/781: Loss 2.9287\n",
      "  Iter  10/781: Loss 3.0524\n",
      "  Iter  20/781: Loss 3.2963\n",
      "  Iter  30/781: Loss 3.1015\n",
      "  Iter  40/781: Loss 2.7526\n",
      "  Iter  50/781: Loss 2.9814\n",
      "  Iter  60/781: Loss 2.7858\n",
      "  Iter  70/781: Loss 2.9743\n",
      "  Iter  80/781: Loss 2.9823\n",
      "  Iter  90/781: Loss 3.1295\n",
      "  Iter 100/781: Loss 2.8608\n",
      "  Iter 110/781: Loss 3.0083\n",
      "  Iter 120/781: Loss 3.2397\n",
      "  Iter 130/781: Loss 2.9790\n",
      "  Iter 140/781: Loss 2.9739\n",
      "  Iter 150/781: Loss 3.0352\n",
      "  Iter 160/781: Loss 3.1890\n",
      "  Iter 170/781: Loss 3.0591\n",
      "  Iter 180/781: Loss 2.6222\n",
      "  Iter 190/781: Loss 2.8939\n",
      "  Iter 200/781: Loss 2.7406\n",
      "  Iter 210/781: Loss 2.7224\n",
      "  Iter 220/781: Loss 3.2509\n",
      "  Iter 230/781: Loss 3.3865\n",
      "  Iter 240/781: Loss 2.8676\n",
      "  Iter 250/781: Loss 2.7900\n",
      "  Iter 260/781: Loss 2.5125\n",
      "  Iter 270/781: Loss 3.0393\n",
      "  Iter 280/781: Loss 3.0826\n",
      "  Iter 290/781: Loss 2.8003\n",
      "  Iter 300/781: Loss 2.7128\n",
      "  Iter 310/781: Loss 2.7962\n",
      "  Iter 320/781: Loss 2.9884\n",
      "  Iter 330/781: Loss 2.8578\n",
      "  Iter 340/781: Loss 2.8837\n",
      "  Iter 350/781: Loss 3.1586\n",
      "  Iter 360/781: Loss 3.3445\n",
      "  Iter 370/781: Loss 3.0255\n",
      "  Iter 380/781: Loss 2.7442\n",
      "  Iter 390/781: Loss 3.0163\n",
      "  Iter 400/781: Loss 2.9336\n",
      "  Iter 410/781: Loss 2.7524\n",
      "  Iter 420/781: Loss 2.8056\n",
      "  Iter 430/781: Loss 2.8128\n",
      "  Iter 440/781: Loss 2.5093\n",
      "  Iter 450/781: Loss 2.6323\n",
      "  Iter 460/781: Loss 2.7216\n",
      "  Iter 470/781: Loss 3.0188\n",
      "  Iter 480/781: Loss 3.1894\n",
      "  Iter 490/781: Loss 2.8396\n",
      "  Iter 500/781: Loss 2.9389\n",
      "  Iter 510/781: Loss 2.7411\n",
      "  Iter 520/781: Loss 3.1527\n",
      "  Iter 530/781: Loss 2.7338\n",
      "  Iter 540/781: Loss 2.9222\n",
      "  Iter 550/781: Loss 2.9675\n",
      "  Iter 560/781: Loss 2.8223\n",
      "  Iter 570/781: Loss 3.1531\n",
      "  Iter 580/781: Loss 2.8191\n",
      "  Iter 590/781: Loss 3.0053\n",
      "  Iter 600/781: Loss 3.1484\n",
      "  Iter 610/781: Loss 2.9032\n",
      "  Iter 620/781: Loss 2.8432\n",
      "  Iter 630/781: Loss 3.0259\n",
      "  Iter 640/781: Loss 2.7857\n",
      "  Iter 650/781: Loss 2.7180\n",
      "  Iter 660/781: Loss 3.0282\n",
      "  Iter 670/781: Loss 2.8324\n",
      "  Iter 680/781: Loss 2.5758\n",
      "  Iter 690/781: Loss 2.7845\n",
      "  Iter 700/781: Loss 2.3205\n",
      "  Iter 710/781: Loss 2.7350\n",
      "  Iter 720/781: Loss 2.7912\n",
      "  Iter 730/781: Loss 2.9154\n",
      "  Iter 740/781: Loss 2.7116\n",
      "  Iter 750/781: Loss 2.9659\n",
      "  Iter 760/781: Loss 2.5791\n",
      "  Iter 770/781: Loss 2.9730\n",
      "  Iter 780/781: Loss 3.0236\n",
      "Train acc: 0.2630, Val loss: 2.6045, Val acc: 0.0112 (Time: 3251.16s)\n",
      "\n",
      "[Epoch 6]\n",
      "  Iter   0/781: Loss 2.7749\n",
      "  Iter  10/781: Loss 2.6480\n",
      "  Iter  20/781: Loss 2.6252\n",
      "  Iter  30/781: Loss 2.5397\n",
      "  Iter  40/781: Loss 2.6708\n",
      "  Iter  50/781: Loss 3.1129\n",
      "  Iter  60/781: Loss 2.7072\n",
      "  Iter  70/781: Loss 2.7959\n",
      "  Iter  80/781: Loss 3.1267\n",
      "  Iter  90/781: Loss 2.5441\n",
      "  Iter 100/781: Loss 2.5039\n",
      "  Iter 110/781: Loss 2.7170\n",
      "  Iter 120/781: Loss 2.7350\n",
      "  Iter 130/781: Loss 2.5590\n",
      "  Iter 140/781: Loss 2.4677\n",
      "  Iter 150/781: Loss 3.0259\n",
      "  Iter 160/781: Loss 3.0148\n",
      "  Iter 170/781: Loss 2.5724\n",
      "  Iter 180/781: Loss 2.5836\n",
      "  Iter 190/781: Loss 2.8138\n",
      "  Iter 200/781: Loss 2.4592\n",
      "  Iter 210/781: Loss 2.8536\n",
      "  Iter 220/781: Loss 2.9940\n",
      "  Iter 230/781: Loss 2.7253\n",
      "  Iter 240/781: Loss 2.9339\n",
      "  Iter 250/781: Loss 2.7214\n",
      "  Iter 260/781: Loss 2.8197\n",
      "  Iter 270/781: Loss 2.6512\n",
      "  Iter 280/781: Loss 2.5843\n",
      "  Iter 290/781: Loss 2.5823\n",
      "  Iter 300/781: Loss 2.8297\n",
      "  Iter 310/781: Loss 2.8533\n",
      "  Iter 320/781: Loss 2.9321\n",
      "  Iter 330/781: Loss 2.8355\n",
      "  Iter 340/781: Loss 2.6552\n",
      "  Iter 350/781: Loss 2.9306\n",
      "  Iter 360/781: Loss 2.6525\n",
      "  Iter 370/781: Loss 2.8294\n",
      "  Iter 380/781: Loss 2.7668\n",
      "  Iter 390/781: Loss 2.5683\n",
      "  Iter 400/781: Loss 2.7721\n",
      "  Iter 410/781: Loss 2.5791\n",
      "  Iter 420/781: Loss 2.5101\n",
      "  Iter 430/781: Loss 2.6684\n",
      "  Iter 440/781: Loss 2.6854\n",
      "  Iter 450/781: Loss 2.4727\n",
      "  Iter 460/781: Loss 2.9179\n",
      "  Iter 470/781: Loss 2.8772\n",
      "  Iter 480/781: Loss 2.7149\n",
      "  Iter 490/781: Loss 2.6929\n",
      "  Iter 500/781: Loss 2.6463\n",
      "  Iter 510/781: Loss 2.8544\n",
      "  Iter 520/781: Loss 2.6542\n",
      "  Iter 530/781: Loss 2.9149\n",
      "  Iter 540/781: Loss 2.6456\n",
      "  Iter 550/781: Loss 2.4573\n",
      "  Iter 560/781: Loss 2.8077\n",
      "  Iter 570/781: Loss 2.5081\n",
      "  Iter 580/781: Loss 2.8364\n",
      "  Iter 590/781: Loss 2.8042\n",
      "  Iter 600/781: Loss 2.6874\n",
      "  Iter 610/781: Loss 2.5578\n",
      "  Iter 620/781: Loss 2.4455\n",
      "  Iter 630/781: Loss 3.0858\n",
      "  Iter 640/781: Loss 2.5382\n",
      "  Iter 650/781: Loss 2.4077\n",
      "  Iter 660/781: Loss 2.6392\n",
      "  Iter 670/781: Loss 2.6712\n",
      "  Iter 680/781: Loss 2.4909\n",
      "  Iter 690/781: Loss 3.0868\n",
      "  Iter 700/781: Loss 2.3076\n",
      "  Iter 710/781: Loss 2.7202\n",
      "  Iter 720/781: Loss 2.4764\n",
      "  Iter 730/781: Loss 3.4469\n",
      "  Iter 740/781: Loss 2.6227\n",
      "  Iter 750/781: Loss 2.4856\n",
      "  Iter 760/781: Loss 2.8220\n",
      "  Iter 770/781: Loss 2.7326\n",
      "  Iter 780/781: Loss 2.9795\n",
      "Train acc: 0.3360, Val loss: 2.4506, Val acc: 0.0112 (Time: 3246.49s)\n",
      "\n",
      "[Epoch 7]\n",
      "  Iter   0/781: Loss 2.9313\n",
      "  Iter  10/781: Loss 3.0010\n",
      "  Iter  20/781: Loss 2.7341\n",
      "  Iter  30/781: Loss 2.5223\n",
      "  Iter  40/781: Loss 2.8247\n",
      "  Iter  50/781: Loss 2.7945\n",
      "  Iter  60/781: Loss 2.2933\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m             trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_cfg\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ema\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mema_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema_decay\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_warmup\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mema_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarmup_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m     run_experiments()\n",
      "Cell \u001b[1;32mIn[8], line 48\u001b[0m, in \u001b[0;36mrun_experiments\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m trainer\u001b[38;5;241m.\u001b[39mema_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.999\u001b[39m\n\u001b[0;32m     47\u001b[0m trainer\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m---> 48\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     49\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_cfg\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_smooth\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msmooth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_cfg\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_smooth\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msmooth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 88\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_per_epoch):\n\u001b[1;32m---> 88\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[0;32m     89\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[7], line 68\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t_batch\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     66\u001b[0m     t_batch \u001b[38;5;241m=\u001b[39m smooth_labels(t_batch, smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss(x_batch, t_batch)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad(x_batch, t_batch))\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip_weights\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[4], line 226\u001b[0m, in \u001b[0;36mResNet20.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m--> 226\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x, train_flg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_error(softmax(y), t)\n",
      "Cell \u001b[1;32mIn[4], line 206\u001b[0m, in \u001b[0;36mResNet20.forward\u001b[1;34m(self, x, train_flg, skip_prob)\u001b[0m\n\u001b[0;32m    204\u001b[0m     out \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3:\n\u001b[1;32m--> 206\u001b[0m     out \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_map \u001b[38;5;241m=\u001b[39m out\n\u001b[0;32m    210\u001b[0m N, C, H, W \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[1;32mIn[4], line 125\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[1;34m(self, x, train_flg, skip_prob)\u001b[0m\n\u001b[0;32m    122\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m    123\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1\u001b[38;5;241m.\u001b[39mforward(out)\n\u001b[1;32m--> 125\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mforward(out)\n\u001b[0;32m    126\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_main \u001b[38;5;241m=\u001b[39m out\n",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m, in \u001b[0;36mConvolution.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m b_q \u001b[38;5;241m=\u001b[39m fake_quantize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\n\u001b[0;32m     59\u001b[0m x_q \u001b[38;5;241m=\u001b[39m fake_quantize(x)\n\u001b[1;32m---> 61\u001b[0m col \u001b[38;5;241m=\u001b[39m im2col(x_q, FH, FW, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad)\n\u001b[0;32m     62\u001b[0m col_W \u001b[38;5;241m=\u001b[39m W_q\u001b[38;5;241m.\u001b[39mreshape(FN, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     63\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(col, col_W) \u001b[38;5;241m+\u001b[39m b_q\n",
      "File \u001b[1;32m~\\ANNproject\\common\\util.py:58\u001b[0m, in \u001b[0;36mim2col\u001b[1;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     55\u001b[0m out_h \u001b[38;5;241m=\u001b[39m (H \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mpad \u001b[38;5;241m-\u001b[39m filter_h)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstride \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     56\u001b[0m out_w \u001b[38;5;241m=\u001b[39m (W \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mpad \u001b[38;5;241m-\u001b[39m filter_w)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstride \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 58\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(input_data, [(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m), (pad, pad), (pad, pad)], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     59\u001b[0m col \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((N, C, filter_h, filter_w, out_h, out_w))\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(filter_h):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\arraypad.py:801\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m    798\u001b[0m padded, original_area_slice \u001b[38;5;241m=\u001b[39m _pad_simple(array, pad_width)\n\u001b[0;32m    799\u001b[0m \u001b[38;5;66;03m# And prepare iteration over all dimensions\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# (zipping may be more readable than using enumerate)\u001b[39;00m\n\u001b[1;32m--> 801\u001b[0m axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(padded\u001b[38;5;241m.\u001b[39mndim)\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    804\u001b[0m     values \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === 자동 튜닝 실험 루프 추가 ===\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train_fine, y_train_coarse),\n",
    "(x_val, y_val_fine, y_val_coarse),\n",
    "(x_test, y_test_fine, y_test_coarse) = load_cifar100()\n",
    "\n",
    "# fine label 기준으로 사용\n",
    "train_data = (x_train, y_train_fine)\n",
    "val_data = (x_val, y_val_fine)\n",
    "test_data = (x_test, y_test_fine)\n",
    "\n",
    "# 고정된 두 실험 조건\n",
    "model_configs = [\n",
    "    {\"lr\": 0.01, \"batch_size\": 64},\n",
    "    {\"lr\": 0.001, \"batch_size\": 32},\n",
    "]\n",
    "# 튜닝 단계별 하이퍼파라미터 구성\n",
    "smoothing_values = [0.05, 0.1, 0.15]\n",
    "ema_warmup_configs = [\n",
    "    {\"ema_decay\": 0.995, \"warmup_epochs\": 3},\n",
    "    {\"ema_decay\": 0.9999, \"warmup_epochs\": 7}\n",
    "]\n",
    "\n",
    "# 각 모델 설정마다 총 9회 실험 (4 + 3 + 2)\n",
    "def run_experiments():\n",
    "    for model_id, cfg in enumerate(model_configs):\n",
    "        print(f\"=== [모델 설정 {model_id+1}] LR={cfg['lr']}, BS={cfg['batch_size']} ===\")\n",
    "        # Best skip_prob = 0.1 고정 (예시), smoothing 튜닝\n",
    "        for smooth in smoothing_values:\n",
    "            model = ResNet20()\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                model_name=f\"ResNet20_cfg{model_id+1}_smooth{smooth}\",\n",
    "                train_data=(x_train, y_train_fine),\n",
    "                val_data=(x_val, y_val_fine),\n",
    "                test_data=(x_test, y_test_fine),\n",
    "                epochs=10,\n",
    "                batch_size=cfg[\"batch_size\"],\n",
    "                optimizer_name=\"adam\",  \n",
    "                lr=cfg[\"lr\"]\n",
    "            )\n",
    "            trainer.skip_prob = 0.0\n",
    "            trainer.smoothing = smooth\n",
    "            trainer.ema_decay = 0.999\n",
    "            trainer.warmup_epochs = 5\n",
    "            trainer.train()\n",
    "            trainer.save_log(f\"log_cfg{model_id+1}_smooth{smooth}.npz\")\n",
    "            trainer.save_model(f\"model_cfg{model_id+1}_smooth{smooth}.pkl\")\n",
    "            \n",
    "\n",
    "        # EMA/Warmup 튜닝\n",
    "        for ema_cfg in ema_warmup_configs:\n",
    "            model = ResNet20()\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                model_name=f\"ResNet20_cfg{model_id+1}_ema{ema_cfg['ema_decay']}_warmup{ema_cfg['warmup_epochs']}\",\n",
    "                train_data=(x_train, y_train_fine),\n",
    "                val_data=(x_val, y_val_fine),\n",
    "                test_data=(x_test, y_test_fine),\n",
    "                epochs=10,\n",
    "                batch_size=cfg[\"batch_size\"],\n",
    "                optimizer_name=\"adam\",\n",
    "                lr=cfg[\"lr\"]\n",
    "            )\n",
    "            trainer.skip_prob = 0.0\n",
    "            trainer.smoothing = 0.1\n",
    "            trainer.ema_decay = ema_cfg[\"ema_decay\"]\n",
    "            trainer.warmup_epochs = ema_cfg[\"warmup_epochs\"]\n",
    "            trainer.train()\n",
    "            trainer.save_log(f\"log_cfg{model_id+1}_ema{ema_cfg['ema_decay']}_warmup{ema_cfg['warmup_epochs']}.npz\")\n",
    "            trainer.save_model(f\"model_cfg{model_id+1}_ema{ema_cfg['ema_decay']}_warmup{ema_cfg['warmup_epochs']}.pkl\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiments()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071245a7-b579-4cb8-957b-e2941c9f5353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_18972\\2582908337.py:30: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(path, members)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-100 downloaded and extracted.\n",
      "CIFAR-100 Dataset Loaded!\n",
      "Train X: (45000, 3, 32, 32), Fine Y: (45000,), Coarse Y: (45000,)\n",
      "Val   X: (5000, 3, 32, 32), Fine Y: (5000,), Coarse Y: (5000,)\n",
      "Test  X: (10000, 3, 32, 32), Fine Y: (10000,), Coarse Y: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "from common.util import shuffle_dataset\n",
    "\n",
    "# 폴더와 파일 강제로 삭제\n",
    "shutil.rmtree('./cifar-100-python', ignore_errors=True)\n",
    "if os.path.exists('cifar-100-python.tar.gz'):\n",
    "    os.remove('cifar-100-python.tar.gz')\n",
    "\n",
    "# CIFAR-100 다운로드 및 압축 해제\n",
    "def download_cifar100(dest=\"./cifar-100-python\"):\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "\n",
    "    def is_within_directory(directory, target):\n",
    "        abs_directory = os.path.abspath(directory)\n",
    "        abs_target = os.path.abspath(target)\n",
    "        return os.path.commonprefix([abs_directory, abs_target]) == abs_directory\n",
    "\n",
    "    def safe_extract(tar, path=\".\", members=None):\n",
    "        for member in tar.getmembers():\n",
    "            member_path = os.path.join(path, member.name)\n",
    "            if not is_within_directory(path, member_path):\n",
    "                raise Exception(\"Attempted Path Traversal in Tar File\")\n",
    "        tar.extractall(path, members)\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "        os.makedirs(dest, exist_ok=True)\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        with tarfile.open(filename, \"r:gz\") as tar:\n",
    "            safe_extract(tar, path=\"./\")\n",
    "        print(\"CIFAR-100 downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"CIFAR-100 already downloaded.\")\n",
    "\n",
    "# 데이터 배치 로딩\n",
    "def load_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data_dict = pickle.load(f, encoding='bytes')\n",
    "        data = data_dict[b'data']\n",
    "        fine_labels = np.array(data_dict[b'fine_labels'])\n",
    "        coarse_labels = np.array(data_dict[b'coarse_labels'])\n",
    "        return data, fine_labels, coarse_labels\n",
    "\n",
    "# 메타데이터 로딩 (fine, coarse label names 포함)\n",
    "def load_meta(data_dir=\"./cifar-100-python\"):\n",
    "    with open(os.path.join(data_dir, \"meta\"), 'rb') as f:\n",
    "        meta_dict = pickle.load(f, encoding='bytes')\n",
    "        fine_label_names = [name.decode('utf-8') for name in meta_dict[b'fine_label_names']]\n",
    "        coarse_label_names = [name.decode('utf-8') for name in meta_dict[b'coarse_label_names']]\n",
    "        return {\"fine_label_names\": fine_label_names, \"coarse_label_names\": coarse_label_names}\n",
    "\n",
    "# 정규화 함수\n",
    "def normalize(x):\n",
    "    mean = np.array([0.507, 0.487, 0.441]).reshape(1, 3, 1, 1)\n",
    "    std = np.array([0.267, 0.256, 0.276]).reshape(1, 3, 1, 1)\n",
    "    return (x - mean) / std\n",
    "\n",
    "# 전체 데이터 로딩\n",
    "def load_cifar100(data_dir=\"./cifar-100-python\"):\n",
    "    x_train, y_train_fine, y_train_coarse = load_batch(os.path.join(data_dir, \"train\"))\n",
    "    x_test, y_test_fine, y_test_coarse = load_batch(os.path.join(data_dir, \"test\"))\n",
    "\n",
    "    x_train = x_train.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "    x_test = x_test.reshape(-1, 3, 32, 32).astype(np.float32) / 255.0\n",
    "\n",
    "    x_train = normalize(x_train)\n",
    "    x_test = normalize(x_test)\n",
    "\n",
    "    val_size = int(0.1 * len(x_train))\n",
    "    x_val, y_val_fine, y_val_coarse = (\n",
    "        x_train[:val_size], y_train_fine[:val_size], y_train_coarse[:val_size]\n",
    "    )\n",
    "    x_train, y_train_fine, y_train_coarse = (\n",
    "        x_train[val_size:], y_train_fine[val_size:], y_train_coarse[val_size:]\n",
    "    )\n",
    "\n",
    "    x_train, y_train_fine = shuffle_dataset(x_train, y_train_fine)\n",
    "    x_train, y_train_coarse = shuffle_dataset(x_train, y_train_coarse)\n",
    "\n",
    "    return (x_train, y_train_fine, y_train_coarse), (x_val, y_val_fine, y_val_coarse), (x_test, y_test_fine, y_test_coarse)\n",
    "\n",
    "\n",
    "# 데이터 다운로드 및 로딩\n",
    "download_cifar100()\n",
    "(x_train, y_train_fine, y_train_coarse), (x_val, y_val_fine, y_val_coarse), (x_test, y_test_fine, y_test_coarse) = load_cifar100()\n",
    "meta = load_meta()\n",
    "\n",
    "# 데이터셋 정보 출력\n",
    "print(\"CIFAR-100 Dataset Loaded!\")\n",
    "print(f\"Train X: {x_train.shape}, Fine Y: {y_train_fine.shape}, Coarse Y: {y_train_coarse.shape}\")\n",
    "print(f\"Val   X: {x_val.shape}, Fine Y: {y_val_fine.shape}, Coarse Y: {y_val_coarse.shape}\")\n",
    "print(f\"Test  X: {x_test.shape}, Fine Y: {y_test_fine.shape}, Coarse Y: {y_test_coarse.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b7b982-85f3-4bb4-9202-75925bc95f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_distribution(labels, label_names=None, title=\"Label Distribution\", filename=\"label_distribution.png\"):\n",
    "    counts = np.bincount(labels)\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.bar(range(len(counts)), counts)\n",
    "    if label_names:\n",
    "        plt.xticks(ticks=np.arange(len(label_names)), labels=label_names, rotation=90)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Label\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# 레이블 분포 시각화\n",
    "visualize_distribution(\n",
    "    y_train_fine, \n",
    "    meta['fine_label_names'], \n",
    "    \"Fine Label Distribution (Train)\",\n",
    "    filename=\"fine_label_distribution_train.png\"\n",
    ")\n",
    "\n",
    "visualize_distribution(\n",
    "    y_train_coarse, \n",
    "    meta['coarse_label_names'], \n",
    "    \"Coarse Label Distribution (Train)\",\n",
    "    filename=\"coarse_label_distribution_train.png\"\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_example_images(images, fine_labels, coarse_labels, fine_names, coarse_names, num_samples=8, filename=\"example_images.png\"):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        img = images[i].transpose(1, 2, 0)  # (3, 32, 32) → (32, 32, 3)\n",
    "        mean = np.array([0.507, 0.487, 0.441])\n",
    "        std = np.array([0.267, 0.256, 0.276])\n",
    "        img = img.transpose(2, 0, 1)  # → (3, 32, 32)\n",
    "        img = img * std[:, None, None] + mean[:, None, None]\n",
    "        img = img.transpose(1, 2, 0)  # → (32, 32, 3)\n",
    "\n",
    "        plt.imshow(np.clip(img, 0, 1))\n",
    "        plt.axis('off')\n",
    "        coarse = coarse_names[coarse_labels[i]]\n",
    "        fine = fine_names[fine_labels[i]]\n",
    "        plt.title(f\"{coarse}\\n({fine})\", fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "# 메타 정보 로드 및 예시 시각화\n",
    "meta = load_meta()\n",
    "show_example_images(\n",
    "    images=x_train,\n",
    "    fine_labels=y_train_fine,\n",
    "    coarse_labels=y_train_coarse,\n",
    "    fine_names=meta['fine_label_names'],\n",
    "    coarse_names=meta['coarse_label_names'],\n",
    "    num_samples=5,\n",
    "    filename=\"example_cifar100_train_samples.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a34e54-0c09-4c99-b4ae-4d7938790eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_quantize(x, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    \n",
    "    if x_max == x_min:\n",
    "        return x  # avoid divide by zero\n",
    "    \n",
    "    scale = (x_max - x_min) / (qmax - qmin)\n",
    "    zero_point = qmin - x_min / scale\n",
    "    zero_point = np.clip(np.round(zero_point), qmin, qmax)\n",
    "\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x = np.clip(np.round(q_x), qmin, qmax)\n",
    "    fq_x = scale * (q_x - zero_point)\n",
    "    return fq_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b38db9-d77c-492b-952e-31c1ef1f7c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.layers import Convolution, Affine, Relu, BatchNormalization\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.util import im2col, col2im\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        # Fake Quantization\n",
    "        W_q = fake_quantize(self.W)\n",
    "        b_q = fake_quantize(self.b)\n",
    "        x_q = fake_quantize(self.x)\n",
    "\n",
    "        out = np.dot(x_q, W_q) + b_q\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)\n",
    "        return dx\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, _, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        # Fake Quantization\n",
    "        W_q = fake_quantize(self.W)\n",
    "        b_q = fake_quantize(self.b)\n",
    "        x_q = fake_quantize(x)\n",
    "\n",
    "        col = im2col(x_q, FH, FW, self.stride, self.pad)\n",
    "        col_W = W_q.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + b_q\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout).transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio=0.3):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "class ResidualBlock:\n",
    "    def __init__(self, in_channels, out_channels, stride=1, dropout_ratio=0.3):\n",
    "        self.stride = stride\n",
    "        self.equal_in_out = (in_channels == out_channels and stride == 1)\n",
    "\n",
    "        self.conv1 = Convolution(\n",
    "            W=np.random.randn(out_channels, in_channels, 3, 3) * np.sqrt(2. / in_channels),\n",
    "            b=np.zeros(out_channels),\n",
    "            stride=stride,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn1 = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "        self.relu1 = Relu()\n",
    "        self.dropout = Dropout(dropout_ratio)  \n",
    "\n",
    "        self.conv2 = Convolution(\n",
    "            W=np.random.randn(out_channels, out_channels, 3, 3) * np.sqrt(2. / out_channels),\n",
    "            b=np.zeros(out_channels),\n",
    "            stride=1,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn2 = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "        self.relu2 = Relu()\n",
    "\n",
    "        if not self.equal_in_out:\n",
    "            self.shortcut = Convolution(\n",
    "                W=np.random.randn(out_channels, in_channels, 1, 1) * np.sqrt(2. / in_channels),\n",
    "                b=np.zeros(out_channels),\n",
    "                stride=stride,\n",
    "                pad=0\n",
    "            )\n",
    "            self.bn_shortcut = BatchNormalization(gamma=np.ones(out_channels), beta=np.zeros(out_channels))\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.x = x\n",
    "\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out, train_flg)\n",
    "        out = self.relu1.forward(out)\n",
    "        out = self.dropout.forward(out, train_flg)  \n",
    "\n",
    "        out = self.conv2.forward(out)\n",
    "        out = self.bn2.forward(out, train_flg)\n",
    "        self.out_main = out\n",
    "\n",
    "        if self.equal_in_out:\n",
    "            shortcut = x\n",
    "        else:\n",
    "            shortcut = self.shortcut.forward(x)\n",
    "            shortcut = self.bn_shortcut.forward(shortcut, train_flg)\n",
    "        self.out_shortcut = shortcut\n",
    "\n",
    "        out += shortcut\n",
    "        out = self.relu2.forward(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.relu2.backward(dout)\n",
    "\n",
    "        dshortcut = dout.copy()\n",
    "        dmain = dout.copy()\n",
    "\n",
    "        dmain = self.bn2.backward(dmain)\n",
    "        dmain = self.conv2.backward(dmain)\n",
    "\n",
    "        dmain = self.dropout.backward(dmain)  \n",
    "        dmain = self.relu1.backward(dmain)\n",
    "        dmain = self.bn1.backward(dmain)\n",
    "        dmain = self.conv1.backward(dmain)\n",
    "\n",
    "        if not self.equal_in_out:\n",
    "            dshortcut = self.bn_shortcut.backward(dshortcut)\n",
    "            dshortcut = self.shortcut.backward(dshortcut)\n",
    "\n",
    "        dx = dmain + dshortcut\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b8f322c-6569-49b9-9def-5baf1c25e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet20:\n",
    "    def __init__(self, input_dim=(3, 32, 32), num_classes=100, dropout_ratio=0.3):\n",
    "        self.params = []\n",
    "        self.trainable_layers = []\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        self.conv1 = Convolution(\n",
    "            W=np.random.randn(16, 3, 3, 3) * np.sqrt(2. / 3),\n",
    "            b=np.zeros(16),\n",
    "            stride=1,\n",
    "            pad=1\n",
    "        )\n",
    "        self.bn1 = BatchNormalization(gamma=np.ones(16), beta=np.zeros(16))\n",
    "        self.relu1 = Relu()\n",
    "\n",
    "        self.layer1 = [ResidualBlock(16, 16, stride=1, dropout_ratio=self.dropout_ratio) for _ in range(3)]\n",
    "        self.layer2 = [ResidualBlock(16 if i == 0 else 32, 32, stride=2 if i == 0 else 1, dropout_ratio=self.dropout_ratio) for i in range(3)]\n",
    "        self.layer3 = [ResidualBlock(32 if i == 0 else 64, 64, stride=2 if i == 0 else 1, dropout_ratio=self.dropout_ratio) for i in range(3)]\n",
    "\n",
    "        self.fc = Affine(W=np.random.randn(64, num_classes) * np.sqrt(2. / 64), b=np.zeros(num_classes))\n",
    "\n",
    "    def clip_weights(self, clip_value=1.0):\n",
    "        self.conv1.W = np.clip(self.conv1.W, -clip_value, clip_value)\n",
    "        self.fc.W = np.clip(self.fc.W, -clip_value, clip_value)\n",
    "        for block in self.layer1 + self.layer2 + self.layer3:\n",
    "            block.conv1.W = np.clip(block.conv1.W, -clip_value, clip_value)\n",
    "            block.conv2.W = np.clip(block.conv2.W, -clip_value, clip_value)\n",
    "            if not block.equal_in_out:\n",
    "                block.shortcut.W = np.clip(block.shortcut.W, -clip_value, clip_value)\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input = x\n",
    "\n",
    "        out = self.conv1.forward(x)\n",
    "        out = self.bn1.forward(out, train_flg)\n",
    "        out = self.relu1.forward(out)\n",
    "\n",
    "        for block in self.layer1:\n",
    "            out = block.forward(out, train_flg)\n",
    "        for block in self.layer2:\n",
    "            out = block.forward(out, train_flg)\n",
    "        for block in self.layer3:\n",
    "            out = block.forward(out, train_flg)\n",
    "\n",
    "        self.feature_map = out\n",
    "\n",
    "        N, C, H, W = out.shape\n",
    "        out = out.mean(axis=(2, 3))  # global average pooling\n",
    "\n",
    "        self.pooled = out\n",
    "        out = self.fc.forward(out)\n",
    "        return out\n",
    "        \n",
    "    def predict(self, x, batch_size=100):\n",
    "        y_list = []\n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            y_batch = self.forward(x_batch, train_flg=False)\n",
    "            y_list.append(y_batch)\n",
    "        return np.concatenate(y_list, axis=0)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.forward(x, train_flg=True)\n",
    "        return cross_entropy_error(softmax(y), t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        acc = 0.0\n",
    "        total = x.shape[0]\n",
    "        for i in range(0, total, batch_size):\n",
    "            x_batch = x[i:i+batch_size]\n",
    "            t_batch = t[i:i+batch_size]\n",
    "            y = self.predict(x_batch)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            if t.ndim != 1:\n",
    "                t_batch = np.argmax(t_batch, axis=1)\n",
    "            acc += np.sum(y == t_batch)\n",
    "        return acc / total\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.fc.backward(dout)\n",
    "        dout = dout.reshape(self.feature_map.shape[0], self.feature_map.shape[1], 1, 1)\n",
    "        dout = dout.repeat(self.feature_map.shape[2], axis=2).repeat(self.feature_map.shape[3], axis=3)\n",
    "        for block in reversed(self.layer3):\n",
    "            dout = block.backward(dout)\n",
    "        for block in reversed(self.layer2):\n",
    "            dout = block.backward(dout)\n",
    "        for block in reversed(self.layer1):\n",
    "            dout = block.backward(dout)\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        dout = self.conv1.backward(dout)\n",
    "        return dout\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c620a452-6019-4330-9c5a-d18e0dfe61e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type)                       Output Shape                Param #\n",
      "===========================================================================\n",
      " 1. Conv1                           (1, 16, 32, 32)                 448\n",
      " 2. Block[1-1]_Conv1                (1, 16, 32, 32)               2,320\n",
      "    Dropout                         (1, 16, 32, 32)                   0\n",
      " 3. Block[1-1]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 4. Block[1-2]_Conv1                (1, 16, 32, 32)               2,320\n",
      "    Dropout                         (1, 16, 32, 32)                   0\n",
      " 5. Block[1-2]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 6. Block[1-3]_Conv1                (1, 16, 32, 32)               2,320\n",
      "    Dropout                         (1, 16, 32, 32)                   0\n",
      " 7. Block[1-3]_Conv2                (1, 16, 32, 32)               2,320\n",
      " 8. Block[2-1]_Conv1                (1, 32, 16, 16)               4,640\n",
      "    Dropout                         (1, 32, 16, 16)                   0\n",
      " 9. Block[2-1]_Conv2                (1, 32, 16, 16)               9,248\n",
      "    └─ Shortcut[2-1]                (1, 32, 16, 16)                 544\n",
      "10. Block[2-2]_Conv1                (1, 32, 16, 16)               9,248\n",
      "    Dropout                         (1, 32, 16, 16)                   0\n",
      "11. Block[2-2]_Conv2                (1, 32, 16, 16)               9,248\n",
      "12. Block[2-3]_Conv1                (1, 32, 16, 16)               9,248\n",
      "    Dropout                         (1, 32, 16, 16)                   0\n",
      "13. Block[2-3]_Conv2                (1, 32, 16, 16)               9,248\n",
      "14. Block[3-1]_Conv1                (1, 64, 8, 8)                18,496\n",
      "    Dropout                         (1, 64, 8, 8)                     0\n",
      "15. Block[3-1]_Conv2                (1, 64, 8, 8)                36,928\n",
      "    └─ Shortcut[3-1]                (1, 64, 8, 8)                 2,112\n",
      "16. Block[3-2]_Conv1                (1, 64, 8, 8)                36,928\n",
      "    Dropout                         (1, 64, 8, 8)                     0\n",
      "17. Block[3-2]_Conv2                (1, 64, 8, 8)                36,928\n",
      "18. Block[3-3]_Conv1                (1, 64, 8, 8)                36,928\n",
      "    Dropout                         (1, 64, 8, 8)                     0\n",
      "19. Block[3-3]_Conv2                (1, 64, 8, 8)                36,928\n",
      "    GlobalAvgPool                   (1, 64)                           0\n",
      "20. FC                              (1, 100)                      6,500\n",
      "===========================================================================\n",
      "Total weight layers:                                        20\n",
      "Total params:                                               277,540\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "def count_params(layer):\n",
    "    count = 0\n",
    "    if hasattr(layer, 'W'):\n",
    "        count += np.prod(layer.W.shape)\n",
    "    if hasattr(layer, 'b'):\n",
    "        count += np.prod(layer.b.shape)\n",
    "    return count\n",
    "\n",
    "def print_resnet20_summary(model, input_shape=(1, 3, 32, 32)):\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Layer (type)':<35}{'Output Shape':<25}{'Param #':>10}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "    x = np.zeros(input_shape)\n",
    "    total_params = 0\n",
    "    layer_idx = 1\n",
    "\n",
    "    # Conv1\n",
    "    x = model.conv1.forward(x)\n",
    "    p = count_params(model.conv1)\n",
    "    print(f\"{layer_idx:>2}. {'Conv1':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "    layer_idx += 1\n",
    "\n",
    "    x = model.bn1.forward(x, train_flg=False)\n",
    "    x = model.relu1.forward(x)\n",
    "\n",
    "    # Residual Blocks\n",
    "    for i, layer_block in enumerate([model.layer1, model.layer2, model.layer3]):\n",
    "        for j, block in enumerate(layer_block):\n",
    "            residual = x.copy()\n",
    "\n",
    "            # Conv1\n",
    "            x = block.conv1.forward(x)\n",
    "            p = count_params(block.conv1)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv1\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn1.forward(x, train_flg=False)\n",
    "            x = block.relu1.forward(x)\n",
    "\n",
    "            # Dropout (표시는 되지만 파라미터는 없음)\n",
    "            x = block.dropout.forward(x, train_flg=False)\n",
    "            print(f\"{'':>3} {'Dropout':<32}{str(x.shape):<25}{'0':>10}\", flush=True)\n",
    "\n",
    "            # Conv2\n",
    "            x = block.conv2.forward(x)\n",
    "            p = count_params(block.conv2)\n",
    "            name = f\"Block[{i+1}-{j+1}]_Conv2\"\n",
    "            print(f\"{layer_idx:>2}. {name:<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "            total_params += p\n",
    "            layer_idx += 1\n",
    "\n",
    "            x = block.bn2.forward(x, train_flg=False)\n",
    "\n",
    "            # Shortcut (optional)\n",
    "            if not block.equal_in_out:\n",
    "                x_sc = block.shortcut.forward(residual)\n",
    "                p = count_params(block.shortcut)\n",
    "                name = f\"└─ Shortcut[{i+1}-{j+1}]\"\n",
    "                print(f\"{'':>3} {name:<32}{str(x_sc.shape):<25}{p:>10,}\", flush=True)\n",
    "                total_params += p\n",
    "                x = x + x_sc\n",
    "                x = block.bn_shortcut.forward(x, train_flg=False)\n",
    "            else:\n",
    "                x = x + residual\n",
    "\n",
    "            x = block.relu2.forward(x)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    x = x.mean(axis=(2, 3))\n",
    "    print(f\"{'':>3} {'GlobalAvgPool':<32}{str(x.shape):<25}{'0':>10}\", flush=True)\n",
    "\n",
    "    # FC\n",
    "    x = model.fc.forward(x)\n",
    "    p = count_params(model.fc)\n",
    "    print(f\"{layer_idx:>2}. {'FC':<32}{str(x.shape):<25}{p:>10,}\", flush=True)\n",
    "    total_params += p\n",
    "\n",
    "    print(\"=\" * 75, flush=True)\n",
    "    print(f\"{'Total weight layers:':<60}{'20'}\", flush=True)\n",
    "    print(f\"{'Total params:':<60}{total_params:,}\", flush=True)\n",
    "    print(\"=\" * 75, flush=True)\n",
    "\n",
    "# ResNet-20 모델 생성 및 요약 출력\n",
    "model = ResNet20()\n",
    "print_resnet20_summary(model, input_shape=(1, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ed8e0a-4c0c-4107-aa21-090a20334ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "class Adam:\n",
    "    \"\"\"Adam optimizer with optional weight decay (L2 regularization).\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, weight_decay=0.0):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.weight_decay = weight_decay  \n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            if self.weight_decay > 0:\n",
    "                grads[key] += self.weight_decay * params[key]\n",
    "\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82b6bc5-4f0a-4e7f-9c53-3878e34debb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from common.functions import softmax\n",
    "from common.util import shuffle_dataset\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, model_name, train_data, val_data, test_data,\n",
    "                 epochs=20, batch_size=64, optimizer_name='adam', lr=0.01,\n",
    "                 weight_decay=0.0, patience=5):\n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.train_x, self.train_t = train_data\n",
    "        self.val_x, self.val_t = val_data\n",
    "        self.test_x, self.test_t = test_data\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        self.patience = patience\n",
    "\n",
    "        self.train_size = self.train_x.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size // self.batch_size, 1)\n",
    "\n",
    "        self.train_loss_list = []\n",
    "        self.val_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.wait = 0\n",
    "\n",
    "        if optimizer_name == 'adam':\n",
    "            self.optimizer = Adam(lr=lr, weight_decay=weight_decay)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    def get_param_dict_and_grad(self):\n",
    "        param_dict, grad_dict = {}, {}\n",
    "        if hasattr(self.model.fc, 'W'):\n",
    "            param_dict['fc_W'] = self.model.fc.W\n",
    "            param_dict['fc_b'] = self.model.fc.b\n",
    "            grad_dict['fc_W'] = self.model.fc.dW\n",
    "            grad_dict['fc_b'] = self.model.fc.db\n",
    "\n",
    "        idx = 0\n",
    "        for layer in self.model.layer1 + self.model.layer2 + self.model.layer3:\n",
    "            for attr in ['conv1', 'conv2', 'shortcut']:\n",
    "                if hasattr(layer, attr):\n",
    "                    conv = getattr(layer, attr)\n",
    "                    param_dict[f'{idx}_W'] = conv.W\n",
    "                    param_dict[f'{idx}_b'] = conv.b\n",
    "                    grad_dict[f'{idx}_W'] = conv.dW\n",
    "                    grad_dict[f'{idx}_b'] = conv.db\n",
    "                    idx += 1\n",
    "        return param_dict, grad_dict\n",
    "\n",
    "    def loss_grad(self, x, t):\n",
    "        y = self.model.forward(x, train_flg=True)\n",
    "        batch_size = x.shape[0]\n",
    "        if t.size == y.size:\n",
    "            return (softmax(y) - t) / batch_size\n",
    "        else:\n",
    "            dx = softmax(y)\n",
    "            dx[np.arange(batch_size), t] -= 1\n",
    "            return dx / batch_size\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.train_x[batch_mask]\n",
    "        t_batch = self.train_t[batch_mask]\n",
    "\n",
    "        loss = self.model.loss(x_batch, t_batch)\n",
    "        self.model.backward(self.loss_grad(x_batch, t_batch))\n",
    "\n",
    "        if hasattr(self.model, 'clip_weights'):\n",
    "            self.model.clip_weights(clip_value=1.0)\n",
    "\n",
    "        params, grads = self.get_param_dict_and_grad()\n",
    "        self.optimizer.update(params, grads)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, x, t):\n",
    "        loss = self.model.loss(x, t)\n",
    "        acc = self.model.accuracy(x, t)\n",
    "        return loss, acc\n",
    "\n",
    "    def get_wrong_indices(self, x, t):\n",
    "        y_pred = np.argmax(self.model.predict(x), axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        return np.where(y_pred != t)[0]\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"[Epoch {epoch + 1}]\", flush=True)\n",
    "            self.train_x, self.train_t = shuffle_dataset(self.train_x, self.train_t)\n",
    "\n",
    "            epoch_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "            for i in range(self.iter_per_epoch):\n",
    "                loss = self.train_step()\n",
    "                epoch_loss += loss\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"  Iter {i:3d}/{self.iter_per_epoch}: Train Loss {loss:.4f}\", flush=True)\n",
    "\n",
    "            avg_loss = epoch_loss / self.iter_per_epoch\n",
    "            val_loss, val_acc = self.evaluate(self.val_x, self.val_t)\n",
    "            train_acc = self.model.accuracy(self.train_x[:1000], self.train_t[:1000])\n",
    "            test_acc = self.model.accuracy(self.test_x, self.test_t)\n",
    "\n",
    "            self.train_loss_list.append(avg_loss)\n",
    "            self.val_loss_list.append(val_loss)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Train acc: {train_acc:.4f}, Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}, Test acc: {test_acc:.4f} (Time: {elapsed:.2f}s)\", flush=True)\n",
    "\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.wait = 0\n",
    "                self.save_model(f\"{self.model_name}_best.pkl\")\n",
    "                print(\">>> Model improved. Saved as best model.\", flush=True)\n",
    "            else:\n",
    "                self.wait += 1\n",
    "                if self.wait >= self.patience:\n",
    "                    print(\">>> Early stopping triggered.\", flush=True)\n",
    "                    break\n",
    "\n",
    "            wrong_idx = self.get_wrong_indices(self.val_x, self.val_t)\n",
    "            np.save(f\"{self.model_name}_wrong_indices_epoch_{epoch+1}.npy\", wrong_idx)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        model_state = {k: v.copy() for k, v in params.items()}\n",
    "\n",
    "        optimizer_state = {\n",
    "            'lr': self.optimizer.lr,\n",
    "            'beta1': getattr(self.optimizer, 'beta1', None),\n",
    "            'beta2': getattr(self.optimizer, 'beta2', None),\n",
    "            'eps': getattr(self.optimizer, 'eps', None),\n",
    "            'm': getattr(self.optimizer, 'm', {}),\n",
    "            'v': getattr(self.optimizer, 'v', {}),\n",
    "            't': getattr(self.optimizer, 't', 0),\n",
    "        }\n",
    "\n",
    "        save_data = {\n",
    "            'model': model_state,\n",
    "            'optimizer': optimizer_state,\n",
    "            'train_loss_list': self.train_loss_list,\n",
    "            'val_loss_list': self.val_loss_list,\n",
    "            'train_acc_list': self.train_acc_list,\n",
    "            'test_acc_list': self.test_acc_list\n",
    "        }\n",
    "\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(save_data, f)\n",
    "\n",
    "    def save_log(self, filename='log.npz'):\n",
    "        np.savez(filename,\n",
    "                 loss=np.array(self.train_loss_list),\n",
    "                 val_loss=np.array(self.val_loss_list),\n",
    "                 train_acc=np.array(self.train_acc_list),\n",
    "                 test_acc=np.array(self.test_acc_list))\n",
    "        print(f\"Log saved to {filename}\", flush=True)\n",
    "\n",
    "    def load_model(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            state = pickle.load(f)\n",
    "\n",
    "        params, _ = self.get_param_dict_and_grad()\n",
    "        for k in params:\n",
    "            if k in state['model']:\n",
    "                params[k][...] = state['model'][k]\n",
    "            else:\n",
    "                print(f\"[WARN] Key {k} not found in checkpoint!\", flush=True)\n",
    "\n",
    "        opt = state['optimizer']\n",
    "        self.optimizer.lr = opt['lr']\n",
    "        self.optimizer.beta1 = opt['beta1']\n",
    "        self.optimizer.beta2 = opt['beta2']\n",
    "        self.optimizer.eps = opt['eps']\n",
    "        self.optimizer.m = opt['m']\n",
    "        self.optimizer.v = opt['v']\n",
    "        self.optimizer.t = opt['t']\n",
    "\n",
    "        # 복원된 로그\n",
    "        self.train_loss_list = state.get('train_loss_list', [])\n",
    "        self.val_loss_list = state.get('val_loss_list', []) \n",
    "        self.train_acc_list = state.get('train_acc_list', [])\n",
    "        self.test_acc_list = state.get('test_acc_list', [])\n",
    "\n",
    "    def load_best_and_evaluate(self):\n",
    "        filename = f\"{self.model_name}_best.pkl\"\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"[ERROR] Best model file not found: {filename}\")\n",
    "            return\n",
    "\n",
    "        self.load_model(filename)\n",
    "        test_loss, test_acc = self.evaluate(self.test_x, self.test_t)\n",
    "        print(f\"[BEST MODEL EVAL] Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "        return test_loss, test_acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd2f577-bc8d-47f1-8cf8-0e24e94983b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Training ResNet20_wd0.0001_0.01_64]\n",
      "[Epoch 1]\n",
      "  Iter   0/703: Train Loss 5.5641\n",
      "  Iter  10/703: Train Loss 5.2881\n",
      "  Iter  20/703: Train Loss 4.8238\n",
      "  Iter  30/703: Train Loss 4.7116\n",
      "  Iter  40/703: Train Loss 4.6836\n",
      "  Iter  50/703: Train Loss 4.7051\n",
      "  Iter  60/703: Train Loss 4.6940\n",
      "  Iter  70/703: Train Loss 4.7275\n",
      "  Iter  80/703: Train Loss 4.5980\n",
      "  Iter  90/703: Train Loss 4.7200\n",
      "  Iter 100/703: Train Loss 4.6568\n",
      "  Iter 110/703: Train Loss 4.7476\n",
      "  Iter 120/703: Train Loss 4.5864\n",
      "  Iter 130/703: Train Loss 4.6765\n",
      "  Iter 140/703: Train Loss 4.6519\n",
      "  Iter 150/703: Train Loss 4.5725\n",
      "  Iter 160/703: Train Loss 4.6541\n",
      "  Iter 170/703: Train Loss 4.6191\n",
      "  Iter 180/703: Train Loss 4.6624\n",
      "  Iter 190/703: Train Loss 4.6019\n",
      "  Iter 200/703: Train Loss 4.6329\n",
      "  Iter 210/703: Train Loss 4.6191\n",
      "  Iter 220/703: Train Loss 4.5911\n",
      "  Iter 230/703: Train Loss 4.6023\n",
      "  Iter 240/703: Train Loss 4.5829\n",
      "  Iter 250/703: Train Loss 4.6818\n",
      "  Iter 260/703: Train Loss 4.6650\n",
      "  Iter 270/703: Train Loss 4.6084\n",
      "  Iter 280/703: Train Loss 4.6608\n",
      "  Iter 290/703: Train Loss 4.6265\n",
      "  Iter 300/703: Train Loss 4.6241\n",
      "  Iter 310/703: Train Loss 4.6160\n",
      "  Iter 320/703: Train Loss 4.6898\n",
      "  Iter 330/703: Train Loss 4.5494\n",
      "  Iter 340/703: Train Loss 4.6366\n",
      "  Iter 350/703: Train Loss 4.6752\n",
      "  Iter 360/703: Train Loss 4.6014\n",
      "  Iter 370/703: Train Loss 4.5930\n",
      "  Iter 380/703: Train Loss 4.5868\n",
      "  Iter 390/703: Train Loss 4.6342\n",
      "  Iter 400/703: Train Loss 4.6707\n",
      "  Iter 410/703: Train Loss 4.6200\n",
      "  Iter 420/703: Train Loss 4.6226\n",
      "  Iter 430/703: Train Loss 4.6296\n",
      "  Iter 440/703: Train Loss 4.6507\n",
      "  Iter 450/703: Train Loss 4.5822\n",
      "  Iter 460/703: Train Loss 4.6454\n",
      "  Iter 470/703: Train Loss 4.6230\n",
      "  Iter 480/703: Train Loss 4.6338\n",
      "  Iter 490/703: Train Loss 4.6184\n",
      "  Iter 500/703: Train Loss 4.6527\n",
      "  Iter 510/703: Train Loss 4.6345\n",
      "  Iter 520/703: Train Loss 4.6256\n",
      "  Iter 530/703: Train Loss 4.6086\n",
      "  Iter 540/703: Train Loss 4.6385\n",
      "  Iter 550/703: Train Loss 4.6551\n",
      "  Iter 560/703: Train Loss 4.6707\n",
      "  Iter 570/703: Train Loss 4.6386\n",
      "  Iter 580/703: Train Loss 4.6352\n",
      "  Iter 590/703: Train Loss 4.5961\n",
      "  Iter 600/703: Train Loss 4.6124\n",
      "  Iter 610/703: Train Loss 4.6401\n",
      "  Iter 620/703: Train Loss 4.6337\n",
      "  Iter 630/703: Train Loss 4.5913\n",
      "  Iter 640/703: Train Loss 4.6337\n",
      "  Iter 650/703: Train Loss 4.6323\n",
      "  Iter 660/703: Train Loss 4.6244\n",
      "  Iter 670/703: Train Loss 4.6343\n",
      "  Iter 680/703: Train Loss 4.5970\n",
      "  Iter 690/703: Train Loss 4.6736\n",
      "  Iter 700/703: Train Loss 4.6244\n",
      "Train acc: 0.0090, Val loss: 4.6211, Val acc: 0.0066, Test acc: 0.0090 (Time: 2920.58s)\n",
      ">>> Model improved. Saved as best model.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m → test_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 81\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet20(dropout_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)  \u001b[38;5;66;03m# 드롭아웃 완전 비활성화\u001b[39;00m\n\u001b[0;32m     10\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     11\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     12\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 24\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     25\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_log.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_epoch10.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 132\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> Early stopping triggered.\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m wrong_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_wrong_indices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_t)\n\u001b[0;32m    133\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_wrong_indices_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, wrong_idx)\n",
      "Cell \u001b[1;32mIn[8], line 89\u001b[0m, in \u001b[0;36mTrainer.get_wrong_indices\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_wrong_indices\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 89\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(x), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     91\u001b[0m         t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(t, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 58\u001b[0m, in \u001b[0;36mResNet20.predict\u001b[1;34m(self, x, batch_size)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], batch_size):\n\u001b[0;32m     57\u001b[0m     x_batch \u001b[38;5;241m=\u001b[39m x[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m---> 58\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x_batch, train_flg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m     y_list\u001b[38;5;241m.\u001b[39mappend(y_batch)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(y_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m, in \u001b[0;36mResNet20.forward\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m     39\u001b[0m     out \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2:\n\u001b[1;32m---> 41\u001b[0m     out \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3:\n\u001b[0;32m     43\u001b[0m     out \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n",
      "Cell \u001b[1;32mIn[4], line 136\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m    133\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1\u001b[38;5;241m.\u001b[39mforward(out)\n\u001b[0;32m    134\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout\u001b[38;5;241m.\u001b[39mforward(out, train_flg)  \n\u001b[1;32m--> 136\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mforward(out)\n\u001b[0;32m    137\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2\u001b[38;5;241m.\u001b[39mforward(out, train_flg)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_main \u001b[38;5;241m=\u001b[39m out\n",
      "Cell \u001b[1;32mIn[4], line 60\u001b[0m, in \u001b[0;36mConvolution.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     57\u001b[0m b_q \u001b[38;5;241m=\u001b[39m fake_quantize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\n\u001b[0;32m     58\u001b[0m x_q \u001b[38;5;241m=\u001b[39m fake_quantize(x)\n\u001b[1;32m---> 60\u001b[0m col \u001b[38;5;241m=\u001b[39m im2col(x_q, FH, FW, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad)\n\u001b[0;32m     61\u001b[0m col_W \u001b[38;5;241m=\u001b[39m W_q\u001b[38;5;241m.\u001b[39mreshape(FN, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     62\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(col, col_W) \u001b[38;5;241m+\u001b[39m b_q\n",
      "File \u001b[1;32m~\\common\\util.py:65\u001b[0m, in \u001b[0;36mim2col\u001b[1;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(filter_w):\n\u001b[0;32m     64\u001b[0m         x_max \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m stride\u001b[38;5;241m*\u001b[39mout_w\n\u001b[1;32m---> 65\u001b[0m         col[:, :, y, x, :, :] \u001b[38;5;241m=\u001b[39m img[:, :, y:y_max:stride, x:x_max:stride]\n\u001b[0;32m     67\u001b[0m col \u001b[38;5;241m=\u001b[39m col\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(N\u001b[38;5;241m*\u001b[39mout_h\u001b[38;5;241m*\u001b[39mout_w, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m col\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    weight_decays = [1e-6, 1e-5]\n",
    "    results = []\n",
    "\n",
    "    for wd in weight_decays:\n",
    "        model_name = f\"ResNet20_wd{wd}_0.01_64\"\n",
    "        print(f\"\\n[Training {model_name}]\", flush=True)\n",
    "\n",
    "        model = ResNet20(dropout_ratio=0.0)  # 드롭아웃 완전 비활성화\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            model_name=model_name,\n",
    "            train_data=(x_train, y_train_fine),\n",
    "            val_data=(x_val, y_val_fine),\n",
    "            test_data=(x_test, y_test_fine),\n",
    "            epochs=10,\n",
    "            batch_size=64,\n",
    "            optimizer_name='adam',\n",
    "            lr=0.01,\n",
    "            weight_decay=wd,\n",
    "            patience=5\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.save_log(f\"{model_name}_log.npz\")\n",
    "        trainer.save_model(f\"{model_name}_epoch10.pkl\")\n",
    "\n",
    "        trainer.load_best_and_evaluate()\n",
    "        trainer.save_confusion_matrix(x_val, y_val_fine, label_names=meta['fine_label_names'],\n",
    "                                      save_path=f\"{model_name}_confusion.png\")\n",
    "        trainer.visualize_misclassified(x_val, y_val_fine,\n",
    "                                        label_names=meta['fine_label_names'],\n",
    "                                        save_path=f\"{model_name}_misclassified.png\")\n",
    "\n",
    "        final_acc = trainer.test_acc_list[-1]\n",
    "        results.append((wd, final_acc))\n",
    "\n",
    "    print(\"\\n[하이퍼파라미터 튜닝 결과 요약]\")\n",
    "    for wd, acc in results:\n",
    "        print(f\"weight_decay={wd} → test_acc={acc:.4f}\", flush=True)\n",
    "\n",
    "    # 두 번째 실험군: lr=0.001, bs=32\n",
    "    for wd in weight_decays:\n",
    "        model_name = f\"ResNet20_wd{wd}_0.001_32\"\n",
    "        print(f\"\\n[Training {model_name}]\", flush=True)\n",
    "\n",
    "        model = ResNet20(dropout_ratio=0.0)\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            model_name=model_name,\n",
    "            train_data=(x_train, y_train_fine),\n",
    "            val_data=(x_val, y_val_fine),\n",
    "            test_data=(x_test, y_test_fine),\n",
    "            epochs=10,\n",
    "            batch_size=32,\n",
    "            optimizer_name='adam',\n",
    "            lr=0.001,\n",
    "            weight_decay=wd,\n",
    "            patience=5\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        trainer.save_log(f\"{model_name}_log.npz\")\n",
    "        trainer.save_model(f\"{model_name}_epoch10.pkl\")\n",
    "\n",
    "        trainer.load_best_and_evaluate()\n",
    "        trainer.save_confusion_matrix(x_val, y_val_fine, label_names=meta['fine_label_names'],\n",
    "                                      save_path=f\"{model_name}_confusion.png\")\n",
    "        trainer.visualize_misclassified(x_val, y_val_fine,\n",
    "                                        label_names=meta['fine_label_names'],\n",
    "                                        save_path=f\"{model_name}_misclassified.png\")\n",
    "\n",
    "        final_acc = trainer.test_acc_list[-1]\n",
    "        results.append((wd, final_acc))\n",
    "\n",
    "    print(\"\\n[하이퍼파라미터 튜닝 결과 요약]\")\n",
    "    for wd, acc in results:\n",
    "        print(f\"weight_decay={wd} → test_acc={acc:.4f}\", flush=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05d220-cbac-4c90-87ee-c5b412ef4e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
